• Mistakes and omissions in ontologies can lead to the inability of applications to achieve the full potential of exchanged data. Good ontologies lead directly to a higher degree of reuse of data and a better cooperation over the boundaries of applications and domains.• People constructing an ontology need a way to evaluate their results and possibly to guide the construction process and any refinement steps. This will make the ontology engineers feel more confident about their results, and thus encourage them to share their results with the community and reuse the work of others for their own purposes.• Local changes in collaborative ontology engineering may effect the work of others. Ontology evaluation technologies allow to automatically check if constraints and requirements are fulfilled, in order to automatically reveal plausibility problems, and thus to decrease maintenance costs of such ontologies dramatically.In this thesis a theoretical framework and several methods breathing life into the framework are presented. The application to the above scenarios is explored, and the theoretical foundations are thoroughly grounded in the practical usage of the emerging Semantic Web. We implemented and evaluated a number of the methods. The results of these evaluations are presented, indicating the usefulness of the overall framework.Short Table of Contents Introduction
What I mean (and everybody else means) by the word 'quality' cannot be broken down into subjects and predicates. This is not because Quality is so mysterious but because Quality is so simple, immediate and direct.(Robert M. The Semantic Web Ontologies are used in order to specify the knowledge that is exchanged and shared between the different systems, and within the systems by the various components. Ontologies define the formal semantics of the terms used for describing data, and the relations between these terms. They provide an "explicit specification of a conceptualization" Ontologies, like all engineering artifacts, need a thorough evaluation. But the evaluation of ontologies poses a number of unique challenges: due to the declarative nature of ontologies developers cannot just compile and run them like most other software artifacts. They are data that has to be shared between different components and used for potentially different tasks. Within the context of the Semantic Web, ontologies may often be used in ways not expected by the original creators of the ontology. Ontologies rather enable a serendipitous reuse and integration of heterogeneous data sources. Such goals are difficult to test in advance.This thesis discusses the evaluation of Web ontologies, i.e. ontologies specified in one of the standard Web ontology languages (RDF(S) Motivation
Ontologies play a central role in the emerging Semantic Web. They capture background knowledge by providing relevant concepts and the relations between them. Their role is to provide formal semantics to terms, so that they can be used in a machine processable way. Ontologies allow us to share and formalize conceptualizations, and thus to enable humans and machines to readily understand the meaning of data that is being exchanged. This enables the automatic aggregation and the proactive use and serendipitous reuse of distributed data sources, thus creating an environment where agents and applications can cooperate for the benefit of the user on a hitherto unexperienced level This section provides three preliminary arguments for the importance of ontology evaluation. The arguments are about (i) the advantages of better ontologies (Section 1.1.1), (ii) increasing the availability and thus reusability of ontologies (Section Advantages of better ontologies
Ontologies are engineering artifacts, and as such they need to be evaluated like all other engineering artifacts. The central role of ontologies in the Semantic Web makes ontology evaluation an important and worthwhile task: mistakes and omissions in ontologies can lead to applications not realizing the full potential of exchanging data. Good ontologies lead directly to a higher degree of reuse and a better cooperation over the boundaries of applications and domains.To just name a few examples of disadvantages of low quality ontologies: readability of an ontology may suffer if the vocabulary or the syntax contains errors. Reasoners may be unable to infer answers in case of inconsistent semantics. Underspecified ontologies hinder automatic mapping approaches. On the other hand, high quality ontologies can be easier reused (since their semantics can be mapped with a higher confidence to a known and grounded ontology within the reading system), can be more readily featured in an existing application (since good user interface elements can be automatically constructed in order to work with the ontology), and will easier discover and also actively omit data errors (since the ontology constrains possible data interpretations).Increasing ontology availability
Ontologies on the Semantic Web are coming from a vast variety of different sources, spanning institutions and persons aiming for different quality criteria. Ontology evaluation techniques help to consolidate these quality criteria and make them explicit. This encourages the publication of ontologies and thus increases the number of available ontologies.This argument has two sides: on the one hand, more ontologies will be published because the ontology engineers are more confident with releasing their work. On the other hand, ontologies can be automatically assessed about their quality and thus can be reused easier, since the confidence of the reusers in the quality of these ontologies increases. Even though this does not raise the actual number of ontologies accessible by a user, it does raise the number of ontologies that will be reused. This fosters cooperation and reuse on the Semantic Web and increases directly its impact and lowers its costs.For anectodal support we refer the reader to the SEKT case studies Lower maintenance costs
Decentralized collaborative ontology creation requires a high independence of tasks. If local changes lead to wide-reaching effects, then the users should be able to understand these effects. Otherwise they will invariably effect the work of many others, which will need a complex system of measures and counter-measures. The prime example is the world's currently biggest knowledge base, Wikipedia.Changes within the knowledge contained by Wikipedia require to be constantly tracked and checked. Currently, thousands of users constantly monitor and approve these changes. This is inefficient, since many of these changes could be tracked automatically using ontology evaluation technologies. They allow to automatically check if certain constraints and requirements are fulfilled. This allows maintainers to quickly and automatically reveal plausibility problems, and thus to decrease maintenance and development costs of such ontologies dramatically.Contribution
The contribution of this thesis is threefold: we (i) introduce a framework for ontology evaluation, (ii) we organize existing work in ontology evaluation within this framework and fill missing spots, and finally (iii) we implement the theoretical results in practical systems to make the results of this thesis accessible to the user.A framework for ontology evaluation
Terminology and content in ontology evaluation research has been fairly diverse. Chapter 3 contains a survey of literature, and consolidates the terminology used. It identifies and defines a concise set of eight ontology quality criteria and ontology aspects that can be evaluated (Section 3.6):Contribution
We identify and describe in detail the following aspects of ontologies (Part II):Methods for ontology evaluation
We surveyed the existing literature on ontology evaluation for evaluation methods and organized them according to the introduced framework. For each method we describe it within the context of its respective aspect.Furthermore, with the help of the framework we identified blind spots and missing methods. We introduce a number of novel evaluation methods to address these gaps. Among them are:• Schema validation (Section 5.3)• Pattern discovery using SPARQL (Section 6.2)• Normalization (Section 7.1)• Metric stability (Section 7.2)• Representational misfit (Section 8.1)• Unit testing (Section 9.1)Implementation
In order to allow the theoretical results of this thesis to be used, we implemented a number of introduced methods within Semantic MediaWiki, an extension for the popular MediaWiki wiki engine. Semantic MediaWiki has become the most popular semantic wiki engine by far, used by several hundreds installations worldwide and nurturing an active open source developer community. Semantic MediaWiki was extended to allow for the collaborative evaluation of the knowledge created within the wiki, and thus lower the costs for maintaining such a knowledge base dramatically (Chapter 10).Readers' guide
This thesis presents a conceptual framework and its implementations for defining and assessing the quality of an ontology for the Web. In this section we will offer an overview of the whole thesis so that readers may quickly navigate to pieces of particular interest to them. The whole thesis is written in such a way that it enables the understanding of the topic in one pass.In the examples in this thesis, whenever a QName is used (see Section 5.2), we assume the standard namespace declarations given in Table Chapter 2 gives the terminology and preliminaries needed to understand the rest of the work. Whereas most of the terms may be familiar to a user knowledgeable about Semantic Web technology, this chapter offers a concise definition of these terms within this thesis. Readers may skip this chapter first, and decide to use it for reference as needed. The index in the appendix can help with finding relevant sections.Chapter 3 introduces the theoretical framework for ontology evaluation that is the main theoretical contribution of this thesis. It describes criteria for ontology evaluation, aspects that can be evaluated, and how the criteria, aspects, and measures can be used in order to achieve defined goals by the user.Aspects
Part II of this thesis describe the six different aspects of ontology evaluation as defined by the evaluation framework. Each chapter in this part is dedicated to one of the aspects.Chapter 4 deals with the vocabulary of an ontology. The vocabulary of an ontology is the set of all names in that ontology, be it URI references or literals, i.e. a value with a datatype or a language identifier. This aspect deals with the different choices with regards to the used URIs or literals.Chapter 5 is about the ontology syntax. Web ontologies can be described in a number of different surface syntaxes such as RDF/XML, N-Triples, OWL Abstract Syntax, the Manchester Syntax, or many else. Often the syntactic description within a certain syntax can differ widely. This aspect is about the different serializations in the various syntaxes.Chapter 6 evaluates the structure of an ontology. A Web ontology describes an RDF graph. The structure of an ontology is this graph. The structure can vary highly even when describing the same meaning.Chapter 7 examines how the semantics of an ontology can be evaluated. A consistent ontology describes a non-empty, usually infinite set of possible models. The semantics of an ontology are the common characteristics of all these models.Chapter 8 takes a look at the aspect of representation. This aspect captures the relation between the structure and the semantics. Representational aspects regard how the explicit structure captures and defines the intended meaning, being the formal semantics of a description logics theory or some other specification of meaning.Chapter 9 finally regards how the context of an ontology can be used for evaluation. This aspect is about the features of the ontology when compared with other artifacts in its environment, which may be, e.g. an application using the ontology, a data source about the domain, or formalized requirements towards the ontology in form of competency questions.Application
The last part describes practical implications and implementations of the work given in the previous part, compares it to other work in the area, and offers conclusions on the results.Chapter 10 describes Semantic MediaWiki, an extension to the MediaWiki wiki engine, that allows for the massive collaborative creation and maintenance of ontologies. It discusses how collaborative approaches towards ontology evaluation are implemented within Semantic MediaWiki.Chapter 11 surveys related approaches and how they relate to the presented framework. Whereas most of the related approaches have been already included in the description of their respective aspect, some of them do not fit into the overall framework presented in this thesis. In this chapter we analyze the reasons for that.Chapter 12 finally summarizes the results and compares them to the motivation presented in this chapter. We collect and comment on research questions that remain open, and outline the expected future work and impact of the research topic.Relation to previous publications
Most of the content in this thesis has been published previously. Here we present the relation to other publications of the author, in order to show which parts of the content have been already peer-reviewed. Most of the publications add further details to the given topic that has not been repeated in this thesis, either because of space constraints or because that part of the work has been performed by one of the coauthors. On the other hand, all the content in this thesis has been updated to adhere to a common framework and to OWL2, which has been released only recently.The outline of this thesis was previously published in a much shorter form in A number of publications present experiences and thoughts which have informed the whole thesis. In Relation to previous publications
MediaWiki ( We also have organized two workshops on the topic of ontology evaluation (EON2006 at WWW2006 in Banff, Canada, and EON2007 at ISWC2007 in Seoul, South Korea) that have contributed heavily to an understanding of the topic of the thesis. The proceedings are available in An earlier version of Chapter 4 was published in The idea of RDF syntax normalization in order to enable XML validation in Section 5.3 has been previously published in The criticism of existing metrics presented in Section 6.1 was first raised in The notions of normalization as presented in Section 7.1 and of stable metrics presented in Section 7.2 have both been previously published in Ontological metrics as described in Section 8.1 have been first published in Unit testing as presented in Section 9.1 was introduced in Semantic MediaWikia (SMW) has been first presented in Chapter 2Terminology and Preliminaries
All good counsel begins in the same way; a man should know what he is advising about, or his counsel will all come to nought.(Socrates, 469 BC-399 BC, Phaedrus Ontologies
An ontology is a (possibly named) set of axioms. Axioms are stated in an ontology language. If all axioms of an ontology are stated in the same ontology language, then the ontology as a whole is in that ontology language.An ontology language defines which language constructs (i.e. which types of axioms) can be used in an ontology in that language. The ontology language also defines the formal semantics of that language. A big number of ontology languages have been suggested in the last twenty years, such as Ontolingua Web ontologies are ontologies that are written in one of the standardized Semantic Web ontology languages. Within this thesis we regard only Web ontologies, i.e. other ontologies using ontology languages do not necessarily have the same properties and thus may not be evaluable with the methods presented here.As of writing of this thesis, the Semantic Web ontology languages are RDF, RDFS (jointly called RDF(S)),and OWL. OWL is available in a number of profiles with specific properties According to the Semantic Web ontology languages, ontologies do not include only terminological knowledge -definitions of the terms used to describe data, and the formal relations between these terms -but may also include the knowledge bases themselves, i.e. terms describing individuals and ground facts asserting the state of affairs between these individuals. Even though such knowledge bases are often not regarded as being ontologies (see An ontology document is a particular serialization of an ontology. As such, it is an information resource, usually a file, and thus an artifact that can be processed by a machine. Web ontologies may be serialized in one of the many W3C standards for ontology serialization, i.e. RDF/XML Web ontologies are often represented with an RDF graph Axioms
and Abstract Syntax document Web ontologies are also often serialized as XML files Ontology elements are both axioms and ontology entities. In Section 2.2 we describe the available types of axioms, followed by the types of entities in Section 2.3.Axioms
An axiom is the smallest unit of knowledge within an ontology. It can be either a terminological axiom, a fact, or an annotation. Terminological axioms are either class axioms or property axioms. An axiom defines formal relations between ontology entities or their names.Facts
A fact or individual axiom is either an instantiation, a relation, an attribute, or an individual (in)equality.An instantiation or class instantiation has the formA (positive) relation or object property instance has the form PropertyAssertion(R a b) with a and b being individual names and R being an object property expression. Informally it means that the property R pointing from a to b holds -e.g. saying that Germany has the capital Berlin. In the example, Germany and Berlin are individual names, and capital is the name of the property that holds between them. The actual instantiation of this property is thus called the relation. Semantically it means that the tuple (a, b) is in the extension of the set R.OWL2 introduces negative relations, i.e. the direct possibility to state that a certain relation is not the case. This uses the following form:NegativePropertyAssertion(R a b) This means that the tuple (a, b) is not in the extension of the set R. Semantically, this was already possible to be indirectly stated in OWL DL by using the following statement:SubClassOf(OneOf(a) AllValuesFrom(R ComplementOf(OneOf(b)))) It is easy to see that the new syntax is far easier to understand.An attribute or datatype property instance uses almost the same form as a relation:PropertyAssertion(R a v) with a being an individual name, R being a datatype property expression and v being a literal. A negative attribute uses the following form respectively:NegativePropertyAssertion(R a v) Individual equality is an axiom stating that two (or more) names refer to the same individual, i.e. that the names are synonyms. An individual inequality on the other hand makes explicit that the names do not refer to the same individual. Ontology languages with the unique name assumption assume that two different names always (or by default) refer to two different individuals. In OWL, this is not the case: OWL does not hold to the unique name assumption, and thus OWL does not make any assumptions about equality or inequality of two individuals referred to by different names. The syntax for these axioms is as follows (for i ≥ 2):SameIndividual(a 1 a 2 ... a i ) DifferentIndividuals(a 1 a 2 ... a i )Class axioms
A terminological axiom is either a class axiom or a property axiom.A class axiom can either be a subsumption, class equivalence, disjoint, or disjoint union.A subsumption has the following form: SubClassOf(C D) with C (the subclass) and D (the superclass) being class expressions. This axiom states that every individual in the extension of C also has to be in the extension of D. This means that individuals in C are described as being individuals in D. For example, SubClassOf(Square P olygon) describes squares as polygons. Subsumptions may be simple subsumptions, complex subsumptions, or descriptions.In a simple subsumption both the subclass and the superclass are class names instead of more complex class expressions. Simple subsumptions form the backbone of class hierarchies.In a complex subsumption both the subclass and the superclass are complex class expressions. A complex subsumption thus sets an intricate restriction on the possible models of the ontology. Such restrictions may be rather hard to understand by users of the ontology.In a description, either the subclass is a class name and the superclass a complex class expression, or the other way around. This describes the named class, i.e. the complex class expression is a condition of the named class. If the named class is the subclass, then the complex class expression offers a necessary condition of the named class, i.e. each individual in the named class must also fit to the complex class expression. If the named class is the superclass, then the complex class expression offers a sufficient condition of the named class, i.e. each individual fitting to the complex class expression will also be an individual of the named class. Descriptions are among the most interesting axioms in an ontology, and they are the namesake of description logics.A class equivalence is stated as follows (for i ≥ 2): EquivalentClasses(C 1 C 2 ... C i ) with C n , 1 ≤ n ≤ i, being class expressions. Class equivalences are simple class equivalences, complex class equivalences, or definitions.In a simple class equivalence, both class expressions of the class equivalence axiom are class names. This is similar to a synonym, since it states that two names mean the same class, i.e. that they have the same extension.In a complex class equivalence both classes are complex class expressions, and thus the axiom defines an intricate condition on the possible models. Just like complex subsumptions such axioms and their implications may be hard to understand.If any of the two classes in a class equivalence is a class name and the other a complex class expression, then the axiom is a definition of the class name. A definition is the strongest statement about a class name, offering both a sufficient and necessary condition by means of the complex class description. Thus, a definition offers the complete meaning of a name by building on the meaning of the names used in the defining class expression. As an example, a mother can be completely described by the following axiom:EquivalentClasses(Mother IntersectionOf(Woman SomeValuesFrom(child Thing)))defining a Mother as a Woman with a child.A disjoint is an axiom of the form (for i ≥ 2): DisjointClasses(C 1 C 2 ... C i ) with C n , 1 ≤ n ≤ i, being class expressions. The axiom states that two classes have no common individuals. This type of axiom is syntactic sugar for the following axiom:A disjoint union has the form (for i ≥ 2): DisjointUnion(C D 1 D 2 ... D i ) stating that the class C is a union of all classes D n , 1 ≤ n ≤ i, and at the same time the classes D n , 1 ≤ n ≤ i are all mutually disjoint. A disjoint union is also called a complete partition or a covering axiom.Property axioms
A property axiom describes formal semantics of properties. Unlike classes, properties can hardly be described or even defined with a single axiom. In other words, whereas an OWL ontology allows to classify individuals based on their descriptions, the same is not true for property instances. Property axioms can be used to define the formal semantics of properties, but this is hardly ever expressive enough to define a property. This is because the only property expressions are inverse property and property chains.The available property axioms either define (i) the relation between two properties, (ii) their domain or range, (iii) their type, (iv) or keys for individuals. The formal semantics of all these axioms are given in Table Relations between properties are subproperties (e.g. mother as a subproperty of parent), equivalent properties (e.g. mother as an equivalent property to mom), disjoint properties (e.g. mother and father have to be disjoint sets), and inverse properties (e.g. child is inverse to parent).A domain definition defines the class an individual belongs to, if that property points from it. A range definition defines the class an individual belongs to, if that property points to it. E.g. the property wife would connect a groom with his bride, and thus have the domain Man and the range Woman (based on a conservative conceptualization not accounting for same-sex marriages). Note that domains and ranges are not constraints, i.e. the system will usually not detect inconsistencies if the related individuals do not belong to the specified class, a behaviour often anticipated by programmers since it resembles the usage of signatures in procedure definitions. In order to do so, the ontology requires either sufficient disjoints between the classes (see Section 9.2.1) or we need to add the ability to formulate domains and ranges as constraints (see A number of axioms can be used to declare specific formal types of properties. These are functional, inverse functional, reflexive, irreflexive, symmetric, asymmetric, and transitive property declarations. The formal semantics of all these properties are given in Table Finally, property axiom can define keys over one or more properties. Keys, just as inverse functional properties are important to infer the identity of individuals, 2Entities
and thus play a crucial role in merging data from heterogeneous sources.Annotations
An annotation connects an element by an annotation property with an annotation value. Elements can be either entities, ontologies, or axioms. An annotation has no impact on the DL semantics, but adds further information about the elements.The most widely deployed annotation is rdf:label. It connects an element with a human-readable label. We will investigate this in Section 4.2.3 in detail.Annotations can express further metadata about the data itself, e.g. who stated a specific axiom, when was a class introduced, which properties are deprecated, etc. Many of these annotations can be used for evaluations, and throughout this thesis we will frequently see examples of such usage. Before the introduction of the punning mechanism in OWL2 (see Ontology annotations add metadata about the whole ontology, e.g. stating the author of the ontology, the version of the ontology, pointing to previous versions, stating compatibility with previous versions, etc. The OWL standard already defines a number of these ontology annotations, but allows for more to be added.Entities
An ontology enitity may be an individual, a class, a property, or an ontology. Since OWL2, the names referencing these entities do not have to be disjoint anymore, i.e. one and the same name may point to both an individual and a class. Consider the following example where Father is the name for a property (connecting a person to its father), a class (of all fathers), and an individual (as an instance of the class role, which in return can be used to query for all roles a person has or can have). Individuals
Individuals can be given by their name or as an anonymous individual. An individual can be any entity with an identity (otherwise it would not be possible to identify that entity with an identifier).An anonymous individual does not have a URI but provides only a local name instead. This means that the individual can not be identified directly from outside of the given ontology, but only through indirect means like inverse functional properties, keys, or nominals. We will discuss anonymous individuals in Section 4.3 in more detail.Classes
A class is a set of individuals. A class is given by a class expression. A class expression may either be a class name or a complex class description. A class name is simply the name, i.e. a URI, of a class. Class names do not carry any further formal information about the class.A complex class expression defines a class with the help of other entities of the ontology. In order to create these expressions, a number of constructs can be used. The formal semantics and exact syntax of all these constructs are given in Table The available set operations are intersections, unions, complements, and nominals. A nominal defines the extension of a class by listing all instances explicitly.The available restrictions are the existential restriction on a property (i.e. a class of all instances where the property exists), the universal restriction (on a property P and a class C, constructing a class where the instances have all their P property values be instances of C), unqualifed number restriction, the qualified number restriction, and the self-restriction (on a property P , stating that an instance has to be connected to itself via P ).As we can see, classes can be expressed with a rich variety of constructs, whereas the same does not hold for individuals and properties.Properties
Properties are given by a property expression. Most often, a property expression is just a property name. The only complex property expressions are inverse properties and property chains.An inverse property is the property expression that is used when the subject and the object exchange their place in a property instantiation. For an example, child is the inverse property of parent. Instead of giving the inverse property the property name parent, we could have used the property expression InverseOf(child) instead.A property chain is the property expression that connects several property expressions in a chain, e.g. the property uncle can be described as a superproperty of the chaining of the properties parent and brother by using the following axiom:SubPropertyOf(PropertyChain(parent brother) uncle) Note that this is not a definition of uncle (since uncle may also be the chaining of the properties parent, sister, and husband). Since there are no boolean operators on properties (i.e. property unions, intersections, and complements) we cannot actually define uncle.Properties can be either object properties or data properties. Object properties connect two individuals with each other. Data properties connect an individual with a data value.A data value is not represented by a URI but rather by a literal, the syntactic representation of a concrete value. The mapping between the literal and the data value is given by a datatype map. For example, the typed literal "4"^^xsd:int is mapped to the number 4. More on literals will be discussed in Section 4.2.Ontologies
An ontology can be either a named or an unnamed ontology. Ontologies can also be regarded as ontology entities and can have axioms to describe them, especially with annotation axioms (e.g. to state the authoring institution or version information).A named ontology is an ontology that explicitly states its name inside the ontology. Within this ontology, and especially in external ontologies this ontology can now be referred to by name.An unnamed ontology is an ontology that has no name given explicitly within the ontology. If a location is available, the location may be used instead of the name in this case. An ontology's name is also always the local name which is represented by the empty string, which should be interpreted as this ontology. For example, in RDF/XML serialization the following statement would say that this ontology was created on January 1 2010. We can see that instead of the name in the first line there is just an empty string, enclosed by double quotes.<owl:Ontology rdf:about=""> <dc:created>2010-01-01</dc:created> </owl:Ontology>Semantics
Throughout this thesis we are using the OWL2 Functional Syntax ∃mother.⊤ ⊑ Female
Although the DL syntax is more concise, the intention of a domain declaration is easier to see from the Functional Syntax. RDF based syntaxes such as N3 on the other hand become very unwieldy and need to deal with many artifacts introduced to the fact that complex axioms need to be broken down in several triples (see the example below).Table To give one example: the DisjointProperties axiom type is given in Table But the axiom type can use an arbitrary number of parameters, e.g.DisjointProperties(R S T )
stating that all the given properties are mutually disjoint, i.e.  Datatype properties are built in an analogous way, but further allow for facets. Facets allow to constrain the range of data values, i.e. one may restrict the age for adults to be bigger than or equal to eighteen years.Table Functional syntax
Set semantics RDF-Graph (N3)     Framework
A definition is the starting point of a dispute, not the settlement.(Neil We introduce a framework for ontology evaluation. The rest of this thesis will be built on the framework in this chapter. First, we give an informal overview of the whole framework in Section 3.1, introducing the relevant terms and their connections. Section 3.2 describes an ontology of ontology evaluation and related concepts, formally specifying the framework presented earlier. Based on that specification, we define different types of ontologies in Section 3.3. We then outline the limits of this work in Section 3.4. The concepts of the ontology are finally discussed in more detail in the following sections: conceptualizations (Section 3.5), quality criteria (Section 3.6), evaluation methods (Section 3.7), and ontology aspects (Section 3.8).Overview
The following framework is inspired by the semiotic meta-ontology O 2 and the ontology of ontology evaluation and selection oQual An ontology (i) specifies a conceptualization, (ii) consists of a set of axioms, (iii) is expressed by an ontology document, and (iv) constraints the construction of models satisfying the ontology. In Section 3.5 we discuss the conceptualizations and their relation to ontologies in detail. The structural definition of an ontology as a set of axioms was given in Section 2.2. The serialization or expression of an ontology as an ontology document was described in Section 2.1. Constraining the models is done by the semantics of an ontology as given in Section 2.4.Ontologies are not artifacts in a narrow sense, but are expressed by ontology documents which in turn are artifacts. Whenever one speaks about ontologies as artifacts they mean ontology documents. Evaluation methods are descriptions of procedures that assess a specific quality of an ontology. Since methods cannot asses an ontology directly (since they are not artifacts), methods directly always evaluate ontology documents. Only indirectly it is possible for an evaluation method to assess an ontology (i.e. by assessing the ontology document that expresses the ontology). Figure Figure An ontology evaluation may be expressed by an ontology, which has the advantage that the result can be reused with the very same tools that we use with the ontologies anyway. It enables us to integrate the results of several different methods and thus to build complex evaluations out of a number of simple evaluations.To give an example: the result of a method such as calculating the ratio between the normalized and unnormalized depth (described in Section 8.2) may be represented as a simple fact in an ontology profile: 3Meta-ontology
PropertyAssertion PropertyAssertion(instancedClassRatio Ontology1 "1.0"^^xsd:decimal) Since the results are both expressed in OWL, we can easily combine the two facts and then create a new class of ontologies based on this metadata about ontologies:EquivalentClasses(FluffyOntology} IntersectionOf(HasValue(normalDepthRatio "1.0"^^xsd:decimal) HasValue(instancedClassRatio "1.0"^^xsd:decimal))) Meta-ontology
The framework presented in Section 3.1 and the terminology presented in Chapter 2 is specified as a meta-ontology Reifying ontologies
A major part of the meta-ontology provides the vocabulary to reify an ontology. This means that we create new entities to represent the axioms and entities in an ontology. Here we will demonstrate how the reification works based on a single axiom type, subsumption.As shown in Section 2.2.2, the syntax for subsumptions is the following:The reification first creates a new individual to represent the axiom (m:Axiom1 in the first line), then reifies the mentioned class names (second and third lines, creating m:Cat and m:Pet), connects the reified terms to the terms in the original axiom (see Section 3.2.2 for more details on reifying terms), and connects them to the individual representing the axiom. The last line adds a property directly relating the reified sub-and superclass, which helps with formulating some analyses (e.g. as given by the AEON approach in Section 6.3). The namespaces used in this example are meta for the meta-ontology itself, m for the reifying ontology, and o for the ontology that is being reified. Figure Every axiom is also explicitly connected to the ontology it is part of, and also the number of axioms is defined. This allows us to close the ontology and thus to classify it (see the classification example in ClassAssertion
FunctionalProperty(meta:subClass) PropertyDomain(meta:subClass meta:Subsumption) 40 PropertyRange(meta:subClass meta:Class) FunctionalProperty(meta:superClass) PropertyDomain(meta:subClass meta:Subsumption) PropertyRange(meta:subClass meta:Class) SubPropertyOf(PropertyChain(InverseOf(meta:subClass) meta:superClass) meta:subClassOf)These axioms determine further formal relations between the terms of the metaontology. These decrease the risk of adding erroneous axioms without becoming inconsistent (i.e. the risk of adding errors that are not detected).The property meta:subClassOf only regards explicitly stated subsumption. In order to expressed inferred subsumption a property meta:inferredSubClassOf exists.Reifying URIs
The reified ontology does not make statements about the classes and individuals of the domain, but about the names used in the ontology. So, for example, even if the ontology claims that two terms are synonymous SameIndividual(o:Denny o:Zdenko) the reified individuals representing these individuals are not the same, since they are different names.DifferentIndividuals(m:Denny m:Zdenko) This means, whereas o:Denny represents the author of this thesis, m:Denny represents the name of the author of this thesis (in this case the URI o:Denny).Even though it would be easy to create new URIs for every reified axiom and entity by simply creating new random URIs, it makes sense to generate the reified URIs for the entities based on their actual URIs. We use an invertible function by simply concatenating an URI-encoded (i.e. escaped) URI to a fixed namespace. This allows us to set up a Web service that returns information about the given reified URI at the fixed namespace.Advantages of a meta-ontology
Throughout this thesis, we will use reified ontologies in order to express the methods succinctly and unambiguously. The following gives an overview of some of the uses the reified ontology can be put to.• Section 3.3 shows how the meta-ontology can be used to classify the ontology itself and thus to make information about the ontology explicit.• SPARQL queries can be used to discover the application of ontology patterns or anti-patterns (see Section 6.for examples).
• There exists no standard OWL query language and SPARQL can not be used to query for the existence of OWL axioms (see Section 6.2 for details). Based on a reified meta-ontology it is possible to use SPARQL for queries against the axiom structure of the ontology (instead merely the RDF graph structure).• Meta-properties and constraints on meta-properties can be directly expressed and reasoned over. The AEON approach uses the OntoClean methodology • Additional axioms can be added to check if the ontology satisfies specific constraints, for example, it is easy to check if a subsumption axiom is ever used to express the subsumption of a class by itself by adding that meta:subClassOf is irreflexive and checking the resulting ontology for consistency.Types of ontologies
Terminological ontology
A terminological ontology is an ontology that consists only of terminological axioms and annotations. This is introduced in order to account for the often found understanding that an ontology indeed is a set of only terminological axioms. This disagrees with the W3C definition of the term ontology, where an ontology may also include facts or even be constituted only of facts. In DL terms, a terminological ontology consists only of a TBox, i.e. terminological knowledge.In terms of the meta-ontology, we define a terminological ontology as follows:This distinction is a fairly common one, even though all facts can be expressed as terminological axioms (using nominals) thus making this distinction irrelevant. We proof this by offering terminological axioms that could replace each type of fact: Knowledge base
A knowledge base is an ontology that consists only of facts and annotations.In DL terms, a knowledge base only has an ABox, i.e. contains only assertional knowledge.EquivalentClasses(Knowledge_base IntersectionOf(Ontology AllValuesFrom(axiom UnionOf(Fact Annotation))))A knowledge base instantiates an ontology if the facts within the knowledge base use property names and class names introduced in that ontology. Often, knowledge bases instantiate several ontologies. An exclusive ontology instantiation uses only property names and class names from a single ontology. Often such instantiating ontologies are named by the ontology, e.g. a FOAF file is an often exclusive ontology instantiation of the FOAF ontology EquivalentClasses(FOAF_file
Semantic spectrum
The semantic spectrum defines one dimension of ontologies, ranging from the most simple and least expressive to the most complex and most precise ones. It was first presented at an invited panel at the AAAI 1999 in Austin, Texas, then published in We aggregate the types of ontologies they report on as the following five, ordered by increasing complexity (see Figure • catalogs / sets of IDs• glossaries / sets of term definitions• thesauri / sets of informal is-a relations • formal taxonomies / sets of formal is-a relations• proper ontologies / sets of general logical constraints
We name the most expressive type of ontologies proper ontologies rather than formal ontologies as in In order to appropriately evaluate an ontology, we have to first determine its type. The type of an ontology determines which evaluation methods can be useful for the ontology, and which make no sense. There are two ways to determine the type of the ontology: prescriptive and descriptive. Prescriptive determination is given by the ontology authors, i.e. they define the task that the ontology should fulfill and based on that the type of ontology that is required. Descriptive determination is given by examining an ontology and say what type of ontology is actually given.Catalog
Based on the meta-ontology described in Section 3.2 we can actually classify an ontology automatically since we can define the above types in the meta-ontology.A catalog is an ontology that consists only of label annotations. This means that a catalog is just a set of URIs with human readable labels.EquivalentClasses(Catalog
IntersectionOf(Ontology AllValuesFrom(axiom Label_annotation)))In the simplified case, Label annotation is defined as an annotation instantiating the rdfs:label annotation property (Label is the reification of rdfs:label).EquivalentClasses(Label_annotation
IntersectionOf(Annotation HasValue(annotation_property Label)))If we want to include not only instantiations of the rdfs:label property but also of its subproperties, e.g. skos:prefLabel or skos:altLabel, we need to redefine Label annotation to also include its subproperties.EquivalentClasses(Label_annotation
IntersectionOf(Annotation AllValuesFrom(annotation_property Label_property))) EquivalentClasses(Label_property Has_Value(inferredSuperProperty Label))Due to OWL's open world semantics this definition is much harder to reason with, since the reification of the ontology we want to classify needs to include sufficient axioms to make other models impossible. We will discuss possible solutions of that in EquivalentClasses(Glossary
IntersectionOf(Ontology AllValuesFrom(axiom Annotation)))A thesaurus is an ontology that, besides annotations, also allows instantiations of classes and properties from the SKOS ontology EquivalentClasses(Thesaurus
A taxonomy or class hierarchy is an ontology that consists only of simple subsumptions, facts, and annotations.EquivalentClasses(Taxonomy
IntersectionOf(Ontology AllValuesFrom(axiom UnionOf(Simple_subsumption Fact Annotation))))A proper ontology finally allows for all possible axioms, as defined in Section 2.2. In the semantic spectrum, each type of ontologies subsumes the simpler types, i.e.Catalog ⊂ Glossary ⊂ Thesaurus ⊂ Taxonomy ⊂ Ontology This means that every glossary is also a taxonomy (though obviously a degenerated taxonomy, since the depth of the class hierarchy is 0), etc.Classification example
This Section gives an illustrative example how an ontology may be classified. Afterwards we discuss problems of such classification and possible solutions.Given Using this with the meta-ontology, a reasoner can infer that m:o1 is a Taxonomy: it is stated that there are exactly two axioms in the ontology (i.e. the ontology is closed), so it is allowed to make inferences from the universal quantifier in the definition of Taxonomy.Many reasoners cannot deal well with cardinality constraints. KAON2 Limits
Ontology evaluations are conducted on several different levels:1. Ontologies can be evaluated by themselves.2. Ontologies can be evaluated with some context.3. Ontologies can be evaluated within an application. This is called application based ontology evaluation 4. Ontologies can be evaluated in the context of an application and a task. This is called task based ontology evaluation In this thesis we will restrict ourselves to the first two possibilities. Note that each of the above levels gains from evaluating the previous levels, i.e. every ontology evaluated within an application should have been evaluated by itself and with some context before that. Many types of errors are much easier discovered on the first and second level than in the much more complex environment of an application or a task. The majority of this thesis deals with the first task (Chapters 4-8), whereas the second point is dealt with in Chapter 9.Ontology-based applications most often have certain requirements regarding the applied ontologies. For example, they may require that the data within the ontology is complete (e.g. a semantic birthday reminder application may require that all persons need to have at least their name and birthday stated), or they may have certain structural or semantic constraints (e.g. the class hierarchy must not be deeper than five levels). Such conditions can often be stated in a way that allows to use the evaluation methods within this thesis in order to ensure the application's requirements are satisfied.Asunción Gómez-Pérez separates ontology evaluation into two tasks: ontology verification and ontology validation Ontology verification is the task of evaluating if the ontology has been built correctly. Verification checks the encoding of the specification. Errors such as circular class hierarchies, redundant axioms, inconsistent naming schemes etc. are detected by ontology verification. Verification confirms that the ontology has been built according to certain specified ontology quality criteria.Ontology validation is the task of evaluating if the correct ontology has been built. Validation refers to whether the meaning of the definitions matches with the conceptualization the ontology is meant to specify. The goal is to show that the world model is compliant with the formal models.Since this thesis mainly concentrates on methods that can be highly automatized we will limit ourselves to ontology verification. Chapter 11 will discuss further methods that lean more towards ontology validation. Often such ontology validation methods assume a simpler understanding of ontologies i.e. they assume that an ontology is more a formal description of a domain than the formal specification of a shared conceptualization. Section 3.5 will discuss in detail the ramifications of this difference.Like applications, ontologies are used in many different domains. Whereas it is easily possible, and often sensible, to create domain-specific ontology evaluation methods, in this thesis we will restrict to domain-independent evaluations. This means that even though the examples presented throughout the thesis will use some domain, the applied methods will always be usable on any other domain as well.To summarize, this thesis will concentrate on the domain-, task-, and applicationindependent verification of Web ontologies. The framework presented in this chapter, and thus the methods described throughout the following chapters, have to be regarded in light of this limitation. Also, the methods presented in Part II are not a complete list of possible evaluation methods. They should rather be regarded as a list of exemplary methods in order to illustrate the usage of the framework presented here. Further methods can easily be created and combined within the framework in order to meet the needs of the evaluator.Conceptualizations
This section describes and formalizes how agents can achieve a shared conceptualization. The formalization is reminiscent of epistemic logics, but differs from them in two main points: first, epistemic logics are about propositions, whereas here we speak mainly about conceptualizations. Second, epistemic logics assume that propositions itself are shareable, whereas here we assume conceptualizations to be private to an agent. In this section we will resolve the discrepancy between shared and private conceptualizations.A rational agent has thoughts and perceptions. In order to express, order, and sort its thoughts and perceptions the agent creates, modifies, and uses conceptualizations. A conceptualization is the agent's mental model of the domain, its understanding of the domain. We define C X (d) to be the conceptualization that agent X has of domain d, C X being a function C X : D → C X with D being the set of all domains and C X being the set of all conceptualizations of agent X.A shared conceptualization represents the commonalities of two (or more) conceptualizations of different agents. We define the operator ∩ for creating a shared conceptualization, but note that this should not be understood as the set-theoretic intersection operator -conceptualizations probably are not sets. It is rather an operation done internally by the agent on two or more of his internal conceptualizations to derive the commonalities.Extracting a shared conceptualization can only be done with conceptualizations of the same agent, i.e. C X (d) ∩ C Y (d) is undefined. This is because conceptualizations are always private (i.e. conceptualizations have no reality external to their agents and thus two agents can not have the same conceptualization). So in order to intersect the conceptualizations of two different agents, an agent needs to conceptualize the other agent's conceptualization (as stated above, anything can be conceptualized, in particular another conceptualization). By interacting and communicating with the other agents, each agent builds conceptualizations of the other agents. These conceptualized agents again are conceptualized with their own private conceptualizations. C X (C Y (d)) is the conceptualization X has of the conceptualization Y has of a domain d (note that C X (C Y (d)) tells us more about C X (Y ) and thus about X than about C Y (d) or about Y ). For simplicity, we assume that each agent's conceptualization of its own conceptualization is perfect, i.e. C X (C X (d)) = C X (d) (this is similar to the KK axiom in epistemic logics An agent can combine its conceptualizations to arrive at a shared conceptualization, i.e. C X (d) ∩ C X (C Y (d)) results in what X considers to be the common understanding of d between itself and Y . Regarding a whole group of n agents we define the (private conceptualization of the) shared conceptualization SC X as follows (let C X be one ofFurthermore, X assumes that ∀i : C X (SC Y i (d)) = SC X (d), i.e. X assumes that everybody in the group has the same shared conceptualization. This is true for all each other, and their respective conceptualizations. members of the group, i.e. ∀i, j :In Figure An ontology O is the specification (defined as the function S) of a conceptualization C, i.e. it is the result of the externalization of a conceptualization O = S(C). For our discussion it is not important, how S was performed (e.g. if it was created collaboratively, or with the help of (semi-)automatic agents, or in any other way).The resulting ontology is a set of axioms that constrain the interpretations of the ontology. This has two aspects, depending on the interpreting agent: a formal agent (e.g. an application using a reasoning engine to apply the formal semantics) will have the possible logical models constrained, and based on these models it will be able to answer queries; a rational agent (e.g. a human understanding the ontology) is constrained in the possible mappings of terms of the ontology to elements of its own internal conceptualization (possibly changing or creating the conceptualization during the mapping). Figure To give an example: if the property married is stated to be a functional property, a formal agent will only allow models to interpret the ontology where the set R denoted by married fulfills the condition (x, y) ∈ R ∧ (x, z) ∈ R → y = z. A rational agent in turn will map the term married to a monogamous concept of marriage, and will not use the term to refer to polygamous marriages. In this case, the rational agent already had a concept that just needed to be mapped to a term from the ontology. In other cases the agent may need to first create the concept before being able to map to it (for example, let a wide rectangle be defined as a rectangle with the longer side being at least three times the size of the short sidein this case readers create a new concept in their mind, and then map the term wide rectangle to it).So after the creation of the ontology O, each member Y of the group can create a conceptualization of O, i.e. internalize it again. So a group member Y i gets the internal conceptualization C Y i (O), and then compares it to its own understanding of the shared specification SC Y i (d). Ideally, all members of the group will agree on the ontology, i.e. ∀i :Note that creating an ontology is not the simple, straight-forward process that is presented in this section. Most of the conceptualizations will be in constant flux during the process. The communication in the group during the creation of the specification may change each member's own conceptualization of the domain, each member's conceptualization of each other member's conceptualization of the domain, each member's shared conceptualization of the domain, and in some cases even the domain itself. Criteria
Ontology evaluation can regard a number of several different criteria. In this section we will list criteria from literature, aggregate them to form a coherent and succinct set, and discuss their applicability and relevance for Web ontologies. The goal of an evaluation is not to perform equally well for all these criteria -some of the criteria are even contradicting, such as conciseness and completeness. It is therefore the first task of the evaluator to choose the criteria relevant for the given evaluation and then the proper evaluation methods to assess how well the ontology meets these criteria.We selected five important papers from literature, where each defined their own set of ontology quality criteria or principles for good ontologies Asunción Gómez-Pérez lists the following criteria (Gómez-Pérez, 2004):• Consistency: capturing both the logical consistency (i.e. no contradictions can be inferred) and the consistency between the formal and the informal descriptions (i.e. the comments and the formal descriptions match)• Completeness: All the knowledge that is expected to be in the ontology is either explicitly stated or can be inferred from the ontology.• Conciseness: if the ontology is free of any unnecessary, useless, or redundant axioms.• Expandability: refers to the required effort to add new definitions without altering the already stated semantics.• Sensitiveness: relates to how small changes in an axiom alter the semantics of the ontology.Thomas Gruber defines the following criteria (Gruber, 1995):• Clarity: An ontology should effectively communicate the intended meaning of defined terms. Definitions should be objective. When a definition can be stated in logical axioms, it should be. Where possible, a definition is preferred over a description. All entities should be documented with natural language• Coherence: Inferred statements should be correct. At the least, the defining axioms should be logically consistent. Also, the natural language documentation should be coherent with the formal statements.• Extendibility: An ontology should offer a conceptual foundation for a range of anticipated tasks, and the representation should be crafted so that one can extend and specialize the ontology monotonically. New terms can be introduced without the need to revise existing axioms.• Minimal encoding bias: An encoding bias results when representation choices are made purely for the convenience of notation or implementation. Encoding bias should be minimized, because knowledge-sharing agents may be implemented with different libraries and representation styles.• Minimal ontological commitment: The ontology should specify the weakest theory (i.e. allowing the most models) and defining only those terms that are essential to the communication of knowledge consistent with that theory.Grüninger and Fox define a single criteria, competency (or, in extension, completeness if all the required competencies are fulfilled). In order to measure competency they introduce informal and formal competency questions Obrst et al. name the following criteria • coverage of a particular domain, and the richness, complexity, and granularity of that coverage• intelligibility to human users and curators• validity and soundness• evaluation against the specific use cases, scenarios, requirements, applications, and data sources the ontology was developed to address• consistency• completeness• the sort of inferences for which they can be used• adaptability and reusability for wider purposes• mappability to upper level or other ontologies Gangemi et al. define the following criteria • Cognitive ergonomics: this principle prospects an ontology that can be easily understood, manipulated, and exploited.54
• Transparency (explicitness of organizing principles): this principle prospects an ontology that can be analyzed in detail, with a rich formalization of conceptual choices and motivations.• Computational integrity and efficiency: this principle prospects an ontology that can be successfully/easily processed by a reasoner (inference engine, classifier, etc.).• Meta-level integrity: this principle prospects an ontology that respects certain ordering criteria that are assumed as quality indicators.• Flexibility (context-boundedness): this principle prospects an ontology that can be easily adapted to multiple views.• Compliance to expertise: this principle prospects an ontology that is compliant to one or more users.• Compliance to procedures for extension, integration, adaptation, etc.: this principle prospects an ontology that can be easily understood and manipulated for reuse and adaptation.• Generic accessibility (computational as well as commercial): this principle prospects an ontology that can be easily accessed for effective application.• Organizational fitness: this principle prospects an ontology that can be easily deployed within an organization, and that has a good coverage for that context.We have taken analyzed the given criteria and summarized them into a concise set. Eight criteria result from this literature survey: accuracy, adaptability, clarity, completeness, computational efficiency, conciseness, consistency, and organizational fitness. All criteria given in the literature are subsumed by the this set. In the following, we define the criteria and how they map to the given criteria described above.We have ignored evaluation criteria that deal with the underlying language used for describing the ontology instead of evaluating the ontology itself. Before OWL became widespread, a plethora of knowledge representation languages were actively used. For some ontologies, specific ontology languages were developed in order to specify that one ontology. Today, OWL is used for the vast majority of ontologies. Therefore we disregard criteria that are based on the ontology language, such as expressivity, decidability, complexity, etc.One example is the criteria expandability from (Gómez-Pérez, 2004). It is defined as the required effort to add new definitions without altering the already stated semantics. Since OWL is a monotonic language it is not possible to retract any inferences that have already been made. Thus the monotonicity of OWL guarantees a certain kind of expandability for all ontologies in OWL. A complete list of methods given in this thesis is available in the appendix.Accuracy
Accuracy is a criteria that states if the axioms of the ontology comply to the knowledge of the stakeholders about the domain. A higher accuracy comes from correct definitions and descriptions of classes, properties, and individuals. Correctness in this case may mean compliance to defined "gold standards", be it other data sources, conceptualizations, or even reality ( For example, all inferences of an ontology should be true. When stating that the foaf:knows property is a superproperty of a married property, then this axiom would only be accurate if indeed all married couples know their respective spouses. If we find counterexamples (for example, arranged prenatal marriages), then the ontology is inaccurate.The following methods in this thesis can be used to measure this criteria: Adaptability
Adaptability measures how far the ontology anticipates its uses. An ontology should offer the conceptual foundation for a range of anticipated tasks (ideally, on the Web, it should also offer the foundation for tasks not anticipated before). It should be possible to extend and specialize the ontology monotonically, i.e. without the need to remove axioms (note that in OWL, semantic monotonicity is given by syntactic monotonicity, i.e. in order to retract inferences explicit stated axioms need to be retracted). An ontology should react predictably and intuitively to small changes in the axioms. It should allow for methodologies for extension, integration, and adaptation, i.e. include required meta-data. New tools and unexpected situations should be able to use the ontology.For example, many terms of the FOAF ontology The following methods in this thesis can be used to measure this criteria: Clarity
Clarity measures how effectively the ontology communicates the intended meaning of the defined terms. Definitions should be objective and independent of the context. Names of elements should be understandable and unambiguous. An ontology should use definitions instead of descriptions for classes. Entities should be documented sufficiently and be fully labeled in all necessary languages. Complex axioms should be documented. Representation choices should not be made for the convenience of the notation or implementation, i.e. the encoding bias should be minimized.For example, an ontology may choose to use URIs such as ex:a734 or ex:735 to identify their elements (and may even omit the labels). In this case, users of the ontology need to regard the whole context of the elements in order to find a suitable mapping to their own conceptualizations. Instead, the URIs could already include hints to what they mean, such as ex:Jaguar or ex:Lion.The following methods in this thesis can be used to measure this criteria: Method Completeness
Completeness measures if the domain of interest is appropriately covered. All questions the ontology should be able to answer can be answered. There are different aspects of completeness: completeness with regards to the language (is everything stated that could be stated using the given language?), completeness with regards to the domain (are all individuals present, are all relevant concepts captured?), completeness with regards to the applications requirements (is all data that is needed present?), etc. Completeness also covers the granularity and richness of the ontology.For example, an ontology to describe the nationalities of all members of a group should provide the list of all relevant countries. Such closed sets in particular (like countries, states in countries, members of a group) can often be provided as an external ontology by an authority to link to, and thus promise completeness.The following methods in this thesis can be used to measure this criteria: Computational efficiency
Computational efficiency measures the ability of the used tools to work with the ontology, in particular the speed that reasoners need to fulfill the required tasks, be it query answering, classification, or consistency checking. Some types of axioms may cause problems for certain reasoners. The size of the ontology also affects the efficiency of the ontology.For example, using certain types of axioms will increase the reasoning complexity. But more important than theoretical complexity is the actual efficiency of the implementation used in a certain context. For example, it is known that number restriction may severely hamper the efficiency of the KAON2 reasoner The following methods in this thesis can be used to measure this criteria: Conciseness
Conciseness is the criteria that states if the ontology includes irrelevant elements with regards to the domain to be covered (i.e. an ontology about books including axioms about African lions) or redundant representations of the semantics. An ontology should impose a minimal ontological commitment, i.e. specify the weakest theory possible. Only essential terms should be defined. The ontology's underlying assumptions about the wider domain (especially about reality) should be as weak as possible in order to allow the reuse within and communication between stakeholders that commit to different theories.For example, an ontology about human resource department organization may take a naïve view on what a human actually is. It is not required to state if a human has a soul or not, if humans are the result of evolution or created directly by God, when 58 3.6 Criteria human life starts or ends. The ontology would remain silent on all these issues, and thus allows both creationists and evolutionists to use it in order to make statements about which department has hired whom and later exchange that data.The following methods in this thesis can be used to measure this criteria: Method Consistency
Consistency describes that the ontology does not include or allow for any contradictions. Whereas accuracy states the compliance of the ontology with an external source, consistency states that the ontology itself can be interpreted at all. Logical consistency is just one part of it, but also the formal and informal descriptions in the ontology should be consistent, i.e. the documentation and comments should be aligned with the axioms. Further ordering principles can be defined that the ontology has to be consistent with, such as the OntoClean constraints on the taxonomy Note that within this thesis we will deal with logical consistency and coherence only superficially. There is an active research community in the area of ontology debugging, that covers discovering, explaining, and repairing errors that lead to inconsistency and incoherence, see for example An example for a non-logical inconsistency is the description of the element ex:Jaguar being "The Jaguar is a feral cat living in the jungle.", but having a logical axiom ClassAssertion(ex:Car manufacturer ex:Jaguar). Such discrepancies are often the result of distributed ontology engineering or a badly implemented change management procedures in ontology maintenance.The following methods in this thesis can be used to measure this criteria: Organizational fitness
Organizational fitness aggregates several criteria that decide how easily an ontology can be deployed within an organization. Tools, libraries, data sources, and other ontologies that are used constrain the ontology, and the ontology should fulfill these constraints. Ontologies are often specified using an ontology engineering methodology or by using specific data sets. The ontology metadata could describe the applied methodologies, tools, and data sources, and the organization. Such metadata can be used by the organization to decide if an ontology should be applied or not.For example, an organization may decide that all ontologies used have to align to the DOLCE upper level ontology The following methods in this thesis can be used to measure this criteria: Method Methods
Evaluation methods either describe procedures or specify exactly the results of such procedures in order to gain information about an ontology, i.e. an ontology description. An evaluation method assesses specific features or qualities of an ontology or makes them explicit. The procedures and result specifications given in Part II are not meant to be implemented literally. Often such a literal or naïve implementation would lead to an unacceptably slow runtime, especially with mid-sized or big ontologies. Many of the methods in this thesis remain for now without an efficient implementation.The relationship between criteria and methods is complex: criteria provide justifications for the methods, whereas the result of a method will provide an indicator for how well one or more criteria are met. Most methods provide indicators for more than one criteria, therefore criteria are a bad choice to structure evaluation methods.A number of the methods define measures and metrics and also offers some upper or lower bounds for these metrics. Note that these bounds are not meant to be strict, stating that any ontology not within the bounds is bad. There are often perfectly valid reasons for not meeting those limits. These numbers should also not lead to the implementation of automatic fixes in order to implement changes to an ontology that make the ontology abide to the given limits, but nevertheless decrease the ontology's overall quality. The numbers given in the method descriptions are chosen based on evaluating a few thousand ontologies in order to discover viable margins for these values. In the case a certain measure or metric goes well beyond the proposed value but the ontology author or evaluator has good reasons, they should feel free to ignore that metric or measure or, better, explain in a rationale why it is not applicable in the 60Aspects
given case.The Semantic Web is still in its infancy. It is to be expected that as we gather more experience with engineering and sharing ontologies, we will find better values for many of these bounds. It is also expected that further methods will be introduced and existing ones may become deprecated. The framework described here allows to accommodate for such changes. The set of methods is flexible and should be chosen based on the needs of the given evaluation.Part II of this thesis describes evaluation methods. In order to give some structure for the description of the methods, the following section introduces different aspects of an ontology. These aspects provide the structure for Part II.Aspects
An ontology is a complex, multi-layered information resource. In this section we will identify different aspects that are amenable to the automatic, domain-and taskindependent verification of an ontology. Based on the evaluations of the different ontology aspects, evaluators can then integrate the different evaluation results in order to achieve an aggregated, qualitative ontology evaluation. For each aspect we show evaluation methods within the following chapters.Each aspect of an ontology that can be evaluated must represent a degree of freedom (if there is no degree of freedom, there can be no evaluation since it is the only choice). So each aspect describes some choices that have been made during the design of the ontology. Some tools do not offer a degree of choice on certain aspects. In such cases evaluation methods for this aspect do not lead to useful insights. In turn, these tools should be evaluated in order to result in the best possible choice for those fixed aspects.• Vocabulary. The vocabulary of an ontology is the set of all names in that ontology. Names can be URI references or literals, i.e. a value with a datatype or a language identifier. This aspect deals with the different choices with regards to the used URIs or literals (Chapter 4).• Syntax. Web ontologies can be described in a number of different surface syntaxes. Often the syntactic description within a certain syntax can differ widely (Chapter 5).• Structure. A Web ontology can be described by an RDF graph. The structure of an ontology is this graph. The structure can vary highly even describing semantically the same ontology. The explicitly given graph is evaluated by this aspect (Chapter 6).• Semantics. A consistent ontology is interpreted by a non-empty, usually infinite set of possible models. The semantics of an ontology are the common characteristics of all these models. This aspect is about the formal meaning of the ontology (Chapter 7).• Representation. This aspect captures the relation between the structure and the semantics. Representational aspects are usually evaluated by comparing metrics calculated on the RDF graph with features of the possible models as specified by the ontology (Chapter 8).• Context. This aspect is about the features of the ontology when compared with other artifacts in its environment, which may be, e.g. a data source that the ontology describes, a different representation of the data within the ontology, or formalized requirements for the ontology in form of competency questions or additional semantic constraints (Chapter 9).Note that in this thesis we assume that logical consistency or coherence of the ontology is given, i.e. that any inconsistencies or incoherences have been previously resolved using other methods. There is a wide field of work discussing these logical properties, and also well-developed and active research in debugging inconsistency and incoherence, e.g. Ontologies are inconsistent if they do not allow any model to fulfill the axioms of the ontology. Incoherent ontologies have classes with a necessarily empty intension Part II Aspects Evaluating the vocabulary aspect of an ontology means to evaluate the names used in the ontology. In this chapter we discuss methods for evaluating the vocabulary of an ontology, and provide a comparison for some of the values based on a large corpus of Web ontologies.The vocabulary of an ontology is the set of all names used in it. Names can be either URIs or literals. The set of all URIs of an ontology is called the signature of the ontology (and is thus the subset of the vocabulary without the literals). URIs are discussed in Section 4.1. Literals are names that are mapped to a concrete data value, i.e. instead of using a URI to identify an external entity, literals can be directly interpreted. Literals are presented in Section 4.2. Finally, we will also discuss blank nodes, i.e. unnamed entities within ontologies (Section 4.3).URI references
Most names in ontologies are URI references (Uniform Resource Identifier, An URI reference should identify one specific resource, i.e. the same URI reference should not be used to identify several distinct resources. A URI reference may be used to identify a collection of resources, and this is not a contradiction to the previous sentence: in this case the identified resource is the collection of resources, and thus a resource of its own. Classes and properties in OWL ontologies are also resources, and thus are identified by a URI reference.A particular type of resources are information resources. Information resources are resources that consist of information, i.e. the digital representation of the resource captures the resource completely. This means that an information resource can be copied without loss, and it can be downloaded via the Internet. Therefore information resources can be located and retrieved with the use of a URL. An example of an information resource is the text of Shakespeare's "Romeo & Juliet" (from which this chapter's introductory quote is taken) which may be referenced, resolved, and downloaded via its URL http://www.gutenberg.org/dirs/etext97/1ws1610.txt Non-information resources can not be downloaded via the Internet. There may be metadata about non-information resources available, describing the resource. An example is the book "Romeo & Juliet": the book itself can not be downloaded via the Internet (in contrast to its content). There may be metadata stating e.g. the weight or the prize of the book. In order to state such metadata we need to be able to identify the book, e.g. using its ISBN number Non-information resources and information resources are disjoint classes (i.e. no information resource can at the same time be a non-information resource and vice versa). A further formalization of information resources can be found e.g. in the "Functional Requirements for Bibliographic Records" ontology FRBR Linked data
URI references are strings that start with a protocol. If the protocol is known and implemented by the ontology based application, then the application may resolve the URI, i.e. use the URI according to the protocol in order to find more information about the identified resource. In case the URI is an URL, the identified information resource can be accessed and downloaded by using the protocol.66
URI references consist of an URI with an optional fragment identifier. Most URI references in Web ontologies are indeed using a protocol that can be resolved by the machine in order to fetch further information about the given URI reference. Most commonly this is achieved by using the HyperText Transport Protocol HTTP Other prominent protocols besides HTTP are file (37,023 times; for local files), mailto Sometimes, QNames (see Section 5.2) can mistakenly be interpreted as an URI, especially in XML serialization Method 1 (Check used protocols)
All URIs in the ontology are checked to be well-formed URIs. The evaluator has to choose a set of allowed protocols for the evaluation task. The usage of any protocol other than HTTP should be explained. All URIs in the ontologies have to use one of the allowed protocols.Resolving an HTTP URI reference will return an HTTP response code and usually some content related to the URI reference. Certain HTTP responses imply facts about the used URI reference. These facts are given in Table Both OWL classes and OWL properties are not information resources. With Table 4.1 this allows us to infer that in OWL DL both class and property names (without fragment identifiers) should not return a 200 or a 301 when resolved via the HTTP protocol. In OWL 2 this is not true anymore, since punning allows URIs to be individuals, properties, and classes at the same time The table also lists the cases when the served resource should describe the name. We can easily check if this is the case, if the description is sufficiently useful, and if it is consistent with the knowledge we already have about the resource.We have tested all HTTP URIs from the Watson EA corpus (see Significance of the difference between hash and slash URIs. For significance testing we apply the two-proportion z-test. Let the null hypothesis be P s = P h , i.e. the probability for a slash URI to return a 200 OK being the same as the probability for a hash The test statistic is a z-score defined as z = ps-p h e ≈ -5.5316. From that it follows that the probability that the null hypothesis is true is p < 0.0001, which means the result is statistically highly significant.Names from the same slash namespace should always return the same response code. Differing response codes indicate some problems with the used terms. For example, the social website LiveJournalMethod 2 (Check response codes)
For all HTTP URIs, make a HEAD call (or GET call) on them. The response code should be 200 OK or 303 See Other. Names with the same slash namespace should return the same response codes, otherwise this indicates an error.Note that this method can only be applied after the ontology has been published and is thus available on the Web.In summary, the usage of resolvable URI references allows us to use the Web to look up more information on a given URI. This can help to discover available mappings, or to explore and add new information to a knowledge based system. This is the major advantage of using the Semantic Web instead of simply an ontology based application.Hash vs slash
There was a long running debate in the Semantic Web community on the usage of fragment identifiers in URI references. The basic question is on the difference between using http://example.org/ontology#joe and http://example.org/ontology/joe in order to refer to a non-information resource. The former type of URI is called a hash URI (since the local part is separated by the hash character # from the namespace), the latter type a slash URI (since the local part is separated by the slash character / from the namespace). The discussion was resolved by the W3C Technical Advisory Group When resolving a hash URI, only the namespace is resolved. All hash URIs with the same namespace thus resolve to the same resource. This has the advantage that the ontology can be downloaded in one pass, but it also has the disadvantage that the file can become very big. Therefore, terminological ontologies and ontologies with a closed, rarely changing, and rather small set of individuals (e.g. a list of all countries) would use hash URIs, whereas open domains with often changing individuals often use slash URIs (see for example in Semantic MediaWiki, Section 10.2.3).We analyzed the Watson corpus to see if there is a prevalence towards one or the other on the Web. We found 107,533,230 HTTP URIs that parse. 50,366,325 were hash URIs, 57,166,905 were slash URIs. Discounting repetitions, there were 5,815,504 different URIs in all, 2,247,706 of them hash URIs, 3,567,789 slash URIs.Regarding their distribution over namespaces, there are much bigger differences: we find that there are only 46,304 hash namespaces compared to 2,320,855 slash namespaces. The hash namespaces are, in average, much bigger than the slash namespaces. 2,197,267 slash namespaces (94.67%) contain only a single name, whereas only 16,990 hash namespaces (36.69%) contain only one name. On the other extreme, only 253 slash namespaces (0.01%) contain more than a 100 names, in contrast to 2,361 hash namespaces (5.10%) that have more than 100 names. Still, as we can see in Table Method 3 (Look up names)
For every name that has a hash namespace make a GET call against the namespace. For every name that has a slash namespace make a GET call against the name. The content type should be set correctly. Resolve redirects, if any. If the returned resource is an ontology, check if the ontology describes the name. If so, N is a linked data conformant name. If not, the name may be wrong.Opaqueness of URIs
URIs should be named in an intuitive way. Even though the URI standard On the other hand, an URI such as http://www.aifb.kit.edu/id/p67 does not have an intuitive denotation, and so they become hard to debug, write manually, and remember. Their advantage is that they do not depend on a specific natural language. An unfortunate number of tools still displays the URI when providing a user interface to the ontology. Therefore intuitive URIs, that actually identify the entities to which their names allude to (for the human reader) and that have readable URIs, should be strongly preferred. Also, since URIs unlike labels should not change often URIs should follow a naming convention. When using natural language based names, the local name of classes may use plural (Cities) or singular forms (City), the local name of properties may use verbs (marries), verbal phrases (married_to), nouns (spouse), or nominal phrases (spouse_of or has_spouse). All of these naming conventions have certain advantages and disadvantages: phrases make the direction of the property explicit and thus reduce possible sources of confusion (given the triple Aristotle Teacher Plato, is it immediately clear who the teacher is, and who the student?). But using simple nouns can help with form based interfaces such as Tabulator (Berners-Lee et al., 2006a), a Semantic Web browser. Tabulator also constructs a name for the inverse property by appending " of " to the word, e.g. the inverse of Teacher would be Teacher of.Capitalization and the writing of multi-word names should also be consistent. The ontology authors should decide which names to capitalize (often, classes and individuals are capitalized, whereas properties are not). Multi-word names (i.e. names that consist of several words, like Wide square) need to escape the whitespace between the words (since whitespaces are not allowed in URIs). Often this is done by camel casing (i.e. the space is removed and every word after a removed space starts with a capital letter, like WideSquare), by replacing the space with a special character (often an underscore like in Wide square, but also dashes or fullstops), or by simple concatenation (Widesquare).Many of these choices are just conventions. The used naming conventions should be noted down explicitly. Metadata about the ontology should state the naming convention for a given vocabulary. Many of the above conventions can then be tested automatically. It is more important to consistently apply the chosen convention than to choose the best convention (especially, since the latter is often unclear).In addition, URIs on the Semantic Web should follow also the same rules that URIs on the hypertext Web should follow. These are (Berners-Lee, 1998):• don't change (i.e. don't change what the resource refers to, nor change the URI of a resource without redirecting the old URI) A proper naming can be checked by comparing the local part of the URI with the label given to the entity or by using lexical resources like Wordnet URI reuse
In order to ease sharing, exchange, and aggregation of information on the Semantic Web, the reuse of commonly used URIs proves to be helpful (instead of introducing new names). At the time of writing many domains still do not provide an exhaustive lexicon of URIs yet, but projects such as FreebaseAnalyzing the Watson EA corpus, we find that 75% of the ontologies use 10 or more namespaces, in 95.2% of the ontologies the average number of names per namespaces is lower than 10, in 76.5% it is lower than 3, in 46.4% lower than 2. This means that most ontologies use many namespaces, but only few names from each namespace. Considering knowledge bases this makes perfect sense: in their FOAF files persons may add information about their location by referencing the Geonames ontology, and about their favourite musician referencing the MusicBrainz ontology. Terminological ontologies often reference the parts of external ontologies relevant to them in order to align and map to their names.Method 5 (Metrics of ontology reuse)
We define the following measures and metrics: • Ratio of name references to unique namesCheck the following constraints. The percentages show the proportion of ontologies that fulfill this constraint within the Watson EA corpus, thus showing the probability that ontologies not fulfilling the constraint are outliers.• R NU < 0.5 (79.6%)• R UNS < 5 (90.3%)• N NS ≥ 10 (75.0%)URI declarations and punning
Web ontologies do not require names to be declared. This leads to the problem that it is impossible for a reasoner to discern if e.g. ex:Adress is a new entity or merely a typo of ex:Address. This can be circumvented by requiring to declare names, so that tools can check if all used names are properly declared. This further brings the additional benefit of a more efficient parsing of ontologies The declarations are axioms, stating not only that a name exists but also its type, i.e. if it is declared as a class, an individual, a datatype, object or annotation property. This feature was introduced in OWL 2, and thus does not yet appear in ontologies outside of test cases.Method 6 (Check name declarations)
Check for every URI if there exists a declaration of the URI. If so, check if the declared type is consistent with the usage. This way it is possible to detect erroneously introduced punning.In OWL 1 DL the set of class names, individual names, and property names were disjoint. This restriction was removed in OWL 2, and now it is allowed to use the names for either the individual, the property or the class. Based on the position in the axiom it is always clear which type of entity the name refers to. There are good reasons to allow punning: for example, the entity lion, depending on the context, may represent the individual lion that is of the type species, or it may be the type of the lion Simba Literals
Ontologies often contain literals that represent data values. These data values can be very varied, e.g. numbers (such as 4.75 ), points in time (such as 2:14 pm CEST on 6th March 2009 ), or strings (e.g. Jaguar ). The importance of literals on the Semantic Web can be shown by their sheer number of occurrences: the Watson corpus consists of 59,749,786 triples, and 26,750,027 of them (42.8%) include a literal.Anything that can be represented by a literal could also be represented by an URI. For example, we could introduce the URI ex:Number4dot75 to be the URI to represent the number 4.75. Using OWL Full, we could state that the literal and the URI are the same individual. Often it is more convenient to use a literal instead of a URI, especially since their meaning is already agreed on. Literals have the disadvantage that in triples they are only allowed to be objects. This means that we cannot make statements about literals directly, e.g. say that 4 is an instance of the class ex:EvenNumber. This can be circumvented by using URIs as proxies for data values. It is currently discussed to drop this limitation and to allow literals to also be subjects in triples.Literals can be typed (see Section 4.2.1) or plain. A plain literal may be tagged with a language tag (Section 4.2.2). The standard does not allow literals to be both typed and language tagged. Language tagged literal would often make little sense: the integer 4 is the same number regardless of the language. Since it makes sense for text, the specification for the new data type rdf:text allows for language tagged typed text literals The RDF standard states that plain literals denote themselves Typed literals and datatypes
A typed literal is a pair of a lexical representation and a data type. The data type is given by an URI that defines the interpretation of the lexical representation. Most ontologies use data types defined by the XML Schema Definition Figure The  should be resolvable just as all other URIs and thus allow a tool to make an automatic check. When evaluating ontologies, the evaluator should decide on a set of allowed data types. This set depends on the use case and the tools being used. All tools should be able to deal with the data types in this set. This closed set will help to discover syntactic errors in the data type declarations of the literals in the ontology.It should be checked if all used data types in the ontology are understood by the tools dealing with these data types. That does not mean that all tools need to understand all the used data types. A tool may be perfectly capable of dealing with an unknown data type as far as the task of the tool itself is concerned. For example, a tool used for converting RDF/XML-serialization into OWL Abstract Syntax does not need to understand the data types. A reasoner who needs to check equality of different values on the other hand needs to understand the used data types.The second check that is relevant for typed literals is to check if the literals are syntactically correct for the given data type. A typed literal of the form "Four" and data type http://www.w3.org/2001/XMLSchema#integer is an error and needs to be corrected. For this it is important for the evaluation tool to be able to check the syntactic correctness of all used data types. This should be considered when choosing the set of allowed data types. Otherwise it will be hard to discover simple syntactic errors within literals.Method 7 (Check literals and data types)
A set of allowed data types should be created. All data types beyond those recommended by the OWL specifications should be avoided. Creating a custom data type should have a very strong reason. xsd:integer and xsd:string should be the preferred data types (since they have to be implemented by all OWL conformant tools).Check if the ontology uses only data types from the set of allowed data types. All typed literals must be syntactically valid with regards to their data type. The evaluation tool needs to be able to check the syntactical correctness of all allowed data types.Language tags
Language tags can be used on plain literals to state the natural language used by the literal. This enables tools to choose to display the most appropriate literals based on their user's language preferences. An example for a literal with a language tag is "university"@en or "Universität"@de. Language tags look rather simple, but are based on a surprisingly large set of specifications and standards.Language tags are specified in the IETF RFC 4646 Language tags can be further refined by a script, regional differences, and variants. All these refinements are optional. The script is specified using ISO codes for scripts It is also possible to define private language tags or tag refiners, denoted by an x. So one could use en-x-semweb as the language tag for the kind of English that is spoken by the Semantic Web research community, where certain terms have meanings deviating from standard English. Private tags should be avoided, and indeed, our analysis of the Watson corpus shows that none are used.Language tags are case insensitive. So it does not matter if one uses en-us, EN-US, en-US, En-uS, or any other combination of upper and lower case characters. On the Semantic Web it seems to be usual to use lower case characters only, with less than 200 occurrences of upper case codes.We examined the usage of language tags on the Semantic Web to find if the standards are applied correctly. For such a complex mesh of standards we found surprisingly few errors. All in all, 17,313,981 literals have a language tag (67.6% of all plain literals). English (en) was by far the most often used tag, applied 16,767,502 times (96.8%), followed by Japanese (ja) with 519,191 tags (3.0%). The other languages are less widely used by far, Suomi (fi) is used 9,759 times, German (de) 4,893 times, French (fr) 1,810 times, and Spanish (es) 866 times.The most often applied refined tags are en-us with 2,284 tags, en-gb 1,767 times, and fr-fr 288 times. The latter could be regarded as redundant, since fr would be sufficient. Further inappropriate usages or errors that can be found in the language tags are en-uk (it should be en-gb, used 90 times), frp (undefined tag, used 30 times), and jp (should be ja). In summary, with respect to the complexity of the used standards, the number of errors regarding language tags is extremely low. This probably stems from the fact that most of the advanced features are never used: no single tag specifies a script (besides one example literal) or a variant, and only a handful of tags specify a region, never using the UN M-49 standard (besides one example literal) Method 8 (Check language tags) Check that all language tags are valid with regards to their specification. Check if the shortest possible language tag is used (i.e. remove redundant information like restating default scripts or default regions). Check if the stated language and script is actually the one used in the literal.Check if the literals are tagged consistently within the ontology. This can be checked by counting n l , the number of occurrences of language tag l that occurs in the ontology. Roughly, n l for all l should be the same. Outliers should be inspected.Labels and comments
Labels are used in order to provide a human readable name for an ontological entity. Every ontological entity should have labels in all relevant languages. Almost none of the ontologies in the Watson corpus have a full set of labels in more than one language, i.e. most ontologies are not multi-lingual. Thus they miss a potential benefit of the Semantic Web, i.e. the language independence of ontologies. Comments add further human-readable explanations to a specific entity, and should also be language tagged. Labels and comments should follow a style guide and be used consistently. A style guide should define if classes are labeled with plural or singular noun, if properties are labeled with nouns or verbs, and under what circumstances comments should be used. Labels and comments should never use camel case or similar escape mechanisms for multi word terms, but instead simply use space characters (or whatever is most suitable for the given language). I.e. an URI http://example.org/LargeCity should have a label "large city"@en. External dictionaries such as WordNet In an environment where ontologies are assembled on the fly from other ontologies Even when subproperties of rdfs:label are defined, there should always be one label (per supported language) given explicitly by using rdfs:label itself. Even 80 4.3 Blank nodes though this is semantically redundant, many tools (especially visualization tools) do not apply reasoning for fetching the labels of an entity but simply look for the explicit triple stating the entity's label.Method 9 (Check labels and comments) Define the set of relevant languages for an ontology. Check if all label and comment literals are language tagged. Check if all entities have a label in all languages defined as being relevant. Check if all entities that need a comment have one in all relevant languages. Check if the labels and comments follow the style guide defined for the ontology.Blank nodes
Blank nodes are an RDF feature that allows to use a node in the RDF graph without giving it a URI. This way the node can only be indirectly referenced (if at all), for example by using an inverse functional property. Blank nodes relieve the author of an RDF graph to come up with good URIs for every node, which would impose additional costs on creating the graph.There are two different scenarios for using blank nodes. First, blank nodes are used in the structural representation of certain OWL axioms within RDF graphs. Second, blank nodes are used for anonymous ontology entities. An example for the first scenario is the representation of a disjoint union axiom in RDF. The axiom DisjointUnion(C D E) will be represented by the following RDF triples:C owl:disjointUnionOf _:x . _:x rdf:first D . _:x rdf:rest _:y . _:y rdf:first E . _:y rdf:rest rdf:nil .Blank nodes in RDF are represented by using the namespace prefix (underscore). In the given example, there are two blank nodes, :x and :y. They do not represent any entities in the domain, but are introduced only out of structural necessity since we cannot state the relationship between three entities (C, D, and E) from the original axiom directly with triples. We defined these kind of blank nodes to be structural blank nodes. Even though they could be given URIs, these URIs would not represent any concept in our conceptualization and thus should be avoided.The second scenario uses blank nodes to represent anonymous ontology entities. For example, in the first few years it was regarded as good practice not to define a URI for persons in FOAF documents but to use blank nodes instead. The argument in favor of using blank nodes for persons was that it was regarded inappropriate to name people via URIs. This echoes the sentiment of "I am not a number", or rather, "I am not a URI". FOAF preferred to identify persons by using inverse functional properties such as eMail-adresses or their hash sums.Web architecture later suggested that all entities of interest should have a URI In summary, blank nodes should be avoided unless structurally necessary.Method 10 (Check for superfluous blank nodes) Tables Each surface syntax warrants their own evaluation methods. Common features that can be evaluated over most of the syntaxes in a fairly uniform way are the proper and consistent indentation in the file and the order of the triples (for graph-based syntaxes) or axioms (for ontology-based syntaxes). Triples forming complex axioms should be grouped together. Groups of axioms forming an ontology pattern should also be grouped together. Terminological axioms should precede facts, and terminological axioms and facts about the same entity should be grouped together. Entities should be declared before usage (see There should be a good reason for using anything but UTF-8 as the file encoding Even though ontologies will rarely be edited in simple text editors, these guidelines will help tremendously once it does happen. Understanding a disjoint union axiom when the triples making up the axiom are scattered throughout the graph serialization creates an unnecessary challenge. We assume that the capability to debug the source file of an ontology increases dramatically when the guidelines given above are implemented. These guidelines are derived from the rules applied for software engineering big programs, where similar rules exist for the layout of the code in the source files In this thesis we refrain from instantiating the given guidelines for any specific surface syntax. The rest of this chapter will first discuss two further specifics of the syntax which apply for most surface syntaxes (syntactic comments in Section 5.1 and qualified names in Section 5.2) and close with introducing a possibility of using XML validation for the completeness test of OWL knowledge bases (Section 5.3).Syntactic comments
Many OWL surface syntaxes allow for comments. For example, an XML-based surface syntax may contain an XML-style comment such as <!--Created with Protege -->. We call these syntactic comments and discern them from RDF-style comments, i.e. comments that are part of the RDF graph (see For many syntaxes it is best practice to include some comments with metadata about the ontology document. XML documents, for example, often start with an introductory comment stating the version, author, and copyright of the given document.In OWL document files this is not necessary, since all these informations can be expressed as part of the ontology itself. For example, the above quoted XML-style comment <!--Created with Protege --> is injected by the Protégé editor ex:Ontology_A ex:authoringtool sw:Protege .Most of the metadata in syntactic comments can be expressed using statements about the ontology. This allows ontology repositories to automatically and uniformly interpret the metadata about the ontologies in the repository, and to provide access and searches over the ontology using the very same sophisticated tools that are used to access the data within the ontology themselves. Qualified names
Most serialization formats include mechanisms to abbreviate URI references. They are often based on known XML-based approaches, such as XML entities When defining abbreviations and prefixes in an ontology document, care should be taken to bind well known abbreviations with the appropriate URIs. For example, the foaf prefix Swoogle Note that using namespaces inconsistently with common usage will not lead to errors. When merging and combining ontologies, the underlying OWL libraries will handle the namespaces correctly, and do not care if we define http://xmlns.com/foaf/0.1/ to be foaf or dc. It merely helps to avoid confusion for the human user not to break expectations regarding the definitions of namespaces, and to follow common practice on the Web.XML validation
The Semantic Web was created as a set of standards building on top of each other, and incorporating already existing and well established standards such as URIs for identifiers A characteristic of the standard RDF/XML syntax In this section an approach resolving this problem is presented. It uses wellestablished standards to sufficiently constrain the serialization of RDF/XML in order to be usable by classic XML processing approaches. We suggest to use the same approach humans would use in order to serialize their conceptualizations in XML, namely following an XML schema. Three popular XML schema languages are currently widely used, the XML-native Document Type Definitions (DTD) Method 11 (Validating against an XML schema)
An ontology can be validated using a standard XML validator under specific circumstances. In order to apply this, the ontology needs to be serialized using a pre-defined XML schema. The semantic difference between the serialized ontology and the original ontology will help in discovering incompleteness of the data (by finding individuals that were in the original ontology but not in the serialized one). The peculiar advantage of this approach is that it can be used with well-known tools and expertise.Depending on the schema, this method may severely impact the extensibility of the ontology document. The advantage of using an XML validating ontology document is that we can check if the document is data complete with regards to a previously made specification. This way tools can check before hand if the data is not only consistent with a given ontology, but also if there is sufficient data in order to perform a specific task.86
The major theoretical challenges lie in the details of the interpretation of the XML schema file as a specification for generating the requested serialization. XML schemas can be roughly regarded as formal grammars meant to check a given input XML document for validity. Instead, we use the very same XML schema file in order to query a given RDF graph and then define how to serialize the results. This interpretation forces us to define a number of constraints on the covered XML schemas (i.e., arbitrary DTDs can not be used as described in We provide a prototype implementation of the approach, and present an example instantiation of the method in order to demonstrate its usefulness and applicability.For our example we take FOAF files from the Web, pipe them through our implementation, and then use standard XML processing techniques (XSLT) in order to generate HTML pages. We provide the example workflow as a Web service.Section 5.3.3 describes the approach to create the serializations. Section 5.3.4 proposes how the dialog-driven DTD creation tool works. This is followed by a description of the current prototype implementation and demonstration in Section 5.3.5. Section 5.3.6 describes related approaches towards the goal we are outlining, before we close with a discussion of further research questions. As a running example we describe a Web service that takes arbitrary FOAF-files Example
An example of an XML-based serialization of RDF is given by RSS 0.9 RSS files are both valid RDF files and valid DTD-described XML files, where the DTD describes the document structure. Listing 5.1The DTD has the root element rdf:RDF (defined in the actual RSS document, see Listing 5.2). The DTD gives the grammar for the XML elements: the root rdf:RDF contains the elements channel, image, item, textinput in arbitrary order and number, but nothing else (note that the * around the brackets in line 1 makes the ? and + Listing 5.1: Document type definition for RSS 0.9 documents. inside the brackets meaningless). All these may contain a further number of elements, which are all flat #PCDATA fields.An RSS document that is valid with regards to the given DTD could look like the example in Listing 5.2. A standard RDF tool will hardly serialize its RDF graph like this. The DTD defines the order of the elements, and it defines the names of the XML elements to be used. For an RDF library the order does not matter, and there are a number of different ways to serialize triples in XML. In order to serialize an RDF graph so that we can use it for validation against a schema, the serializer must be aware of the schema. Now we can check the RSS document against the DTD given above, and the XML validator will provide us with a list of all validation errors. If the file validates, we do not only know that it is a valid XML document and a valid RDF file, but that the entities described in the graph have certain properties filled (i.e. all items will have a title property).Unlike the cardinality axiom SubClassOf(Item ExactCardinality(1 title)) that allows us to infer that every item has to have a title property (be it given or not), validating against the above DTD will guarantee us that we know the actual value of the title property for every item. This allows us to state data completeness. This means that for certain properties we do not only know that they exist, but we also know that we know their values (this is similar to the usage of autoepistemic logic with OWL as described in Section 9.2.3).Motivation
Today, one of the major hindrances towards achieving the wide deployment of Semantic Web technologies is the lack of sufficient expertise. If at all, Semantic Web technologies have only recently been introduced to students' curricula, and most general audience books on Semantic Web technology have only been published in the last two years Also, even though the number of Semantic Web tools is growing rapidly, many of them require a deeper understanding of the technology than is readily available.Strengthening the ties between RDF and XML allows not only to reuse existing expertise, but also to re-enable the already existing tools.To illustrate the problems with using RDF/XML, consider the following ontology, serialized in N3 (Berners-Lee, 2006): All documents have very different XML infosets and very different XML serializations, but equal RDF semantics. Thus, creating a simple list of all persons according to the RDF semantics is not trivial without using an RDF parser. An XQuery or an XSLT transformation will be cumbersome to write. In order to simplify the authoring of XML-based solutions, we provide a workflow that will normalize the RDF/XML serialization following a given XML schema.The example in Listing 5.7 shows an XML DTD that normalizes the way to express FOAF data in XML files. It states that the root element has an arbitrary number of foaf:Persons (line 1) and each foaf:Person must have a foaf:name and may have some mail address (given by foaf:mbox, line 6). Lines 2-5 define the namespaces Normalizing the serialization
In order to create the normalized serialization we use the provided DTD to generate a series of SPARQL queries. The results of the queries are then used to write the actual serialization. In this section we describe how this is accomplished.The given DTD has to fulfill a number of constraints that will be listed explicitly in the following section. First we need to read and define all given namespaces from the root element and declare them appropriately in the SPARQL query. Furthermore we add the RDF and RDFS namespaces, if not already given (for the namespaces used here, see Table Next we go through every possible child element type of the root element rdf:RDF. In our example we can see in line 1 that there is only one possible child element type, foaf:Person. This is the type of the sought for instances, i.e. we translate it into the following query fragment:SELECT ?individual WHERE {
?individual rdf:type foaf:Person . } Next, for every required child element of the found elements, we add a respective line to the WHERE-clause. In our example, foaf:Person requires only foaf:name (line 6, foaf:mbox is optional). So we add the following line (introducing a new variable every time):?individual foaf:name ?v1 .If the foaf:name itself would have pointed to another element, this element would be the type of ?v1, and in return we would add all required properties for ?v1 by adding the subelements of this type, and so on.The result of this query is a list of all individuals that are described with all the necessary properties as defined by the DTD. In the example they are, thus, not only 92 instances of foaf:Person but also of Kfoaf : name.⊤, i.e. we know that they have at least one foaf:name given (see Next we iterate over the result list, asking for every required and optional property individually. In the example, we would issue the following two queries: SELECT ?result WHERE { aifb:Rudi_Studer foaf:name ?result . } LIMIT 1 SELECT ?result WHERE { aifb:Rudi_Studer foaf:mbox ?result . } Since line 6 of the DTD states that there is only one foaf:name, we limit the first query to 1. Again, if the subelement of a property would have been stated in the DTD, it would have been required to add all required properties for ?result, just as it was done for ?v1 above. Furthermore, each required and optional property of ?result also has to be gathered in the same way as we do for all the instances of the root element's children.With the results of the queries, we can start generating the serialization. Listing 5.8 shows such a serialization that was created with the normalization tool.A further note on the approach: since the resulting queries are all describing conjunctive queries, we can use the queries on a reasoning-aware SPARQL endpoint. This allows us to employ the power of RDFS and OWL to provide for a fully transparent ontology mapping. For example, even if the individuals in the input file are actually having the class swrc:Student, as long as this is a subclass of foaf:Person the results will come back as expected and the normalized serialization will contain all instances of direct and indirect subclasses. This provides a powerful and easy way to quickly add further knowledge to existing XML-based tool chains.Creation of compliant schemas
The provided DTDs have to fulfill a number of constraints so that they can be used by the normalization tool. This section lists those constraints explicitly. Since it would require quite some expertise with regards to RDF/XML-serializations (which we explicitly do not expect) to create DTDs fulfilling these constraints, we also propose a dialog-based tool to easily create such DTDs.An RDF-compliant DTD, that can be used by the normalizer described in the previous section, has to fulfill the following constraints:• the resulting XML-file must be a valid RDF/XML-file• all used namespaces have to be fixed in the DTD and defined in the root element of the resulting XML-file• the root element of the XML-file has to be rdf:RDF• since DTDs are not context-sensitive, the DTD can use each element only a single timeEspecially the last constraint is a severe restriction necessary due to the shortcomings of DTDs. In Section 5.3.7 we will take a look at possible remedies for this problem using other, more modern XML schema languages.The first constraint is basically impossible to fulfill without deep knowledge of the RDF/XML serializations. Because of that we suggest a tool that analyses a given dataset or ontology and then guides the developer through understandable dialog options to create a conformant DTD. The tool follows the following approach:1. the tool loads a number of RDF files. It does not matter if these files contain terminological ontologies, knowledge bases, or populated ontologies.2. the tool offers the developer to select a class from the given RDF files. rdfs:Class and owl:Class are both considered. The developer has to decide if the result should contain exactly one instance, or an arbitrary number of instances.3. for the selected class, all sensible properties are offered. Sensible properties are those that either are defined with a domain being the given class, or where the instances of the given class have assertions using the property. For each selected property the developer has to decide if this property is required, can be repeated arbitrary times, or both.4. for each selected property the developer has to decide on the type of the filler, especially if it is a datatype value or an individual, and if the latter, if it is of a specific class (which again will be selected from a provided list, based both on the range of the property and the classes of the actual fillers in the knowledge base).5. if a class was selected, enter recursively to Step 3.6. as soon as a property is selected and fully described, the developer can select another property by repeating from Step 3 on.7. as soon as a class is fully described, the developer can continue with another class by repeating from Step 2.94
The tool has to be careful not to allow the selection of any element twice. This constraint is stronger than required, and we expect future work and more thorough analysis to relax it and thus extend the expressivity of the tool. This is especially true when moving to more powerful schema languages that are aware of the context an element is used in. We are also aware that the list of sensible properties may be incomplete. We discuss this in Section 5.3.7.Implementation
In order to demonstrate the feasibility of our approach, we have developed a prototype Web service implementation. The input is an arbitrary RDF file. It should describe a person using the FOAF vocabulary. The Web service uses an extension of the DTD given in Listing 5.7. The full DTD can be accessed online from our demonstration site. Now developers can post-process the output of the serializer with the same ease they would have with any XML files, and they do not need to have any further knowledge of Semantic Web technologies to do so. There are numerous technologies for the further processing of XML files. In our example implementation we use the XML transformation language XSLT Listing 5.9: XSLT for transforming FOAF to vCard The demonstration site uses XalanRelated approaches
In this section we discuss alternative approaches towards bridging the gap between the Semantic and the Syntactic Web.1. The main related approach is to combine or extend XSLT with capabilities to seamlessly deal with RDF, and still continue to provide the same output formatting power. There are a number of implementations towards this goal, such as RDF Twig, 7 TreeHugger, 8 or RDFXSLT. 9 XSPARQL 10 provides similar capabilities, but by extending SPARQL. These approaches have all proven feasible, but do not resolve the original issue: they all require the developer to understand RDF.2. A very early approach is DTDMmaker It also reports about the same issues identified here (about the shortcomings of DTDs) but is not flexibly customizable and also does not create RDF-conformant DTDs.3. Another approach is to write custom hard-coded translators, that first read the RDF graph and then create the serialization. This may be the easiest way, but it requires hard-coding a proprietary solution for each use case, and it requires the programmer to learn the API of at least one RDF parser. Our approach does not require any newly written, proprietary code.4. One approach is to use SPARQL first, and then use the SPARQL XML result format as the basis for the XSLT translations. This approach is viable, but it requires the developer both to understand SPARQL and the SPARQL result format, and then create XSLT scripts to deal with the representation of their data in the SPARQL result format instead of an XML vocabulary that is close to the domain.5. The serializers of RDF generating tools could be rewritten so that they always return the required serialization. But this would mean that only output of these tools can be used, and we lack interoperability with other tools using the same ontology, since the XML serialization would usually not be specified. If the serialization indeed is specified, then this would have been done using an XML schema, and therefore it just hard codes our proposed approach into the RDF source. This is the approach that RSS has chosen, as shown in Section 5.3.1.6. Another approach is to change the tool using the data so that it becomes RDF aware, i.e. instead of translating the RDF graph to the required format we can enable the tool to use RDF. Even though we personally would prefer this approach, in our example use case this would require to extend all existing tools that can consume vCard to also accept RDF descriptions of persons. This renders this solution unlikely.We conclude that the major difference of our approach is in the zero-knowledge assumption in using it: no knowledge of Semantic Web technologies is required. Expertise in wide-spread XML technologies is sufficient to start using Semantic Web knowledge bases as data sources.Open questions for XML schema-based RDF validation
We described and implemented an approach towards bridging the gap between classic XML-technologies and the novel Semantic Web technology stack. The current implementation is already usable, but it exhibits a number of limitations. We list these limitations here in order to explicitly name open challenges.• The given approach can be redone using other, more powerful and modern XML schema languages. These languages add further features, e.g. cardinalities.• DTDs do not allow for context sensitive grammars. Therefore elements can only appear once in every DTD, which severely constraints their expressive power. For example, it is not possible to ensure that every person in a FOAF-file requires a name and mailbox and may have friends (which are persons), and at the same time define that friends should not have a mailbox. Using a context-sensitive XML schema language can remedy this.• Even without moving to more powerful schema languages, the given constraints in Section 5.3.4 can be relaxed. Further analysis is required to understand this bigger language fragment.• For now the prototype implementation ignores that a property that appears several times should have several different values, i.e. is basically a cardinality declaration. This could be expanded.• A number of features have not been explored for this first implementation, that future work will take into account, e.g. datatypes, language tags, the collection and xmlliteral parsetypes, and blank nodes.• As we have seen, DTDs have to use fixed namespaces and prefixes, whereas XML namespace-aware schema languages could deal with them more elegantly.For now we provide the current implementation and a Web-accessible demonstration workflow to show the advantages of the described approach. We expect that the approach will be applied in a number of early use cases in order to gain more insight in the actual usage and to direct the future development of the given project.98
Chapter 6Structure
Caught up in circles confusion is nothing new (Cindy The most widely explored measures used on ontologies are structural measures. Graph measures can be applied on the complete or partial RDF graph describing the ontology. An example of an extensively investigated subgraph would be the one consisting only of edges with the name rdfs:subClassOf and the nodes connected by these edges (i.e. the explicit class hierarchy). This subgraph can be checked to see if the explicit class hierarchy is a tree, a set of trees, or if it has circularities, etc. If it is indeed a tree, the depth and breadth of the tree can be measured. Current literature defines more than forty different metrics • they can be calculated effectively from the ontology graph. Graph metrics libraries are available and can be used for this task.• they simply yield numbers. This makes tracking the evolution of the ontology easy, because even in case the meaning of the number itself is not well understood, its change often is.• their results can be checked automatically against constraints. For example, constraining the maximal number of outgoing edges of the type rdfs:subClassOf from a single node to five can be checked on each commit of the ontology to a version control system. Upon violation, an appropriate message can be created.• they can be simply visualized and reported.Due to these advantages and their simple implementation, most ontology toolkits provide ready access to a number of these metrics. Also ontology repositories often provide annotations and filtering options of the ontologies in the repositories using these metrics.But in practice, structural metrics are often not well-defined. That is, based on their definitions in literature, it is hard to implement them unambiguously. Also there is often confusion with regards to their meaning. Often they define a new graph structure (and do not use the existing RDF translation), but then fail to define a proper translation of the ontology features to the graph (e.g. how to translate that a property is transitive, how to translate domain and ranges, etc.). Section 6.1 examines four metrics taken from literature and provides exemplary analysis of the shortcomings of their definition, and also offers remedies for these definitional problems.Besides measures counting structural features of the ontology, the structure can also be investigated with regards to certain patterns. The best known example is to regard cycles within the taxonomic structure of the ontology as an error Structural metrics in practice
In this thesis we focus on the foundational aspects that form the base for automatically acquirable measures. Therefore we will not define a long list of metrics and measures, but rather take a step back and discuss conditions that measures have to adhere to in order to be regarded as semantically aware ontology metrics. This also helps to understand clearly what it means for a metric to remain on a a structural level.Thus the scope of this work compares best to other metric frameworks, such as the QOOD (quality oriented ontology description) framework Simple structural measures can uncover a number of interesting features such as the reasoning complexity of the given ontology. Complexity is actually a measure of an ontology language, and defines the complexity for reasoning over instances of that language. The complexity of the OWL languages are known, but that does not give us much information on particular ontologies. The expressivity of the used language merely defines an upper bound on the complexity that applies to the reasoning tasks. With a simple list of the constructs used within the ontology one can further refine the used language fragment, and thus arrive at a possibly lower complexity bound and thus a better estimation of the reasoning task. For example, OWL DL is known to correspond to the description logic SHOIN (D) Most OWL ontologies do not use the more expressive constructs Method 12 (Ontology complexity)
We define measures counting the appearance of each ontology language feature. We do this by first defining a filter function O T : O → O with T being an axiom or an expression type (as given in Chapter 2). O T returns all the axioms of axiom type T or all axioms having an expression of type T .We can further define a counting metricThen we can further define a few shortcuts, derived from the respective letters defining DL languages the number of subsumption axioms in the ontology With these numbers we can use a look-up tool such as the description logics complexity navigator We further define H(O) : O → O as the function that returns only simple subsumptions in O, i.e. only those SubClassOf axioms that connect two simple class names.The rest of this section discusses exemplary measures from the literature, shows how they are ambiguous, and offers actions to remedy these ambiguities (in favor of a purely structural application). In Chapter 8 these measures will be revisited in order to redefine them with the help of normalization, introduced in Chapter 7. Note that the following is criticizing only the actual description of the single measures, not the framework they are described in. The remedies are often simple, and after applying them the now remedied measures can be used with the original intention.Maximum depth of the taxonomy
OntoMetric (Lozano-Tello and Gómez-Pérez, 2004) is a methodology for selecting an ontology, based on five dimensions which in turn are organized into selection factors. The content dimension is the one most related to the work presented in this thesis, organized in the four factors concepts, relations, taxonomy, and axioms. We will take a closer look at one of the metrics related to the factor taxonomy, the maximum depth.The maximum depth of the concept hierarchy is "defined as the largest existing path following the inheritance relationships leading through the taxonomy". In Section 7.1 we will introduce the technique of normalization, which will allow us to use the maximum depth metric as it is intuitively meant to be used (see In order to define a purely structural maximum depth, it would be required to first rename the metric so that the meaning of the resulting number is clear. The appropriate name obviously depends on the actual definition of the metric, and one possible definition could be: the biggest number of subsumption axioms connecting consecutively simple class names without the same class name ever being repeated. An appropriate name for such a metric could be maximum subsumption path length.Compared to the original definition, this definition can deal with cycles in the subsumption graph. Some issues still remain. Consider the following ontology, given in Figure The longest path is four (either ABCD or ACBD), even though we would expect the maximal depth of the class hierarchy to be 3 (since B and C are on the same hierarchy level). But this is an inherent flaw of structural metrics. Only by renaming the metric from a name that implies that it measures a semantic feature (maximum depth of the taxonomy) to a name that states explicitly that it measures a structural feature (maximum subsumption path length) we can break the original intuition and thus achieve a metric that does not deceive due to its name.Class / relation ratio
In Measure (M29) in But applying the definition yields the ratio between the number of nodes representing classes and the number of nodes representing relations within the ontology graph, which will be a different number since a number of nodes, and thus names, can all denote the same class or relation. Therefore the metric is improperly named, since it does not yield the ratio between classes and relations, but rather between class names and relation names.In Section 8.3 we will return to this metric and redefine it properly to capture the intuition behind the name. For now, in order to achieve a structural metric, it is again required to rename the metric, e.g. to "class name / property name ratio". It is unclear if this metric is useful, but since it is far easier obtained than the actual "class / relation ratio" metric and will often correlate to it (compare Chapter 8), we expect it to remain displayed by ontology tools.Relationship richness
OntoQA is an analysis method that includes a number of metrics As an example we choose the metric (RR) "relationship richness". (RR) is defined as RR = |P | |SC|+|P | , "as the ratio of the number of relationships (P) defined in the schema, divided by the sum of the number of subclasses (SC) (which is the same as the number of inheritance relationships) plus the number of relationships" According to the authors, this metric "reflects the diversity of relations and place- Semantic similarity measure
Another example of a structural metric is given by In Figure In fact, one could always connect any class C with an explicit subsumption to the top class without changing the semantics, and thus have any two classes be connected in two links via the top class. Note that when introducing ssm, SPARQL for finding patterns
Patterns are crucial elements in all mature fields of engineering One of the major tasks when working with patterns is to recognize them. In order to offer the best support, ontology engineering tools need to recognize patterns and offer appropriate user support for working with them. Otherwise patterns can easily become compromised by tools that are unaware of them. We introduce an approach for detecting patterns in ontologies using readily available SPARQL query engines We investigate several issues and problems that arise when using SPARQL to detect patterns in OWL ontologies. We discuss three approaches towards resolving these problems and report empirical results when applying one of them. In order to keep the examples simple we will concentrate on one of the best known ontology engineering patterns, the partition pattern. We expect many of the problems and solutions investigated here to also apply to more complex patterns.The following section describes how DL patterns can be translated to RDF graphs in numerous different way. In Section 6.2.2 we discuss several approaches to remedy these problems, and empirically test the most naïve approach with surprisingly good results. We then apply the results on the notion of anti-patterns in Section 6.2.5.Translating patterns from OWL to RDF
The partition pattern, as defined by The partition pattern is displayed in Figure Since SPARQL is a query language for RDF we need to consider how this pattern is represented in RDF. This means that the pattern needs to be translated from OWL to RDF (as shown in Table 2).
Unfortunately, this does not necessarily lead to a unique representation in RDF. In order to make the relation to SPARQL more obvious, we use the Notation3 syntax for RDF (Berners-Lee, 2006) instead of the more verbose standard RDF/XMLserialization Querying with SPARQL
The RDF graphs can be translated straight-forward to the following SPARQL query. select distinct ?A ?B1 ?B2 where { ?B1 rdfs:subClassOf ?A . ?B2 rdfs:subClassOf ?A . ?B1 owl:disjointWith ?B2 . } With an OWL DL aware SPARQL implementation this query should return the same result on all the variants above. But none of the implementations we are aware of gives this result (this result is unsurprising, since SPARQL is not a query language that fully addresses querying OWL DL ontologies, and the relation between the two is underspecified in the SPARQL standard Implementations that are not aware of OWL DL can not be used to discover all instances of the pattern with the given SPARQL queries. In this case all possible patterns would be needed to be queried to gain a complete result, which is not practical.In order to be able to search for knowledge engineering patterns using SPARQL, we thus can follow three possible approaches:1. Implementing and using an OWL DL aware SPARQL engine 2. Materializing or normalizing the relevant parts of the ontology and then use an OWL DL unaware SPARQL engineCommitting to an incomplete search by searching only specific variants of the pattern's implementations
We consider the first option to be out of scope for this thesis, and thus we will discuss the other two options in the following two sections.Ontology normalization
In Section 7.1 we define ontology normalization as an approach to make features of the semantics of an ontology explicit within the ontology's structure. By using an OWL DL unaware SPARQL engine, SPARQL is useful only for querying the structure of the ontology, i.e. its explicit RDF graph.In order to use the query successfully with a SPARQL engine that is not aware of the OWL DL semantics, we first have to use an OWL DL reasoner to materialize all relevant parts of the ontology. In the case of the partition, we would need to materialize 110 the subClassOf-and disjointWith-constructs. But only direct subClassOf-relations should be materialized. Stating disjointness between classes that are not siblings is not considered a partition. If we materialize the complete subClassOf-graph, we could not recognize direct siblings anymore. Therefore we need to use the first three normalization steps introduced in Section 7.1 in order to avoid this problem.Furthermore, the disjointWith-graph should not be fully materialized either, since the same problem would emerge. Subclasses of a class A that was declared disjoint to a class B would all be disjoint to B as well. Such inherited disjointness should not be materialized.Note that the normalization steps that would be required for recognizing the partition pattern are valid only for the partition pattern. If we wanted to recognize other patterns, we would have to consider anew which normalization steps would be required, and which could be left out.Incomplete searches
In this section we explore empirically what happens if we drop the requirement of guaranteeing the discovery of a knowledge pattern. We took the SPARQL partition query from Section 6.2.2 and ran it against our corpus (see Of the 1331 ontologies of the Watson OWL corpus (see Using the SPARQL query to detect partition patterns, we discovered the partition pattern in 47 of these 55 ontologies (85.5%). This means that most of the time when the disjointness axiom is used, the partition pattern is also applied. This fact was recognized by the DAML+OIL ontology language We have taken a look at the 8 ontologies that did not show the partition pattern but still used disjointness (B90, B98, C37, E71, G13, H71, L22, and L66). E71, G13, and L22 turned out to be versions of the time ontology that implement an antipattern as discussed in Section 6.2.5. We will argue there that these ontologies should instantiate the partition pattern, but fail to do so. B90, B98, and L66 are obviously test ontologies and not meant to be used. C37 uses disjointness to partition the whole domain. Finally, H71 contains a partition that is unrecognized by the SPARQL pattern, because the partitioned class is not subclassed directly but rather through an IntersectionOf-construct.In order to see if any of the other two variants (or even similar implementations) given in Section 6.2.1 were used, we issued further queries. In order to detect variant 1 and similar patterns, we used the essential part of the variant 1 pattern: select ?a where { ?a rdfs:subClassOf owl:Nothing . } This query did not return any results. Whereas in DL subclassing ⊥, i.e. the empty class, is often used to state constraints, in OWL this possibility goes practically unused. Since OWL includes numerous powerful constructs to state such constraints directly, the rather counterintuitive usage of the empty class is usually avoided. Based on this result we can conclude that variant 1 and any similar patterns were not used to create partitions.Based on these empirical results we may claim that any appearance of owl:Nothing in an OWL ontology is an anti-pattern (see Section 6.2.5) and thus indicates problems with the formalization. Furthermore, also any appearance of owl:disjointWith outside of a partition is a strong indicator for either a problem in the ontology, or an incomplete formalization. Further anti-patterns should be gathered from the Web.In order to detect instantiations of variant 3, we search for any subclasses that are used to define a complement: select ?a ?b where { ?c owl:complementOf ?b . ?b rdfs:subClassOf ?a . } This query returned 12 ontologies (A23, B14, C35, C53, F96, I27, J57, K13, L03, L77, M31, and N11). We examined them manually and found that in all but two cases the complement was used to state a restriction on the values of a property (e.g. Vegetarian ≡ ∀eats.¬Meat). In the remaining two cases, the complement appeared once in a complex class descriptionFuture work needs to investigate further patterns in order to see if these positive results will be confirmed for other, more complicated patterns. We assume though that current ontology engineering practice is lagging well behind the state of the art in ontology pattern research. Therefore possibilities for large-scale experiments will be rather limited.Querying for anti-patterns
To detect so called anti-patterns is at least as important as detecting patterns in ontologies. Anti-patterns are strong indicators for problems in an ontology. The notion of anti-patterns was introduced to software engineering by The pattern was detected in five ontologies (E71, F54, G13, L22, and N25). Upon manual inspection they all turned out to be different versions of the time ontology In the time ontology, a TemporalEntity has a start and an end time. There are two kind of TemporalEntity: Instant and Interval. An Instant is defined as a TemporalEntity that starts and ends at the same time, whereas a ProperInterval is defined as an Interval that does have a start time that is different from the end time. Interval was introduced to capture also degenerate intervals that start and end at the same time. But such degenerate intervals are equivalent to Instant, and thus Interval is equivalent to TemporalEntity. The upper level of the time ontology could thus be simplified by removing the Interval class and creating a partition of TemporalEntity into ProperInterval and Instant (actually, a complete partition).Cleaning up the ontology in such a way increases the understandability, and avoids confusion (users would rightly assume a difference between interval and temporal entitiy, why else would there be two distinct classes?). Changing to a complete partition as discussed above, and removing a class makes the ontology smaller, and thus easier to understand, and brings the invoked conceptual model when studying the ontology closer to its formal semantics.Method 13 (Searching for Anti-Patterns) SPARQL queries over the ontology graph can be used to discover potentially problematic patterns. For example results to the following queries have been found to be almost always problems.Detecting the anti-pattern of subsuming nothing: select ?a where { ?a rdfs:subClassOf owl:Nothing . } Detecting the anti-pattern of skewed partitions: 114 select distinct ?A ?B1 ?B2 ?C1 where { ?B1 rdfs:subClassOf ?A . ?B2 rdfs:subClassOf ?A . ?C1 rdfs:subClassOf ?B1 . ?C1 owl:disjointWith ?B2 . } A bigger library of such anti-patterns would help to flag areas in ontologies that warrant further investigations. Since such a library can be checked automatically, we can include it easily in an ontology build system.The AEON approach
OntoClean From a practical perspective OntoClean provides means to derive measurable mismatches of a taxonomy with respect to an ideal structure which takes into account the semantics of subsumption. Such mismatches have a structural nature, e.g. one is able to derive that a certain concept should not be the subconcept of another concept. On-toClean provides an explanation of why mismatches occur which subsequently might help to improve the taxonomic structure. The philosophical notions of OntoClean may be the subjects of long discussions, however, strictly speaking, this is not part of the evaluation but of the ontology engineering because deciding the proper nature of a class forces the ontology to commit itself to a more specified meaning, which in turn allows for a more objective evaluation technique.The application of OntoClean consists of two main steps. First, all classes are tagged with regards to the OntoClean meta-properties. Second, the tagged classes are checked against predefined constraints, with constraint violations indicating potential misconceptualizations in the subsumption hierarchy. Although OntoClean is well documented in numerous publications In order to leverage the adoption of OntoClean, we have developed AEON, an approach to automatize both steps of OntoClean. By means of AEON, we can automatically tag any given ontology with respect to the OntoClean meta-properties and perform the constraint checking. For creating the taggings, our implementation of AEONTo evaluate our automatic tagging approach we created a gold standard, i.e. we created a manually tagged middle-sized real-world ontology, and compared AEON results against it. A number of OntoClean experts as well as ontology engineering experts were involved in the creation of the more than 2,000 taggings in the gold standard. Each expert had to tag the PROTON ontology In order to check the OntoClean constraints automatically, we decided to reuse an existing OWL DL formalization of the OntoClean meta-properties and constraints (OntoClean ontology). We used the meta-ontology given in Section 3.2 to represent the tagged ontology and were then able to automatically check the tagged ontology according to the OntoClean ontology. We expected two types of errors when analyzing the inconsistencies. First, the tagging of a concept is incorrect, and second, the corresponding taxonomic relationship is incorrect. We found both kinds of errors in our experimental data and looked at some of the errors in more detail to understand the rationale behind.In the next section, we briefly introduce the idea behind OntoClean. Then we describe the four meta-properties and the most important OntoClean constraints. The automatic tagging has been mostly developed by Johanna Völker and is described in OntoClean in theory
We provide a brief introduction to OntoClean, for a more thorough description refer for example to Applying the OntoClean methodology consists of two main steps.• First, every single class of the ontology to be evaluated is tagged with occurrences of the core meta-properties, which are described in Section 6.3.2. Thus, every class has a certain tagging such as +R+U-D+I, where for example +R denotes that a concept carries Rigidity and +U denotes that the concept carries Unity. We call an ontology with tagged classes a tagged ontology (with regards to OntoClean, to be precise).• Second, after the tagging, all subsumptions of the ontology are checked according to the OntoClean constraints (described in Section 6.3.3). Any violation of a constraint indicates a potential misconceptualization in the subsumption hierarchy.The key idea of OntoClean is to constrain the possible taxonomic relations by disallowing subsumptions between specific combinations of tagged classes. This way, OntoClean provides a unique approach by formally analyzing the classes intensional content and their subsumption relationships. In other words, applying OntoClean means comparing the taxonomical part of a tagged ontology versus a predefined ideal taxonomic structure which is defined by the combination of meta-properties and constraints.After performing the two steps the result is a tagged ontology and a (potentially empty) list of misconceptualizations. According to this list an ontology engineer may repair (in an OntoClean sense) the ontology, in order to remove all discovered misconceptualizations.OntoClean meta-properties
As already indicated, the main ingredients of OntoClean are four meta-properties and a number of rules. The four meta-properties are: rigidity (R), unity (U), dependence (D) and identity (I). They base on philosophical notions as developed by • Rigidity. Rigidity is based on the notion of essence. A class is essential for an instance iff it is necessarily an instance of this class, in all worlds and at all times. Iff a class is essential to all of its instances, the class is called rigid and is tagged with +R. Iff it is not essential to some instances, it is called nonrigid, tagged with -R. An anti-rigid class is one that is not essential to any of its instances. It is tagged ∼R. An example of an anti-rigid class would be teacher, as no teacher has always been, nor is necessarily, a teacher, whereas human is a rigid class because all humans are necessarily humans and neither became nor can stop being a human at some time.• Unity. Unity tells us what is part of the object, what is not, and under what conditions the object is whole • Dependence. A class C 1 is dependent on a class C 2 (and thus tagged +D), iff for every instance of C 1 an instance of C 2 must exist. An example for a dependent class would be food, as instances of food can only exist if there is something for which these instances are food. This does not mean that an entity being food ceases to exist the moment all animals die out that regarded it as food, it just stops being food. Another way to regard dependency is to distinguish between intrinsic and extrinsic class. Intrinsic classes are independent, whereas extrinsic classes need to be given to an instance by circumstances or definitions.• Identity. A class with identity is one, where all instances can be identified as themselves, by virtue of this class or a superclass. This means that the class carries an identity criterion (IC). It is tagged with +I, and with -I otherwise. It is not important to answer the question of what this IC is (this may be hard to answer), it is sufficient to know that the class carries an IC. For example, the class human carries an IC, as we are able to identify someone as being the same or not, even though we may not be able to say what IC we actually used for that. On the other hand, a class such as red would be tagged -I, as we cannot tell instances of red apart because of their color.OntoClean differentiates between the two tags I and O, whereby the first means, that the concept simply carries an Identity Criterion (also through inheritance) and the second that it carries its own Identity Criterion. The difference is not relevant for this thesis, as the tagging +O may just be treated like the tagging +I, as +O implies +I anyway and there are no subsumption constraints about the tag O.OntoClean constraints
A number of OntoClean rules is applied on the tagged ontology. We use the existing OntoClean rules to check a tagged ontology for consistency. Here, we will give some illustrative example for these rules. For a full list refer to • ∼R can't subsume +R. Having a class C 1 subsuming the class C 2 , with C 1 tagged ∼R and C 2 tagged +R, would lead to the following inconsistency: C 2 must always hold true for all of its instances (this is the meaning of the tagging: C 2 is a rigid concept). C 2 , as a subsumed concept, would always imply C 1 for all of its instances. Therefore there are at least some instances of C 1 that are necessarily C 1 as they are C 2 . Thus C 1 can not be anti-rigid, as the tagging says, because this would mean that it is not necessarily true for any of its instanceswhich would be a contradiction. Example: food, an anti-rigid class, subsuming apple, a rigid class. As it is explained in • +I can't subsume -I. If this rule was broken, it would mean that instances of the subsumed class can not be identified -although they are also instances of the subsuming class, which explicitly allows for the identification of the instances. This would be a contradiction, revealing an error in our taxonomy (or tagging).• +D can't subsume -D. food is an example for a dependent class. Modeling the class candy, we decide that everything with more than 20% sugar is candy, thus the class would be independent. We let food subsume candy, and the formal analysis shows this rule is broken. This points us to either an error in the taxonomy or in the tagging. In this example we see that the quality of the taxonomical analysis is only as good as the quality of the applied tagging.Constraint checking
Equipped with the OntoClean taggings we are able to check the hierarchical part of the ontology with regards to the meta-property constraints defined by OntoClean. In order to check these constraints automatically, we use the meta-ontology described in Section 3.2 and extend it in order to provide a formalization of the constraints in OWL in order to check the reified ontology.Listing 6.1: OntoClean constraints meta-ontology in OWL. The formalization given in Listing 6.1 is based on the OntoClean formalization in OWL DL as described in We took the tagging created in the previous sections and formalized them in OWL DL as well, taking each tagging and interpreting it as a class instantiation of the respective class described in the OWL DL constraint ontology. Then we added the reification of the class hierarchy. For each tag, declare the individual that relates to the class from the original ontology to belong to the class corresponding to that tag within the constraint ontology. For example, if a class C is tagged +R, take the reifiied class i C and add the fact ClassAssertion(oc:RigidClass i C ). If a class C is tagged -U, take the individual i C and add the fact ClassAssertion(oc:NonUnityClass i C ), etc.The thus created ontology can be simply checked for satisfiability by an OWL DL reasoner (actually, even much weaker reasoners would be sufficient due to the limited language fragment used in the constraint ontology). Standard reasoning services and ontology debugging tools can be applied in order to discover and repair inconsistencies.For our experiments, we used RaDON The disadvantage of this algorithm is that it does not compute the absolute number of minimal inconsistent subontologies, because in step (ii) it potentially fixes more than one inconsistency. And finally, it is not deterministic. Therefore, different experiments can result in different sets of minimal inconsistent subontologies.Take the classes apple and food. apple is tagged +R, whereas food is tagged ∼R (as described in Section 6.3.3). Now for the sake of the example let's assume that apple is defined as a subclass of food. We reify this axiom as described above, which results in the following formalization: This leads to an unsatisfiability: apple is a RigidClass (b), which has a local range axiom for the subClassOf relation ( Reifying the subsumption hierarchy in OWL DL Analysis and Examples
For evaluating our approach, we have created a set of manual taggings for an ontology provided by three independent annotators A 1 , A 2 and A 3 How does this compare to the automatically generated taggings? After training a classifier on each of the data sets we obtained seven fully automatic taggings. Since AEON so far has not been trained to distinguish between an anti-rigid and non-rigid (respectively, anti-unity and non-unity) tagging we converted all taggings to their  stricter counterpart wherever possible, in order to obtain an upper bound for the number of constraint violations. The results of the inconsistency diagnosis computed for these taggings are presented in Table In the following we present some illustrative examples for inconsistencies which were detected in the ontology, and discuss possible reasons for the corresponding constraint violations.We expected two different kinds of errors when analyzing the unsatisfiabilities: (i) the tagging was incorrect, e.g., because the annotators interpreted a concept in a different way than its author (presumably) did, or (ii) the subsumption relation was wrong, i.e. contradictory to a plausible meta-property assignment. We assumed that the OntoClean constraints as given in Listing 6.1 are correct.An example of an error of the first kind is given by the following facts: The facts contradict axiom An error of the second kind was discovered in the subsumption relation between Group and PoliticalParty, which is only an indirect relation: Group is, in PROTON, actually a superclass of Organization which in turn is a superclass of PoliticalEntity which is a superclass of PoliticalParty. The problem here is indeed in the subsumption relation between Group and Organization: a group is defined in PROTON as "a group of agents, which is not organized in any way." Another example of a subsumption relation that required closer inspection was the subsumption of Hotel by Building. Is Hotel rather the role or the function of a building, whereas the building describes the object itself? Then the building would have a height, whereas the hotel would have a manager. A good example to illustrate the difference would be the number of rooms: it is counted differently for hotels than for buildings. This illustrates that it is not obvious if Hotel should be subsumed by Building or not, as the constraint violation suggested.All the given examples are taken from the automatically tagged ontology. They illustrate that AEON points to possible taxonomic errors in an ontology, and guides the ontology engineer towards problematic parts of the ontology.Method 14 (OntoClean meta-property check)
An ontology can be tagged with the OntoClean meta-properties and then automatically checked for constraint violations. Since the tagging of classes is expensive, we provide an automatic tagging system AEON.All constraint violations, i.e. inconsistencies in the meta-ontology, come from two possible sources:• an incorrect meta-property tagging, or• an incorrect subsumption.The evaluator has to carefully consider each inconsistency, discover which type of error is discovered, and then either correct the tagging or redesign the subsumption hierarchy.125
Chapter 7Semantics
So, so you think you can tell heaven from hell, blue skies from pain? There is a recurring set of problems with existing ontology metrics and measures. We have argued in Chapter 6 that most metrics are based on structural notions without taking into account the semantics which leads to incomparable measurement results. First, most ontology metrics are defined over the RDF graph that represents an OWL DL ontology and thus are basically graph metrics solely. Second, a very small number of metrics take the semantics of OWL DL into account (subsumption etc.). Third, few metrics consider the open world assumption. We believe that foundational work addressing these issues will substantially facilitate the definition of proper ontology metrics in the future.In this chapter we will describe these issues in more detail, and suggest methods to avoid them. These issues are not always problematic: we will also explore under which circumstances they are problematic, and when they can be considered irrelevant. We will outline the foundations for a novel set of metrics and measures, and discuss the advantages and problems of the given solutions. Our approach is based on notions of ontology normalization for measuring (Section 7.1), of stable metrics (Section 7.2), and of language completeness (Section 7.3). Normalization will help to properly define better ontology metrics in subsequent research in this area. Stability and completeness will help us understand metrics better.Normalization
We define ontological, or semantic, metrics to be those which do not measure the structure of the ontology, but rather the models that are described by that structure. In a naïve way, we could state that we base our metrics not on the explicit statements, but on every statement that is entailed by the ontology.But measuring the entailments is much harder than measuring the structure, and we definitively need a reasoner to do that. We also need to make a difference between a statement X that is entailed by an ontology O to be true (O |= X), a statement that is not entailed by an ontology (O |= X), and a statement that is entailed not to be true (O |= ¬X). To properly regard this difference leads us to so called stable metrics that can deal with the open world assumption of OWL DL.Note that measuring the entailments is rather an intuitive description than an exact definition. In many cases -for example for a measure that simply counts the number of statements in an ontology -measuring all entailed statements instead of measuring all explicit statements often leads to an infinite number of statements. Just to give one example, the ontology and so on, an endless chain of SomeValuesFrom-descriptions. But only terminating measures are of practical interest, and thus we need approaches that allow us to capture ontological metrics in a terminating way.In order to gain the advantage of the simple and cheap measurement of structural features, we can transform the structure of the ontology. These transformation need to preserve the semantics of the ontology, that is, they need to describe the same models. But they also need to make certain semantic features of the ontology explicit in their structure -thus we can take structural measures of the transformed ontology and interpret them as ontological measures of the original ontology. We call this kind of transformations normalization We define five normalization steps:1. name all relevant classes, so no anonymous complex class descriptions are left (Section 7.1.1)2. name anonymous individuals (Section 7.1.2)3. materialize the subsumption hierarchy and normalize names (Section 7. Normalization offers the advantage that metrics are much easier defined on the normalized ontology since some properties of the graph are guaranteed: the ontology graph will have no cycles, the number of normal class names and actual classes will be equal, and problems of mapping and redundancy are dealt with. We give an example in Section 7.1.6 to illustrate some of the steps.Often normalizations do not result in canonical, unique results (like, for example conjunctive normal forms). The normalization as described here can be extended in order to result in canonic normalized forms, but the benefits of such an extension are not clear. Considering that common serializations, like the RDF/XML serialization of OWL ontologies Further note that the algorithms provided in this section are merely effective but not efficient. They are given for the purpose of understanding normalization, and not as a blueprint for implementing them. Implementing the given algorithms will be unnecessarily slow, and more clever strategies for efficiently normalizing ontologies remain an open issue.First normalization
In the first normalization our aim is to get rid of anonymous complex class descriptions.After the first normalization, there will be only two types of class axioms left: definitions (i.e. class equivalence axioms between a simple class name and a class description) and simple subsumptions (i.e. subsumptions between two simple class names). Other class axioms (i.e. disjoints, disjoint unions, class equivalences involivng more than one complex class description, and subsumptions involving any complex class descriptions) will be reformulated. Class and property assertions will both use only simple class or property names, and no complex class descriptions.The first normalization can be done as follows:1. replace all axioms of the form DisjointUnion(C D E ...) by the following axioms: replace all axioms of the form EquivalentClasses(C A)
where C is a complex class description and A a simple class name with the axiom EquivalentClasses(A C)6. in all axioms having one of the following forms:where C is a complex class description, replace C with A (being a new simple class name) and add the following axiom:None of these structural changes change the possible models, that means, that they are semantically equivalent. They do introduce new class names to the ontology, which may not be desirable in all cases (for example for presentation purposes, for counting the classes, and so on), but it has to be noted that normalization is done only for measuring processes, and not for the purpose of engineering and processing the ontology (i.e., a normalized ontology is not meant to be published). Note that this way named classes could be introduced that are unsatisfiable. This does not mean that the ontology becomes unsatisfiable, but solely these newly introduced classes. In the third step (Section 7.1.3) we can remove these additional names again.Second normalization
The second normalization gets rid of anonymous individuals. This means that every blank node that is an (asserted or inferred) individual needs to be replaced with an URI reference. Especially in FOAF files It is possible that these newly introduced individual names add a further name to already existing (or other newly introduced) individuals. But since OWL DL does not adhere to a unique name assumptions, this is no problem. Furthermore, the next step of normalization will take care to resolve such synonyms.Third normalization
The third normalization will materialize the subsumption hierarchy and normalize the names. The first step requires a reasoner.1. for all pairs of simple class names (A, B) in the ontology, add the axiom SubClassOf(A B) if the ontology entails that axiom (that is, materialize all subsumptions between simple named classes)2. detect all cycles in the subsumption structure. For each set 3. regarding solely the subontology that consists of all subsumption axioms of the ontology, remove all redundant subsumption axioms (that is, remove all subsumption axioms that are redundant due to the transitivity of the subsumption relation alone). This also removes all subsumption axioms involving owl:Thing and owl:NothingThe subsumption structure now forms a directed acyclic graph that represents the complete subsumption hierarchy of the original ontology. We define a set of normal class names of an ontology as follows: every class name that participates in a subsumption axiom after the third normalization of an ontology is a normal class name of that ontology. Now in all axioms of the types ClassAssertion, PropertyDomain, PropertyRange, and HasKey we can replace all not normal class names with its equivalent normal class names.Note that instead of creating a new class name for each detected cycle, often it will make more sense to choose a name from the set of classes involved in that cycle, based on some criterion (e.g. the class name belonging to a certain namespace, the popularity of the class name on the Web, etc.). For many ontology metrics, this does not make any difference, so we disregard it for now, but we expect the normalizations to have beneficial effects in other scenarios as well, in which case some steps of the normalization need to be revisited in more detail.Compared to classes, properties are often neglected. Besides inverse properties no other complex property descriptions can be stated in OWL. Therefore property normalization can be regarded as normalizing inverses and property names analogous to class name normalization. All normal property names have to be stated explicitly to be equivalent to all other property names they are equal to (that is, we materialize the equality relations between the normal property names and the non-normal ones). All occurrences of non-normal property names (besides within the axiom stating equality with the normal property name, and besides within annotation property instances) are replaced with the normal property name. We can also normalize the property hierarchy just as we normalized the class hierarchy.The same holds true for individuals. In case an individual has more than one name, we decide on or introduce a normal one and state explicitly equality to the normal name, and then replace all occurrences of the non-normal individual names with the normal one (besides within the axiom stating equality with the normal individual name, and besides within annotation property instances).We disregard annotation property instances since they may be used to state annotations about the URI, and not about the actual class, property, or individual. There could be annotations that describe when a certain URI was introduced, who created it, its deprecation state, or that point to a discussion related to the introduction of the URI. Some annotations on the other hand may be useful for the normal name as well -especially labels, or sometimes comments. Since annotations do not have impact on the DL semantics of the ontology anyway, they may be dropped for the purpose of measuring semantic metrics. Nevertheless, if the normalization is done for some other purpose, and it is planned to further use the normalized version of the ontology in some scenario, then the possible replacement of names within annotation property instances depends both on the scenario and the instantiated annotation property (for example, it may be useful to normalize the label when the ontology will be displayed on the user interface, but it may be bad to normalize versioning information that is captured within annotations).Fourth normalization
The fourth normalization aims towards instantiating the most specific classes and properties, as this conveys the most information explicitly (and deriving instantiations of higher levels is very cheap because of the asserted explicitness of the hierarchy due to third normalization). This does not mean that every instance will belong to only one class, multiple instantiations will still be necessary in general.Here is a possible algorithm to perform the fourth normalization of an ontology O. We do not want to remove all redundant facts from the ontology at this step, since there may be some facts that are redundant due to an interplay of different other terminological axioms. For example, in the following ontologyClassAssertion(Person Adam) PropertyAssertion(likes Adam Eve) PropertyDomain(likes Person)
the first statement is actually redundant, but would not be removed by the above algorithm. This is because we only remove axioms that are redundant within the subontology I O , and the axiom stating the domain of likes would not be part of it.Fifth normalization
The fifth normalization finally normalizes the properties: we materialize property instances of symmetric, reflexive and inverse properties, and we clean the transitivity relationship. This can be done similar to the creation of the subsumption hierarchy in the third normalization: after materializing all property instances, we remove all that are redundant in the subontology T O , which contains only the property instances of all transitive properties, and the axioms stating the transitivity of these properties.Examples of normalization
The metric we will regard in this example is the maximum depth of the taxonomy as defined by We have introduced a new class name A that replaces the members of the cycle (D, E). Now the depth of the ontology is 3, as we would have expected from the start, since the cycle is treated appropriately.Existing structural metrics, as discussed in Chapter 6, often fail to capture what they are meant for. Normalization is a tool that is easy to apply and that can easily repair a number of such metrics. Even seemingly simple metrics, as demonstrated here with the ontology depth, are defined in a way that makes too many assumption with regards to the structure of the measured ontologies.As we can see in this chapter, simple structural measures on the ontology do yield values, and often these values may be highly interesting. If we know that md resolves to ∞, then this tells us that we have a cycle in the subsumption hierarchy. Also a high number of classes and complex axioms, but a low md may indicate an expensive to reason about ontology, since the major part of the taxonomy seems to be implicitly stated (but such claims need to be evaluated appropriately). But both results do not capture what the measure was meant to express, that is, the depth of the class hierarchy.Stability
Another aspect of semantic metrics is their stability with regards to the open world assumption of OWL Stability
When an ontology is built, a stable metric will indicate if and how an ontology can be changed. An ontology engineer can prevent certain changes that will render the ontology useless. By adding more heavy-weight axioms to the ontology (for example complete partitions) the minimal depth may raise, indicating a more robust ontology with regards to future changes. Stable metrics are indicators for stable ontologies.Stability with regards to the knowledge base can also be used by closing certain classes. In some cases we know that a knowledge base offers complete coverage: for example, we may publish a complete list of all members of a certain work group, or a complete list of all countries. In this case we can use nominals to close off the class and declare its completeness. But note that such a closure often has undesirable computational side effects in many reasoners.Often metrics intend to capture features of the ontology that are independent of the actual representation of the ontology. But as we have seen, structural transformations of the ontology description often lead to differences in the metrics even though the semantics remained untouched. Normalization offers a way to overcome these problems in many cases.In order to illustrate metrics stability, consider the following ontology:PropertyAssertion(author paper York) PropertyAssertion(author paper Denny) PropertyAssertion(author paper Zdenko)Now let us ask the simple question: how many authors does the paper have? It seems that the answer should be 3. But now, if you knew that Zdenko is just another name for Denny, and thus state SameIndividual(Zdenko Denny) then you suddenly would change your answer to 2, or even, becoming more careful, giving an answer such as "I am not sure, it is either 1 or 2". So finally we can state thatDifferentIndividuals(York Denny)
and thus arrive at the answer that the paper indeed has 2 authors (and even that is possibly wrong if we consider that we could add statements any time in an open world that add further authors to the paper -all we know as of now is that the paper has at least two authors).When creating a metric, we have to ask ourselves the following, similar question: how does the metric behave when additions to the ontology happen? Since ontologies are meant to be smushed and integrated constantly and dynamically, can we predict how certain properties of the ontology will behave, that is, if M (O 1 ) and M (O 2 ) for a metricStability
EquivalentClasses(C UnionOf(D E)) EquivalentClasses(owl:Nothing
md of this ontology is 3 (C, E, F). But besides the actual depth, we can also calculate the minimal depth of this ontology, that is, no matter what axioms are added, what is the smallest number of levels the ontology class hierarchy will have (under the condition that the ontology remains satisfiable)?In the given example, if we add the axiom EquivalentClasses(F E) md will decrease to 2. But on the other hand, no matter what axiom we further add, there is no way to let C collapse with D and E, therefore C is a proper superset of both (that is, it contains more individuals than each D or E alone). And because C cannot become owl:Thing (due to k being outside of C), the minimum depth of the ontology is 2.The maximum depth of an ontology is usually ∞ (since we can always add axioms about an arbitrarily long class hierarchy). Therefore we need to define a maximum depth in a slightly different way in order to be of practical value. In the following, we will discuss two possible definitions.Instead of allowing for arbitrary axioms that may be added, we only allow to add axioms of the form SubClassOf(A B) with A and B being normal class names of the normalized ontology. Thus, in the above example, we may add the axiom SubClassOf(H F) to the ontology in order to increase md from 3 to 4. No longer subsumption path is possible, since all the other named classes would become unsatisfiable when added to an existing path. So this metric will provide with a maximum depth of the ontology, assuming no new class names are added.Another possibility to constrain the axioms to be added, is to allow only for axioms that do not relate to the existing ontology, that is, the intersection of the signatures of the two ontologies is empty. The signature of an ontology is the set of all names used in the ontology (besides the names from the OWL, RDF, RDFS, and XSD namespaces). In this case, md of the merged ontology is the maximal md of the single ontologies, since no interaction between the axioms can happen that may increase or reduce md. We can thus defineStable metrics are metrics that take the open world assumption into account. Stable metrics will help us to evaluate ontologies for the Wide Wild Web. Since we expect ontologies to be merged on the Web dynamically, stable metrics allow us to state conditions that the ontology will fulfill in any situation. The depth of an ontology may be a too simple example to demonstrate the advantages of stable metrics, but imagine a dynamic, ontology-based graphical user interface. Having certain guarantees with regards to the future development of the properties of the ontology may help the designer of the user interface tremendously, even if it is such a seemingly trivial statement such as "the depth of the ontology is never less than 3".There is no simple recipe to follow in order to turn a metric into a stable metric, but the question outlined at the beginning of this section, and then discussed throughout the rest -how does the ontology behave when axioms are added? -can be used as a guideline in achieving a stable metric.Method 15 (Ensuring a stable class hierarchy) Calculate a normalized class depth measure, i.e. calculate the length of the longest subsumption path on the normalized version of the ontology md(N (O)). Now calculate the stable minimal depth of the ontology md min (O). Ifthen the ontology hierarchy is not stable and may collapse.We expect that the ready availability of metrics that take the open world assumption into account will lead to more robust ontologies. Since ontology engineers will have these numbers available at engineering and maintenance time, they will learn easier how to achieve their actual goals. For example, ontology engineers that want to create a class hierarchy that will not collapse to less levels can always check if the minimum depth as described above corresponds to the asserted depth. This would be useful when regarding a class hierarchy with a certain number of levels, which are known not to collapse (e.g. a biological taxonomy). The ontology engineer now could check if the well known number of levels indeed corresponds to the calculated minimum depth.Tools could guide the ontology engineer towards achieving such goals. Ontology engineers get more aware of such problems, and at the same time get tools to measure, and thus potentially control them. 140 7.3 Language completenessLanguage completeness
Language completeness is defined on a certain ontology with regards to a specific ontology language (or subset of the language). Given a specific signature (i.e. set of names), language completeness measures the ratio between the knowledge that can be expressed and the knowledge that is stated. For example, if we have an ontology with the signature Adam, Eve, Apple, knows, eats, Person, Food, we can ask which of the individuals are persons, and which of the individuals know each other.Thus assuming a simple assertional language such as RDF, language completeness with regards to that language (or short: assertional completeness) is achieved by knowing about all possible ground facts that can be described by the ontology (i.e. for each fact {ClassAssertioncan say if it is true or not, and none of them is unknown).An expressive ontology language allows numerous more questions to be asked besides ground facts: is the domain of knows a Person? Is it the range? Is eats a subproperty of knows? In order to have a language complete ontology with regards to the more expressive language, the ontology must offer defined answers for all questions that can be asked with the given language.Method 16 (Measuring language completeness) We define a function Υ i with the index i being a language fragment (if none is given, the assertional fragment is assumed) from an ontology O to the set of all possible axioms over the signature of O given the language fragment i.We introduce C i as language completeness over the language fragment i.Note that the language fragment the completeness measure is using is not tied to the language fragment the ontology is using. Consider for example the following ontology using the above example signature.Disjoint(Food Person) PropertyDomain(knows Person) PropertyRange(eats Food) ClassAssertion(Person Adam) PropertyAssertion(knows Eve Adam) PropertyAssertion(eats Eve Apple)
With the help of Table or by adding terminological axioms that allow to infer that certain facts hold, such as SymmetricProperty(knows) Class
Instead of using assertional completeness, which indeed is not suited for capturing intensional completeness, we have to use a more expressive language fragment. For example, by adding the symmetry axiom to the language fragment used for computing language completeness, we see that the second set indeed displays a higher completeness (0.77) than the first set (0.73). The more expressive the language fragment used for calculating the completeness, the more the measure will reflect the value of intensional axioms.142
Chapter 8Representation
Ceci n'est pas une pipe.(René Representational aspects of an ontology deal with the relation between the semantics and the structure, i.e. the way the semantics are structurally represented. This will often uncover mistakes and omissions within the relation between the formal specification and the shared conceptualization -or at least the models which are supposedly isomorphic to the conceptualizations.Ontological metrics
Normalization helps with the definition of ontological metrics, since they will help a metric designer in being explicitly aware in their choices when creating the metric. Furthermore they offer the ontology designer ready to use methods to easier capture what they mean to express with the designed metric. Sometimes simple structural metrics are sufficient for the task at hand, and many structural metrics exist today. Sometimes an understanding of the semantic model is required, and we have introduced a way to gain that. The aspect of representation covers how the structure represents the semantic. In order to evaluate features of the representation, we compare the results of the structural measures to the results of the semantic measures. Using the normalization described in Chapter 7, we can even often use the same (or a very similar) metric as described in Chapter 6, applied before and after normalization, and compare the respective results. Any deviations between the results of the two measurements indicate elements of the ontology that require further investigation. For example, consider the ontology given in Figure By contrasting the two ontology structures in Figure In the remainder of this Section, we will discuss the four metrics we have introduced in Section 6.1 as examples of how they can be turned into semantic metrics that actually reflect their descriptions. This is a prerequisite for the discussion of the representational metrics introduced subsequently, and their meaning. For notation, we formalize the five steps of normalization as the functions N Maximum depth of the taxonomy
In Section 6.1.1.we have introduced the maximum depth of the taxonomy metric as defined by with H : O → O a function that selects only the simple subsumptions. This calculates all single subsumptions that are not part of the hierarchy, and the other way around. Each axiom x ∈ D is thus a potential problematic axiom that should be checked.Class / relation ratio
In Section 6.1.2 we discussed the (M29) class / relation ratio from and the ratio of properties and property names > 1 then this indicates that not all interesting classes or properties have been given a name, i.e. the coverage of classes and properties with names may not be sufficientIn this metric we see that we cannot just mechanically replace the ontology with its normalized version in order to yield a metric that fits to the desired definition. It is also not true that simply comparing the metrics of the normalized and the original ontology yields interesting metrics that provides us with more insight in the quality of the ontology.Relationship richness
We redefine the original relationship richness metric from (with H as defined in Section 8.2 and P as defined in Section 8.3). As discussed in Section 6.1.3 this value is pretty much meaningless as it is.In order to repair this we cannot just redefine RR(O) = RR * (N (O)) because P (N (O)) does not yield the number of properties. Instead we could use the number of normal property names, resulting inwhich is probably the closest we can get to the original intended definition.Regarding the rationale for this metric, the authors are looking for a metric that "reflects the diversity of relations and placement of relations in the ontology" and is based on the assumption that an ontology that "contains many relations other than class-subclass relations is richer than a taxonomy with only class-subclass relation-ships". We question this assumption: it seems more straightforward to simply use the number of classes than the number of class-subclass relations. We do agree with the original definition that the relationship richness should be of the formIn order to understand the difference we first investigate the relationship between the number of class-subclass relations and the number of classes in an ontology.We understand the number of class-subclass relations to be |H(N (O))|, i.e. the number of simple subsumptions in the normalized ontology. The number of classes is |C N (O)|, i.e. the number of normal class names in an ontology. Now we can define the set of all root classes, i.e. of all classes that have no given superclass (besides owl:Thing) asFurther we can define the treelikeness of the class hierarchy as. The closer the value to 1 the more treelike the class hierarchy is. So if there is exactly one simple subsumption for each class that is not a root class then the treelikeness of the class hierarchy is 1 (this allows us to easily give a formal definition for the terms tree and set of trees describing the taxonomy of an ontology: a tree is given if t| leads to a less tree-like class hierarchy, but has no other obvious effect on the ontology. The treelikeness of the hierarchy seems to be independent of the relationship richness. Therefore we suggest to choose another function for X : obvious candidates seem to be the size of the ontology, i.e. the number of axioms |O|, the number of terminological axioms, or simply the number of classes. We think that the number of classes is a better choice, since a growth in the number of axioms but having a fixed number of entities indicates an overall growth of richness. Therefore it would be counterintuitive for the relational richness to decrease if existing classes are described in more detail. This effect does not happen if we choose X to be instead the number of classes, i.e. |C N (O)|.So we suggest the best metric to capture relational richness that is still close to the original metric as defined by Again it would not make sense to compare this metric to the original metric. But we see that normalization is a useful tool to define metrics more precisely, and to get 148 8.5 Semantic similarity measure closer to what we want to capture with a given metric.Semantic similarity measure
The semantic similarity measure is a function ssm : O × C × C → R that describes the similarity of two classes in an ontology. It is defined as the reciprocal value of the distance over the subsumption graph of the ontology. Continuing the argumentation from Section 6.1.4, we can see that first normalizing the ontology and then calculating the distance avoids a number of pitfalls.The problem we run into here, though, is that normalization removes all explicit subsumption axioms connecting owl:Thing with the root classes R(O) thus potentially leading to a number of disconnected graphs. In this case, we can either define ssm to be 0, or we can add a further step to the normalization procedure, namely ∀C ∈ R(O) add the axiom SubClassOf(C owl:Thing) to the ontology. This way we connect the whole subsumption graph and will always have a finite number for the distance.When we introduced normalization, we stated explicitly that the given steps are neither necessary nor sufficient for all tasks. They provide a metric engineer with a new tool to define and test metrics, but as we see in this example we need to further extend the preprocessing of the ontology before we can measure the value of a specific metric. Again, the given step is a semantic-preserving syntactic transformation, but it is required in order to measure the ontology.It is unclear what the comparision of ssm over the original and the normalized ontologies will yield. Whereas one may think that the normalized ontology provides a better base for calculating a semantic similarity, it may indeed be the case that the opposite is true: usually, even though a redundant subsumption axiom may be represented in the ontology, the inclusion of such an axiom often has a rationale. In learned ontologies, it may be based on the evidence for exactly this relation, in humanengineered ontologies it may be to put emphasis on a specific relation (even though that does not mean anything in the formal semantics). So explicit connections may indicate some semantic closeness that the formal semantics do not properly capture.This evaluation is out of scope for this thesis. Our task is to provide a framework in which to discuss and define metrics for ontology qualities. We have shown in this section a number of metrics as example and demonstrated the usefulness of the tools provided in this thesis.149
Chapter 9Context
Neo: I just have never. . . Rama-Kandra: . . . heard a program speak of love? Neo: It's a. . . human emotion. Rama-Kandra: No, it is a word. What matters is the connection the word implies.(Matix Revolutions There are a number of approaches in the literature describing the creation and definition of artifacts accompanying an ontology. An evaluating tool can load both the additional artifact and the ontology and then perform further evaluations. The additional artifact thus provides a context for the ontology and the evaluation method. Note that we do not consider the whole of the Semantic Web to be a context in this sense. Thus we use the Web for evaluations of other aspects as well, be it for checking the vocabulary (as for linked data in Section 4.1.1) or for automatically deriving properties of classes (as in AEON in Section 6.3). Only specific further artifacts used as input to an evaluation process is considered context within our framework. One of the earliest approaches toward ontology evaluation was the introduction of competency questions, i.e. questions that the ontology should be able to answer But in some cases a list of formalized competency questions and their correct answers is not feasible or possible to generate. Instead, we often can state certain constraints the answers need to fulfill in order to be possibly correct. Again we have to come to the conclusion that we cannot create an automatic system that allows us to check if the ontology is correct -but merely a system that sends out warnings when something seems to be wrong. The constraints in turn may often require expressivity beyond what OWL offers.We address problems in ontology engineering and maintenance that arose during the work with ontologies within the case studies in the European FP6 project SEKT. In Section 9.2 we discuss how further expressivity that goes well beyond the available standardized languages can be used in order to guarantee that the evaluated ontologies fulfill certain, formalized properties. Since these semantics cannot be expressed within OWL there have to be necessarily regarded as contextual in the sense of our definition, i.e. as extra-ontological artifacts that can be used for the evaluation of the ontology.Unit tests
In the SEKT project one of the case studies aimed at providing an intelligent FAQ system to help newly appointed judges in Spain As the ontology evolved and got refined (and thus changed), the legal experts noticed that some of their changes had undesired side effects. To give a simplified example, consider the class hierarchy depicted in Figure In software engineering, the idea of unit testing Ontologies behave quite differently than program units. As there is no notion of information hiding in ontology engineering, and thus no black box components, at first the idea of unit testing for ontologies seems not applicable. Therefore we need to adapt the idea for ontologies.The main purpose of unit tests for ontologies is similar to their purpose in software engineering: whenever an error is encountered with an axiom which is falsely inferred or respectively incorrectly not inferred, the ontology maintainer may add this piece of knowledge to the appropriate test ontology. Whenever the ontology is changed, the changed ontology can be automatically checked against the test ontology, containing the formalized knowledge of previously encountered errors.We investigate the benefits of unit testing applied to ontologies, especially their possibilities to facilitate regression tests, to provide a test framework that can grow incrementally during the maintenance and evolution phase of the ontology, and that is reasonably simple to use. In order for unit tests for ontologies to be useful, they need to be reasonably easy to use and maintain. This will depend heavily on the given implementation. The following approach is informed by the idea of design by contract, i.e. we enable to formalize what statements should and should not derive from an ontology being developed or maintained, either as formalized competency questions (Section 9.1.1) or as explicit ontology statements (Sections 9.1.2 and 9.1.3).Formalized competency questions
Competency questions, as defined by some methodologies for ontology engineering (such as OTK Method 19 (Checking competency questions against results)
Formalize your competency question as a SPARQL query. Write down the expected answer as a SPARQL query result, either in XML SELECT ?Good_mother WHERE {
?child :mother ?Good_mother .154
?child :friend ?Good_mother . }We consider this method especially useful not for the maintenance of the system, but rather for its initial build, in order to define the extent of the ontology. Note that competency questions usually are just exemplary questions -answering all competency questions does not mean that the ontology is complete. Also note that sometimes, although the question is formalizable, the answer does not necessarily need to be known at the time of writing the question. This is especially true for dynamic ontologies, i.e. ontologies that reflect properties of the world that keep changing often (like the song the user of the system is listening to at query time). In that case we can define some checks if the answer is sensible or even possible (like that the answer indeed needs to be a song).How can we test such constraints? Instead of using a SPARQL SELECT query, we can use a SPARQL CONSTRUCT query to create a new ontology with the given results (again, namespace declarations are omitted):CONSTRUCT { ?Good_mother rdf:type :Good_mother } WHERE { ?child :mother ?Good_mother . ?child :friend ?Good_mother . } This will result in an ontology that consists only of class instantiations for the Good_mother class. We can now merge the ontology resulting from the SPARQL CONSTRUCT query with the background ontology (usually the ontology used for query answering) and a constraint ontology that includes constraints on the results, like the following.DisjointClasses(Good_mother Father)
The ontology states that fathers cannot be good mothers. Now should one of the results actually be an instance of father, the resulting merged ontology will be inconsistent.Method 20 (Checking competency questions with constraints)
Formalize your competency question for ontology O as a SPARQL CONSTRUCT query that formulates the result in RDF as ontology R. Merge R with O and a possibly empty ontology containing further constraints C. Check the merged ontology for inconsistencies.Affirming derived knowledge
Unit tests for ontologies test if certain axioms can or can not be derived from the ontology We create two test ontologies T + (called the positive test ontology) and T -(the negative test ontology), and define that an ontology O, in order to fulfill the constraints imposed by the test ontologies, needs to fulfill the following conditions: each axiomNote that T + trivially fulfills the first condition if O is not satisfiable, whereas an empty ontology trivially fulfills the second condition. So it is not hard to come up with ontologies that fulfill the conditions, which shows that unit tests are not meant to be complete formalizations of the requirements of an ontology, but rather helpful indicators towards possible errors or omissions in the tested ontologies.To come back to our previous example in Section 9.1 a simple test ontology T + that consists of the single axiom SubClassOf(Professor University_member) would have been sufficient to discover the problem described. So after the discovered error, this statement is added to the test ontology, and now this same error will be detected next time automatically by running the unit tests.The test ontologies are meant to be created and grown during the maintenance of the ontology. Every time an error is encountered in the usage of the ontology, the error is formalized and added to the appropriate ontology (like in the example above). Experienced ontology engineers may add appropriate axioms in order to anticipate and counter possible errors in maintenance.In software engineering it is often the case, that the initial development of a program is done by a higher skilled, better trained, and more consistent team, whereas the maintenance is then performed by a less expensive group, with less experienced members, that change more frequently. So in software engineering, the more experienced developers often anticipate frequent errors that can happen during maintenance, and create unit tests accordingly in order to put appropriate constraints on the future evolution of the software. We expect a similar development in ontology engineering and maintenance, as soon as ontologies become more common components of information systems. The framework proposed here offers the same possibilities to an ontology engineer. 156Unit tests
Why should an ontology engineer not just add the axioms from T + to O, and ¬A - i for each A - i in T -? There are several reasons:1. not every axiom A - i can be negated. For example, a subsumption statement cannot be negated without inventing new entities.2. adding such axioms increases redundancy in the ontology, and thus makes it harder to edit and maintain.3. the axioms may potentially increase reasoning complexity, or else use language constructs that are not meant to be used within the ontology.4. as discussed in Section 9.1.3, the axioms in T -may be contradictory.finally, due to the open world assumption,
so that the negative test ontology can actually not be simulated with the means of OWL DL.A Protégé plug-in implementing an OWL Unit Test frameworkMethod 21 (Unit testing with test ontologies) For each axiom A + i in the positive test ontology T + test if the axiom is being inferred by the tested ontology O. For every axiom that is not being inferred, issue an error message.For each axiom A - i in the negative test ontology T -test if the axiom is being inferred by the tested ontology O. For every axiom that is being inferred, issue an error message.Formalized competency questions and positive unit test ontologies can sometimes be translated from one into the other, but are largely complementary. We already have shown that SPARQL enables queries beyond the expressivity of OWL, and also test ontologies are much more natural to express OWL constructs than SPARQL is (compare with Section 6.2). Finally, the notion of negative test ontologies expand the possibilities of unit testing well beyond formalized competency questions.Asserting agnosticism
Whereas ontologies in general should be consistent in order to be considered useful, this is not true for the negative test ontology T -. Since T -is simply a collection of all axioms that should not be inferrable from the tested ontology O, it does not need to be satisfiable. Thus they may be two (or more) sets of axioms (subsets of T -) that contradict each other. What does this mean? The existence of such contradicting sets mean that O must not make a decision about the truth of either of these sets, thus formalizing the requirement that O must be agnostic towards certain statements.For example an ontology of complexity classes (where each complexity class is an individual) should have the following two axioms in its negative test ontology:It is obvious that the test ontology is inconsistent, but what it states is that any tested ontology will neither allow to infer that P and NP are equal, nor that they are different. Every ontology that is successfully tested against this negative test ontology will be agnostic regarding this fact.Besides known unknowns (such as the P=NP? problem) we can also assert agnosticism in order to preserve privacy. If we want to ensure that it cannot be inferred if Alice knows Bob from an ontology (i.e. a set of statements, possibly gathered from the Web), we can state that in a test ontology: PropertyAssertion(foaf:knows Alice Bob) NegativeProperyAssertion(foaf:knows Alice Bob)Note that if such a test ontology is indeed used for privacy reasons, it should not be made public since knowing it may lead to important clues for the actual statements that were meant to remain secret.Increasing expressivity for consistency checking
Certain language constructs, or their combination, may increase reasoning time considerably. In order to avoid this, ontologies are often kept simple. But instead of abstaining from the use of more complex constructs, an ontology could also be modularized with regards to its complexity: an ontology may come in different versions, one just defining the used vocabulary, and maybe their explicitly stated taxonomic relations, and a second ontology adding much more knowledge, such as disjointness or domain and range axioms. If the simple ontology is properly built, it can lead to an ontology which often yields the same results to queries as the complete ontology (depending, naturally, on the actual queries). The additional axioms can be used in 158 order to check the consistency of the ontology and the knowledge base with regards to the higher axiomatized version, but for querying them the simple ontology may suffice.We investigate the relationship of heavy-to lightweight ontologies, and how they can interplay with regards to ontology evaluation in Section 9.2.1. We then move to an exemplary formalism going well beyond the expressivity of OWL, by adding rules to the ontology for evaluating it in Section 9.2.2. Autoepistemic operators lend themselves also to be used in the testing of ontologies, especially with regards to their (relative) completeness, since they are a great way to formalize the introspection of ontologies (Section 9.2.3). We also regard a common error in ontology modeling with description logics based languages, and try to turn this error into our favor in Section 9.2.4.Expressive consistency checks
Ontologies in information systems often need to fulfill the requirement of allowing reasoners to quickly answer queries with regards to the ontology. Light weight ontologies usually fulfill this task best. Also, many of the more complex constructors of OWL DL often do not add further information, but rather are used to restrict possible models. This is useful in many applications, such as ontology mapping and alignment, or information integration from different sources.For example, a minimal cardinality constraint will, due to the open world assumption, hardly ever lead to any inferred statements in an OWL DL ontology. Nevertheless the statement can be useful as an indicator for tools that want to offer a user interface to the ontology, or for mapping algorithms that can take this information into account.Further expressive constraints on the ontology, such as disjointness of classes, can be used to check the ontology for consistency at the beginning of the usage, but after this has been checked, a light weight version of the ontology, that potentially enables reasoners to derive answers with a better response time, could be used instead.Formally, we introduce a test ontology C for an ontology O, including additional axioms of the entities used in O that constrain the possible models of the ontology, and check for the satisfiability of the merged ontology O ∪ C.For example, consider the following ontology O about a family:PropertyAssertion(father Adam Seth) PropertyAssertion(mother Eve Seth)Now consider ontology C, formalizing further constraints on the terms used in the ontology:SubPropertyOf(father parent) SubPropertyOf(mother parent) PropertyDomain(parent Person) PropertyRange(father Male) PropertyRange(mother Female) DisjointClasses(Male Female) Now, O ∪ C will be recognized as inconsistent by a reasoner. This is because of the definitions of the properties father and mother showing that it should point to the father respectively the mother, whereas in O it is used to point from the father respectively the mother. This is recognized because C adds range axioms on both, thus letting us infer that Seth has to be an instance of both Female and Male, which is not possible due to the DisjointClasses axiom in C.Method 22 (Increasing expressivity)
An ontology O can be accompanied by a highly axiomatized version of the ontology, C. The merged ontology of O∪C has to be consistent, otherwise the inconsistencies point out to errors in O.Consistency checking with rules
The consistency checks with context ontologies do not need to be bound by the expressivity of OWL, but can instead be using languages with a different expressivity, such as SWRL Consider (a repaired version of) the family ontology O given in Section 9.2.1. We also add years of birth for Adam and Seth: PropertyAssertion(father Seth Adam) PropertyAssertion(mother Seth Eve) PropertyAssertion(birthyear Adam "-3760"^^xsd:gYear) PropertyAssertion  Translating it to datalog will yield the following result LP(O): 160 father(Seth, Adam) mother(Seth, Eve) birthyear(Adam, -3760) birthyear Now we regard further integrity constraints such as the following R (< meaning before):R states that if the year of birth of the parent's child is before the year of birth of the parent, then we have an inconsistency. We can now concatenate LP(O), LP(C) and R and check for inconsistencies -and indeed, one will be raised since Adam was born 130 years after Seth, and thus the ontology must be inconsistent.Method 23 (Inconsistency checks with rules)
Translate the ontology to be evaluated and possible constraint ontologies to a logic program. This translation does not have to be complete. Formalize further constraints as rules or integrity constraints.Concatenate the translated ontologies and the further constraints or integrity constraints. Run the resulting program. If it raises any integrity constraints, then the evaluated ontology contains errors.Use of autoepistemic operators
One approach using increased expressivity is to describe what kind of information an ontology is supposed to have, using autoepistemic constructs such as the K-and A-operators In On the Semantic Web, such a formalism will prove of great value, as it allows to simply discard data that does not adhere to a certain understanding of completeness. For example, a crawler may gather event data on the Semantic Web. But instead of simply collecting all instances of event, it may decide to only accept events that have a start and an end date, a location, a contact email, and a classification with regards to a certain term hierarchy. Although this will decrease the recall of the crawler, the data will be of a higher quality, i.e. of a bigger value, as it can be sorted, displayed, and actually used by calendars, maps, and email clients in order to support the user.The formalization and semantics of autoepistemic operators for the usage in Web ontologies is described in Domain and ranges as constraints
When novice ontology engineers have a background in programming, they often find the semantics of domain and range confusing In order to simulate a mechanism similar to the expectations of programmers, we need to introduce two new axiom types, PropertyDomainConstraint to describe a constraint on a domain of a property, and PropertyRangeConstraint for the range respectively. Besides their name they have the same syntax as the PropertyDomain respective PropertyRange axioms. Using autoepistemic logic, the axiom PropertyDomainConstraint(R C) translates to the following semantics ∃KR.⊤ ⊑ AC stating that everything that is known to be in the domain of R has to be a known instance of C.Instead of using a new axiom type, we could also use the fact that ontology engineers often add domain and range axioms erroneously, not in order to add more inferences but with the meaning intended by the new axiom types, i.e. as type constraints. Based on that we suggest to consciously misinterpret the semantics of the already existing domain and range axioms for the sake of ontology evaluation (note, that this explicitly is not meant as an reinterpretation for using the ontology, but merely for evaluating it before usage). The evaluator will quickly figure out if this is a useful misinterpretation for a given ontology or not. We checked both the Watson EA and the Watson 130 corpora (see Section 11.3) and we did not find a single case where the domain axiom of a property added an inferred class assertion to an individual using that property with the individual not being already explicitly asserted to be an instance of that class. This indicates that this approach could indeed provide fruitful. Collaborative ontology evaluation in Semantic MediaWiki
Given enough eyeballs, all bugs are shallow.(Eric Wikis have become popular tools for collaboration on the Web, and many vibrant online communities employ wikis to exchange knowledge. For a majority of wikis, public or not, primary goals are to organize the collected knowledge and to share information. Wikis are tools to manage online content in a quick and easy way, by editing some simple syntax known as wikitext. This is mainly plain text with some occasional markup elements. For example, a link to another page is created by enclosing the name of the page in brackets, e.g. by writing But in spite of their utility, the content in wikis is barely machine-accessible and only weakly structured. In this chapter we introduce Semantic MediaWiki (SMW) • Consistency of content: The same information often occurs on many pages.How can one ensure that information in different parts of the system is consistent, especially as it can be changed in a distributed way?• Accessing knowledge: Large wikis have thousands of pages. Finding and comparing information from different pages is challenging and time-consuming.  • Reusing knowledge: Many wikis are driven by the wish to make information accessible to many people. But the rigid, text-based content of classical wikis can only be used by reading pages in a browser or similar application.SMW is a free and open source extension of MediaWiki, released under the GNU Public License. Figure Annotation of wiki pages
The main prerequisite of exploiting semantic technologies is the availability of suitably structured data. For this purpose, SMW introduces ways of adding further structure to MediaWiki by means of annotating the textual content of the wiki. In this section, we recall some of MediaWiki's current means of structuring data (Section 10.1.1), and introduce SMW's annotations with properties (Section 10.1.2). Finally, a formal semantic interpretation of the wiki's structure in terms of OWL is presented (Section 10.1.3).Content structuring in MediaWiki
The primary method for entering information into a wiki is wikitext, a simplified markup language that is transformed into HTML pages for reading. Accordingly, wikitext already provides many facilities for describing formatting, and even some for structuring content. For defining the interrelation of pages within a wiki, hyperlinks are arguably the most important feature. They are vital for navigation, and are sometimes even used to classify articles informally. In Wikipedia, for example, articles may contain links to pages of the form The primary structural mechanism of most wikis is the organization of content in wiki pages. In MediaWiki, these pages are further classified into namespaces, which distinguish different kinds of pages according to their function. Namespaces cannot be defined by wiki users, but are part of the configuration settings of a site. A page's namespace is signified by a specific prefix, such as User: for user homepages, Help: for documentation pages, or Talk: for discussion pages on articles in the main namespace. Page titles without a known namespace prefix simply belong to the main namespace. Most pages are subject to the same kind of technical processing for reading and editing, denoted Page display and manipulation in Figure Many wiki engines generally use links for classifying pages. For instance, searching for all pages with a link to the page Another structuring problem of large wikis are synonymous and homonymous titles. In case of synonyms, several different pages for the same subject may emerge in a decentralized editing process. MediaWiki therefore has a redirect mechanism by which a page can be caused to forward all requests directly to another page. This is useful to resolve synonyms but also for some other tasks that suggest such forwarding (e.g. the mentioned articles A final formatting feature of significance to the structure of the wiki is MediaWiki's template system. The wiki parser replaces templates with the text given on the template's own page. The template text in turn may contain parameters. This can be used to achieve a higher consistency, since, for example, a table is then defined only once, and so all pages using this table will look similar. The idea of capturing semantic data in templates has been explored inside WikipediaIn addition to the above, MediaWiki knows many ways of structuring the textual content of pages themselves, e.g. by sections or tables, presentation markup (e.g. text size or font weights), etc. SMW, however, aims at collecting information about the (abstract) concept represented by a page, not about the associated text. The layout and structure of article texts is not used for collecting semantic annotations, since they should follow didactic considerations.Semantic annotations in SMW
Adhering to MediaWiki's basic principles, semantic data in SMW is also structured by pages, such that all semantic content explicitly belongs to a page. Using the terms from Section 2.3, every page corresponds to an ontology entity (including classes and properties). This locality is crucial for maintenance: if knowledge is reused in many places, users must still be able to understand where the information originated. Different namespaces are used to distinguish the different kinds of ontology entities: they can be individuals (the majority of the pages, describing elements of the domain of interest), classes (represented by categories in MediaWiki, used to classify individuals and also to create subcategories), properties (relationships between two individuals or an individual and a data value), and types (used to distinguish different kinds of properties). Categories have been available in MediaWiki since 2004, whereas properties and types were introduced by SMW. SMW collects semantic data by letting users add annotations to the wiki source text of pages via a special markup. The processing of this markup is performed by the components for parsing and rendering in Figure Properties in SMW are used to express binary relationships between one individual (as represented by a wiki page) and some other individual or data value. Each wikicommunity is interested in different relationships depending on its topic area, and therefore SMW lets wiki users control the set of available properties. SMW's property mechanism follows standard Semantic Web formalisms where binary properties also are a central expressive mechanism. But unlike RDF-based languages, SMW does not view property statements (subject-predicate-object triples) as primary information units. SMW rather adopts a page-centric perspective where properties are a means of augmenting a page's contents in a structured way.MediaWiki offers no general mechanism for assigning property values to pages, and a surprising amount of additional data becomes available by making binary relationships in existing wikis explicit. The most obvious kind of binary relations in current wikis are hyperlinks. Each link establishes some relationship between two pages, without specifying what kind of relationship this is, or whether it is significant for a given purpose. SMW allows links to be characterized by properties, such that the link's target becomes the value of a user-provided property. But not all properties take other wiki pages as values: numeric quantities, calendar dates, or geographic coordinates are examples of other available types of properties.For example, consider the wikitext shown in SMW provides a number of datatypes that can be used with properties. Among those are String (character sequences), Date (points in time), Geographic coordinate (locations on earth), and the default type Page that creates links to other pages. Each type provides own methods to process user input, and to display data values. SMW supplies a modular Datatype API as shown in Figure Mapping to OWL
The formal semantics of annotations in SMW, as well as their mapping for the later export (see OWL further distinguishes object properties, datatype properties, and annotation properties. SMW properties may represent any of those depending on their type. Types themselves do not have OWL semantics, but may decide upon the XML Schema type used for literal values of a datatype property. Finally, containment of pages in MediaWiki's categories is interpreted as class membership in OWL.SMW offers a number of built-in properties that may also have a special semantic interpretation. The above property has type, for instance, has no equivalent in OWL and is interpreted as an annotation property. Many properties that provide SMWspecific meta-information (e.g. for unit conversion) are treated similarly. MediaWiki supports the hierarchical organisation of categories, and SMW can be configured to interpret this as an OWL class hierarchy (this may not be desirable for all wikis Exploiting semantics
However simple the process of semantic annotation may be, the majority of users will neglect it as long as it does not bear immediate benefits. In the following we introduce several features of SMW that show contributors the usefulness of semantic markup.Browsing
As shown in Figure These links can be used to browse the wiki based on its semantic content. The page title in the factbox heading leads to a semantic browsing interface that shows not only the annotations within the given page, but also all annotations where the given page is used as a value. The magnifier icon behind each value leads to an inverse search for all pages with similar annotations (Figure Querying
SMW includes a query language that allows access to the wiki's knowledge. The query language can be used in three ways: either to directly query the wiki via a special query page, to add the answer to a page by creating an inline query (Figure Inline queries enable editors to add dynamically created lists or tables to a page, thus making up-to-date query results available to readers who are not even aware of the semantic capabilities of the underlying system. Figure Concepts are the intensional counterparts to MediaWiki's extensional categories. They occupy a new namespace (Concept:) and allow there to define a query that describes the class. Individual pages can not be tagged explicitly with a concept, instead an individual instantiates a concept implicitly by fulfilling the query description. This allows to define concepts such as ISWC Conference as [[Category:Conference]] [[series::ISWC]] All conferences that are properly annotated will then automatically be recognized as ISWC conferences. Concepts can be used in queries just as normal categories, and allow a higher abstraction than categories do.The syntax of SMW's query language is closely related to wiki text, whereas its semantics corresponds to specific class expressions in OWL. [[located in::England]] is the atomic query for all pages with this annotation. Queries with other types of properties and category memberships are constructed following the same principle. Instead of single fixed values one can also specify ranges of values, and even specify nested query expressions.A simplified form of SMW's query language is defined in operators (depending on the context), <q> and </q> as (sub)query delimiters, + as the empty condition that matches everything, and <, >, and ! to express comparison operators ≤, ≥, and = (note that the given grammar assumes a way of expressing these comparison on literal values directly in OWL, whereas they actually need to be defined using appropriate concrete domain definitions). Some nonterminals in Figure Giving back to the Web
The Semantic Web is all about exchanging and reusing knowledge, facilitated by standard formats that enable the interchange of structural information between producers and consumers. Section 10.1.3 explained how SMW's content is grounded in OWL, and this data can also be retrieved via SMW's Web interface as an OWL export. As shown in Figure SMW makes sure to generate valid URIs for all entities within the wiki. It does not burden the user with following the rules and guidelines for "cool URIs" Related systems
Before SMW, some other semantic wikis have been created, but most of them have been discontinued by now The most notable (and stable) related system currently is KiWi Besides text-centered semantic wikis, various collaborative database systems have appeared recently. Examples of such systems include OntoWiki Collaborative ontology evaluation
Semantic wikis have shown to be feasible systems to enable communities to collaboratively create semantically rich content. They enable users to make the knowledge within the wiki explicit. This also allows the wiki to automatically check and evaluate the content. In this section we present a number of approaches in order to provide facilities to ensure the content quality of a wiki, including the application of constraint semantics and autoepistemic operators in ways that are easy accessible for the end user. Wikis such as Wikipedia do not work solely because of their underlying software, but due to their rather complex socio-technical dynamics that work due to often implicit community processes and rules Concept cardinality
Concept cardinality states how many results a query within the wiki should have. Besides exact numbers also minimal and maximal cardinalities are allowed. For the description of the implementation we assume that the query is captured by a concept. Then Template:Cardinality can be added to the concept page.{{#ifeq:{{#ask:[[Concept:{{PAGENAME}}]]|format=count}} |{{{1}}}|OK|Not OK}}
The template assumes one parameter, the cardinality. Thus the template can be instantiated on a concept page such as US states as follows: {{Cardinality|50}}The format count returns simply the number of results. The #ifeq MediaWiki parser function checks if the first parameter, i.e. the query result, is equal to the second parameter, i.e. the first parameter of the template call (in our example 50). If they are equal, the third parameter will be returned (OK), otherwise the fourth parameter will be printed (Not OK). In a production setting the resulting text should be more refined to make sure that the user understands the raised issue.In case we want not to check exact cardinality, but rather maximal or minimal cardinality we can use the MediaWiki parser function #ifexpr that checks if an expression is true or not.  The triple-brackets are replaced with the first and second parameter to the template call, respectively. The inline query will return OK, if no results exists, or the list of individuals prepended with the Intro, as shown below. This template can be called on any wiki page like this: {{Disjoint|Man|Woman}} The template call will result in a little report that could look like this:Testing Man and Woman -Inconsistent individuals: [[Hermes]], and [[Aphrodite]]
Since the inconsistent individuals are already linked, the user can quickly naviagte to their pages and check them.Note that the template is not necessary neither in this case nor in the previous. We could simply add the query directly on any wiki page. The template is merely used to allow other users to easily add tests to pages without having to understand how the SMW query language works.Property cardinality constraints
Property cardinalities are statements about how often a property should be used on a specific page or point to a specific page. We can start with a similar approach as for concept cardinalities, introducing Template:Property cardinality: {{#ifexpr:{{#ask: 182
Note that the expressivity of SMW itself is more restricted than OWL, but exports from SMW have been used in combination with more expressive background ontologies (as suggested in Section 9.2.1) and then evaluated by external OWL inference engines Social and usability aspects
An evaluation framework such as the one available in SMW allows the organic growth and addition of further content evaluations as deemed necessary by the wiki community. We expect that some wikis will introduce specific evaluation approaches that only apply to the domain at hand. Making the above constraints in the wiki must be simple enough to allow contributors to actually apply these features. The given selection is based on the fact that they can be represented within the wiki simply and unambiguously, i.e. contributors will always know where to look for a specific piece of information. This is a necessary requirement in order to keep a wiki maintainable. This is not the case for the actual evaluations themselves. They can be put on any page -in particular, users can create their own private pages where they define their own evaluation criteria and run their own tests. Or they can be collaboratively run and maintained on a more central page. This way each member of the wiki community can decide on their own how and what part of the ontology to monitor.A major decision embedded in the given ontology evaluation framework is to indeed allow contributors to introduce inconsistencies. When a page is modified, the wiki could check if that edit would turn it inconsistent and then cancel the edit. But even disregarding if a real time check would be computationally feasible, this seems to heavily conflict with the wiki paradigm. Instead of prohibiting edits that lead to inconsistencies we introduce means to report discovered problems and allow to repair them efficiently by linking to the respective pages.Having all the evaluations in the wiki being implementable ad-hoc by any contributor, without the requirement of programming PHP and having access to the server and the underlying code, we expect numerous evaluations to bloom. The prize of this freedom, though, is a high computational tax. Executing multiple intervowen layers of template calls and inline queries by the MediaWiki parser is computationally expensive. We expect that some template combinations will turn out to be both useful and expensive, in which case new extensions can be written natively in PHP to replace these template calls with calls to new parser functions. This has happened before in Wikipedia with several features Chapter 11
Related work
Wanting connections, we found connections -always, everywhere, and between everything. The world exploded in a whirling network of kinships, where everything pointed to everything else, everything explained everything else . . . This thesis presents a framework for ontology evaluation. In this chapter we discuss other frameworks with a similar intention, and relevant aspects in the creation of the underlying framework in Section 11.1. We discuss further methods and approaches to ontology evaluation in Section 11.2 and show how they fit into our framework. We expect that future work -especially newly developed evaluation methods, further implementations of existing methods, and evaluations regarding existing methodscan be similarly integrated in our framework. Section 11.3 then closes this chapter by discussing the corpora used for ontology evaluation within this thesis. Note that the relation to our own previously published work is given in Section 1.4.Frameworks and aspects
There are already a number of frameworks for ontology evaluation. The main goal of the framework in this thesis is the assessment of the qualities of one given ontology.The other frameworks often have slightly different, although related goals. The most common such goals are ontology ranking and ontology selection.Ontology ranking has the goal of sorting a given set of ontologies based on some criteria. Often these criteria can be parameterized with a context, usually a search term. Ontology search engines such as FalconS Ontology selection can be regarded as a specialization of ontology ranking as it selects only a single result, often for being reused in an ontology engineering task. The Cupboard system (d'Aquin and Lewen, 2009) presents a similar, though more fine-grained approach that allows not the selection of whole ontologies but rather the selection of single axioms. It incorporates a topic-specific open rating system The framework presented here differs from ranking and selection frameworks as it does not regard and sort sets of given ontologies, but only assesses individual ontologies by themselves. Many of the methods presented here can also be used in a ranking or selection framework, but some have to be adopted: a number of the methods do not yield a numerical score but rather a list of problematic parts of an ontology (such as Method 1 on page 67 or Method 11 on page 86), others may yield a number but this number is not simply proportional to ontology quality but may have a complex relation to it (if at all -compare the metrics introduced in Method 12 on page 101).Some ontology evaluation frameworks are based on defining several criteria or attributes; for each criterion, the ontology is evaluated and given a numerical score. Additionally a weight is also assigned (in advance) to each criterion, and an overall score for the ontology is then computed as a weighted sum of its per-criterion scores. (Lozano-Tello and Gómez-Pérez, 2004) defines an even more detailed set of 117 criteria, organized in a three-level framework. The criteria cover various aspects of the formal language used to describe the ontology, the contents of the ontology, the methodology used, the costs (hardware, software, licensing, etc.) of using the ontology, and the tools available.Our methodology for selecting the ontology evaluation criteria is presented in Section 3.6. For the selection of criteria for the ontology evaluation framework presented in this thesis we have focused on ontology evaluation literature. There are a number of related research fields, such as information and data quality, software engineering (especially the evaluation of software architectures and software models), and database engineering (especially in the field of database schemas and data modeling).In systems engineering, quality attributes as non-functional requirements are sometimes called ilities due to the suffix many of the words share We regard the criteria catalog in Section 3.6 not as a fixed, unchanging list, but as the current state of the art in ontology evaluation research based on our literature survey. The criteria are used to categorize ontology evaluation methods and to help users to find methods relevant for their tasks quickly. The list may be extended or changed and thus may potentially include further relevant criteria originating in related fields of studies or in further work in ontology evaluation.As noted in Section 3.4 this thesis covers more the field of ontology verification (as opposed to ontology validation). A complementing work covering the area of ontology validation is provided in • the evaluation of the use of an ontology in an application (see Section 11.2.2).• the comparison against a source of domain data (see • assessment by humans against a set of criteria. Human experts are used to gauge an ontology against a set of principles "derived largely from common sense".• natural language evaluation techniques. This evaluates the ontology within a natural language processing application such as information extraction, question answering or abstracting.• using reality as a benchmark. Here the notion of a "portion of reality" (POR) is introduced, to which the ontology elements are compared.It is not claimed that the list of techniques is a complete list. We further point out that the list is not even a list of techniques: one of the points describes a dimension of evaluation methods (whether the evaluation is performed automatically, semi-automatically, or manually, i.e. by humans), one is a specialization of the other (natural language evaluation techniques specialize the application-based evaluation), and the last one is describing a methodological condition for ontology evaluation techniques (using reality as a benchmark). As shown in Section 3.5 we do not commit to a strong theory of reality, i.e. an accessible objective ideal that can be used to be compared with an ontology. We furthermore do not think that ontologies should be restricted in specifying conceptualizations of domains within reality, but should be also allowed to specify conceptualizations of fictional domains, such as the family tree of the mythological Greek gods, the history of Tolkien's Middle-earth, or scientific theories about the luminifereous aether.Whereas we disagree with the framework described in Methods and approaches
Work in ontology evaluation has grown considerably during the last few years, also due to two workshops on ontology evaluation A major line of research that has been hardly discussed in this thesis is the logical consistency or satisfiability of an ontology and the debugging of inconsistencies Integrity constraints are extended on top of The larger a group that commits to an ontology (and the shared conceptualization it purports), the harder it is to reach a consensus -but also the larger the potential benefit. Thus the status of the ontology with regards to relevant standardization bodies in the given domain is a major criteria when evaluating an ontology. Ontologies may be standardized or certified by a number of bodies such as W3C, Oasis, IETF, and other organizations that may standardize ontologies in their area of expertise A related question is the grounding of the terms in an ontology In The most widely evaluated aspect in current research is the context aspect. Frequently ontologies are paired with a context and then evaluated with regards to this context. In the survey of ontology evaluation methods provided by • comparing the ontology to a golden standard,• task-based ontology evaluation, i.e. using the ontology with an application and evaluating the application• data-driven ontology evaluation, and• evaluations performed by humans against a set of predefined criteria, standards or requirements.We think that the last point does not belong to this list as it describes a dimension of the ontology evaluation method independent of the other approaches. The other approaches, though, are all part of what the framework in this thesis defines as being an evaluation of the context aspect, with the context being a golden standard, a task or application, or a set of external data respectively. In the following, we will regard evaluation methods that belong to these three categories of context.Golden standard -similarity-based approaches
The evaluation based on a golden standard builds on the idea of using similarity measures to compare an ontology with an existing ontology that serves as a reference. This approach is particularly useful to evaluate automatically learned ontologies with a golden standard. The similarity between ontologies can be calculated using similarity functions measuring the degree of similarity between two ontologies. Typically, such measures are reflexive and symmetric. The similarity function to compare the ontologies can be used directly as the evaluation function, if we keep one of the arguments -the golden standard GS -fixed. On the vocabulary aspect, the similarity between two strings can be measured by the Levenshtein edit distance The vocabulary can also be evaluated using precision and recall, as known in information retrieval. In this context, precision is the fraction of the labels that also appear in the golden standard relative to the total number of labels. Recall is the percentage of the golden standard lexical entries that also appear as labels in the ontology, relative to the total number of golden standard lexical entries. A disadvantage of these definitions is that they are strict with respect to spelling (e.g. different use of hyphens in multi-word phrases would not match, etc.). Given a golden standard, evaluation of an ontology on the semantic aspect can also be based on precision and recall measures, just like on the lexical layer. Task-based evaluations
The output of an ontology-based application, or its performance on a given task, will be better or worse depending on the utility of the ontology used in it. Often ontologies are tightly interwoven with an application, so that the ontology cannot be simply exchanged. It may drive parts of the user interface, the internal data management, and parts of it may be hard-coded into the application. On the other hand, the user never accesses an ontology directly but always through some application. Often the application needs to be evaluated with the ontology, regarding the ontology as merely another component of the used tool. Such a situation has the advantage that wellknown software evaluation methods can be applied, since the system can be regarded as an integrated system where the fact that an ontology is used is of less importance.A utility-based evaluation is presented in The evaluation function presented in Utility-based approaches often have drawbacks:• They allow one to argue that the ontology is good or bad when used in a particular way for a particular task, but it is difficult to generalize this observation (what if the ontology is used for a different task, or differently for the same task?)• the evaluation may be sensitive in the sense that the ontology could be only a small component of the application and its effect on the outcome may be relatively small (or depend considerably on the behavior of the other components)• if evaluating a large number of ontologies, they must be sufficiently compatible that the application can use them all (or the application must be sufficiently flexible)Data-driven evaluation -fitting the data set
An ontology may also be evaluated by comparing it to existing data about the domain to which the ontology refers. This can, e.g., be a collection of text documents. For example, Methods and approaches
Domain completeness is given when an ontology covers the complete domain of interest. This can be only measured automatically if the complete domain is accessible automatically and can be compared to the ontology. A way to assess the completeness of an ontology with respect to a certain text corpus is to use natural language processing techniques to detect all relevant terms in a corpus In the case of more extensive and sophisticated ontologies that incorporate a lot of factual information such as Cyc Many techniques for the automated generation of ontologies, e.g. ontology learning algorithms, provide different kinds of evidences with respect to the correctness and the relevance of ontology elements for the domain in question. For instance, in order to learn subsumptions, Text2Onto Watson corpus
For a number of experiments throughout this thesis we have used corpora derived from the Watson collection. This section describes the corpora and how we created them.In order to test our approach on a realistic corpus of Web ontologies we have created and made available a corpus of ontologies based on the Watson corpus. Watson is a search engine developed by the Knowledge Media Institute Some of the evaluations are based on an earlier corpus. We received an early copy of the Watson corpus in spring 2007, containing 5873 files. We filtered these ontologies to receive only valid OWL DL ontologies, so that only 1331 ontologies remained (checking using KAON2 The ontologies were given short labels for easier reference (A00 to N30). All ontologies can be retrieved in order to examine the results in this thesis within the context of the complete ontology. The metadata about the ontologies offers several key metrics about the ontology, e.g. the number of class names, the number of axioms, etc. Using the metadata file one can easily filter and select ontologies with specific properties. This corpus is by no means meant to be a full view of the Semantic Web, but just a partial, representative snapshot that should allow us to draw conclusions about current OWL DL ontology engineering practice on the Web. We assume that Watson is a random sample of the WebThe experiments were partially run using the RDFlibChapter 12
Conclusions
Uh huh. Uh huh. Okay. Um, can you repeat the part of the stuff where you said all about the. . . things. Uh. . . the things?(Homer Simpson
The Simpsons, Season 7, Episode 17 When we started with this thesis, we had the naïve goal of achieving a simple, fully automatically computable, real-valued quality function:Given two ontologies O 1 and O 2 we wanted to be able to use the measure, get results such as Q(O 1 ) = 0.73 and Q(O 2 ) = 0.62, and thus not only being able to state that one ontology is better than the other, but also how much better it is.As said, it was a naïve goal. In But also this quest for quality did not lead to a satisfying result, especially since "Quality cannot be defined" This final chapter summarizes the achievements of this thesis in Section 12.1 and lists the many open research questions and development challenges in Section 12.2.Achievements
The result of this thesis is a comprehensive framework for the evaluation of ontologies. The framework organizes ontology evaluation methods in two dimensions: ontology quality criteria (accuracy, adaptability, clarity, completeness, computational efficiency, conciseness, consistency, and organizational fitness) and ontology aspects (vocabulary, syntax, structure, semantics, representation, and context). For all criteria and for all aspects we presented methods to evaluate the given criteria or aspect. We added a number of new techniques to the toolbox of an ontology engineer, such as stable metrics, XML based ontology validation, reasoning over a meta-ontology, and others.Unlike other evaluation frameworks and methods we separated an ontology into the given aspects, thus making it clear what is actually being evaluated. A common error in current research is to mix up semantics and structure. Chapters 6-8 show how to keep these levels separate, and offers the tool of normalization in order to assess exactly what the metrics engineer claims to assess. This will clarify the conceptualization surrounding the evaluation of ontologies, and help with describing new ontology evaluation methods and what their benefits will be.The framework in this thesis is also novel as far as it puts some emphasis on the evaluation of the "lower aspects" of the ontology, i.e. vocabulary, syntax, and structure . Only recently, with the strong shift towards Linked Data, have these lower levels gained increased scrutiny. This is not yet reflected so much in research work but rather in informal groups such as the Pedantic Web. Open questions
In the following we list a number of open questions and research challenges raised by this thesis. Many of the methods have their own, specific list of open issues: XML schema validation can be extended with more powerful schema languages, normalization may offer benefits in other areas besides ontology evaluation, and it would be interesting to investigate if it is possible to define a method that turns a normal metric into a stable metric. We pointed out to specific research questions throughout this thesis. Here we will list more general research questions that pertain to the framework as a whole.It is obvious that a domain-and task-independent verification, as discussed here, provides some common and minimum quality level, but can only go so far. In order to properly evaluate an ontology, the evaluator always needs to come up with methods appropriate for the domain and task at hand, and decide on the relative importance of the evaluation criteria. But the minimum quality level discussed here will at least provide the ontology engineer with the confidence that they eliminated many errors and can publish the ontology. Providing a framework for creating and understanding domain-and task-aware evaluation methods, integrating the rich work in this field, remains an open issue.As we have seen, numerous quality evaluation methods have been suggested in literature. But only few of them have been properly designed, defined, implemented, and experimentally verified. The relation between evaluation methods and ontology quality criteria is only badly understood and superficially investigated, if at all. For example, Most of the presented methods in this thesis are only prototypically implemented, be it as tools of their own (like the XML schema-based validation) or be it as part of a toolset (like the structural metrics implemented in the KAON2 OWL tools). What this thesis did not achieve is the implementation of a comprehensive application that applies the various described evaluation methods and provides a summarizing report, either as a part of an ontology development environment or as a stand-alone application. We expect such a validation tool to be of great use. Also many of the current implementations are not efficient. We have defined the results formally, but for a number of our prototypical implementations we do not expect them to scale to realistically sized ontologies. We expect that future research will realize efficient implementations of those methods that have proven useful.We have implemented a number of the evaluation methods within a collaborative semantic authoring system, Semantic MediaWiki. SMW was developed and implemented during the creation of this thesis. We expect the field of collaborative ontology evaluation to become an increasingly important topic for collaborative knowledge construction. But what we see today is just the beginning of this interesting, new research track. We expect the close future to show hitherto unknown levels of cooperation between groups of humans and federations of machine agents, working together to solve the wicked problems we face today.  List of Tables
List of Figures
1
Figure 3 . 1 :
Figure 3 . 2 :
Figure 3 . 3 :
Figure 3 . 4 :
Figure 3 . 5 :
Method 3 :
Method 6 :
1 :
Method 3 :
Method 6 :
5 :
Method 3 :
1 :
Figure 4 . 2 :
84 5 . 2
Listing 5 . 8 :
•
Figure 6 . 1 :
Figure 6 . 2 :
Figure 6 . 3 :
Figure 6 . 4 :
( 1 )
1 . 3 ) 4 .
Figure 8 . 1 :
•
Figure 9 . 1 :
Figure 10 . 1 :
1
Figure 10 . 2 :
Figure 10
Figure 10 . 4 :
Figure 10 . 5 (Figure 10 . 5 :
11
10. 4 . 2
Fig. 1 A
Figure 11 . 1 :
3. 1
Table 2 .
Table 2 .
{x|∃((x, y) ∈ R ∧ y ∈ C)} :x rdf:type owl:Restriction. :x owl:onProperty R. :x owl:someValuesFrom C. AllValuesFrom(R C) {x|∀(x, y) ∈ R → y ∈ C} :x rdf:type owl:Restriction. :x owl:onProperty R. :x owl:allValuesFrom C. HasValue(R a) {x|∃(x, a) ∈ R} :x rdf:type owl:Restriction. :x owl:onProperty R. :x owl:hasValue a. HasSelf(R) {x|∃(x, x) ∈ R} :x rdf:type owl:Restriction. :x owl:onProperty R. :x owl:hasSelf true. MinCardinality(n R) {x|#{y|(x, y) ∈ R} ≥ n} :x rdf:type owl:Restriction. :x owl:onProperty R. :x owl:minCardinality n. MaxCardinality(n R) {x|#{y|(x, y) ∈ R} ≤ n} :x rdf:type owl:Restriction. :x owl:onProperty R. :x owl:maxCardinality n. ExactCardinality(n R) {x|#{y|(x, y) ∈ R} = n} :x rdf:type owl:Restriction. :x owl:onProperty R. :x owl:cardinality n. MinCardinality(n R C) {x|#{y|(x, y) ∈ R ∧ y ∈ C} ≥ n} :x rdf:type owl:Restriction. :x owl:onProperty R. :x owl:onClass C. :x owl:minQualifiedCardinality n. MaxCardinality(n R C) {x|#{y|(x, y) ∈ R ∧ y ∈ C} ≤ n} :x rdf:type owl:Restriction. :x owl:onProperty R. :x owl:onClass C. :x owl:maxQualifiedCardinality n. ExactCardinality(n R C) {x|#{y|(x, y) ∈ R ∧ y ∈ C} = n} :x rdf:type owl:Restriction. :x owl:onProperty R. :x owl:onClass C. :x owl:qualifiedCardinality n. PropertyChain(R S)* {(a, b)|∃(a, x) ∈ R ∧ (x, b) ∈ S} :x owl:propertyChain :y. :y rdf:first R. :y rdf:rest :z. :z rdf:first S. :z rdf:rest rdf:nil.Table 2 . 2
: Semantics of OWL expressions using object properties (datatypes properties are analogous). Expression types with * may hold more parameters.Table 4 .
URI. According to our tests on the Watson EA corpus, we set p s = n s /t s ≈ 0.7513 and p h = n h /t h ≈ 0.8585 (with sample sizes n s = 14, 856 resp. n h = 509 and t s = 11, 161 resp. t h = 437 positive samples, i.e. URIs returning 200 OK codes). We calculate the pooled sample proportion p = psns+p h n h ns+n h ≈ 0.7548. The standard error resultsUID:http: // www . aifb . kit . edu / id / Rudi_Studer N ; CHARSET = UTF -8 :Studer ; Rudi ;;; FN ; CHARSET = UTF -8 :Rudi Studer EMAIL ; TYPE = i n t e r n e t : m a i l t o : r u d i . studer@kit . edu URL:http: // www . aifb . kit . edu / web / Rudi_Studer CLASS:PUBLIC ORG ; CHARSET = UTF -8 : ; END:VCARD ✝ ✆Table 6 .
i A 1 ), . . . , EquivalentClasses(B i A n ) to the ontology. If B i is unsatisfiable, take owl:Nothing instead of B i . If B i is equal to owl:Thing, take owl:ThingTable 7 .
For more on the syntax and semantics of the axioms throughout this thesis, see Section2.4.   A 3XX response means an HTTP response in the range 300-307, a group of responses meaning redirectionhttp://www.livejournal.comhttp://www.freebase.comhttp://www.w3.org/2000/10/XMLSchema#nonNegativeInteger, used 157 timeshttp://www.w3.org/2002/12/cal/icaltzd#dateTime, used 128 timesBoth cited example literals are based on the W3C's write up on its internationalization effort, highlighting the possibilities of languages tags. See here: http://www.w3.org/International/ articles/language-tags/Overview.en.phphttp://ebiquity.umbc.edu/blogger/2007/09/23/top-rdf-namespaces/http://pingthesemanticweb.com/stats/namespaces.phphttp://prefix.ccOriginal source at http://my.netscape.com/rdf/simple/0.9/ unavailable, copy according to http://www.rssboard.org/rss-0.9.dtdhttp://km.aifb.uni-karlsruhe.de/services/RDFSerializer/http://xml.apache.org/xalan-j/http://rdftwig.sourceforge.net/ http://rdfweb.org/people/damian/treehugger/index.html http://www.wsmo.org/TR/d24/d24.2/v0.1/20070412/rdfxslt.html 10 http://xsparql.deri.org/spec/Translated from the original Spanish: "La profundidad máxima en la jerarquía de conceptos: definida como el mayor camino existente siguiendo las relaciones de herencia que puede alcanzar la taxonomía." (Lozano-Tello, 2002, p. 72)   F96: ViewableFile ≡ File ⊓ ¬(MediaFile ⊓ ¬ImageFile)N11: IntangibleEntity ≡ ¬TangibleEntity 112The discussed semantics of the time ontology are not fully captured in the OWL version of the time ontology, but are formalized as described in their first order logic descriptionhttp://ontoware.org/projects/aeon/The original ontology can be found at http://www.ontoclean.org/ontoclean-dl-v1.owl 120The agreement statistics represent lower bounds as they are computed in a cautious manner with respect to the number of possible inconsistencies. If at least one of the individual annotators tagged the regarding concept as -R whereas the others agreed upon ∼R, we assumed the agreement to be -R (or -U, respectively), i.e. the weaker tagging.122http://cordis.europa.eu/ist/kct/sekt_synopsis.htmhttp://www.co-ode.org/downloads/owlunittest/The example follows ex.3.3 in (Donini et al., 2002)    162See, e.g., http://de.wikipedia.org/wiki/Hilfe:Personendaten.SMW's query language has never been officially named, but some refer to it as AskQLhttp://simile.mit.edu/timeline/http://simile.mit.edu/wiki/Longwellhttp://www.openrecord.orghttp://www.freebase.comhttp://www.omegawiki.orghttp://www.mediawiki.org/wiki/Extension:Semantic_Formshttp://smwforum.ontoprise.com 180http://www.mediawiki.org/wiki/Help:Extension:ParserFunctions#.23ifexprWe currently assume that SMW together with parser functions is Turing-complete. If this is the case, any evaluation that can be done automatically at all could be expressed within SMW. But it can turn out that an actual implementation is too hard to be feasible. This is what we mean with obviously implementable.There is also a section on ontology accreditation, certification, and maturity model, but it is made clear that this is a discussion about the future of ontology evaluation and not describing a technique per sehttp://www.mindswap.org/2004/SWOOP/http://minsky.dia.fi.upm.es/odeval 190http://challenge.semanticweb.org/http://www.aifb.uni-karlsruhe.de/WBS/dvr/research/corpushttp://www.aifb.uni-karlsruhe.de/WBS/dvr/research/corpus/meta.rdfAvailable from http://rdflib.net/ 196http://pedantic-web.orgDan Brickley and Libby Miller. The Friend Of A Friend (FOAF) vocabulary specification, July 2005.