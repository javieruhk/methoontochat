(c) to provide a textbook for undergraduate and postgraduate courses on ontology engineering.Introduction
The Semantic Web is characterized by the existence of a very large number of distributed semantic resources, which subscribe to alternative but often overlapping modeling schema (i.e., ontologies). Together these resources define a network of ontologies related through a variety of different meta-relationships such as versioning, inclusion, inconsistency, similarity, and others. This emerging scenario is radically different from the relatively narrow contexts in which ontologies have been traditionally developed and applied, and calls for new methods and tools to support effectively the development of a new kind of network-oriented semantic applications.Hence, ontologies on the Web are not stand-alone artifacts. They relate to each other in ways that might affect their meaning, and are inherently distributed in a network of interlinked semantic resources. More precisely, a network of ontologies or an ontology network is a collection of ontologies related together via a variety of relationships, such as alignment, modularization, version, and dependency. Accordingly, a networked ontology is an ontology included in such a network, sharing relationships with a potentially large number of other ontologies.Intuitively, this aspect of considering ontologies as included in a network implies that they are defined not only through their content but also in terms of ontology metadata, which provide information about their provenance, purpose, and the relations with other ontologies and semantic resources, among other things.One of the most common ways for two ontologies to relate is to be dependent on each other. More precisely, it is often the case that in order to define its own model, an ontology refers to the definitions included in another ontology. The OWL language includes a primitive (owl:imports) allowing an ontology developer to declare such a relationship, merging the definitions of the imported ontology with those from the importing one.Aligning ontologies is a way to put different models in correspondence by declaring which entities in one ontology are the same as those in another ontology, or a generalization or specialization. The main purpose of alignments is to ensure semantic interoperability, making it possible to merge ontologies in a meaningful way by representing information in one ontology in terms of the entities in another.Large, monolithic ontologies are hard to manipulate, use, and maintain. Modular ontologies on the contrary divide the ontological model in self-contained, interlinked components, which can be considered independently, while at the same time participate to the definition of a specific aspect of an ontology. Therefore, modules share the relation that they are common components of a larger ontology, and often include dependencies and alignments to other modules.Finally, versioning relates to the activity of keeping track of the different versions of an ontology. This is of particular importance in a collaborative ontology engineering environment, where the ontology evolution process needs to be carefully monitored and managed. The OWL language includes primitives to declare versioning relations between ontologies, but these do not consider fine-grained changes and are not often used in practice.In this networked world, ontology practitioners need both methodological and technological support for the development and use of ontology networks. We aim to provide such a support in this book.NeOn Methodology Framework
One of the main contributions of this book is the NeOn Methodology framework, which is described in the first part of this book. Although methodological approaches already exist in the literature -e.g., METHONTOLOGY, On-To-Knowledge, and DILIGENT, they do not provide the comprehensive set of methods described in the NeOn Methodology, especially with respect to key activities in a network-centric scenario, such as those related to reusing and managing the dynamics of ontologies.The NeOn Methodology (Chap. 2) uses a scenario-based approach to ontology development and provides a comprehensive set of methods and guidelines for carrying out the variety of activities required when developing ontologies in a networked world.The NeOn Methodology includes (1) a set of nine scenarios that involve different activities for collaboratively building ontologies and ontology networks, (2) a glossary of processes and activities relevant to ontology development, (3) a collection of ontology life cycle models, and (4) a set of methodological guidelines. The NeOn Methodology defines each process or activity in a precise manner, stating its purpose, inputs and outputs, the actors involved, when its execution is more appropriate, and a set of proposed methods, techniques, and tools to be used. The methodology is presented in a prescriptive way to facilitate its adoption by students and practitioners.Current methodologies for ontology engineering, such as METHONTOLOGY, On-To-Knowledge, and DILIGENT, mainly include guidelines for single ontology construction, ranging from ontology requirements specification to ontology implementation, and they are mainly targeted to ontology researchers. In contrast to the aforementioned approaches, the NeOn Methodology does not prescribe a rigid workflow but instead suggests pathways and activities for a variety of scenarios. The nine scenarios described in the book cover commonly occurring situations, e.g., when existing ontologies need to be reengineered, aligned, modularised, localized to support different languages and cultures, or integrated with nonontological resources (NORs), such as folksonomies or thesauri.Another important aspect of the NeOn Methodology is the pattern-based design approach described in Chap. 3. In this chapter, different types of ontology design patterns (ODPs) are presented as well as an associated method (named eXtreme Design) to assist in ontology development. Ontology design patterns provide modeling solutions which can be applied to solve recurrent ontology design problems. The availability of a library of ontology design patterns is an important step toward achieving the ultimate goal of turning ontology design into a structured and reproducible engineering process. The pattern library also includes patterns for reengineering non-ontological resources (such as thesauri, classification schemas, etc.) into ontologies.In addition, as part of the methodological framework, three models are proposed to represent information about ontology networks. They play a critical role, as they allow keeping track of the provenance, purpose, and design of ontologies, as well as covering multilinguality issues. The three models are:• The Ontology Metadata Vocabulary (OMV). An ontology that defines classes and relations to describe authoring aspects, ontology type, purpose, etc. • The Collaborative Ontology Design Ontology (C-ODO). An ontology network that enables designers to describe design entities (ontologies, modules, ontology elements, requirements, activities, tools, reusable knowledge, teams, people, etc.). • The Linguistic Information Repository (LIR). An ontology that defines a set of linguistic classes, whose nature accounts for the localization of ontology terms in a particular language.Ontology Engineering Activities
The second part of the book provides the reader with a description of the key activities relevant to the ontology engineering life cycle in a networked world. For each activity, a general introduction, methodological guidelines, practical examples (where possible), and the technological support within the NeOn Toolkit (if available) are provided. Methodological guidelines are explained using a common structure, which includes process or activity definition, goal, input and output, actors involved, and a graphical workflow, which describes how the process or activity should be carried out. This structured way of explaining the guidelines maximizes the pedagogical value of the book. The starting point to develop an ontology network is the gathering of the requirements the ontology should fulfill. This activity is called ontology requirements specification and is described in Chap. 5.Once requirements are collected, ontology practitioners are encouraged to follow a reuse approach in the ontology building process, which allows speeding up the ontology network development process, saving time and money, and promoting the application of good practices. In this context, both non-ontological resources (Chap. 6) and ontological resources (Chap. 7) can be reused.An important aspect in a networked world, which involves different natural languages and cultures, is the localization of the ontologies. This activity is described in Chap. 8.Another key aspect in the ontology network development is the ontology evaluation activity, which is performed at different levels and according to different criteria, as explained in Chap. 9.Additionally, modularization also needs to be taken into account in the ontology network development according to three different aspects: (1) designing modular ontologies, (2) modularizing existing ontologies, and (3) reusing ontology modules. Methodological guidelines for modularizing existing ontologies are presented in Chap. Ontology networks need to be kept up to date in order to reflect changes and updates. To this purpose, methodological guidelines for ontology evolution are provided in The NeOn Toolkit
The third part of the book presents an overview of the NeOn Toolkit (Chap. 13), focusing in particular on the user interaction side and a detailed description of the plugins, which are most critical to the ontology development process.Proper management of ontology engineering projects in a networked world requires careful planning, and to this purpose it is recommended that an ontology project plan and schedule is defined. To support this activity, a NeOn Toolkit plugin, called gOntt, has been developed, which is described in Chap. 14.The tasks of locating, selecting, and accessing NeOn Toolkit plugins are supported by the Kali-ma plugin Visualizing and navigating ontology networks is a key issue for ontology engineering. In this sense, the NeOn Toolkit provides a novel plugin called ), which exploits an innovative ontology summarization method to support a "middle-out ontology browsing" approach, where it becomes possible to navigate ontologies starting from the most information-rich nodes (key concepts).Finally, reasoning with ontology networks is another key activity in ontology engineering. Chapter 17 presents (a) the NeOn Toolkit query plugin, which allows users to query ontologies in the NeOn Toolkit via the RDF query language SPARQL, (b) the NeOn Toolkit reasoning plugin, which allows for standard reasoning tasks, such as materializing inferences and checking consistency in ontologies, and (c) the RaDON plugin, which supports users in diagnosing and resolving inconsistencies in networked ontologies.Case Studies
The fourth and last part of the book describes how the NeOn methods and tools have been applied in three real-world case studies in the fishery and pharmaceutical domains.• Knowledge management at it concerns (a) other ontologies, such as DOLCEGiven this new vision of ontology engineering by reuse, it then becomes important to provide strong methodological support for the collaborative development of ontology networks.Methodological frameworks are widely accepted in different mature fields (Ferna ´ndez-Lo ´pez 1999), like Software Engineering and Knowledge Engineering. Such methodological frameworks cover aspects, such as development process, life cycle models, as well as the methods, techniques, and tools that can be used to support the development process. Accordingly, a mature methodology for developing ontologies should also cover these aspects.This chapter describes the NeOn Methodology for building ontologies and ontology networks, a scenario-based methodology that supports different aspects of the ontology development process, as well as the reuse and dynamic evolution of networked ontologies in distributed environments, where knowledge is introduced by different people (domain experts, ontology practitioners) at different stages of the ontology development process.This methodology includes the following components:• The NeOn Glossary (Sect. 2.2), which identifies and defines the processes and activities potentially involved in the ontology network construction. • A set of nine scenarios for building ontologies and ontology networks, which are described in Sect. 2.3. Each scenario is decomposed in different processes and activities taken from those included in the NeOn Glossary. • Two ontology network life cycle models (Sect. 2.4) that specify how to organize the processes and activities of the NeOn Glossary into phasesIn addition to applying the NeOn Methodology to the development of the ontology networks associated with use cases of the NeOn project as shown in Finally, it is worth mentioning that the NeOn Methodology can also be used within the Linked Data initiative The NeOn Glossary
The NeOn Glossary identifies and defines the processes and activities potentially involved in the ontology network construction. This glossary has been established by a consensus reaching process among ontology experts and is a first step in addressing the lack of a standard glossary in Ontology Engineering -in contrast with the Software Engineering field that can claim the IEEE Standard Glossary of Software Engineering Terminology Nine Scenarios for Building Ontology Networks
In the NeOn Methodology framework, a set of nine flexible scenarios for collaboratively building ontologies and ontology networks, placing special emphasis on reusing and re-engineering knowledge resources (ontological and non-ontological), has been identified. Figure This section includes, as independent subsections, the most common scenarios that may unfold during the ontology network development. However, the reader should keep in mind that this list is not meant to be exhaustive.• Scenario 2: Reusing and re-engineering non-ontological resources. This scenario covers the case where ontology developers need to analyze non-ontological resources and decide, according to the requirements the ontology should fulfill which non-ontological resources can be reused to build the ontology network. The scenario also covers the task of re-engineering the selected resources into ontologies. • Scenario 3: Reusing ontological resources. Here, ontology developers reuse ontological resources (ontologies as a whole, ontology modules, and/or ontology statements). • Scenario 4: Reusing and re-engineering ontological resources. Here, ontology developers both reuse and re-engineer ontological resources. • Scenario 5: Reusing and merging ontological resources. This scenario unfolds only in those cases where several ontological resources in the same domain are selected for reuse and when ontology developers wish to create a new ontological resource from two or more ontological resources. • Scenario 6: Reusing, merging, and re-engineering ontological resources. This scenario is similar to Scenario 5; however, here developers decide not to use the set of merged resources as it is, but to re-engineer it. • Scenario 7: Reusing ontology design patterns (ODPs). Ontology developers access ODPs repositories to reuse them.   • Scenario 8: Restructuring ontological resources. Ontology developers restructure (modularizing, pruning, extending, and/or specializing) ontological resources to be integrated in the ontology network being built. • Scenario 9: Localizing ontological resources. Ontology developers adapt an ontology to other languages and culture communities, thus producing a multilingual ontology.Knowledge acquisition, documentation, configuration management, evaluation, and assessment should be carried out during the whole ontology network development, that is, in any scenario used for developing the ontology network. The intensity of such support activities depends on the concrete phase of the development progress.It is worth mentioning that these scenarios can be combined in different and flexible ways, and that any combination of scenarios should include Scenario 1 because this scenario is made up of the core activities that have to be performed in any ontology development. Indeed, as Fig. The following subsections present the various scenarios identified; each subsection includes (a) motivation for the scenario; (b) sequence of processes, activities, and tasks to be carried out, where the processes and activities included are taken from the NeOn Glossary of Processes and Activities (Sect. 2.2); and (c) outcomes for the scenario.Scenario 1: From Specification to Implementation
This scenario refers to the development of ontologies from scratch. The scenario is made up of the core activities that have to be performed in any ontology development and should be combined with the rest of scenarios.In this scenario, ontology developersAfter the ontology requirements specification activity, it is recommended to carry out a look for candidate knowledge resources (ontologies, non-ontological resources, and ontology design patterns) to be reused in the development, using as input terms included in the ORSD. These candidate resources provide clues for the identification of the scenarios to be followed during the ontology development. Then, the scheduling activity must be carried out, using the ORSD and the results of such a look for resources. During the scheduling activity, the team establishes the ontology network life cycle and the human resources needed for the ontology project. Chapter 14 presents guidelines and a tool for performing the scheduling of ontology development projects.Then, the ontology developers assigned to the ontology project should carry out (1) the ontology conceptualization activity, in which knowledge is organized and structured into meaningful models at the knowledge level; (2) the ontology formalization activity, in which the conceptual model is transformed into a semicomputable model; and (3) the ontology implementation activity, in which a computable model (implemented in an ontology language) is generated.The principal output is a network of ontologies that represents the expected domain implemented in an ontology language (OWLScenario 2: Reusing and Re-engineering Non-Ontological Resources
Currently, ontology developers are realizing the benefits of "not reinventing the wheel" at each ontology development. They are starting to reuse as much as possible non-ontological resources, such as classification schemes, thesauri, lexicons, and folksonomies, built by others that already have reached some degree of consensus, with the aim of speeding up the ontology development process The activities for carrying out the non-ontological resource reuse process are briefly explained below; prescriptive methodological guidelines for this activity are described in Chap. 6:1. Activity 1. Search non-ontological resources. The goal of the activity is to find non-ontological resources in highly reliable websites, domain-related sites, and resources within organizations. The input for this activity is the ontology requirements specification document (ORSD). 2. Activity 2. Assess the set of candidate non-ontological resources. The goal of this activity is to assess the set of candidate non-ontological resources obtained in Activity 1. To carry out this activity, the following criteria should be used: coverage, precision, and consensus about the knowledge and terminology used in the resource, which is a subjective criterion. 3. Activity 3. Select the most appropriate non-ontological resources. The goal of this activity is to select the most appropriate non-ontological resources from those candidates obtained in Activity 2.As mentioned before, the goal of the non-ontological resource re-engineering process is to transform a non-ontological resource into an ontology. This process can be divided into the following activities, and prescriptive methodological guidelines for performing them are included in Chap. 6:1. Activity 1. Non-ontological resource reverse engineering. The goal of this activity is to analyze a non-ontological resource in order to identify its underlying components and create representations of the resource at the different levels of abstraction (design, requirements, and conceptual). 2. Activity 2. Non-ontological resource transformation. The goal of this activity is to generate a conceptual model from the non-ontological resource. 3. Activity 3. Ontology forward engineering. The goal of this activity is to output a new implementation of the ontology on the basis of the new conceptual model identified in Activity 2.The principal output is an ontology network that represents the expected domain implemented in an ontology language (OWL, F-Logic, etc.). Furthermore, a broad range of documents containing the requirements specification, the ontology documentation, the ontology evaluation, etc. will be generated as output of different activities. Additionally, the non-ontological resources selected to be reused have been "ontologized" by means of the non-ontological resource re-engineering activity.Scenario 3: Reusing Ontological Resources
As more ontological resources are available in ontology repositories and on the Internet As Fig. 1. Activity 1. Ontology search. Ontology developers search for candidate ontological resources that satisfy the requirements in repositories and registries like SwoogleAfter selecting the most appropriate ontological resources, ontology developers should define the reuse mode; that is, ontology developers need to decide how they will reuse the selected ontological resources. There are three possible modes:• The ontological resources selected will be reused as they are.• The ontology re-engineering activity should be carried out with the ontological resources selected. • Some ontological resources will be merged to obtain a new ontological resource.Before reusing the selected ontological resources by means of any reuse mode, it is also convenient to evaluate these resources through the ontology evaluation activity. 5. Activity 5. Ontology integration. Ontology developers should include, as they are, the ontological resources selected (the code) in Activity 4 into the ontology network being built following the activities of Scenario 1 (Sect. 2.3.1).Prescriptive methodological guidelines to reuse general ontologies are provided in Chap. 7.The principal output is an ontology network that represents the expected domain implemented in an ontology language (OWL, F-Logic, etc.). Additionally, a broad range of documents including the requirements specification, the ontology documentation, the ontology evaluation, etc. will be generated as output of different activities.Scenario 4: Reusing and Re-engineering Ontological Resources
This scenario unfolds in those cases in which ontology developers have at their disposal ontological resources useful for their problem, which can be reused in the ontology network development. However, such resources are not exactly useful as they are, so they should be modified (i.e., re-engineered) to serve to the intended purpose or problem. As Fig. Specifically, ontology developers should carry out some activities as part of the ontological resource reuse process; such activities are the following: ontology search, ontology assessment, ontology comparison, and ontology selection as already explained in Scenario 3 (Sect. 2.3.3).After the ontology selection activity, ontology developers should decide how they will reuse the ontological resources. They should also decide whether to perform the ontological resource re-engineering process with the selected ontological resources because these resources may not absolutely correct for the concrete use case as they are and they need to be transformed in some way.The ontological resource re-engineering process proposed here has been created taking as inspiration the software re-engineering process Additionally, this process is related to the levels of abstraction shown in Fig. • Specification is the highest level of abstraction. In this level, requirements, purpose, and scope, among other components of the specification, are described. • In the conceptualization level, ontology characteristics such as structure and components are described. The knowledge that the ontology represents is organized following a set of knowledge representation primitives Figure • At implementation level: from ontological resource 1 code to ontological resource 2 code • At formalization level: reverse engineering (from code 1 to formalization 1), restructuring formalization 1 to obtain formalization 2, and forward engineering to obtain code of resource 2 • At conceptualization level: reverse engineering (from code 1 to conceptualization 1), restructuring conceptualization 1 to obtain conceptualization 2, and forward engineering to obtain formalization or implementation 2 Specification Conceptualization Formalization Implementation Fig. • At specification level: reverse engineering (from code 1 to specification 1), restructuring specification 1 to obtain specification 2, and forward engineering to obtain conceptualization, formalization, or implementation 2The choice of a concrete path depends on the ontological resource characteristics that have to be changed. Thus, in Fig. • Re-specification. If the ontology developer restructures the requirements specification, she changes requirements, purpose and scope, among other elements of the requirements specification. For example, changes in requirements, addition or deletion of requirements, etc. • Re-conceptualization. If she restructures the conceptualization, changes might refer to modification of ontology structure, modification of granularity and richness of the knowledge, removal or addition of axioms, restructuration of ontology architecture (modularization), inclusion of new concepts, use of ontology design patterns, etc. • Re-formalization. If she restructures the formalization level, the changes refer to formalization characteristics (such as changing the ontology paradigm from description logic to frames). • Re-implementation. If she restructures the implementation level, the changes are focused on implementation characteristics that are tightly related to the ontology implementation language (e.g., translation from RDF(S) to OWL). Other changes could be conforming to coding standards, improving code readability, renaming code items, etc.Ontology developers should decide at which level they need to carry out the ontological resource re-engineering process. Once ontology developers have decided the level, they should carry out the ontological resource re-engineering process, and then they should integrate the result of such a process (code, The principal outcome is an ontology network that represents the expected domain implemented in an ontology language Scenario 5: Reusing and Merging Ontological Resources
This scenario unfolds in those cases where several ontological resources in the same domain can be selected for reuse and when the ontology developer wishes to create a new ontological resource from two or more, possibly overlapping, ontological resources. It could also occur that the ontology developer wishes only to establish alignments among the ontological resources selected in order to create the ontology network.As Fig. 1. Activity 1. Ontology aligning. Ontology developers carry out this activity with the aim of obtaining a set of alignments among the selected ontological resources. Prescriptive methodological guidelines for this activity are described in Chap. 12. 2. Activity 2. Ontology merging. Ontology developers can merge the selected ontological resources using the alignments (output of Activity 1) to obtain a new ontological resource from the overlapping selected ones.Ontology developers have here two different possibilities: (1) to establish the mappings among such selected resources and (2) to establish the mappings and also to merge the selected resources.After this activity, ontology developers should use the resultant merged ontological resource as input of some of the activities included in Scenario 1 (explained in Sect. 2.3.1), as shown in Fig. The principal outputs are (a) a set of alignments among the selected ontological resources and (b) a set of new ontological resources to be integrated as they are in the ontology network.2.3.6 Scenario 6: Reusing, Merging, and Re-engineering Ontological ResourcesThis scenario unfolds in those cases in which several ontological resources in the same domain can be selected to build the ontology network. Ontology developers decide to create a new ontological resource merging two or more, possibly overlapping, ontological resources. Such a merged ontological resource is not useful as it is, so it should be modified (i.e., re-engineered) to serve to the intended purpose.As Fig. The principal output is an ontology network that represents the expected domain implemented in an ontology language Furthermore, a merged ontological resource, taken from those selected for reuse, and a re-engineered merged ontological resource are generated. Alignments between the ontological resources selected are also outputs of this scenario.Scenario 7: Reusing Ontology Design Patterns
Recently, within the Ontology Engineering field, ontology design patterns (ODPs) have emerged as (1) a way of helping ontology developers to model OWL ontologies Ontology developers work on the development of an ontology network and very often encounter problems regarding the way in which certain knowledge should be modeled. This may happen during the ontology conceptualization activity, the ontology formalization activity, or during the ontology implementation activity. In these situations, ontology developers can access on-line libraries in order to find modeling solutions.Ontology developers should perform the ontology design pattern reuse process to select the most suitable ODPs for building the ontology network. The principal output of this reuse process is a set of ontology design patterns integrated into the ontology network being developed. Guidelines to perform this reuse are provided in Chap. 3.Scenario 8: Restructuring Ontological Resources
This scenario unfolds in those cases where the knowledge contained in the conceptual model of the ontology network should be corrected and reorganized to obtain the network that covers the ontology requirements.Ontology developers should perform the ontology restructuring activity to modify the ontology network being built, after the ontology conceptualization activity. The ontology restructuring activity can be performed by executing any of the following sub-activities, combining them in any manner and order:• Ontology modularization activity. Ontology developers create different ontology modules in the ontology network, which facilitates the reuse of the knowledge included in the network. Prescriptive methodological guidelines to carry out this activity are presented in Chap. 10. • Ontology pruning activity. Ontology developers prune those branches of the taxonomies included in the ontology network that are considered not necessary to cover the ontology requirements. • Ontology enrichment activity. This activity can be carried out by performing any of the two sub-activities that follow:-Ontology extension activity. Ontology developers extend the ontology network, including (in width) new concepts and relations. -Ontology specialization activity. Ontology developers specialize those branches of the ontology network that require more granularity and include more specialized concepts and relations.Note that this activity (ontology restructuring) can be performed (1) in an independent way as explained in this scenario or (2) as part of the ontological resource re-engineering process, as described in Scenario 4 in The principal output is a conceptual model of the ontology network that represents the expected domain.Scenario 9: Localizing Ontological Resources
Although access to top-quality ontologies (e.g., Galen, CYC, or AKT) is, in many cases, free and unlimited for users all around the world, most of these ontologies are available only in English. Due to the language barrier, non-English users therefore often encounter problems when trying to access ontological knowledge in their own languages. Moreover, more and more ontology-based systems are being built for multilingual applications (e.g., multilingual machine translation or multilingual information retrieval). For these reasons, the need for multilingual ontologies has increased. Thus, this scenario unfolds in those cases in which the ontology network to be developed should be written in different natural languages.Ontology developers should perform the ontology localization activity once the ontology has been conceptualized and restructured. This activity requires the translation of all the ontology terms into another natural language (Spanish, French, German, etc.) different from the language used in the conceptualization, using multilingual thesauri and electronic dictionaries (e.g., EuroWordNet 1. Task 1. Selecting the most appropriate linguistic assets. The goal of this task is to select the most appropriate linguistic assets that help to reduce the cost, to improve the quality of the localization, and to increase the consistency of the localization activity. 2. Task 2. Selecting ontology label(s) to be localized. The goal of this task is to select the ontology label(s) to be localized. 3. Task 3. Obtaining ontology label translation(s). The goal of this task is to obtain the most appropriate translation in the target language for each ontology label. 4. Task 4. Evaluating label translation(s). The goal of this task is to evaluate the label translations in the target language.5. Task 5. Updating the ontology. The goal of this task is to update the ontology with the label translations obtained for each localized label. The task output is an ontology enriched with labels in the target language associated to each localized term.Prescriptive methodological guidelines for localizing ontologies are presented in Chap. 8.After this localization activity, the resulting conceptual model should be integrated in the conceptualization activity of Scenario 1 (Sect. 2.3.1).The principal outcome is a conceptual model of the ontology network in different natural languages (i.e., a multilingual conceptual model) that represents the expected domain.Two Ontology Network Life Cycle Models
Ontologies are artifacts designed for the purpose of satisfying certain requirements and needs that are emerging in the real world.Thus, the ontology network development process is defined as the process by which user's needs are translated into an ontology network. This means that the ontology network development process can be seen as a specific case of the software development process.An ontology network life cycle model is defined as a model to describe how to develop (and maintain) an ontology network project; in other words, how to organize the processes and activities of the NeOn Glossary into phases or stages.This section includes the two ontology network life cycle models, which include the waterfall model (Sect. 2.4.1) and the iterative-incremental model (Sect. 2.4.2). Additionally, it is worth mentioning that these two models are intrinsically related to the set of nine flexible scenarios for collaboratively building ontologies and ontology networks, presented in Sect. 2.3. Such a relation is due to the creation of both models and scenarios, taking into account the importance of reusing and reengineering knowledge resources and merging resources.Waterfall Ontology Network Life Cycle Models
The main characteristic of the waterfall life cycle model family proposed for the ontology network development is the representation of the stages of an ontology network as sequential phases. This model represents the stages as a waterfall. In this model, a concrete stage must be completed before the following stage begins, and no backtracking is permitted except in the case of the maintenance phase.The main assumption for using the waterfall ontology network life cycle model proposed is that the requirements are completely known, without ambiguities, and unchangeable at the beginning of the ontology network development.This model could be used in the following situations:• In ontology projects with a short duration (e.g., 2 months) • In ontology projects in which the goal is to develop an existing ontology in a different formalism or language • In ontology projects in which the requirements are closed, for instance, to implement an ontology based on an ISO standard, or based on resources with previous consensus in the included knowledge • In ontology projects when ontologies cover a small, well-understood domain Taking into account the characteristics of the ontology development scenario, this model includes a set of support activities that should be performed in all of the phases. This set of support activities includes the acquisition of knowledge in the domain in which the ontology network is being developed, the evaluation (from a content-oriented perspective) and the assessment (from user and need perspectives) of the different phase outputs, project and configuration management, and documentation.Because of the importance of reusing and re-engineering knowledge resources and merging ontological resources, the following five significantly different versions of the waterfall ontology network life cycle model have been defined. These versions have been created incrementally (i.e., the four-phase is the basis for the five-phase, the five-phase is the basis for the six-phase, etc.).Before detailing the different versions, they can be summarized in the following way:• The four-phase waterfall model. It represents the stages of an ontology network, starting with the initiation phase and going through the design phase and the implementation phase to the maintenance phase. • The five-phase waterfall model. It extends the four-phase model with the reuse of ontological resources as they are. • The five-phase + merging phase waterfall model. It is a special case of the fivephase model. It includes the merging phase to obtain a new ontological resource from two or more ontological resources previously selected in the reuse phase. • The six-phase waterfall model. It extends the five-phase model with re-engineering phase. It allows the re-engineering of knowledge resources (ontological and nonontological). It could happen that several knowledge resources are transformed into ontologies in the re-engineering phase. • The six-phase + merging phase waterfall model. It extends the six-phase model by including the merging phase after the reuse phase.The Four-Phase Waterfall Ontology Network Life Cycle Model
This model represents the stages of an ontology network, starting with the initiation phase and going through the design phase, the implementation phase to the maintenance phase.The model proposed is shown in Fig. • Initiation phase. In this phase, it is necessary to produce an ontology requirement specification document (ORSD) (explained in Chap. 5), including the requirements that the ontology network should satisfy and taking into account knowledge about the concrete domain. Also in this phase, the approval or rejection of the ontology network development should be obtained. This phase has also as requisite to identify the development team and to establish the resources, responsibilities, and timing (i.e., the scheduling for the ontology project). • Design phase. The output of this phase should be both an informal model and a formal one that satisfy the requirements obtained in the previous phase. The formal model cannot be used by computers, but it can be reused in other ontology networks. • Implementation phase. In this phase, the formal model is implemented in an ontology language. The output of this phase is an ontology implemented in RDF(S), OWL, or other language that can be used by semantic applications or by other ontology networks.It is worth mentioning that the last two phases (design and implementation ones) are normally performed together when ontology development tools (such as NeOn Toolkit, Prote ´ge ´, etc.) are used.• Maintenance phase. If, during the use of the ontology network, errors or missing knowledge are detected, then the ontology development team should go back to the design phase. Additionally, in this phase the generation of new versions for the ontology network should also be carried out.The Five-Phase Waterfall Ontology Network Life Cycle Model
This model extends the four-phase model with a new phase in which the reuse of already implemented ontological resources is considered. The main purpose in the reuse phase is to obtain one or more ontological resources to be reused in the Fig. For the other phases, the purposes and outcomes are the same as those presented in the four-phase model.The Five-Phase + Merging Phase Waterfall Ontology Network Life Cycle Model
This model is a special case of the five-phase model. Now, a new phase (the merging phase) is added after the reuse one. This merging phase has as a main purpose to obtain a new ontological resource from two or more ontological resources selected in the reuse phase.For the other phases, the purposes and outcomes are the same as those presented in the five-phase model.The Six-Phase Waterfall Ontology Network Life Cycle Model
In this model, the five-phase model is taken as general basis, and a new phase (re-engineering phase) is included after the reuse one. This model allows the reuse of knowledge resources (ontological and non-ontological) and their later reengineering. In this model, the reuse phase has as output one or more knowledge resources to be reused in the ontology network that is being developed. After this phase, the non-ontological resources are transformed into ontologies in the reengineering phase; the ontological resources, on the other hand, can or cannot be re-engineered, a decision that should be taken by the ontology development team.For the other phases, the purposes and outcomes are the same as those presented in the six-phase model.The Six-Phase + Merging Phase Waterfall Ontology Network Life Cycle Model
This model, extended from the six-phase model, includes the merging phase after the reuse phase. For the other phases, the purposes and outcomes are the same as those presented in the six-phase model.Iterative-Incremental Ontology Network Life Cycle Model
The main feature of this model is the development of ontology networks organized in a set of iterations (or short mini-projects with a fixed duration). Each individual iteration is similar to an ontology network project that uses any type of waterfall model from those presented in Sect. 2.4.1, as shown schematically in Fig. • In ontology projects with large groups of developers having different profiles and roles • In ontology projects in which the development involves several different domains that are not well understood • In ontology projects in which requirements are not completely known or can change during the ontology development Ontology requirements specified in the ORSD can be divided in different subsets. The result of any iteration is a functional and partial ontology network that meets a subset of the ontology network requirements. Such a partial ontology network can be used, evaluated, and integrated in any other ontology network.This model is based on the continuous improvement and extension of the ontology network resulted from performing multiple iterations with cyclic feedback and adaptation. In this way, the ontology network grows incrementally along the development. Generally, in each iteration new requirements are taken into account, but, occasionally, in a particular iteration, the partial ontology network could be only enhanced.This model focuses on a set of basic requirements; from these requirements, a subset is chosen and considered in the development of the ontology network. The partial result is reviewed, the risk of continuation with the next iteration is analyzed and the initial set of requirements is increased and/or modified in the next iteration until the complete ontology network is developed. The main benefit of this model is to identify and alleviate the possible risks as soon as possible. Other benefits are:• The development team is motivated by rapidly producing an adequate ontology.• Some priorities can be established in the set of requirements.• The development can be possibly adapted to changes in the requirements.• The scheduling of each iteration can be adapted based on the experience of previous iterations.It is worth mentioning that at the beginning of the ontology network project, the number of iterations during the ontology project is influenced by:• The decision of performing a more complete and detailed ontology requirements specification. In this case, the number of iterations will be lower. • The decision of carrying out a simpler and less complete requirements specification, in which case more number of iterations and more revisions will be needed.Figure • No backtracking is allowed between phases in a particular iteration, because the refinement should be performed in the next iterations. • Revising the ontology network requirements and the global plan should be carried out in the initiation phase of each iteration. Additionally, a detailed plan for the particular iteration should be performed.Relation Between Scenarios and Life Cycle Models
The set of nine flexible scenarios for building ontologies and ontology networks presented in Sect. 2.3 and the two proposed ontology network life cycle models presented in this section are intrinsically related because both scenarios and life cycle models have been created (1) taking into account the importance of reusing and re-engineering knowledge resources (ontological and non-ontological) and merging ontological resources and (2) assuming a controlled setting for ontology engineering in which approaches such as mining ontologies from tags are not considered. Table • Scenario 1 (as stated in Sect. 2.3.1) is for building ontology networks from scratch. The scenario mainly includes core activities such as specification, conceptualization, and implementation. This way of building ontologies fits with the stages represented in the four-phase waterfall model (initiation phase, design phase, implementation phase, and maintenance phase). • Scenario 2 (as stated in Sect. 2.3.2) is for building ontology networks by reusing and re-engineering non-ontological resources, which is represented in the six-phase waterfall model. • Scenario 3 (as stated in Sect. 2.3.3) is for building ontology networks by reusing ontological resources. This way of building ontologies is represented by the five-phase waterfall model. • Scenario 4 (as stated in Sect. 2.3.4) refers to the development of ontology networks by reusing and re-engineering ontological resources. This way of building ontologies is represented by the six-phase waterfall model. • Scenario 5 (as stated in Sect. 2.3.5) is for building ontology networks by reusing and merging ontological resources, which is represented by the five-phase + merging phase waterfall model. • Scenario 6 (as stated in Sect. 2.3.6) refers to the development of ontology networks by reusing, merging, and re-engineering ontological resources. This way of building ontology networks is represented by the six-phase + merging phase waterfall model. • Scenario 7 (as stated in Sect. 2.3.7) is for building ontology networks by reusing ontology design patterns, which is represented by the five-phase waterfall model. • Scenario 8 (as stated in Sect. 2.3.8) is for building ontology networks by restructuring ontological resources. This is mainly related to the core activities already mentioned in Scenario 1. Thus, this Scenario 8 is also represented by the four-phase waterfall model. • Scenario 9 (as stated in Sect. 2.3.9) refers to the development of ontology networks by localizing ontologies. This way of building ontologies is mainly related to Scenario 1 and thus represented by the four-phase waterfall model. As explained in Sect. 2.4.2, the iterative-incremental model is basically formed by a set of iterations that can follow any version of waterfall ontology network life cycle model. Thus, the relation between scenarios and the iterative-incremental model depends on the different versions of waterfall model used in the iterativeincremental one, and for this reason, the relations presented in Table Methodological Guidelines for Processes and Activities
In the second part of this book (called Ontology Engineering Activities), methodological guidelines for a subset of the processes and activities included in the NeOn Glossary are provided. To describe each of the processes and activities included in the NeOn Methodology presented in this book, the following content is provided for most of the cases:• A general introduction to the process or activity, where the value of the process or activity is discussed. • The detailed guidelines proposed for carrying out the process or the activity, including the following fields: (a) definition, which is taken from the NeOn Glossary of Processes and Activities and included in Sect. 2.2; (b) goal, which explains the main objective intended to be achieved by the process or the activity; (c) input, which includes the resources needed for carrying out the process or the activity; All the aforementioned information is provided in the so-called filling cards. These filling cards explain the information of each process and activity of the NeOn Methodology in a practical and easy way. Each card is filled according to the filling card template shown in Table It should be noted that in the framework of the NeOn Methodology, there are a wide range of prescriptive methodological guidelines for carrying out different processes and activities. Along this book, the reader can find guidelines for Scenario 1, particularly for ontology requirements specification (Chap. 5) and scheduling (Chap. 14), Scenario 2 (Chap. 6), Scenario 3 (Chap. 7), Scenario 5 (Chap. 12), Scenario 7 (Chap. 3), Scenario 8, for ontology modularization (Chap. 10), and Scenario 9 (Chap. 8). In addition, there are also guidelines for ontology evaluation (Chap. 9) and for ontology evolution Introduction
One of the most challenging and neglected areas of ontology design is reusability, which is getting more and more important partly due to the increased spread of the Linked Data concept Then we focus on content ODPs (CPs), which are domain-dependent practices of modeling, encoded as reusable computational components.ODPs have recently been the subject of a series of workshops What Are Ontology Design Patterns (ODPs)?
During the past decade, as remarked by Another typical scenario includes so-called "reference" or "core" ontologies that are supposed to be directly reused and specialized. Unfortunately, even if well designed, they are usually large and cover more knowledge than what a designer might need. In this case, it is hard to reuse only the "useful pieces" of the ontology, and consequently, the cost of reuse can be higher than developing a new ontology from scratch. On the other hand, the success of very simple and small ontologies, such as FOAFUnder the assumption that there exist classes of problems that can be solved by applying common solutions (as has been experienced in software engineering), it is suggested to support reusability on the design side specifically. We need a way to express commonly applicable solutions and "best practices" and what ontological requirements they solve (see Chap. 5); this is where ODPs come into play. An ODP is a modeling solution to a recurrent ontology design problem Structural ODPs include Logical ODPs and Architectural ODPs. Logical ODPs are compositions of logical constructs that solve a problem of expressivity. They help solving design problems when the used representation language does not directly support certain logical constructs, such as representing n-ary relations in OWL Figure Content Ontology Design Patterns (CPs)
CPs solve design problems for the domain classes and properties that populate an ontology; therefore, they solve content -domain-specific -problems The time interval CP shown earlier, in Fig. • When does a certain time interval start? • When does a certain time interval end?• What are the points in time that belong to a certain time interval?Typically, a CP can also be associated with a set of SPARQL queries that formally encode its competency questions. This is very useful for an ontology designer who wants to test an ontology containing a CP against sample data since the SPARQL queries can be used to test the coverage of the CQs.The time interval CP (see Fig. • A time interval always has exactly one starting point and exactly one end point.This requirement is, in the OWL realization of the CP, addressed by cardinality restrictions on the data type properties that identify the start and the end of a time interval.Finally, this CP is also associated with the following reasoning requirements:• The start and end dates of a time interval belong to the interval.• Two time intervals with the same start and end dates should be recognized to be the same interval.The first reasoning requirement is, in the OWL realization of the CP, addressed by defining the two data type properties startDate and endDate as subproperties of hasDate, which is the property indicating that something belongs to the interval. This enables ontologies reusing the CP to also include the start and end date of an interval, when the model is queried for dates belonging to the interval, using the hasDate property. The second reasoning requirement is addressed by defining a hasKey[startDate, endDate] axiom, on the class TimeInterval. This enables an inference engine to infer that the owl: sameAs property holds between two instances of TimeInterval whenever they have the same start and end date values.Where do CPs come from? This is a highly relevant question since we need a considerable catalog of CPs in order for them to be useful in practice. A CP can emerge from existing conceptual models as well as from data. It can be extracted from foundational CPs can also be extracted from Linked Data CPs can also be created by composing or specializing other CPs or by expanding them (see Sect. 3.3.1 describing operations on CPs). Figure Since CPs can be represented as reusable building blocks, e.g., OWL modules, a natural question is how they are distinguished from any other small ontology. CPs show a number of pragmatic characteristics that allow to distinguish them from other ontologies. CPs are:• Computational components. They are represented and encoded in a computational logic language, e.g., OWL, so that they can be processed and reused as building blocks in ontology design, e.g., through the OWL import construct. • Small, autonomous components. Smallness and autonomy of CPs facilitate the design of ontology networks, because they enforce modularization; by composing CPs, designers can better govern the complexity of the whole resulting ontology as opposed to governing a monolithic ontology. • Inference enabling components. Each CP allows some logical conclusions to be drawn from the model. This means that a single element, e.g., a single class without any associated axioms, cannot be a CP since it does not enable any inferences (even simple ones) to be made. • Hierarchical components. All CPs participate in a partial order, where the ordering relation is called specialization (see Sect. 3.3.1). Specialization requires that at least one entity of the more specific pattern, e.g., a class or property, is subsumed by at least one entity of another, more general, pattern.  Additionally, CPs often match linguistic structures called frames. This could be formulated as an additional characteristic of being linguistically relevant, and the essence of most CPs can be expressed quite straightforward in natural language. The richest repository of frames is FrameNet As opposed to the concept of CP, there is that of AntiCP. AntiCPs are ontologies that implement wrong modeling practices, e.g., examples of bad practices or common mistakes. In other words, they are based on erroneous assumptions or rationales. For example, modeling transitive parthood relationships through subsumption, e.g., City rdfs:subClassOf Country, is considered an AntiCP. AntiCPs produce the side effect of inferring wrong or undesired knowledge, e.g., Rome rdf:type Country, or of preventing the capability to infer the desired knowledge. It is important to distinguish between ontologies that are not CPs and AntiCPs, i.e., only a subset of ontologies that are not CPs are AntiCPs. Operations on Content Patterns
CPs are a special kind of ontologies, as discussed above, and their creation and usage rely on a set of operations that can be summarized as follows:Cloning consists of duplicating an ontology entity (possibly into a new namespace), which can be reused in a CP or used as a prototype for the definition of a new ontology entity defined in a CP. This operation is, for instance, used when extracting CPs from foundational and core ontologies, i.e., a part of the larger structure becomes a CP through being cloned and given a new namespace. Composition relates two CPs and results in a new ontology (which could in turn be a CP -as seen in Fig. eXtreme Design: An Agile Methodology for Pattern-Based Ontology Development
With the name eXtreme Design (XD), we refer to a family of methods that support the pattern-based design approach as depicted in Fig. In the following sections, we describe the XD method, inspired by software engineering's eXtreme Programming (XP) eXtreme Design Principles
Similarly to XP, XD has evolved around a set of main principles. The principles both describe the essence of XD and act as guidelines when performing the design process.The first principle is named customer involvement and feedback. Ideally, the customer should be involved in the ontology development team continuously. This means that the customer should identify representatives that can be easily contacted during the development for quick feedback. Such representatives have to be aware of all parts, and needs, of the project. Here, depending on the project configuration, the "customer" could be either the organization containing the end users of the system to be built (including the ontology) or simply the software developers needing the ontology in order to perform some particular functionality in the overall system.The second principle states that all requirements should be based on customer stories, from which CQs, contextual statements, and reasoning requirements are derived. The customer representatives describe the ontology requirements and the ontology tasks in terms of small stories. Designers work on those small stories and transform them into more rigorous and precise requirements, e.g., in the form of CQs, contextual statements, and reasoning requirements.Next, an important principle is that of iterative development. XD is an iterative and incremental process. Each iteration produces a number of modules that contribute to an incremental release, produced through an integration phase.Test-driven design means, in the case of XD, that testing is used as an integrated means for completing the modules. Stories, CQs, reasoning requirements, and contextual statements are used in order to develop unit tests, e.g., CQs can be transformed into SPARQL queries. By deciding how the query should be formed, a developer is actually partly designing the model, hence, the notion "test-driven" design. The ontology module representing a customer story can be passed to the integration phase, i.e., to be included in the next release, only if all its associated unit tests run successfully. This principle also enforces the task-oriented approach of the method, i.e., the principle that modules should realize exactly what is required (their intended task), nothing more and nothing less.It has to be noted that ontology unit testing, first introduced by (Vrandec ˇic ´and Gangemi 2006), has a different meaning than software unit testing. An ontology module developed for addressing (part of) a user story is tested by developing unit tests, i.e., dedicated ontology modules containing sample facts and appropriately documented with testing metadata, each importing the ontology module to be tested, based on one of the following three approaches: (1) through verification tests to test the fulfillment of basic requirements, i.e., SPARQL queries based on CQs that are run against valid sample data in order to check if expected results are returned by the SPARQL engine, (2) inference tests, i.e., through inference materialization performed on sample data which is expected to cause certain inferences to be materialized in accordance with the reasoning requirements and (3) through stress tests, e.g., through consistency checking performed on invalid sample data violating the contextual statements, thus expecting to provoke inconsistencies. While (1) and (2) are mainly intended for verifying the correct implementation of requirements, (3) could be viewed as more similar to the kind of software testing when a system is fed random or erroneous data, to make sure such cases are handled correctly, and there are no unexpected side effects or crashes.One of the core principles of XD is ODP reuse, which inherently leads to a modular design. Iterations are based on identifying reusable CPs through matching CQs and other requirements. Every time there is a positive match, the identified CPs are considered for reuse. If the solution space does not provide an adequate readyto-use CP, a specific solution is developed in a modular way, preferably in the form of a new CP so that it can be shared with the team (and ideally on the web) for future reuse. This principle favors the creation of a common "language" based on shared patterns and eases both the understandability and the integration of developed modules. In addition, the divide-and-conquer paradigm leads to a natural modularisation of the problem, which facilitates a distributed ontology development, and assists in scoping the modeling issues that are addressed within a single iteration.To handle this incremental ontology development, collaboration and integration are two essential principles. Integration is a key aspect of XD, as the ontology is developed in a distributed, modular, and collaborative way. Collaboration and continuous sharing of knowledge is needed when running an XD project. The result of each iteration, i.e., one or more ontology modules, is integrated with the rest of the ontology modules before releasing an increment. Typically, a sub-team of designers is devoted to the integration task.As mentioned previously, task-oriented design is another main principle. The XD approach is based on developing a task-oriented ontology, covering only part of a domain of knowledge according to a specific application task. This is opposed to the more philosophically oriented approach of formal ontology design, where the aim is to comprehensively cover a certain domain of knowledge. XD proposes to provide solutions to the exact requirements stated, in the sense that the concepts should be defined according to the intended task of the ontology, rather than in some common sense notion of their "true" nature. Each XD iteration focuses on a specific part of the domain requirements, expressed in terms of a user story.On the more organizational side, XD promotes pair design. The team of designers is organized in pairs. This practice is analogous to the pair programming of XP. While pair programming has empirically been proven efficient in software development, it still remains to rigorously test the efficiency for ontology engineering. Currently, this has to be considered a hypothesis, based on experience and observations made through collecting feedback of trainees and developers, through informal discussions and questionnaires after the execution of XD with different teams. Most of them felt that they benefit from on-the-fly brainstorming, and perceived to improve the effect of learning-by-doing within the pair design setting.The eXtreme Design Process
Figure The customer representatives are then invited to write stories, preferably from real, documented scenarios, which act as samples of the typical facts that should be stored in the resulting ontology, and exemplify how these facts are connected and used (Step 3. Collecting requirement stories). All stories are organized in terms of priority, and possible dependencies between them are identified and made explicit. Each story is described by means of a small card, like the one depicted in Table Once a sufficient number of stories for starting the development have been collected, each pair of designers selects a story that will be the focus of their work for the next iteration (Step 4. Eliciting requirements and constructing module(s) from CPs). The selection is based on the experience and competencies of the design pair and on the priority of the story. A new wiki page for the story is created, and its content is set up based on the information reported on the card. By performing this task, a pair enters a development iteration (the dashed rectangle in Fig. Once a story has been completely modeled, it is carefully documented and released internally for integration into the next release (Step 5. Releasing module(s)). This task constitutes the end of a story iteration for a pair, and the result is one or more ontology modules, i.e., small ontologies. Before releasing a module, it is important to make sure that all tests run successfully and that the module is well documented, both in the shared wiki as well as through annotations in the module itself. All ontological elements have to have appropriate labels, they have to be commented (as well as the module itself), and the module should be associated with a description of its purpose, the requirements it solves, and even links to the unit In 2004, the resource of species "tuna" in water area 24 was observed to be fully exploited in the tropical zone at pelagic depth Priority High tests that have been used. The modules are assigned a URI and are shared by the whole team. If a module can be publicly shared, and is considered highly reusable, it can be published in open web registries, such as the ODP portal.Once a new module is released, it has to be integrated with all the others that constitute the current version of the ontology network (Step 6. Integrating partial solutions, evaluating, and revising). Usually, one pair is in charge of performing the integration and related tests. New unit tests are defined for the integrated ontology network, and all existing ones (unit tests of individual modules) are again executed as regression tests before moving to next task. All contextual statements and reasoning requirements are taken into account, and all necessary alignment axioms are defined. The modules are now under the complete control and editing of the team in charge of the integration, and refactoring of the ontology modules may be performed in case inconsistent modeling choices are discovered. Integration can be done in multiple fashions, and an integration policy should be defined at the start of the project. For instance, if decoupling of modules is an essential feature of the resulting ontology network, then a minimum of refactoring should be performed in order to remove overlap between modules, instead integration should simply align the modules. On the other hand, in some cases, a coherent and non-redundant model is desired, whereby an alternative policy would be to refactor the modules, remove as many redundant definitions as possible and instead add import dependencies between them. The products of this step are new unit tests and alignment axioms, and possibly a set of changes to the ontology modules (results of refactoring), all properly documented in the wiki.When all unit tests run successfully during the integration step, a new incremental version of the ontology network can be released (Step 7. Releasing new version of ontology network). The ontology is given a new version number, it is appropriately documented, and it is associated with its own version of the wiki documentation. It is important to note that the process depicted in Fig. Step 4 of the XD process identifies the core iteration performed by a design pair, which is focused on the development of the ontology module(s) representing one user story. Figure First, the development pair analyzes the selected user story and derives a set of CQs, contextual statements and reasoning requirements from it (Step 4.1. Eliciting requirements). In order to do that, designers could involve the customer for having feedback and clarifications. For example, the story "Tuna observation" (see Table • CQ 1 : What is the exploitation state observed and the vertical distance in a given climatic zone for a certain resource? • CQ 2 : What resources have been observed during a certain period in a certain water area?Additionally, assume that the following contextual statements and reasoning requirements are derived based on a discussion with the customer representative:• A resource contains one or more species.• Species are associated to vertical distances. As a consequence, the vertical distance of a resource is inferred through the vertical distance of its species.The iteration continues by further breaking down the task, before starting to address it through modeling. This is done by selecting one of the competency questions, or a small set of them that constitutes a coherent modeling issue, and then start matching them to the competency questions associated with available CPs in order to identify candidates for reuse (Step 4.2. Matching and selecting patterns).In our example, let CQ 1 and CQ 2 be the selected competency questions. Candidate CPs for reuse would be situation and time interval. The competency question of situation -"What entities are in the setting of a certain situation?" -can be said to match the observation of the resource and the parameters that are in the setting of that observation. Additionally, the time interval CP may be seen as matching the question of what period a certain observation was made (CQ 2 ), although this could also be solved with just a simple data type property.The following step is to select which of those patterns should actually be used for solving the modeling problem. In our case, there are only two patterns, and neither is an alternative solution to the other, but in many cases, this step involves making some modeling choices, i.e., deciding which pattern is most suitable for the particular case. Nevertheless, in our example, we still need to decide if both patterns are really needed or if they add too much overhead to our model. For instance, we may decide that time interval adds too many extra elements to the model since perhaps our customer simply wants to store the year of the observation, rather than an exact period of dates, in which case we will only select situation.After selecting a set of CPs, it is time to start modeling, i.e., reusing the CPs (Step 4.3. Reusing and integrating CPs). The term "reuse" here refers to the application of typical operations that can be applied to CPs, i.e., import, specialization and composition (see In our example, we import and specialize situation in order to address CQ 1 , as shown in Fig. The goal of the following task (Step 4.4. Testing module) is to validate the ontology module against the requirements it is supposed to address, i.e., CQs, contextual statements, and reasoning requirements, through developing and executing verification tests, inference tests, and stress tests (see description of test types in Sect. 3.4.1). The ontology modules are revised until all unit tests run successfully. All unit tests are documented in the project wiki and are properly linked to their motivating user story, and requirement(s), in order to document the testing activity as well as to preserve the unit tests for the integration process. In our example, a unit verification test associated with CQ 2 could be the following SPARQL query, retrieving the exploitation state (?exp), vertical distance (?dist), climatic zone (?zone), and resources (?resource) of available observations (?obs):SELECT ?exp ?dist ?resource ?zone WHERE { ?obs a:AquaticResourceObservation. ?obs aboutAquaticResource ?resource. ?obs hasClimaticZone ?zone ?obs hasExploitationState ?exp. ?obs hasVerticalDistance ?dist } If all requirements derived from the story have been solved, and all tests run successfully, the design pair proceeds to internally release the module(s) (Step 5. Releasing module(s)), which are then ready for integration in the current overall increment iteration.Example: A Music Industry Ontology
To illustrate the process of creating ontologies through XD, a small hypothetical project is described in this section. The domain, which is music industry, should be intuitive to most readers. The example is not intended as a case for validating the methodology, but merely as an illustration how it could be used in practice.Step 1 -Project initiation and scoping. Let us assume that an ontology is needed in an online community platform for people who want to discuss music, share news, playlists, and music recommendations. The ontology will be used to store and retrieve information about music recordings and artists, as well as to reason over musical genres. As ontology developers, we are working closely together with the software developers, implementing the online community software; however, we also have access to some future administrators of the community, who are used as the "customer representatives", i.e., domain experts, during the project. A wiki is set up for the development project, where all information is stored and shared, both between developers and with the customer representatives.Step 2 -Identifying CP catalogs. Let us assume that we decide to focus on CP reuse and to mainly reuse CPs from the ODP portal.Step 3 -Collecting requirement stories. As soon as the project environment is set up, we ask the customer representatives to start entering their stories into the wiki. Some stories become examples of typical information that is to be stored by the ontology, while other stories focus more on reasoning tasks of the ontology, depending on how the customer representatives formulate them.Each story is entered into the story template, e.g., given a title and priority, and as soon as several stories are in place, they can be related to each other.A collected story can be seen in Table Step 4 -Eliciting requirements and constructing modules from CPs. At this point, the design team is divided into pairs, in order to develop the ontology modules using pair design. One pair is dedicated to the integration task, i.e., proceed directly to prepare Steps 6-7, while the rest of the pairs choose their first stories from the pool of collected ones. Each story with high priority has to be solved before the lower priority ones are addressed. Each pair then starts to elicit requirements from their chosen story, i.e., tries to derive CQs, contextual statements, and reasoning requirements. Let us imagine that you are part of the design pair who picks up the story in Table 1. What are the tracks of this album? 2. What is the genre of this track or album? There could however be other CQs possible; hence, the final list needs to be agreed with a customer representative, in order to ensure appropriate coverage of the domain and task and to avoid misinterpretations of the story. In addition, it is evident from the way the story is written that a reasoning requirement is needed, i.e., the following:• An album should be automatically assigned all the genres of it contained tracks.In other cases, it may not be as self-evident what needs to be possible to infer; however, in many cases, software requirements and interactions with the customer can clarify such issues.Additionally, contextual statements can be proposed based on common sense knowledge, e.g., in our case:• An album always has at least one track.Other contextual statements may be given by the customer representative or be implicit in the software requirements, e.g., limitations set by the way the software will use the ontology. We have thus collected two CQs, one contextual statement and one reasoning requirement, based on the story in Table Next, the pair proceeds to select a subset of the requirements, which represent some particular modeling issue. When analyzing the CQs, we note that the first one is focused on the album as a collection of tracks, while the second one adds the notion of genre. These are actually quite separate concerns, and in order to decouple these modeling issues, we decide to create one module for each CQ.Choosing to start with the first CQ and the contextual statement, we now need to match the CQ to the requirements covered by the CPs in the ODP portal. When searching the portal's CP submission table 14 , we find that there are several interesting CPs, e.g., there is the collection CP 15 for representing membership, and the part of CP for part-whole relations. In this case, both patterns have matching CQs, so the choice is instead based on how we wish to view the album, i.e., as an object divided into parts or as a collection that is the sum of its members. One of the main differences between the patterns is that the part of CP defines the part-whole relation as transitive, while the membership relation in the collection CP is not. Since we are not interested in creating a hierarchy of parts, we decide on the collection CP and document this choice in the project wiki (including our argumentation).Then it is time to start modeling. Since we are creating a new module each time, we start by creating an empty ontology, with a new namespace (following a namespace convention agreed in Step 1). Next, we import the OWL building block of the collection CP into our empty ontology and start specializing it. As a subclass of collection:Collection, we create a new class Album, and then another new class called Track (subclass of owl:Thing). To complete the specialization, we create sub-properties of the pattern properties, with more domain-specific names, e.g., containsTrack and containedInAlbum, set them to be inverses, and define domain and range axioms. Figure When the design pair is satisfied, it is time to test the module they have created. First, we formulate the CQ as a SPARQL query. Most often, missing parts are discovered already when formulating the query since the query formulation involves an inspection of the model. However, a new ontology (i.e., a "test case") is created, importing the ontology to be tested, and some test instances are added in the test case ontology. If the SPARQL query gives the expected result, based on our test data, then the test is successful. We can proceed to perform some stress testing. In this case, we should add data that violates some constraint, e.g., a contextual statement, and see that the ontology is able to detect the problem, e.g., through finding an inconsistency, and that there are no undesired When all tests run successfully, and the module is fully annotated and documented in the wiki, it is time to proceed with the next set of requirements, i.e., the second CQ and the reasoning requirement. Similarly as before, we start by matching the requirements to the list of CPs in the ODP portal. This time, we do not immediately find a match, i.e., there is no pattern for music genres; however, there are patterns for expressing descriptions and parameters of a concept. Nevertheless, let us assume that we find these too abstract for our case, and instead choose to create the model on our own. Just as in the previous iteration, we start by creating a new empty ontology with its own namespace. However, this time we realize that we need the tracks and albums that we just modeled in the previous module; hence, we import it into our new ontology module. Then we add the class Genre and a property hasGenre (including its inverse genreOf). The domain of hasGenre is set to the union of Track and Album, while the range is set to the Genre class. In addition, to solve the reasoning requirement, we add a property chain definition to the hasGenre property, stating that hasGenre can be derived from the combination of the hasTrack and hasGenre properties, meaning that if an album has a track which in turn has a certain genre, then that album should also be directly connected to the same genre.Testing this time involves testing the CQ using one or more SPARQL queries but also to test the inferences produced based on the property chain, i.e., to confirm that the reasoning requirement is fulfilled. To do the latter, we create a new empty ontology, import our module to be tested and add some test data that should produce the correct inference. For instance, an album instance can be added, then associated to a track (through hasTrack), and the track's genre set to rock (through hasGenre). When the inferences are materialized, we expect to see that the album is now also associated with the genre rock. As soon as all tests run successfully, and the ontology module is appropriately commented, we are now ready to release the complete solution of the customer story in Table Step 5 -Releasing module(s). The modules, and all their wiki documentation, are now made available to the pair in charge of integration.Step 6 -Integrating partial solutions, evaluating, and revising. As soon as the integration team have more than one solution to work with, i.e., more than one story is covered, they start integrating the modules. Integration is a crucial part and involves a trade-off between refactoring, to reduce overlap between modules, and keeping the decoupling of modules to facilitate later changes and reusability of individual modules. In some cases, integration is quite easy, e.g., the modules can directly be imported into one new ontology, and tested together, without any additional modeling, while in other cases, the integration means to add some "glue" to resolve conflicts and make sure that the requirements of the stories treated so far can be covered all together. However, the use of CPs facilitates the integration since it makes explicit the modeling choices made, assures that the development team has a shared vocabulary for talking about modeling choices, and in some cases even makes the integration semi-automatic, i.e., if the same CP is imported in several modules they are inherently aligned. While the integration pair starts their task, our design pair can now go back to the list of remaining user stories, and select a new one, to start another development iteration. This process is continued until no more stories are to be covered.Step 7 -Releasing new version of ontology network. After each new module has been integrated into the resulting ontology (i.e., ontology network), a new release is created, letting the customer and other parties, e.g., software developers, review and test the ontology at all stages of development.Tool Support
In this section, we briefly present the ODP portalFigure XD Tools also include a wizard for guiding users in the process of specializing a CP. Figure In addition, XD Tools provide several help functions, such as inline info boxes, help sections in the Eclipse help center and "cheat sheets" describing the XD methodology for CP reuse.Conclusion
In this chapter, we have presented ontology design patterns (ODPs), which are reusable modeling solutions that encode modeling best practices, by briefly discussing their different types and characteristics. ODPs are the main tool for performing pattern-based design of ontologies, which is an approach to ontology development that emphasizes reuse and promotes the development of a common "language" for sharing knowledge about ontology design best practices. ODPs are associated with a set of requirements that are explicitly expressed in order to favor their selection through a matching procedure. Content ODPs (CPs) have been the main focus of this chapter, which has shown through some examples how they can be used for building an ontology according to a set of elicited requirements. CPs are domain-dependent patterns, the requirements of which are expressed by means of competency questions, contextual statements, and reasoning requirements. In order to reuse CPs, we have defined a set of operations that include importing, specializing, and composing them to the aim of building a new ontology (or ontology network).In the second part of the chapter, we have described an agile methodology for pattern-based ontology design named eXtreme Design (XD), an iterative and incremental process, which is characterized by a test-driven and collaborative development approach. The XD methodology is supported by a set of software components named XD Tools, which assist users in the process of pattern-based design.The XD methodology has been tested in numerous ontology development projects, including user-based experiments conducted in controlled environments. The results of those experiments have been reported by Chapter 4
The NeOn Ontology Models Alessandro Adamou, Rau ´l Palma, Peter Haase, Elena Montiel-Ponsoda, Guadalupe Aguado de Cea, Asuncio ´n Go ´mez-Pe ´rez, Wim Peters, and Aldo Gangemi Abstract Interoperability on multiple levels, concerning both the ontologies themselves and their engineering activities, is a key requirement for ontology networks to be efficient, with minimal redundancy and high reuse. This requirement has a strict binding for software tools that can support some interoperability levels, yet they can be hindered by a lack of shared models and vocabularies describing the resources to be handled, as well as the ways of handling them. Here, three examples of metalevel vocabularies are proposed, each covering at least one peculiar interoperability aspect: OMV for modeling the artifacts themselves, LIR for managing a multilingual layer on top of them, and C-ODO Light for modeling collaboration-supportive life cycle management tasks and processes. All of these models lend themselves to handling by dedicated software tools and are all being employed within NeOn products.Introduction
Authoring ontologies and modeling domains of interest are only part of an ontology life cycle management process. If these activities are carried out in a monolithic fashion, from scratch and without reusing readily available knowledge models, this may lead to costly "reinventions of the wheel" and contradicts the Semantic Web philosophy of an open knowledge world. On the other hand, even when the intention and sentiment to follow this philosophy are present, they might not be encouraged by appropriate tool support. This, in turn, depends on the availability of formal models of processes and artifacts in ontology design, i.e., their metalevel. This model may sometimes be implicitly hardwired in the software itself, but if it is not, then it may be useful to share and exploit it for the sake of interoperability, be it conceptual, linguistic, functional, or social.This chapter focuses on three contributions to the practice of ontology design by metalevel handling. Each contribution, presented itself as an ontology network, covers a specific design perspective, i.e., reuse (OMV), localization (LIR), and collaborative engineering (C-ODO Light). By the end of the chapter, the reader will have a practical insight as to how a model of the ontology metalevel can be employed to build effective software tools to automate engineering tasks.Ontology Metadata Vocabulary (OMV)
Ontologies have undergone an enormous development and have been applied in many domains within the last years, especially in the context of the Semantic Web. Currently, however, efficient knowledge sharing and reuse, a prerequisite for the realization of the Semantic Web vision, is a difficult task. It is hard to find and share existing ontologies because of the lack of standards for documenting and annotating ontologies with metadata information. Without ontology-specific metadata, developers are not able to reuse existing ontologies, which leads to interoperability problems, as well as duplicate efforts. In order to provide a basis for an effective access and exchange of ontologies across the web, it is necessary to agree on a standard for ontology metadata. This standard then provides a common set of terms and definitions describing ontologies and is called metadata vocabulary.Limitations. The need for a metadata vocabulary for describing ontologies has been acknowledged in the past by previous efforts Thereupon, in this chapter we describe our contribution to the alleviation of this situation: the ontology metadata standard OMV (Ontology Metadata Vocabulary), which specifies reusability-enhancing ontology features for human-and machineprocessing purposes. It allows to clarify the relations between the available ontologies so that they are easy to search, to characterize, and to maintain. Moreover, it provides the means for making explicit the virtual and implicit network of ontologies.Ontology Metadata Requirements. As a result of a systematic survey of the state of the art in the area of ontology reuse, we have elaborated an inventory of requirements for the metadata model. Besides analytical activities, we conducted extensive literature research focused on theoretical methods Several aspects to be considered in ontology metadata representation are definitely similar to those of other more general metadata standards such as Dublin Core. Differences arise, however, if we consider the semantic nature of ontologies, which are much more than plain web information sources. The main requirements identified in this process are the following:Accessibility. Metadata should be accessible and processable for both humans and machines. Whereas the human-driven aspects are ensured by the usage of natural language concept names, the machine-readability requirement can be implemented by the usage of web-compatible representation languages (such as XML or Semantic Web languages, see below). Furthermore, having metadata in processable format will facilitate the implementation of tools that use or manage ontology-related metadata (e.g., ontology changes). Usability. A metadata model should (1) reflect the needs of the majority of ontology users, as reported by existing case studies in ontology reuse, but at the same time (2) allow proprietary extensions and refinements in particular application scenarios (e. g., ontology change management). From a content perspective, usability can be maximized by taking into account multiple metadata types, which correspond to specific viewpoints on the ontological resources and are applied in various application tasks. Despite the broad understanding of the metadata concept and the use cases associated to each definition, several key aspects of metadata information have already been established across computer science fields (NISO 2004):• Structural metadata relate to statistical measures on the graph structure underlying an ontology. In particular, we mention the number of specific ontological primitives (e.g., number of classes and individuals). The availability of structural metadata influences the usability of an ontology in concrete application scenarios, because size and structure parameters constrain the type of tools and methods that are applied to aiding the reuse process. For instance, as it has been analyzed in the past (e.g., Interoperability. Similar to the ontology it describes, metadata information should be available in a form that facilitates metadata exchange among applications.While the syntactical aspects of interoperability are covered by the usage of standard representation languages (see 'Accessibility'), the semantic interoperability among machines handling ontology metadata information can be ensured by means of a formal and explicit representation of the meaning of the metadata entities (by conceptualizing the metadata vocabulary itself as an ontology).OMV Overview
This section presents the ontology metadata vocabulary (OMV); the first part provides an overview of the core design principles applied to the development of the OMV metadata model; then, we describe in detail the core of such a model; next, we present implementation and practical aspects; finally, we provide an introduction to the OMV extensions.Core and Extensions
Following the usability constraints identified during the requirements analysis, we decided to design the OMV schema modularly, distinguishing between the OMV core and various OMV extensions. The former captures information that is expected to be relevant to the majority of ontology reuse settings. However, in order to allow ontology developers and users to specify task-or application-specific ontology-related information, we allowed for the development of OMV extension modules, which are separated from the core schema while remaining compatible to it. That is, the terms are supposed to mean the same thing in the core and the extensions. Essentially, extensions reuse the core knowledge and provide specialized information for different ontology aspects.Metadata Organization and Categorization
In the following, we present the organization and categorization of metadata (entities) in two dimensions, which provide a structured overview of the OMV ontology:Property Appropriation. We organize metadata entities according to the impact on the prospected reusability of the described ontological content as presented in the following list:• Required -mandatory metadata elements. Any missing entry in this category leads to an incomplete description of the ontology. • Optional -important metadata facts, but not strongly required.• Extensional -specialized metadata entities, which are not considered to be part of the core metadata schema.Property Categorization. Orthogonal to the previous classification, we organize the metadata elements according to the type and purpose of the information contained as follows:• General -elements providing general information about the ontology.• Availability -information about the location of the ontology (e.g., its URI or the URL where the ontology is published on the web). • Applicability -information about the intended usage or scope of the ontology. • Format -information about the physical representation of the resource.In terms of ontologies, these elements include information about the representation language(s) in which the ontology is formalized. • Provenance -information about the organizations contributing to the creation of the ontology. • Relationship -information about relationships to other resources. This category includes versioning, as well as conceptual relationships such as extensions, generalization/specialization, and imports. • Statistics -various metrics on the underlying graph topology of an ontology (e.g., number of classes). • Other -information not covered in the categories listed above.Note that the classification dimensions introduced above (appropriation and categorization) are intended to be considered when implementing several metadata support facilities. The first dimension is relevant for a metadata creation service, since it ensures a minimal set of useful metadata entries for each of the described resources. The second can be used in various settings, mainly to reduce the userperceived complexity of the metadata schema, whose elements can be structured according to the corresponding categories.OMV Core Metadata Entities
The main classes and properties of the OMV ontology are illustrated in Fig. Besides the main class Ontology, the metadata model contains elements describing various aspects related to the creation, management, and usage of an ontology. We will briefly discuss these in the following text. In a typical ontology engineering process, person(s) or organization(s) develop ontologies. We group these two classes under the generic class Party by a subclass-of relation. A Party can have several locations by referring to a Location individual and can create and contribute to ontological resources, i.e., Ontology class. Review details and further information can be captured in an extensional OMV module. Further on we provide information about the engineering process the ontology originally resulted from in terms of the classes OntologyEngineeringMethodology, OntologyEn-gineeringTool, and the attributes version, status, creationDate, and modificationDate. Again these can be elaborated as an extension of the core metadata schema. The usage history of the ontology is modeled by classes such as the OntologyTask and LicenceModel. The scheme also contains a representation of the most significant intrinsic features of an ontology. Details on ontology languages are representable with the help of the classes OntologySyntax, OntologyLanguage, and KnowledgeRepresenta-tionParadigm. Ontologies might be categorized along a multitude of dimensions. One of the most popular classifications differentiates among application, domain, core, task, and upper-level ontologies. A further classification relies on their level of formality and types of Knowledge Representation (KR) primitives supported, introducing catalogs, glossaries, thesauri, taxonomies, frames, etc., as types of ontologies. The former categories can be modeled as individuals of the class OntologyType, while generic formality levels are introduced with the help of the class FormalityLevel. The domain the ontology describes is represented by the class OntologyDomain that references a predefined topic hierarchy such as the DMOZ hierarchy. Further content information can be provided as values of the attributes description, keywords, and documentation. Moreover, the metadata schema provides information about the imported ontologies (useImports) and versioning relations (hasPriorVersion, isBackwardCompatibleWith, and isIncompatibleWith) -analogously to the OWL ontology properties. Finally, OMV gives an overview of the graph topology of an Ontology with the help of several graph-related metrics represented as integer values of the attributes numberOfClasses, numberOfProperties, numberOfAxioms, and numberOfIndividuals.Ontological Representation
Following the accessibility and interoperability requirements, as well as the nature of the metadata, which are intended to describe ontologies, the conceptual model designed in the previous steps was implemented in OWL2Additionally, a metadata element is modeled either by means of classes and individuals or by means of valued properties. The former alternative, represented using additional classes linked by object properties, was chosen to model those metadata elements representing entities that can be referred to. The latter alternative, represented using datatype properties, was chosen to model metadata elements with value/content that can be easily mapped to conventional data types (numerical, literal, list values).Finally, OMV implements the appropriate properties of metadata entities by different means: The required and optional metadata entities are implemented in OMV core with the appropriate cardinality restrictions, while the extensional metadata entities are implemented in the different OMV extensions.OMV Extensions
The OMV core metadata is intended to evolve toward a commonly agreed schema for Semantic Web ontologies. In contrast to this ambitious goal, we are aware that for specific domains, tasks, or communities, extensions in any direction might be required. These extensions should be compatible to the OMV core, but at the same time, they should fulfill the requirements of a domain-, task-, or community-driven setting.The character of an OMV extension is a metadata ontology itself that imports the OMV core ontology. There are no restricting modeling guidelines to be met. However, developers are encouraged to follow the design principles described above (see Some of the existing OMV extensions were developed in collaboration with different institutions. The available extensionsUses and Benefits
OMV plays an important role in the ontology reuse task by facilitating the discovery and exchange of ontologies, fostering the widespread dissemination of ontology-driven technologies and the development of full-fledged ontology repositories and registries on the web. Furthermore, applications that work with the creation or (re)use of ontologies can benefit from having a standard schema for ontology metadata. By using the same vocabulary to describe ontology metadata, applications can exchange this information easily.Several applications are already using OMV to describe ontology metadata. In this section, we present a selection of these applications that use OMV at various stages of the ontology development life cycle. First, the NeOn ToolkitOyster Moreover, Oyster uses ontologies extensively to provide its main metadata management functions (registry metadata, formulating queries, routing queries, and processing answers). The ontology metadata entries are aligned and formally represented according to two ontologies: (1) the OMV that describes the properties of the ontology and (2) a topic hierarchy to define the domain of the ontology (c.f. Sect. 4.3).NeOn Applications. The NeOn Toolkit includes different OMV-related plugins. The Oyster-API plugin enables programmatic access to all Oyster registry functionalities within any other NeOn Toolkit plugin. This plugin can either use a local or remote Oyster instance. Similarly, the Oyster-GUI plugin provides a graphical user interface to interact with Oyster servers and other OMV-enabled servers (e.g., Centrasite) implementing the OMV-based web service. This plugin allows submitting, updating, and removing instances of OMV core classes, submitting queries to search ontologies based on different criteria and importing from the Internet ontologies matching the search criteria. Furthermore, the change-capturing plugin implements methods and strategies for the capturing and synchronization of ontology changes that are formally represented as instances of the change ontology (an OMV extension). This OMV extension is also used by several other plugins, such as Cicero plugin (to enable discussions on changes), Evolva plugin (to represent the changes proposed by the plugin based on background knowledge) and GATE Web service plugin (to represent the changes generated from textual sources).Additionally, the Cupboard system produced in NeOn for ontology publishing, sharing, and reuse also relies on OMV to implement some of its features. Besides letting users add their ontologies in a personal space -hosting, indexing, linking, and exposing them through APIs and SPARQL -Cupboard is designed to be a community tool. It helps ontology users and practitioners (including ontology developers) in finding and reusing ontologies, through the use of rich ontology metadata (thanks to Oyster and OMV) and advanced ontology review mechanisms.Finally, the latest update produced in NeOn of the collaborative ontology design ontology (C-ODO), called codolight (c.f. Sect. 4.4), has been aligned with OMV. Compared to the original C-ODO ontology design metamodel, codolight is now linked to requirements and application tasks, has been used for tool descriptions, is aligned to external vocabularies, is lighter in complexity, and improves association between the social and software layers of ontology design aspects. From a design viewpoint, the metadata provided by OMV have a semantics that is potentially compatible to that of other metamodels, and this alignment helps with metadata interoperability.Protege Plugin. The Protege MetaAnalysis plugin calculates various metadata for ontologies and facilitates the export of those metadata to the OMV. The plugin is a tab widget consisting of four panels: the numbers panel, the design panel, the OWL panel, and the extras panel. The plugin computes metadata for a given ontology and displays them in these panels. The ontology metadata can be exported to an extension of the OMV. If the ontology already exists in OMV, the metadata for that ontology are updated. Otherwise, a new instance of the ontology is created in OMV and populated with the computed metadata.BioPortal. While Oyster is a distributed ontology repository, BioPortal is a centralized repository of biomedical ontologies, where authors submit their ontologies. As part of the submission process, authors also fill in the form to describe their metadata. In the future, it is planned to add the capability for the authors simply to point to the location of an OWL file that has the OMV individuals and to have BioPortal import the information from that file.BioPortal uses the ontology metadata in ontology search and navigation. Users can specify, e.g., whether they want their search term to appear only in concept definitions or in metadata as well. BioPortal will also use OMV extensions. For example, one of the functions of BioPortal is to be a repository of mappings between concepts in biomedical ontologies. Each mapping comes with its own set of metadata (e.g., the mapping author, the algorithms used, the application context in which the mapping is valid, etc.) (Fridman Another feature of BioPortal is the use of peer reviews for ontology evaluation. Ontology users can rate BioPortal ontologies along different dimensions, such as coverage and degree of formality, based on their experience with the ontology in their own applications The Watson Semantic Web gateway contains a repository of ontologies and provides export of their metadata in the OMV format. When Watson users search for ontologies, they can click on an ontology URI from the search results, and then on the overview page for that, click on "Get OMV" for the metadata export.OMEGA is an algorithm that addresses the problem of populating metadata elements Linguistic Information Repository (LIR)
The symbiosis between ontologies and natural language has proven more and more relevant in the light of the growing interest and use of Semantic Web technologies. Ontologies that are well-documented in a natural language not only provide humans with a better understanding of the world model they represent, but also a better exploitation by the systems that may use them. This "grounding in natural language" is believed to provide improvements in tasks such as ontology-based information extraction, ontology learning, and population from text or ontology verbalization Nowadays, there is a growing demand for ontology-based applications that need to interact with information in different natural languages, i.e., with multilingual information. This is the case of numerous international organizations currently introducing semantic technologies in their information systems, such as the Food and Agriculture Organization or the World Health Organization, to mention just a few. Such organizations have to manage information and resources available in more than a dozen of different natural languages and have to customize the information they produce to a similar number of linguistic communities.For all these reasons, solutions have to be provided to model multiple natural language descriptions in ontologies. Such an undertaking needs to consider several requirements imposed by the characteristics of the domain of knowledge modeled in the ontology and by the type of linguistic descriptions that are required by the final application.Requirements. Although the number of multilingual ontologies is still quite small compared with the total amount of ontologies available in the web• Including multilingual labels in the ontology model • Combining the ontology model with a mapping model between different natural languages or a common interlingua • Associating the ontology model with an external linguistic modelThe first modeling option relies on the RDF(S) and OWL properties rdfs: label and rdfs:comment to associate word forms and descriptions to ontology elements. The main disadvantage of this option is that it is not possible to define any relation among the linguistic annotations, so that the linguistic information is restricted and the model is difficult to scale. The second option assumes the existence of several ontologies in the same domain with labels expressed in different natural languages, which are mapped to each other in a pairwise fashion, or through a common conceptualization or interlingua. This option has been considered in projects such as EuroWordNet Whereas some models have been explicitly designed to enrich ontologies with linguistic information, such as the ones mentioned above, they mainly focus on morphosyntactic descriptions of ontological entities and have not handled multilingualism issues, as the ones that arise when aiming at reusing the same ontology in different linguistic and cultural settings. This is particularly relevant in the case of ontologies that represent categorizations of reality that are not completely valid for all the cultures and languages involved. In this context, we have to consider the possibility of providing relations among the linguistic descriptions in different languages associated to the same ontology elements.Finally, we refer to the need for encoding the linguistic descriptions captured in the linguistic model according to standard models in order to guarantee interoperability, reuse, and commitment to best practices. The potential integration of terminological and lexical knowledge bases into our model requires interoperability with existing and proposed standards. In this sense, we have analyzed some standardization initiatives that have been developed in order to capture linguistic information that can be reused for various purposes. As the most important initiatives, we mention a number of standards from the International Organization for Standardization (ISO) and the World Wide Web Consortium (W3C) that capture terminological and lexical information. We are referring to Terminological Markup Framework (TMF) In particular, in the case of the linguistic model, this allows the existence of a complex model that contains as much linguistic information as required by the final application and, additionally, in different languages. Localization: the capability for providing a subset of linguistic descriptions to account for the linguistic realization of an ontology in different natural languages and representing term variants within one language and cultural specificities among different languages. Interoperability: the flexibility of interoperating with existing standards for the representation of lexical and terminological information. By interoperating with standard models, there also exists the possibility for the model of interchanging knowledge with the standards and being extended with further linguistic description elements, if so required by the final application. Accessibility: the fact of being implemented in a syntax or representation language that can provide tool support available to manage it, as well as access to external resources from which information can be obtained to semi-automatically support the model.LIR Overview
This section presents the Linguistic Information Repository or LIR, a model that has been created with the twofold purpose of fulfilling the needs of portability and association of multilingual information to domain ontologies, on the one hand, and adapting ontologies to the needs of the languages involved in the localization activity, on the other.The LIR has been implemented as an ontology in OWL. Its main purpose is not to provide a model for a lexicon of a language but to cover a subset of linguistic description elements that account for the linguistic realization of a domain ontology in different natural languages. A complete description of the current version of the LIR can be found in The lexical and terminological information captured in the LIR is organized around the LexicalEntry class. Lexical entry is considered a union of word form (Lexicalization) and meaning (Sense). This ground structure has been inspired by the Lexical Markup Framework (LMF). The compliance with this standard is important for two main reasons: (a) Links to lexicons modeled according to this standard can be established, and (b) the LIR can be flexibly extended with modular extensions of the LMF (or standard-compliant) modeling specific linguistic aspects, such as deep morphology or syntax, not dealt by LIR in its present stage. For more details on the interoperability of the LIR with further standards see The rest of the classes that make up the LIR are Language, Definition, Source, Note, and UsageContext (see Fig. Uses and Benefits
The main benefit of the LIR model is that it provides a very granular specification of relationships between elements of an ontology. In particular, it identifies well-defined relationships among the linguistic descriptions used to represent ontological concepts, specifically:• Well-defined relations within lexicalizations in one language • Well-defined relations within lexicalizations across languages Both cases are illustrated in the following. The example in Fig. The second example highlights the possibility given by the LIR model to represent scientific names and use them across languages (scientific names are in Latin and are internationally accepted over scientific communities). Variants in the same language (e.g., Buffaloes (syncerus)) can therefore be connected to the same scientific term, such as the English and Japanese translations. We have illustrated in Fig. Then, we have a lexicalization in Latin that represents the scientific name, and it is accordingly related with the rest of lexical entries by means of the object property hasScientificName. Finally, 04:LexicalEntry belongs to the Japanese language, which is also the common denomination in Japanese of the Syncerus caffer scientific name and, at the same time, the translation of the two lexicalizations in English.To conclude, we refer to the LabelTranslator NeOn plugin, a translationsupporting tool Collaborative Ontology Design Ontology (C-ODO) Light
Authoring and maintaining Semantic Web ontologies is generally not an individual, monolithic activity but is intrinsically grounded on social and collaborative processes, more so when ontologies are configured in a networked architecture. Continuous interaction between knowledge engineers and domain experts is key and so is that with resource providers whenever reuse or re-engineering enters the life cycle.However, without dedicated tool support, collaboration may occur across general-purpose software tools and communication channels, in which case a manual effort is required to coordinate and bring the outcome of these activities together. Thus, on one hand, tool support to ontology engineering activities (e.g., reusing existing ontologies and design patterns; re-engineering thesauri, lexica, and database schemas; validating the outcome) is required. On the other hand, tools are often unable to support these activities in a collaborative setting, e.g., aiding the discussion and consensus-based assessment of an ontology element and the rationale behind it. Among other reasons, this can also be ascribed to an inadequate requirement analysis describing the actual processes and data involved therein, and the lack of a conceptual framework that formally expresses these notions so that they can be unified and reasoned upon.C-ODO Light Overview
C-ODO Light (aka codolight) is one such formal knowledge framework. It is a pattern-based OWL-DL ontology network that provides a metamodel for describing collaborative ontology projects In particular, the network displays the following features:1. The ability to formalize ontology design tool descriptions in terms of input/ output data (knowledge types), functionalities, interface objects, and interaction patterns 2. Smooth integration between human-oriented and tool-oriented descriptions of ontology design aspects 3. Alignment to existing vocabularies such as DOAP, OMV, etc. 4. Light axiomatization, e.g., no use of anonymous classes in restrictions 5. Modular development by pattern-based design (cf. Chap. 3), in compliance with the ontologydesignpatterns.org practices Additionally, the codolight core is extended to support specific ontology application tasks, such as:1. Browsing semantic data about ontology projects, tools, data, repositories, solutions, discussion, evaluation, etc. 2. Searching and selecting design components based on design aspects, knowledge types, individual needs, user profiles, etc. 3. Creating design configuration interfaces that aid or automate task 2 4. Help collecting ontology requirements, design functionalities, and ontology application tasks for an ontology project 5. Providing a shared network of vocabularies to create/query/reason on annotations and data related to ontology projects, including integration between annotations of heterogeneous provenance, such as those coming from collaborative discussions and changeIt is here anticipated that part of these tasks are implemented within NeOn in the form of the Kali-ma tool, to be described in Chap. 15.Structure
The C-ODO Light network of ontologies is organized as a layered architecture, where these layers are connected with different types of bindings. Besides the two bottom layers that define the main structure, there are three additional layers that bridge it with existing applications, vocabularies, and functionalities:Pattern layer: It contains reusable content ontology design patterns (Content ODP) The corolla pattern minimizes dependencies and enforces loose coupling between the modules of an ontology network: In the codolight core example, all modules, but codinteraction (described below), directly depend on the kernel module exclusively. Also, the modules are built so that their structure suggests an organization of the network, by which different aspects of the domain of interest are represented by each petal module. The criterion by which an ontology network can be broken apart into a corolla can be: What are the main aspects of the domain described by the ontology network?The kernel module defines core concepts, shared by all aspects. As shown in Fig. All classes defined in the kernel module are specializations of classes from the pattern layer (such as DesignFunctionality subsuming Task from the taskrole patternThe following petal modules are defined for codolight:Data (coddata): contains the main notions that classify the data managed when designing an ontology: ontologies, ontology elements, Knowledge Organization Systems (KOS), KOS elements, rules, modules, encoding syntaxes, and more. For each class of knowledge resources, a knowledge-type instance is provided. Projects (codprojects): contains the minimal vocabulary for representing ontology design projects and their executions. An ontology project is here taken as a social entity, whose computational counterpart (e.g., a project created in the NeOn Toolkit) is a software entity that collects resources and descriptions related to an ontology project. Workflows (codworkflows): contains classes and properties to represent workflows from within ontology projects: collaborative workflows, accountable agents, need for an agent or a design functionality, etc. Argumentation (codarg): contains the basic classes and properties to represent argumentation concepts: arguments, threads, ideas, positions, rationales, etc. Solutions (codsolutions): contains classes and properties to represent ontology design solutions: competency questions, ontology design patterns, ontology requirements, unit tests, etc. Tools (codtools): contains classes and properties to represent ontology design tools: tools, pieces of code, code entities, computational tasks, input and output data relations, etc. Interfaces (codinterfaces): contains classes and properties that represent some typical user interface entities, such as interface objects, panes, and windows. Interaction (codinteraction): contains classes and properties that represent some typical entities related to human-computer and human-ontology interaction, e.g., user types, computational tasks, and workflows. These can in turn be combined so as to construct interaction pattern models.One advantage of employing this aspect-oriented architecture is selective extensibility. A software application intended to exploit only a given subsystem of ontology life cycle management, such as reasoning on the usage of interaction patterns and user interface widgets, can import only the codinterfaces and codinteraction modules. Possibly, they can also extend these modules with ontologies that model further additional interaction patterns or GUI elements originally not intended for the application domain at hand.Alignments
This section provides an insight on some alignments that hold between codolight and other vocabularies that are widely used on the Semantic Web or are introduced as part of the methodology described by this book (c.f. Chap. 2), such as OMV that is described in this very chapter.OWL• The original RDF, RDFS, and OWL vocabularies from the W3C • The OWL1 metamodel designed for NeOn • The OWL2 metamodel also designed for NeOnThe reason why so many different vocabularies represent entities from a same language is mainly due to the pragmatic evolution of semantic technologies. The original vocabularies by W3C are not extremely detailed in distinguishing the constructs available in OWL (and RDF, RDFS); e.g., it is difficult to describe existential restrictions explicitly, because these are just instances of owl: Restriction. On the other hand, W3C vocabularies are implemented in all APIs and tools for ontology engineering, so in order to maximize interoperability, an ontology design vocabulary like codolight must be aligned to the main data vocabularies. The OWL metamodels developed in NeOn try to overcome the referential coarseness of OWL constructs, e.g., by providing the class owlodm1:ExistentialRestriction. On the other hand, these metamodels are not intended to be a replacement for the W3C OWL data model.Ontology Metadata Vocabulary (OMV)Description of a Project (DOAP) NeOn Access Rights ModelSoftware Ontology Model (SOM) Conclusions
A methodology for managing ontology networks is best designed if formal models of the resources and processes involved come along with it. To that end, the NeOn Methodology proposes three stand-alone models, i.e., the OMV, LIR, and C-ODO Light ontologies, that can nonetheless be interconnected in order to represent and reason on the structural, linguistic, and engineering aspects of ontology life cycle. Ontology alignments are provided across these models to ensure logic interoperability, without hampering their stand-alone usage possibilities. These ontologies also serve as a back end for software applications provided as plugins for the NeOn Toolkit (see For this reason, this chapter presents detailed methodological guidelines for specifying ontology requirements as part of the NeOn Methodology (see Chap. 2). Such methodological guidelines are based on the use of the so-called competency questions (CQs) (Gr€ uninger and Fox 1995) and are inspired by how methodologies for building ontologies propose to perform the ontology requirements specification activity Methodological Guidelines for Ontology Requirements Specification
The ontology requirements specification activity has a main goal to state why the ontology is being built, which its intended uses are, who the end users are, and what specific requirements the ontology should fulfill are. For specifying the specific ontology requirements, the competency questions technique proposed in (Gr€ uninger and Fox 1995) is used. Before identifying the set of competency questions, the purpose and scope of the ontology should be identified, as well as its level of formality, and its intended uses and end users.The NeOn Methodology framework for building ontology networks proposes the filling card, for the ontology requirements specification activity, as it is shown in Fig. The tasks for carrying out the ontology requirements specification activity can be seen in Fig. The tasks for carrying out the ontology requirements specification activity are explained in detail next.Task 1. Identifying the purpose, scope, and implementation language. The objective is to determine the main goal of the ontology, its coverage and foreseeable granularity, and its implementation language (e.g., OWL, RDFSImplementation language
The formal language that the ontology should have 4Intended end usersThe intended end users expected for the ontology 5Intended usesThe intended uses expected for the ontology 6Ontology requirements (a) Non-Functional requirements The general requirements or aspects that the ontology should fulfill, including optionally priorities for each requirement (b) Functional requirements: Groups of competency questionsThe content specific requirements that the ontology should fulfill, in the form of groups of competency questions and their answers, including optionally priorities for each group and for each competency question 7Pre-glossary of terms (a) Terms from competency questions The list of terms included in the competency questions and their frequencies (b) Terms from answersThe list of terms included in the answers and their frequencies (c) ObjectsThe list of objects included in the competency questions and in their answersThe task output is a list containing the intended end users of the ontology to be built; the list is included in slot 4 of the template shown in Table Task 3. Identifying the intended uses. The development of an ontology is mainly motivated by scenarios related to the application that will use the ontology. The goal of this task is to obtain the intended uses and use scenarios of the ontology. The ontology development team holds a set of interviews with the users and domain experts in order to carry out this task, taking as input a set of ontological needs; the purpose here is to obtain the uses of the ontology within the application, and to have a general idea of the application requirements, in terms of knowledge to be represented.The task output is a list of intended uses in the form of scenarios, which is included in slot 5 of the template shown in Table Task 4. Identifying requirements. The goal of this task is to acquire the set of requirements that the ontology should satisfy. Ontology requirements, similar to software requirements• Non-functional ontology requirements refer to the characteristics, qualities, or general aspects not related to the content that the ontology should represent.Examples of non-functional requirements are (a) whether the terminology to be used in the ontology must be taken from standards, (b) whether the ontology must be multilingual, or (c) whether the ontology should be written following a specific naming convention. • Functional ontology requirements, which can be seen as content specific requirements, refer to the particular knowledge to be represented by the ontology and the particular terminology to be included in the ontology. In the SEEMP case study (Villazo ´n-Terrazas et al. 2011), for example, the knowledge and the terminology are about curriculum vitae with candidate skills, education level, expertise, previous work experience, or about job offers with information on job location, salary, etc.The ontology development team should interview the users and domain experts, taking as input a set of ontological needs, and they should obtain as result the initial set of ontology requirements (non-functional and functional) of the ontology to be built. To identify functional requirements, they use as main technique the writing of the requirements in natural language in the form of the so-called CQs. They can use mind map tools • Top-down: The team starts with complex questions that are decomposed in simpler ones. • Bottom-up: The team starts with simple questions that are composed to create complex ones. • Middle out: The team starts just writing down important questions that are composed and decomposed later on to form abstract and simple questions, respectively.The output of this task is (1) a list of non-functional ontology requirements written in natural language, which is included in slot 6a of the template shown in Table Task 5. Grouping functional requirements. The goal of this task is to group into several categories the list of functional ontology requirements in the form of CQs and their associated answers obtained in Task 4. The users, the domain experts, and the ontology development team should classify the list of CQs written in natural language with a hybrid approach that not only combines preestablished categories such as time and date, units of measure, currencies, location, languages, etc., but also creates categories for those terms that appear with the highest frequencies in the list of CQs.Techniques such as card sorting can be used when the grouping is done manually. In addition, mind map tools can help to display graphically and in groups the CQs; or Cicero if the grouping is done collaboratively.The task output is the set of groups of functional requirements in the form of CQs and their associated answers, which is included in slot 6b of ORSD template shown in Table Usually this task is carried out in parallel with Task 4. To group CQs is useful because it permits to identify the essential parts to be covered by the ontology. The different groups of CQs will be used by the ontology development team for developing the ontology with a modularization approach. In addition, such groups can be used to organize the development in a collaborative fashion in which different teams are in charge of a set of CQs groups.Task 6. Validating the set of requirements. The aim here is to identify possible conflicts between ontology requirements, missing ontology requirements, and contradictions between them. Users, domain experts, and ontology developers must carry out this task taking as input the set of requirements identified in Task 4 (it includes both non-functional and functional requirements) to decide if each element of the set is valid or not.The task output is the confirmation of the validity of the set of non-functional and functional ontology requirements.The criteria that can be used in this validation task and that are mainly inspired by Priorities will be used by the ontology development team for planning and scheduling the ontology development and for deciding which parts of the ontology are going to be developed first. This task is optional, but recommended. In fact, if no priorities are given to the groups of CQs, ontology developers will start modeling the ontology without any guidance regarding the functional requirements that should be implemented first; in this case, the waterfall ontology life cycle model should be selected during the scheduling of the ontology project. On the contrary, if different priorities have been assigned to functional ontology requirements, the iterative-incremental ontology life cycle model should be selected in the scheduling activity.Task 8. Extracting terminology and its frequency. The goal of this task is to extract a pre-glossary of terms with their frequencies from the list of CQs and their answers identified in Task 4. The ontology development team carries out this task using terminology extraction techniques and tools supporting such techniques.This pre-glossary of terms is divided in three different parts: terms from the CQs, terms from their answers, and terms identified as named entities.• From the requirements in the form of CQs, the ontology development team should extract terminology (names, adjectives, and verbs) that will be formally represented in the ontology by means of concepts, attributes, relations, or instances (in the case of named entities). • From the answers to the CQs, the ontology development team should extract terminology that could be represented in the ontology as concepts or as instances. • From both CQs and corresponding answers, the ontology development team should extract named entities such as countries or currencies, which are objects in the universe of discourse.The output is included in the slots 7a, 7b, and 7c of the template shown in Table The set of terms with higher appearance frequencies will be used later on for searching knowledge resources that could be potentially reused in the ontology development. The following heuristic can be applied: the set of more frequent terms is that requires more effort during the ontology development; for this reason, frequencies are important to know which knowledge resources allow to save more effort.Ontology Requirements Specification in the Semantic
Nomenclature Case Study This section provides one example of how to use the guidelines proposed for the ontology requirements specification activity and what results are expected from any of the tasks detailed in the guidelines. The example shows an excerpt of the ORSD obtained after performing the ontology requirements specification activity following the methodological guidelines proposed in this chapter.The example presented refers to the requirements specification of the ontology network developed within the Semantic Nomenclature case study (see The main objectives of the Semantic Nomenclature case study were (a) helping in the systematization of creating, maintaining, and keeping up-to-date drugrelated information, and (b) allowing an easy integration of new drug resources. In order to do that, the case study tackles the engineering of a pharmaceutical product ontology network implemented in OWL based on the nomenclature of products in the pharmaceutical sector in Spain. This ontology network represents the general aspects of the main terms and objects related to drugs, and it classifies these pharmaceutical terms according to the Anatomical Therapeutic Chemical (ATC)Next we described the tasks followed for the ontology requirements specification activity within the Semantic Nomenclature case study based on the methodological guidelines proposed in this chapter.Task 1. Identifying purpose, scope, and implementation language. The development of the Semantic Nomenclature ontology network is motivated by scenarios related to the end-user application that will use the ontology network. Such scenarios describe a set of the ontology requirements that the ontology should satisfy after being formally implemented. The motivating scenarios are described in Task 2. Identifying the intended end users. The analysis of the motivating scenarios described in (Go ´mez-Pe ´rez et al. 2006) allowed ontology developers to identify the following intended end users of the ontology:• User 1. Pharmacists who navigate across the ontology searching for drug information. • User 2. GSCoP (General Spanish Council of Pharmacists) technicians who navigate across the ontology network and search for information or relations about a given concept (drug, active ingredient, etc.). GSCoP technicians also extract the latest information from different sources and update their database.• User 3. Spanish government analysts who study the situation of the pharmaceutical product information in the Spanish market or update the content.Task 3. Identifying the intended uses. The analysis of the motivating scenarios described in • Use 1. To search for updated information about the characteristics of pharmaceutical products • Use 2. To connect heterogeneous pharmaceutical models • Use 3. To update pharmaceutical product information databases Task 4. Identifying requirements. The non-functional ontology requirement identified was:• NFR1. The ontology must support a multilingual scenario in the following languages: Spanish, Catalan, Basque, and Galician.For specifying the functional ontology requirements, the competency question technique was used. In addition, the bottom-up approach for identifying them was used because it was the more direct way to work with the domain experts. Competency questions were stored in an Excel file and then rewritten in a mind map tool as appears in Fig. In total, 61 competency questions were identified; they are described in detail in (Go ´mez-Pe ´rez et al. The criteria for grouping the competency questions were based on the identified uses, the identified users, and the domain experts' suggestions. Apart from the three aforementioned groups, ontology developers have created a new group, called Composite (CQG4), which includes the result of combining simple CQs to obtain more general and complex CQs.Figure Task 6. Validating the set of requirements. During the overall process, ontology developers received recommendations, suggestions, and advice from the domain experts, and they iterated several times until the final approval by the end users was achieved. Domain experts and ontology developers used the following criteria for validating the set of requirements (both non-functional and functional requirements):• Correctness. Domain experts and ontology developers checked the correctness of each non-functional requirement and of each competency question, verifying that its formulation and answers were correct. • Consistent. Domain experts also verified that the non-functional requirements and the competency questions did not have any possible inconsistency.Task 7. Prioritizing requirements. Within the Semantic Nomenclature case study, ontology developers did not carry out this step. This means the first version of the ontology network must be able to represent the knowledge contained in all the competency questions and be able to cover all the nonfunctional requirements.Task 8. Extracting terminology and its frequency. From the competency questions and their answers, ontology developers manually extracted the terminology that will be formally represented in the ontology network by means of concepts, attributes, and relations. In addition, ontology developers identified the terms and the objects in the universe of discourse. Examples of the terms identified are shown in Table After following these tasks, the output of the ontology requirements specification activity is the ontology requirements specification document. An excerpt of this document, which has been written for this chapter, is shown in Table The ontology has to focus just on the Spanish and European pharmaceutical domain 3 Implementation languageThe ontology has to be implemented in OWL Conclusions
One of the critical activities when developing ontologies is to identify their functional and non-functional requirements. In this chapter, the ontology requirements specification activity has been systematized by proposing detailed and prescriptive methodological guidelines for specifying ontology requirements, based on CQs, and by providing a template for writing the ontology requirement specification document (ORSD). The ORSD will play a key role during the ontology development process because it facilitates different activities. In that sense, it will be shown in later chapters that the ontology requirements specification document (1) is a crucial input for the scheduling of ontology development projects (see Chap. 14) and ( Chapter 6
Reusing and Re-engineering Non-ontological Resources for Building Ontologies Boris Villazo ´n-Terrazas and Asuncio ´n Go ´mez-Pe ´rez Abstract With the goal of speeding up the ontology development process, ontology developers are reusing as much as possible available ontological and nonontological resources such as classification schemes, thesauri, lexicons, and folksonomies, that have already reached some consensus. The reuse of such nonontological resources necessarily involves their re-engineering into ontologies. Based on this new trend, this chapter presents a general method for re-engineering non-ontological resources into ontologies, taking into account that non-ontological resources are highly heterogeneous in their data model and contents. The method is based on the so-called re-engineering patterns, which define a procedure that transforms the non-ontological resource components into ontology representational primitives. This chapter also presents the description of a software library that implements the transformations suggested by the patterns. Finally, the chapter depicts an evaluation of the method.Introduction and Motivation
Research on ontology engineering methodologies has provided methods and techniques for developing ontologies from scratch. Well-recognized methodological approaches such as METHONTOLOGY (Go ´mez-Pe ´rez et al. During the last decade, specific methods, techniques, and tools were proposed for building ontologies from available knowledge resources. First, ontology learning methods and tools were proposed to extract relevant concepts and relations from structured, semi-structured, and non-structured resources (Go ´mez-Pe ´rez and Manzano-Macho 2004; The literature presents a wide set of methods and tools for the ontologization of non-ontological resources. This ontologization of resources has led to the design of several specific methods, techniques, and tools The analysis of the ontologies developed by distinct research groups in different international and national projects have revealed that there are different alternative ways or possibilities to build ontologies by reusing and re-engineering the available knowledge resources used by a particular community. However, at this stage, we can state that all the projects perform an ad hoc transformation of the resources available for building ontologies.Therefore, a new ontology development paradigm started approximately in 2007, whose emphasis was on the reuse and possible subsequent reengineering of knowledge resources, as opposed to custom-building new ontologies from scratch. However, in order to support and promote such reuse-based approach, new methods, techniques, and tools are needed.The remainder of the chapter is organized as follows: Section 6.2 presents our categorization of non-ontological resources. Then, Sect. 6.3 describes the methodological guidelines for reusing non-ontological resources. Next, Sect. 6.4 provides the pattern-based method for re-engineering non-ontological resources into ontologies. Section 6.5 introduces the technological support for our re-engineering method. Then, Sect. 6.6 describes an example of the methodological guidelines presented here. Finally, Sect. 6.7 presents the conclusions and future work.Types of Non-ontological Resources
The knowledge resources, reused in several projects for building ontologies, contain readily available a wealth of category definitions and reflect some degree of community consensus. In this chapter, we refer to non-ontological resources (NOR)Our analysis of the literature has revealed different ways of categorizing nonontological resources. Thus, Therefore, one of the contributions of this chapter is the categorization of NORs, according to the following three features presented in Fig. According to the type of NORs, we classify them into:• Glossaries: A glossary is an alphabetical list of terms or words found in or related to a specific topic or text. It may or may not include explanations, and its vocabulary may be monolingual, bilingual, or multilingual (Wright and Budin 1997). An example of glossary is the FAO Fisheries GlossaryThesauri are mainly used for indexing and retrieving articles in large databases (ISO 2788). An example of thesaurus is the AGROVOCThe knowledge encoded by the resource can be represented in different ways, known as data models. A data model Next we present several data models for classification schemes, shown in Fig. • Path enumeration • Record-based model (2) a term-term relationship entity, in which each record contains two different term codes and the relationship between them; and (3) a relationship source entity, which contains the overall resource relationships.Next we present a data model for lexica.• Record-based model According to the implementation, we classify NORs into:• Databases: A database is a structured collection of records or data stored in a computer system. • Spreadsheets: An electronic spreadsheet consists of a matrix of cells where a user can enter formulas and values. • XML file: EXtensible Markup Language is a simple, open, and flexible format used to exchange a wide variety of data on and off the web. XML is a tree structure of nodes and nested nodes of information where the user defines the names of the nodes. • Flat file: A flat file is a file usually read or written sequentially. In general, a flat file is a file containing records with no structured interrelationships.In summary, Fig. To exemplify the non-ontological categorization presented with a real life classification scheme, we use an excerpt from the FAO water area classification presented in (Fig. Moreover, we can map available non-ontological resources to our categorization. Next we present a brief list of them.• The United Nations Standard Products and Services Code, UNSPSC • UMLS12 12 is a very large, multipurpose, multilingual thesaurus that contains information about biomedical and health-related concepts. It is modeled with the record-based model and stored in a flat file. • MeSh 13 , the Medical Subject Headings, is a classification scheme, modeled with the path enumeration data model. • The Art and Architecture Thesaurus 14 is modeled with the record-based data model and implemented in XML. Methodological Guidelines for Reusing Non-ontological Resources
Once we have defined and categorized the non-ontological resources to be dealt with, we present the methodological guidelines for reusing them. The goal of the non-ontological resource reuse process is to choose the most suitable non-ontological resource for building ontologies. Domain experts, software developers, and ontology practitioners carry out this process by taking as input the ontology requirements specification document (ORSD)Activity 1. Search Non-ontological Resources
The goal of the activity is to search non-ontological resources from highly reliable web sites, domain-related sites, and resources within organizations. Domain experts, software developers, and ontology practitioners carry out this activity, taking as input the ORSD. They use the terms that have the highest frequency in the ORSD to search for the candidate non-ontological resources that cover the desired terminology. The activity output is a set of candidate non-ontological resources that may belong to any of the identified typologies described in Sect. 6.2.Activity 2. Assess the Set of Candidate Non-ontological Resources
The goal of the activity is to assess the set of candidate non-ontological resources. Domain experts, software developers, and ontology practitioners carry out this activity, taking as input the set of candidate non-ontological resources. We propose to consider the following measurable criteria: (1) coverage, (2) precision plus two subjective criteria, (3) qualityTask 2.1 Extract Lexical Entries
The goal of this task is to extract the lexical entries of the non-ontological resources.The task is carried out by software developers and ontology practitioners by taking as input the non-ontological resources and extracting their lexical entries with terminology extraction tools.Task 2.2 Calculate Precision
The goal of this task is to calculate the precision of the candidate non-ontological resources. Precision is a measure widely used in information retrieval (Baeza-Yates and Ribeiro-Neto 1999) and is defined as the proportion of retrieved material that is actually relevant. This task is carried out by software developers and ontology practitioners by taking as input the lexical entries extracted for the non-ontological resources and the terminology gathered in the ORSD. To adapt this precision measure into our context, we need to define:• NORLexicalEntries as the set of lexical entries extracted from the non-ontological resource • ORSDTerminology as the set of identified terms included in the ORSD Now we can define the precision, in our context, as the proportion of the lexical entries of the non-ontological resource that are included in the identified terms of the ORSD over the lexical entries of the non-ontological resource. This is expressed as follows:The goal of this task is to calculate the coverage of the non-ontological resources.Coverage is based on the recall measure used in information retrieval (Baeza-Yates and Ribeiro-Neto 1999). Recall is defined as the proportion of relevant material actually retrieved in answer to a search request. This task is carried out by software developers and ontology practitioners by taking as input both the lexical entries extracted from the non-ontological resources and the terminology gathered in the ORSD. To adapt this measure into our context, we use the aforementioned definitions of NORLexicalEntries and ORSDTerminology. In this context, coverage is the proportion of the identified terms of the ORSD that are included in the lexical entries of the non-ontological resource over the identified terms of the ORSD. This is expressed as follows:Evaluate the Consensus
The goal of this task is to evaluate the consensus of the non-ontological resources.Consensus is a subjective and not quantifiable criterion. This task is carried out by domain experts, taking as input the non-ontological resources for stating whether the non-ontological resources contain terminology agreed upon by the community or not. We propose a preliminary starting point for this evaluation. Domain experts have to check whether the resource is coming from:• A standardization body or any entity whose primary activity is to develop, coordinate, promulgate, revise, amend, reissue, or otherwise maintain standards; for example, the International Organization for Standardization (ISO), the American National Standards Institute (ANSI), and the World Wide Web Consortium (W3C) • Large organizations across national governments, such as the Food and Agriculture Organization of the United Nations (FAO), the World Health Organization (WHO), the United Nations Educational, Scientific and Cultural Organization (UNESCO), and the International Olympic Committee (IOC) • A large enough user community to make it profitable for developers to use it as a means of general interoperability Either the resource is coming from any of the aforementioned parties or not, domain experts may state that the resource has reached some degree of consensus.Task 2.5 Evaluate the Quality
The goal of this task is to evaluate the quality of the resource. We do not intend to provide a deep analysis of the quality of the resource but to offer some preliminary considerations about it. In this chapter, we propose to check the following quality attributes:• Good documentation of the resource.• Lack of anomalies of the non-ontological resource, such as redundancies or inconsistencies. • Reliability of the non-ontological resource. This means analyzing whether we can trust the resource or not.Task 2.6 Build the Assessment Table
The goal of this task is to create an assessment table of the non-ontological resources. Software developers and ontology practitioners carry out this task, taking as input the non-ontological resources with their respective values for precision, coverage, consensus, and quality criteria, for the construction of the assessment table. This table is shown in Table Activity 3. Select the Most Appropriate Non-ontological Resources
The goal of this activity is to select the most appropriate non-ontological resources to be transformed into an ontology. This activity is carried out by domain experts, software developers, and ontology practitioners, taking as input the non-ontological resource assessment table. The selection is performed manually and we recommend looking for resources with:• Consensus. This criterion is taken into account in the first place because if the resource to be reused contains terminology agreed upon by the community, the effort and time spent in finding out the right labels for the ontology terms will decrease considerably. • Quality. This criterion is taken into account in the second place because if the resource to be reused has an acceptable level of quality, then the resultant ontology should also have it. • High value of coverage. This criterion is taken into account in the third place because our third concern is to consider most of the ORSD terms identified. • High value of precision. This criterion is taken into account in the fourth place because our fourth concern is the proportion of non-ontological lexical entries over the identified terms of the ORSD.The activity output is a ranked list of non-ontological resources that, to some extent, covers the expected domain. These resources will be ready for the reengineering process.Methodological Guidelines for Re-engineering NORs into Ontologies
In this section, we depict the prescriptive methodological guidelines for reengineering NORs. The goal of the method for re-engineering non-ontological resources is to transform a non-ontological resource into an ontology. The output of the process is an ontology. Figure The NOR re-engineering process consists of the three activities depicted in Fig. Activity 1. Non-ontological Resource Reverse Engineering
The goal of this activity is to analyze a non-ontological resource, to identify its underlying terms, and to create representations of the resource at the different levels of abstraction (design, requirements, and conceptual).  Conceptual Abstraction
The goal of this task is to identify the schema of the non-ontological resource including the conceptual components and their relationships. If the conceptual schema is not available in the documentation, the schema should be reconstructed manually or with a data modeling tool.Task 1.3 Information Exploration
The goal of this task is to find out how the conceptual schema of the nonontological resource and its content are represented in the data model. If the nonontological resource data model is not available in the documentation, the data model should be reconstructed manually or with a data modeling tool.Activity 2. Non-ontological Resource Transformation
This activity has as a goal to generate a conceptual model from the non-ontological resource. We propose the use of patterns for re-engineering non-ontological resources (PR-NOR) to guide the transformation process.Task 2.1 Search for a Suitable Pattern for Re-engineering Non-ontological Resource
The goal of this task is to find out if there is any applicable re-engineering pattern that transforms the non-ontological resource into a conceptual model. The search is performed in the ODP Portal24Task 2.2.a Use Re-engineering Patterns to Guide the Transformation
The goal of this task is to apply the re-engineering pattern obtained in Task 2.1 (see Sect. 6.4.2.1) to transform the non-ontological resource into a conceptual model. If a suitable pattern for re-engineering non-ontological resources is found, then the conceptual model is created from the non-ontological resource following the procedure established in the pattern for re-engineering. Alternatively, the software library, described later in Sect. 6.5, can be used for generating the ontology automatically.Task 2.2.b Perform an Ad Hoc Transformation
The goal of this task is to set up an ad hoc procedure that transforms the non-ontological resource into a conceptual model when a suitable pattern for re-engineering cannot be found. This ad hoc procedure may be generalized to create a new pattern for reengineering non-ontological resources.Task 2.3 Manual Refinement
The goal of this task is to check whether any inconsistency appears after the transformation. Software developers and ontology practitioners, with the help of domain experts, can fix manually any inconsistencies generated from the transformation.Activity 3. Ontology Forward Engineering
The goal of this activity is to generate the ontology. We use the ontology levels of abstraction to depict this activity because they are directly related to the ontology development process. The conceptual model obtained in Task 2.2.a (Sect. 6.4.2.2) or 2.2.b (Sect. 6.4.2.3) is transformed into a formalized model, according to a knowledge representation paradigm such as description logics, first order logic, or F-logic. Then, the formalized model is implemented in an ontology language.Technological Support
Our technological support consists in (1) a PR-NOR pattern library that includes the set of patterns for re-engineering non-ontological resources and the implementation of (2) NOR 2 O, a software library that implements the transformation process suggested by the patterns. Patterns for Re-engineering Non-ontological Resources
In this section, we introduce the 16 patterns that perform the transformations of NORs into ontologies. Patterns for re-engineering NORs (PR-NOR) define a procedure that transforms the NOR terms into ontology representational primitives. Next, we present the template proposed that describes the patterns for reengineering non-ontological resources (PR-NOR). We have modified the tabular template used in Villazo ´n- According to the NOR categorization presented in Sect. 6.2, we propose patterns for re-engineering classification schemes, thesauri, and lexicons (see Table The re-engineering patterns take advantage of the use of the ontology design patterns 26 for creating the ontology code. So, most of the code generated follows the best practices already identified by the community (see section Process on Table Semantics of the Relations Among the NOR Terms
The TBox transformation approach converts the resource content into an ontology schema. TBox transformation tries to impose a formal semantics on the resource by making explicit the semantics hidden in the relations of the NOR terms. To this end, each NOR term is mapped to a class, and then, the semantics of the relations among those entities must be discovered and then made explicit. Thus, patterns that follow the TBox transformation approach must discover first the semantics of the relations among the NOR terms. To perform this task, we rely on WordNet, which organizes the lexical information into meanings (senses) and synsets. What makes WordNet remarkable is the existence of various relations explicitly declared between the word forms (e.g., lexical relations, such as synonymy and antonymy) and the synsets (meaning to meaning or semantic relations, e.g., hyponymy/hypernymy relation, meronymy relation). Here, we want to prove that we can rely on an external resource for making explicit the relations. For this purpose, first, we rely on WordNet, and then, as a future line of this work, we may rely on other information resources, such as DBpedia Algorithm 1 describes how to make explicit the semantics of the relations in the NOR terms. The abbreviation of the algorithm name is getRelation.NOR 2 O
This section presents NOR 2 O, a Java library that implements the transformation process suggested by the patterns for re-engineering non-ontological resources (PR-NOR). The library performs the ETL processNOR Connector
The NOR Connector loads classification schemes, thesauri, and lexicons modeled with their corresponding data models, and implemented in databases, XML, flat files, and spreadsheets.This module utilizes an XML configuration file for describing the NOR. An example of the XML configuration file is presented in Listing 6.1. The Listing shows how the file describes a thesaurus. The thesaurus has two schema entities, Term and NonPreferredTerm, is modeled following the record-based data model, and is implemented in XML.Listing 6.1 NOR Connector configuration file example
Scheme" name=" c e p a94 " <Nor type=" C l a s s i f i c a t i o n > <Schema> <S c h e m aE n t i t i e s> <SchemaEntity name=" CSItem"> <A t t r i b u t e name=" C S I d e n t i f i e r " valueFrom=" cepa . CodeNumber" type=" s t r i n g " /> <A t t r i b u t e name="CSName" valueFrom=" cepa . D e s c r i p t i o n E n g l i s h " type=" s t r i n g " /> <R e l a t i o n name=" subType" u s i n g=" PathEnumeration" d e s t i n a t i o n="CSItem"/> <R e l a t i o n name=" superType " u s i n g=" PathEnumeration" Transformer
This module performs the transformation suggested by the patterns by implementing the sequence of activities included in the patterns. The module transforms the NOR elements, loaded by the NOR Connector module, into internal model representation elements. It also interacts with the Semantic Relation Disambiguator module for obtaining the suggested semantic relations of the NOR elements.The Transformer also utilizes an XML configuration file, called prnor. xml, for describing the transformation between the NOR elements and the ontology elements. This XML configuration file has only one section, PRNOR, which includes the description of the transformation from the NOR schema components (e.g., schema entities, attributes, and relations) into the ontology elements (e.g., classes, object properties, data properties, and individuals). Additionally, it indicates the transformation approach (e.g., Two examples of the XML configuration file are shown in Listings 6.2 and 6.3. Listing 6.2 indicates that the pattern follows the TBox transformation approach and that it transforms the elements of the CSItem schema component into ontology Semantic Relation Disambiguator
This module is in charge of obtaining the semantic relation between two NOR elements. Basically, the module receives two NOR elements from the Transformer module and returns the semantic relation between them. First, the module verifies whether it can obtain the subClassOf relation by identifying attribute adjectivesThe TBox transformation approach converts the resource content into an ontology schema. To this end, each NOR term is mapped to a class, and then the semantics of the relations among those entities is made explicit. Thus, patterns that follow the TBox transformation approach must make explicit the semantics of the relations among the NOR terms. To perform this task, we rely on WordNet, which organizes the lexical information into meanings (senses) and synsets.Algorithm 1, presented in Sect. 6.5.1.1, describes how to make explicit the semantics of the relations in the NOR terms.It is worth mentioning that, when asserting the partOf relation the algorithm takes advantage of the use of the PartOf content patternExternal Resource Service
The External Resource Service is in charge of interacting with external resources for obtaining the semantic relations between two NOR elements. At this moment, the module interacts with WordNet. We are now implementing the access to DBpediaOR Connector
The Ontological Resource (OR) Connector generates the ontology in OWL Lite. To this end, this module relies on the OWL APIAn example of the XML configuration file is shown in Listing 6.4. The listing indicates that the ontology generated will be stored in the asfa.owl file, that its name will be asfa ontology, and that it will be implemented in OWL.Listing 6.4 OR Connector configuration file example <Or name=" a s f a o n t o l o g y " ontologyURI=" h t t p : // mccarthy . d i a . f i . upm. e s / o n t o l o g i e s / a s f a . owl " o n t o l o g y F i l e=" a s f a . owl " i m p l e m e n t at i on="OWL" a l r e a d y E x i s t="no" s e p a r a t o r="#"> </Or> Finally, to conclude the description of the software library, it is worth mentioning that the implementation of this library follows a modular approach; therefore, it is possible to extend it and include other types of NORs, data models, and implementations in a simple way, as well as to exploit other external resources for making explicit the hidden semantics in the relations of the NOR terms.Example
In order to evaluate the methodological guidelines proposed in this chapter, we conducted two experiments in real case scenarios within the SEEMPSEEMP Project
The main objective of this project was to develop an interoperable architecture for public employment services (PES). The resultant architecture consisted of (1) a reference ontology, the core component of the system, that acts as a common "language" in the form of a set of controlled vocabularies that describes the details of a job posting; (2) a set of local ontologies, each PES uses its own local ontology, which describes the employment market in its own terms; (3) a set of mappings between each local ontology and the reference ontology; and (4) a set of mappings between the PES schema sources and the local ontologies.In the following sections, we describe the application of our methodological guidelines for reusing and re-engineering non-ontological resources when building an occupation ontology.Reusing Non-ontological Resources
This section presents the application of the method for reusing non-ontological resources within the SEEMP project. It shows the process we followed for selecting the non-ontological resources to be reused when building the occupation domain ontology.Activity 1. Search Non-ontological Resources
Following the suggestions of some domain experts, we searched for the occupation classifications at (1) the Ramon Eurostat PortalTask 1. Extract Lexical Entries Within this task, we extracted the lexical entries of the aforementioned occupation classifications. We developed an ad hoc extraction tool for performing automatically the extraction task.Task 2. Calculate Precision
Since we were dealing with occupations related to the IT domain, it was impossible to cover all the IT domain occupations already identified in the ontology requirements specification document. Thus, we used a constant that represents the complete set of IT domain occupations. In this case, the cardinality of the complete set is K. Therefore, the intersection of the complete set with the set of terms available in the ORSD is the set of terms of the ORSD. Next, we present the precision for each occupation classification:Again, since we were dealing with the occupations related to the IT domain, it was impossible to cover all the IT domain occupations in the ORSD. Thus, we used a constant K that represents the complete set of IT domain occupations. Next, we present the coverage for each occupation classification:Evaluate the Consensus
It was important for the project that resources focused on the current European reality because the user partners involved in SEEMP are European, and the outcoming prototype has to be validated in European scenarios. Thus, domain experts confirmed whether the resources were built with the consensus of the European community or not. They also explained that ISCO-88(COM) and EURES proprietary occupation classification contains terminology that had already reached a consensus. Table Activity 3. Select the Most Appropriate Non-ontological Resources Following Table Re-engineering Non-ontological Resources
In this section, we present the application of the method for re-engineering non-ontological resources within the SEEMP project. Once we select the nonontological resource, we have to transform it into an ontology. Next, we describe the process of generating an occupation ontology from the EURES proprietary occupation classification. In this activity, we gathered documentation on the EURES occupation classification from the European Dynamics SEEMP user partner. From this documentation, we extracted the schema of the classification scheme, which consists of two tables, CVO OCCGROUP and CVO OCCUGROUP NAME. Since the data model was not available in the documentation, it was necessary to extract it for the resource implementation itself. The EURES occupation classification is modeled following the snowflake data model and is implemented in a MS Access database.Activity 2. Non-ontological Resource Transformation
Within this activity, we carried out the following tasks:1. We identified the transformation approach, the TBox transformation, i.e., transforming the resource content into an ontology schema. 2. Then, we searched our local pattern repository for a suitable pattern to reengineer NORs, taking into account the transformation approach (TBox transformation), the non-ontological resource type (classification scheme), and the data model (snowflake data model) of the resource. 3. The most appropriate pattern found for this case was the PR-NOR-CLTX-03 pattern. This pattern takes as input a classification scheme modeled with a snowflake data model and produces an ontology schema.Activity 3. Ontology Forward Engineering WSMLBecause of the number of occupations of the EURES classification, it was not practical to create the ontology manually. Therefore, we created an ad hoc wrapper, implemented in Java, that reads the data from the resource implementation and automatically creates the corresponding classes and relations of the new ontology following the suggestions given by the pattern for re-engineering NORs and the conceptual model. We followed this process for all the resources identified, being the patterns used those presented in Table In order to illustrate the dimension of the ontology and the ontological engineers' efforts required to build it, some statistical data are shown in Table Our experience in SEEMP has shown us that the approach of building ontologies by reusing and re-engineering non-ontological resources already agreed upon allows building ontologies faster, with less resources, and with an immediate consensus. This approach permits making explicit the knowledge implicitly coded in organization models and standards. By building ontologies in this fashion, we facilitate that ontologies become reference ontologies in their respective domains.With respect to the application of the method for reuse and re-engineering, this was especially useful for guiding the steps of the ontological engineers involved since this method provides detailed and sufficient guidelines. In addition, the existence of a well-defined and structured process for building the ontology network in the e-employment domain eased the planning, coordination, and communication with other non-Semantic Web members of the development team, which in turn helped to convey reliability to the final result.mIO! Project
The main objective of the mIO! project is to develop ubiquitous services in an intelligent environment, adapted to every user and its context by means of mobile interfaces. The project relies on ontologies for modeling the knowledge.The following sections describe the application of our methodological guidelines for reusing and re-engineering non-ontological resources when building a geographical ontology, which includes continents, countries, and regions.Reusing Non-ontological Resources
This section describes the activities carried out for reusing non-ontological resources.Activity 1. Search Non-ontological Resources
Following some of the suggestions made by the domain experts, we searched geographical location resources on highly reliable web sites. Next, we list the geographic location classifications:• ISO 3166• Guide to regions of the worldTask 2.1 Extract Lexical Entries Within this task, we extracted the lexical entries of the aforementioned geographic location classifications. For this purpose, we used TreeTaggerTask 2.2 Calculate Precision
It was impossible to cover all the geographic locations in the ORSD. Thus, we used a constant K that represents the cardinality of the complete set of geographical locations. Next, we present the precision for each geographic location classification:Again, it was impossible to cover all the geographic locations in the ORSD. Thus, we used a constant K that represents the cardinality of the complete set of geographic locations. Next, we present the coverage for each geographic location classification:Evaluate the Consensus
It was important for the project that resources focused on the current worldwide reality because the outcoming prototype will be validated by users. Thus, domain experts evaluated whether the resource was built with the consensus of the worldwide community or not. They confirmed that ISO 3166 has the full consensus of the community, whereas the other resources have not.Task 2.5 Evaluate the Quality In this case, domain experts evaluated whether the resource was built with an acceptable level of quality. They confirmed that ISO 3166 has an acceptable level of quality.Task 2.6 Build the Assessment Table Activity 3. Select the Most Appropriate Non-ontological Resources
According to Table Activity 1. Non-ontological Resource Reverse Engineering
In this activity, we gathered documentation about ISO 3166 from its web site. From this documentation, we extracted the schema of the classification scheme, which consists of one entity ISO 31661 Entry. Since the data model was not available in the documentation, it was necessary to extract it for the resource implementation itself. ISO 3166 is modeled following the snowflake data model and implemented in XML.Activity 2. Non-ontological Resource Transformation
In this activity, we carried out the following tasks:1. We identified the transformation approach, the ABox transformation, i.e., the transformation of the resource schema into an ontology schema, and the resource content into ontology instances. 2. Then we searched our local pattern repository for a suitable pattern to reengineer NORs, taking into account the transformation approach (ABox transformation), the non-ontological resource type (classification scheme), and the data model (snowflake data model) of the resource. 3. The most appropriate pattern for this case is the PR-NOR-CLAX-12 pattern.This pattern takes as input a classification scheme modeled with a snowflake data model. 4. Finally, we followed the procedure defined by the pattern selected for transforming the resource components into ontology elements.Activity 3. Ontology Forward Engineering
In this activity, we formalized and implemented the ontology in OWL. The ontology is available at http://mccarthy.dia.fi.upm.es/ontologies/.Analysis of the Applicability of the Method
The network of ontologies of the mIO! project was developed following the NeOn Methodology In order to illustrate the dimension of the ontology and the efforts required by the ontological engineers to build it, we outline some data in Table Our experience in mIO! has served us to demonstrate that the approach of building ontologies by reuse and re-engineering non-ontological resources already agreed upon allows building ontologies faster, with less resources, and with consensus. With respect to the application of the method for reuse and re-engineering, this was especially useful for guiding the steps of the ontological engineers involved since the method provides detailed and sufficient guidelines.Conclusions
In this chapter, we have provided a method and its technological support that rely on re-engineering patterns in order to speed up the ontology development process by reusing and re-engineering as much as possible available non-ontological resources. Moreover, we have introduced a three-level categorization of NORs according to three different features: type of NOR, data model, and implementation. Finally, we have presented two use cases of the proposed approach.Introduction
Ontologies play an important role in many knowledge-intensive applications, by formally defining the conceptualization used by the application and by facilitating interoperability. Building ontologies from scratch can in general be expensive. In this sense, one way of reducing the time and costs associated with the ontology development process is by reusing available ontological resources. Ontologies developed by reuse can also build on existing good practices (from well-developed ontologies), thus increasing the overall quality of the results.As mentioned in Chap. 2, the NeOn Methodology presents nine scenarios for building networks of ontologies. One of these scenarios is Building Ontology Networks by Reusing Ontological Resources. In this scenario, ontology developers analyze whether existing ontological resources can be reused in the context of building an ontology.The reuse of ontological resources is encouraged by a recent increase in the number of ontologies available online.According to our experience, the reuse of ontological resources is useful for (a) saving time and resources during the ontology development and (b) refining the Ontology Requirement Specification Document (ORSD) (see Chap. 5) taking into account the knowledge represented in the candidate ontological resources to be reused. The latter case refers to the situation in which the engineer finds axioms and/or definitions of terms that did not appear in the ORSD. For example, in the development of a drug ontology, the engineer may find a type of drug that had not been considered in the ORSD. For the sake of simplicity, in this chapter, it is assumed that the reuse does not imply modifications in the ORSD. If such modifications are required, an iterative-incremental life cycle model should be followed (see Chap. 2).The ontological resource reuse process is often influenced by the type of ontology to be reused. Ontologies can model domain entities (e.g., drug, disease, pharmaceutical product) or generic entities, which are considered to be generic across many fields However, the reuse of large ontologies such as WordNet• Ontologies can be reused as a whole if they closely meet the expectations and the needs of the ontology engineer. • In certain cases, only one part or moduleFor example, when building an ontology about lung cancer, it is not always necessary to reuse an entire ontology about the human body; it suffices to reuse a module describing concepts related to the lung.• In other cases, only some knowledge components from the ontology (the description of a particular entity, the branch in the taxonomic hierarchy in which an entity appears, or entity neighborhoods in the ontology) are relevant for the development needs. In these cases, the reuse of ontological knowledge is performed at the statementThis chapter focuses on providing methodological guidelines for the reuse of generic ontologies, although most of the recommendations are also applicable to the reuse of domain ontologies.Methodological Guidelines for Reusing Generic Ontologies
Table The activities shown in Fig. Along the exposition, an example of reusing a generic ontology in the development of the pharmaceutical product ontology network (PPO) (see Chap. 20) is presented. This ontology will be used as a bridge between proprietary systems for managing financial and product knowledge interoperability in pharmaceutical laboratories, companies, and distributors in Spain. In this ontology reuse task, we have taken into account the four competency questions (CQs) shown in Table Activity 1: Selecting the Generic Ontology to be Reused
The goal of this activity is to select the most appropriate generic ontologies to be reused in the ontology being developed. It is worth mentioning that instead of reusing available ontologies, practitioners can implement from scratch the necessary axioms and definitions according to some existing formalization, for example, the one appearing in Annex. On the one hand, the advantage of reusing available ontologies implemented in a formal language is that ontology developers will save effort in the transformation of a formalization that is not suitable for run-time reasoning. On the other hand, the advantage of starting from an existing formalization is that ontology developers will save effort in the searching, comparison, and evaluation of candidate ontologies to be reused. In this chapter, we focus on the reuse option. This activity takes as input the ORSD (Chap. 5) and is divided in the following tasks:Task 1.1 Reformulating the CQs and adding linking axioms. The main goal of this task is to reformulate the CQs included in the ORSD of the ontology that is being developed with vocabulary that could potentially belong to ontologies to be reused but that do not explicitly appear in the CQs. Additionally, another goal of this task is to identify axioms that link terms of the CQs to terms that could be reused. The first column of Table The third column shows the action to carry out in each case. Finally, as an example, the second and fourth columns present the PPO CQ that matches each case and the result of applying the action corresponding to the case. For example, given that the case 2 (Table Task 1.2 Identifying the definitions and axioms of the ontology to be reused. The goal here is to identify which definitions and axioms can be potentially reused in the ontology to be developed. The terms whose definition could be reusable from other ontologies are those terms appearing in the pre-glossary of the ORSD (specifically in slot 7) (see Chap. 5) and the new terms that appear in the reformulated CQs obtained in Task 1.1. The second column of Table For instance, iron is part of ferrous sulfite, and this is, in its turn, part of Mol Iron ® , which is a drug. Moreover, the definition of the term is proper part of should be reused to answer questions like CQ 2 , where the interest is not located in the drug itself. Task 1.3 Search for ontologies. The ontology development team should search for ontologies that implement the axioms and definitions identified in Task 1.2.To perform this task, ontology developers can use a general purpose search engine (e.g., GoogleD.1. Is proper part of definition
The case 1 (see In addition to its user interface, Watson includes a set of open APIs making it possible for application developers to find and exploit online ontologies directly from the provided infrastructure. This API has been used to create an interface to Watson from the NeOn Toolkit, where definitions of specific classes and properties can be found and reuse: the Watson plugin In addition to the Watson plugin, other developments have been integrated with Watson with the goal of facilitating ontology search and reuse. For example, in addition to extracted information, Watson provides a simple visual summary of each ontology using the key concept extraction mechanism described in As an example, Table Task 1.4 Performing a comparative study. The goal here is to compare the candidate ontologies obtained in Task 1.3 with the axioms and definitions identified in Task 1.2. This comparative study is represented in the form of a table to facilitate its use. In the table, each row represents the set of definitions (or axioms) identified in Task 1.2, and each column, the ontologies found in Task 1.3.As an example, a comparative table of ontologies implementing mereologies is shown in Table Task 1.5 Determining the most appropriate ontology to be reused. The goal of this task is to determine which of the candidate ontologies identified in Task 1.3 is the most appropriate to be reused in the ontology being developed. To determine such an ontology, the analysis following Fig. Variable to be assigned a value  overcoming the lack of compliance in non-functional properties. The exact value of each weight can be obtained using different procedures. One of them is by means of the utility theory With the objective of having a reference to compare the scores, the score of an ideal ontology has been considered as a normalization denominator. Let us note that if this ideal reference is not provided, it is not easy to know the significance of the difference between the ontology scores. Thus, for example, without this ideal reference, if the difference between ontologies o 1 and o 2 is 0.4, the engineer cannot necessarily determine how large such difference is.Given an ontology ont, the following formula to calculate the score of the functional features analysis is used:where value ont (functionalFeature i ) is the value of functional feature i for the ontology ont, and value idealOnt (functionalFeature i ) is the value of functional feature i for an ideal ontology, that is, the number of features (axioms and definitions) obtained in Task 1.4. Concerning non-functional features analysis, it is carried out on the basis of the following four dimensions:• Reuse economic cost. It refers to the estimate of the economic cost needed for accessing and using the candidate ontology. If the candidate ontology has any type of license, then the cost of acquisition and/or exploitation should be taken into account (Go ´mez-Pe ´rez and Lozano-Tello 2005). • Understandability effort. It refers to the estimate of the effort needed for understanding the candidate ontology. In this case, the following criteria should be analyzed:-Quality of the documentation. It refers to whether there is any communicable material used to describe or explain different aspects of the candidate ontology (e.g., modeling decisions). The documentation should explain the statements contained in the ontology so that a nonexpert could understand them • Integration effort. It refers to the estimate of the effort needed for integrating the candidate ontology into the ontology being developed. In this case, the following criteria should be analyzed:-Adequacy of knowledge extraction. It refers to whether it is easy to identify parts of the candidate ontology to be reused and to extract them. For example, in large and not modularized ontologies (e.g., SUO), the difficulty to extract the part of the knowledge we are interested in is especially high. -Adequacy of naming conventions. It refers to whether both ontologies (the candidate and the one being developed) follow the same rules for naming the different ontology components (e.g., concept names should start with capital letters, relation names should start with non-capital letters). -Adequacy of the implementation language. It refers to whether both languages (the candidate ontology's and the ontology's being developed) are the same, or at least are able to represent similar knowledge with the same granularity. -Knowledge clash. It refers to whether there are contradictory bits of knowledge between the candidate ontology to be reused and the ontology being developed (e.g., discrete time versus continuous time assumption). -Adaptation to the reasoner. It refers to whether the adaptation of definitions and axioms that satisfy the existing restrictions of the reasoner is needed (e.g., explicit definitions can be included in OWL ontologies; however, this kind of definitions cannot be included in ontologies written in Prolog). -Necessity of bridge terms. It refers to whether it is necessary to create new linking axioms and/or relations to integrate the candidate ontology to be reused into the ontology being developed.• Reliability. It refers to an analysis of whether ontology developers can trust the candidate ontology to be reused. In this case, the following criteria should be considered:-Design criteria. It refers to whether the ontology has been built according to the design criteria assumed by the development team of the domain ontology. Table Thus, ontology developers should fill a table and analyze the candidate ontologies with respect to the abovementioned criteria, taking into account the different ways to measure each criterion and the possible values that can be assigned.Having filled Table • To transform linguistic values, the following transformation rules are proposed: Given that we want to penalize ontologies about which we have less knowledge, we have assigned a value of 0 to unknown.• The score that synthesizes the non-functional features contribution is the following weighted mean:where:-ScoreNon-FunctionalFeatures ont is the score for the candidate ontology ont for the set of criteria j is a particular criterion of those included in Table After applying the previous formula to all the candidate ontologies, ontology developers should select the candidate ontology with the best normalized scored. As an example, in the context of the PPO case, we have filled in the values associated with the OWL versions of SUO-OWL and Dolce-Lite, which are shown in Table The results in Task 1.5 have been very close. Given that we have found top-level concepts of SUMO-OWL like biologically active substance and molecule (and their ancestors) useful for PPO, the criterion adequacy of knowledge extraction has been assigned high for this ontology and, consequently, has obtained the best score (Table For this example, we have used a spreadsheet. For the future, we plan to support the automation of this task in NeOn Toolkit. The goal of this activity is to customize the ontology selected in Activity 1 according to the needs of the domain ontology being developed. This activity consists of the following tasks: Task 2.1 Pruning the ontology to be reused according to the needed features. The goal of this task is to prune the selected ontology taking into account the features needed in the domain ontology that is being developed. Thus, for example, if the definition of overlap is defined in the generic ontology, but it is not necessary in the resulting ontology, it should be removed.Task 2.2 Enriching the ontology to be reused. The goal of this task is to extend the ontology selected with the new conceptual structures needed in the domain ontology being developed. In the PPO example, we have added transitivity to the part and properPart object properties, reflexivity and antisymmetry to part, and asymmetry and irreflexivity to properPart.When pruning and enriching the ontology, it is necessary to take into account that the axioms and definitions to be reused may be applicable to a category that does not completely include all the individuals of interest in our domain ontology. If this happens, an adaptation of the axioms and definitions should be performed.Task 2.3 Translating the ontology to be reused into the implementation language of the domain ontology being developed. The goal of this task is to translate the selected ontology into the implementation language of the domain ontology being developed if those two ontologies are in different languages.An ontology can be translated in an automatic or manual way. It is important to point out that a complete translation into different languages is not always possible. For example, let us suppose the following implementation in Prolog of overlaps and disjoint:The rule corresponding to disjoint cannot be implemented in OWL. In fact, let us note that given that Prolog works under the closed world assumption, if common parts of substance1 and substance2 have not been represented, the answer to the query:?:-disjoint(substance1, substance2).will be true. However, it is not possible to attain this effect directly with OWL (open world assumption). Task 2.4 Adapting the ontology to be reused to the design criteria followed in the ontology to be developed. The following modifications have to be done in most cases: (a) changing names (concepts, properties) to adapt them to the naming conventions used in the ontology network being developed and (b) adding range to properties. For example, we have adapted the names to the convention used in PPO. Thus, part has been changed to isPartOf.Task 2.5 Evaluating the obtained ontology. The goal of this task is to evaluate from a content perspective if there are no errors in the ontology. This task is described in detail in Chap. 9.Activity 3: Integrating the Generic Ontology to be Reused in the Ontology Being Developed
The goal of this activity is to integrate the ontology obtained in Activity 2 in the ontology being developed. The development team should decide whether:• To import the customized ontology. The advantage is that the resulting developed ontology will be structured in different modulesIn any case, links between terms of the reused ontology and the ontology to be developed should be established. In the case of PPO, we have taken advantage of the possibilities that SUMO-OWL offers us to easily represent different perspectives of the notion of drug, for example, drug as a substance that acts in our organism and drug as a product that can be sold. Moreover, given that transitivity, antisymmetry, etc. involve individuals, we have added an individual for each type of substance and product. Therefore, the application that uses the PPO maintains the individuals corresponding to particular entities (e.g., Frenadol C243, corresponding to Frenadol box with manufacturing lot C243) and the individuals that represent products and substances in a general way. Thus, for example, the system can infer that caffeine is part of Frenadol because there is an individual of caffeine (also with tag "caffeine") that is part of an object Frenadol, that is, an individual of Frenadol. We have also added the axioms identified in Table To answer CQ 4 , we have added this rule to the ontology:interactsWith(?x, ?y), isPartOf(?x, ?z) -> interactsWith(?z, ?y)   That is, if a substance ?x interacts with another substance ?y, then the latter interacts with every part of ?x. Thus, for example, given that paracetamol interacts with the ethyl alcohol, Frenadol ® also interacts with ethyl alcohol.A partial view of the resulting ontology is shown in Fig. Conclusions and Future Work
The reuse of (well-developed) ontologies allows spreading good practices and increasing the overall quality of ontological models. In this chapter, we have presented how to carry out this process. The guidelines shown here provide the methodological assistance to Scenario 3 in the NeOn Methodology (Chap. 2).Given that the reuse of an ontology usually implies pruning it, ontology reuse usually implies statement reuse (see It is also worth mentioning that interesting knowledge represented in ontologies may be found by chance. For instance, part of the knowledge on substances and Which is the main active ingredient of Frenadol®?Formal CQ Example of answer .Table In addition, it would be interesting to perform a comparison of the costs of (a) reusing generic ontologies versus (b) developing what is required from scratch.Annex: Mereology
A mereology is a formal theory of parts and associated concepts In the following paragraphs, we will show one of the mereologies presented by Theory M. Most of the authors agree on the following core of axioms (named with A) and definitions (named with D) • A.1. Reflexivity. Every object of the universe of discourse is a part of itself. For instance, the EU is part of the EU. • A.2. Antisymmetry. If an object x is a part of y, and y is a part of x, then x and y are the same object. For instance, if the territory T 1 is part of the territory T 2 , then the only way so that T 2 is part of T 1 is being T 1 and T 2 the same territory. Theory M may be viewed as embodying the common core of any mereological theory. A.1-A.3 should be extended to build a mereology.Minimal mereology (MM). A way to extend M is assuming the following principle • A.4. Weak supplementation principle. Every object x with a proper part y has another part z that is disjoint from y. The domain of territories, for example, fulfills this principle. For example, given that Spain is proper part of the EU, then the EU has other parts that are disjoint from Spain: the Netherlands, Luxemburg, Sweden, etc.Most of the authors strengthen that A.4 should be incorporated to M as a further fundamental principle on the meaning of part of. Other authors provide scenarios that could be counterexamples of this principle. However, it is far from being demonstrated that such supposed counterexamples have implications in computer applications.The Abstract In the context of the Semantic Web, resources on the net can be enriched by well-defined, machine-understandable metadata describing their associated conceptual meaning. These metadata consisting of natural language descriptions of concepts are the focus of the activity we describe in this chapter, namely, ontology localization. In the framework of the NeOn Methodology, ontology localization is defined as the activity of adapting an ontology to a particular language and culture. This adaptation mainly involves the translation of the natural language descriptions of the ontology from a source natural language to a target natural language, with the final objective of obtaining a multilingual ontology, that is, an ontology documented in several natural languages. The purpose of this chapter is to provide detailed and prescriptive methodological guidelines to support the performance of this activity.Motivation
As with the World Wide Web, the success or failure of the Semantic Web will be determined to a large extent by easy access to and availability of high-quality and diverse content In the case of the Semantic Web, the problem is similar; most of the ontologies that have been built so far have English as their basis. Nevertheless, although English is now the de facto language for science and technology, other spoken languages are used, and it is important to provide methods and tools both to support the definition of ontologies expressed in languages other than English and also to support interoperability across ontologies written in different languages. Currently, a great effort is being applied to the construction of ontologies. Although access to top-quality ontologies (e.g., Galen 2 , CYC 3 , or AKT 4 ) is in many cases free and unlimited for users around the world, most of these ontologies can be said to be essentially monolingual, that is, documented in one natural language, and this language is often English as an international lingua franca. However, there is a growing need for multilingual ontology resources that overcome communication barriers arising from cultural-linguistic differences, lack of excellent command of English, need for high precision in communication, etc. In fact, multilingual knowledge is even more prevalent in those countries that have more than one official language Moreover, the use of ontologies has grown not only in terms of the number of application domains but also in the number of natural languages chosen to build domain-specific knowledge bases. Thus, multilingual ontologies are nowadays demanded by institutions worldwide with a huge number of resources available in different languages. Basically, usage of multilingual ontologies traverses many disciplines and has become an urgent need in certain organizations. For instance, in agriculture, the Food and Agriculture Organization (FAO) has expressed the need for semantically structuring the information they have in different natural languages. Since all FAO official documents must be made available in Arabic, English, Chinese, French, Russian, and Spanish, a large amount of research has been carried out in translating large multilingual agricultural thesauri (Chun and Wenlin 2002), in mapping methodologies for thesauri A further factor that has increased the need for multilingual ontologies is the development of some ontology-based systems that need to interact with information in natural languages. Some examples of these applications are cross-lingual information retrieval These examples can serve to highlight the importance of adding multilingualism, that is, multilingual information, to ontologies before trying to solve the numerous pending problems that still exist with the current monolingual approach. But, while there is a clear need for ontology localization, there are no well-defined and broadly accepted definitions of what the ontology localization activity entails. Moreover, to our knowledge, no other study has focused on the methodological guidelines for this activity. For this reason, in this chapter we present efficient, prescriptive, and detailed methodological guidelines for the ontology localization activity.The rest of the chapter is organized as follows: Sect. 8.2 presents the scope of the methodological guidelines together with a detailed and systematic account of the most important items from practical and application perspectives. Section 8.3 describes the guidelines for the ontology localization activity (including the filling card and the activity workflow). Section 8.4 includes an example of how the proposed guidelines for ontology localization are used in practice and the results obtained. Finally, Sect. 8.5 summarizes the main conclusions from this work.Scope of the Methodological Guidelines
Methodological guidelines for ontology localization can be discussed from different perspectives:• Guidelines for the development of internationalized ontologies • Guidelines for the localization of existing ontologies • Guidelines for reaching a mature ontology localization processIn what follows, we briefly describe these types of guidelines with respect to three aspects: (1) the objective and scope of the guidelines, (2) the target audience, and (3) the work related to them. Then, we will deal with the design principles we considered in order to define the guidelines used in this chapter.Guidelines for the Development of Internationalized Ontologies
Objective and scope: The aim of these guidelines is to improve the design and implementation of internationalized ontologies in order to reduce the cost of localization. Based on the software localization field (LISA), we understand internationalized ontologies as ontologies built with the aim of supporting multilingual descriptions of the conceptualizations they provide, since they are to be used in a multilingual scenario. However, this means that in their design phase, languageand culture-specific concepts are to be stored externally or removed, so as to enable possible reuse, and only those common concepts are captured in the ontology.Target audience: These guidelines are intended particularly for ontology developers, who are concerned with the design and development of ontologies. In addition, they are intended for international institutions interested in planning, designing, and implementing internationalized domain ontologies.Related work: These guidelines provide recommendations for naming ontology elements. The key goal here is that the ontology elements be clear (avoid ambiguity) and simple (easy to translate to other languages). Recently, some approaches have been defined Guidelines for the Localization of Existing Ontologies
Objective and scope: The aim is to carry out the localization process of ontologies already conceptualized. Usually, these ontologies are designed without taking into account the multilingual and localization aspects. Therefore, these guidelines aim at reducing costs, improving its quality, and increasing the consistency of the localization activity.Target audience: These guidelines are particularly intended for ontology stakeholders such as localization managers, translators, and reviewers, who are concerned with the ontology localization activity. In addition, they are intended for communities interested in localizing ontologies and those international firms that may promote multilingualism in their working environments for a variety of reasons.Related work: These guidelines describe different stages for the localization activity. At each stage of the process, they explain the activities or tasks to be performed with the same style and granularity level as used by software development methodologies. To the best of our knowledge, no guidelines exist for supporting the ontology localization activity. However, software localization methodologies could be adapted for ontology localization, as these methodologies are very general.Guidelines for Realizing a Robust Localization Process
Objective and scope: These guidelines are meant to help reach a robust localization process within an organization. Usually, most organizations pass through different stages of maturity before reaching a robust localization process. Therefore, these guidelines describe behaviors or best practices adopted by successful projects.Target audience: We envisage that they can be intended for organizations dedicated to the development of Semantic Web applications that require the use of multilingual ontologies.Related work: These guidelines should describe different maturity levels used to improve and appraise the ability of an organization to perform the functions required in the ontology localization activity. The localization maturity model (DePalma 2007) (LMM) is a new advance in the deployment of software localization. While LMM compliance will not guarantee success, it does increase the likelihood of succeeding by helping planners understand what others have experienced and learned before them. As this methodology is quite general, we believe it can be adapted to ontology engineering methodologies.In this chapter, we propose general guidelines that cover the localization of existing ontologies (second group above). In the following sections, we will define the actors involved in the different tasks of the ontology localization activity. Then, we will describe in detail the tasks for carrying out this activity.Methodological Guidelines for Ontology Localization
In this section, our purpose is to explain the guidelines set out to help ontology developers in the ontology localization activity. The principles that guide the construction of such guidelines are the following:• The guidelines should be general enough in the sense that they should help software developers and ontology practitioners to localize ontologies in different natural languages and domains. • The guidelines should define each activity or task precisely; they should clearly state its purpose, its inputs and outputs, the actors involved, when its execution is more convenient, and the set of methods, techniques, and tools to be used for executing them. • To facilitate a prompt assimilation of the ontology localization by software developers and ontology practitioners, we present the guidelines in a prescriptive way, not specifically oriented to researchers.First, we present the different kinds of actors involved in the ontology localization activity. Then, we describe the guidelines for localizing ontologies to different natural languages.Ontology Localization Actors
The different tasks involved in the ontology localization activity are carried out by different actors according to the kind of roles that must be performed in each task. In the following, we describe briefly the main actors involved in the localization activity:• Domain experts and ontology development team. The domain expert or experts and the ontology development team (ODT) are responsible for performing one of the first tasks in the ontology localization activity. Their work consists of selecting the right resources and tools to perform the ontology localization activity. • Localization manager. The localization manager plays a key role in the localization activity, as he or she must prepare all technical aspects of the localization activity, including the localization material (e.g., identifying the ontology elements to be localized) and distributing it to the localization team, setting up the localization team, as well as assigning and monitoring the tasks. Another task to be performed by the localization manager is the updating and final quality revision of the translated ontology. • Linguists. These specialists can either be:-Translators (localization specialist). Once the localization manager assigns the localization tasks to each member, the translator or localization specialist takes care of discovering the most appropriate translations for each ontology element. -Reviewers (QA specialist). The reviewer or quality assurance (QA) specialist reviews the translated ontology elements. A reviewer does not necessarily focus on the quality of the translations but on the linguistic and stylistic quality of the translated ontology elements. The revision is a final language check for spelling errors, grammar mistakes, and consistency.The current industry trend is to use external localization service providers in the translation task to avoid the high fixed cost of using in-house translators and use translators focused on the target markets and knowing the up-to-date usage of particular languages. We conceived a similar situation for ontology localization, in which translators and reviewers can be internal or external to the organization that develops the ontology and who work in a distributed environment. Figure Ontology Localization Guidelines
The ontology localization guidelines have been created in the context of the NeOn Methodology (see Chap. 2) to build ontology networks. Thus, taking into account the aforementioned methodological work, we provide the filling card for the ontology localization shown in Fig. The methodological guidelines for carrying out the ontology localization activity can be seen in Fig. The tasks for carrying out the ontology localization activity are explained in detail in the following: Task 1. Select the most appropriate linguistic assets. The goal of this activity is to select the most appropriate linguistic assets that help in the localization activity. Domain experts and ODT carry out this activity, taking as input the ontology to be localized. The activity output is a set of linguistic assets that can help to reduce the cost, improve the quality, and increase the consistency of the localization activity. The choice of a specific resource is performed manually, taking into account that the linguistic assets comply with the following characteristics:• Consensus. Resources used should contain multilingual terminology consensually accepted by the community (authoritative resources), thus the effort and time spent in finding out adequate translation labels for ontology terms would decrease considerably. In this sense, internal resources, such as terminology glossary of financial terms or a legal dictionary) when translating domain ontologies, since they will contain the appropriate terminology. Also, since each resource supports different features and language sets, the selected resources should cover all target languages for current and possible future ontology localization projects. To select the appropriate translation tool for performing the ontology localization activity, the preliminary guidelines presented in Table By default, all labels of concepts, attributes, relations, and instances will be selected to be translated. However, it may happen that the ontology has been partially localized, and only the remaining labels need to be translated or Ontology localization tool
The main difficulty at this level is related to the fact that labels for concepts, attributes, and relations are usually short (isolated) labels, not inserted in a sentence or text. Therefore, a tool designed for the purpose of translating ontologies is required at this stage to extract the label specification and its context correctly. The label context is in its turn required for discovering the label sense (for disambiguation purposes) Consider for example the word "plant," which depending on the context can be translated into Spanish as "planta" in the sense of "living organism" or "fa ´brica" in the sense of "industrial plant" Ontology instances Computer-aided translation tool or ontology localization toolThe main complication at this level is to decide which instances should be translated and which ones should not A big part of the instances are represented by proper names and, therefore should not be translated (e.g., a label containing "Michael Schumacher" should not be translated). However, other instances such as "South America" should be translated to other natural languages, as they have traditionally well-established and accepted translations Ontology term annotationsTranslation memory tool
The major cost involved at this level is the difficulty in translating correctly long pieces of text To provide a human-readable description of a term, the RDF(S) and OWL ontology languages use for example the rdfs: comment statement, where a textual comment can be added. Thus, this level involves the difficulty to translate a whole sentence (not isolated labels or terms) which is part of the annotation of concepts, attributes, or instances in ontologies retranslated if, according to the localization manager, they have not been properly translated.The task output is a set of ontology labels and their contextTask 3. Obtain ontology label translation(s). For each ontology label, the goal of this task is to obtain the most appropriate translation in the target language. Translators carry out this task, taking as input the ontology label(s) to be localized. Different machine translation (MT) techniques • The type of domain knowledge represented in the ontology. We mainly consider here two types of domains: internationalized domains, that is, domains whose categorization usually finds consensus among different cultures, and culturally dependent domains, that is, domains whose categorization is normally influenced by a certain culture. On the one hand, ontologies categorized within the first domain type will require translation techniques that allow identifying direct correspondences between words. Techniques based on linguistic resources such as dictionaries, terminologies, etc., can be used in this case. On the other hand, ontologies representing a culturally dependant domain (e.g., the judiciary), in which categorizations tend to reflect the particularities of a certain culture, will require translation techniques that allow identifying semantic correspondences.• The type of ontology element to be localized. A second factor to be considered is the type of ontology elements to be localized. Depending on the ontology elements considered for localization, the algorithms of localization can be more or less complex. For example, the localization of ontology concepts and relations has a higher level of complexity than the localization of ontology instances because a big part of the instances are represented by proper names and have previously agreed translation or should not be translated.The task output is a ranked set of labels in the target language for each ontology label(s). Task 4. Evaluate label translation(s).Translation quality measurements must accomplish two basic criteria:• Repeatable. Two assessments of the same sample must yield similar results.• Reproducible and objective. Different evaluators should arrive at a similar assessment for the same piece of translation.The goal of this task is to evaluate label translations in the target language. At this stage, translators and/or reviewers carry out this activity taking as input the labels in the target language. The output of this task is a set of labels with its corresponding evaluation. Different linguistic criteria can be used for the evaluation of label translations. We propose two levels of evaluation criteria and for each level a set of tests, which should be automated as far as possible.• Semantic fidelity evaluation. The aim of this evaluation criterion is to control that the label translation is conceptually equivalent to the ontology label in the source language. A way of evaluating the semantic fidelity is to perform a backward translation test, which provides a quality-control step demonstrating that the quality of the translation is such that the same meaning is derived when the translation is moved back into the source language. • Stylistic evaluation. The aim is to control the clarity and syntax of the target language, which depends on the style of the source language and on the features of the individual idiolect. Special attention should be paid to certain stylistic aspects (e.g., "transport service" instead of "service of transport"), misspellings, and typos (e.g., "women" instead of "woman," "ig" instead of "big," etc.).Task 5. Ontology update.The goal of this task is to update the ontology with the label translations obtained for each localized label. The localization manager/QA department carries out this task taking as input the selected label translations. The activity output is an ontology enriched with labels in the target language associated to each localized term. The ontology enrichment can follow two different modeling options. If only labels in different languages are to be included in the ontology, we can make use of the rdfs:label and rdfs:comment properties of the OWL language (model 1). If, on the other hand, the final application demands further linguistic data than just labels, an external model capturing linguistic descriptions can be associated to the ontology (model 2). The choice of the modeling option for the linguistic information will be mainly determined by two factors:• The type of domain of knowledge represented by the ontology • The amount of linguistic information required by the final application Taking these variables into account, we envision the two following scenarios:• If the conceptualization represents a consensual domain, we can opt for the inclusion of multilingual information in the ontology (model 1) or for the association of an external model with the ontology (model 2). The decision between these two options will depend on the linguistic needs of the final application. An illustrative example of model 2 can be seen in Fig. (see Sect. 8.4). If morphosyntactic data are needed for the purpose of information retrieval or information extraction, for example, the most suitable option will be the association of an external model. In the state of the art, we find some suitable models in this sense, such as LingInfo Example
In this section, we include an example of the use of the proposed guidelines for the ontology localization activity and the obtained results. In particular, the example refers to the automatic localization of the "economy activity" ontology, an ontology developed in the SEEMPLabelTranslator has been designed with the aim of automating ontology localization, and it has been implemented as a plugin of the ontology editor NeOn Toolkit. In its current version, it can localize ontologies in English, German, and Spanish. In its design, the guidelines proposed above have been followed.In order to illustrate the results obtained by our system, we will consider the extract of the sample economy activity ontology shown in Fig. In the following, we briefly describe how the tasks are performed by our system and which techniques and tools are used for each task.Task 1. Select the most appropriate linguistic assets. The linguistic assets used by the current version of the LabelTranslator plugin are multilingual resources (Wiktionary or IATE), translation web services (Google Translate, BabelFish, etc.), Semantic Web resources (EuroWordNet and thirdparty resources retrieved through WatsonFor each ontology element, LabelTranslator retrieves its local context, its neighbor terms, which is interpreted by the system using a structure-level approach. In our approach, the context of an ontology term is used to disambiguate the lexical meaning of an ontology term. To determine the context of an ontology term, the system retrieves the labels of the set of terms associated with the term under consideration. The list of context labels comprises a set of names which can be direct label names and/or attributes label names, depending on the type of term that is being translated.To mitigate risks associated with system performance, LabelTranslator limits the number of context labels used to disambiguate the translated label. Every context label is compared with the ontology label under consideration using a measure based on normalized Google distance (NGD) In Fig. • In Step 1, the system obtains equivalent translations for all selected labels by accessing the linguistic assets listed in Task 1. • In Step 2, the system retrieves a list of semantic senses for each translated label, querying Watson and EuroWordNet. Each sense is represented as a tuple:where s is the list of synonym names, grph describes the sense by means of the hierarchical graph of hypernyms and hyponyms of synonym terms found in one or more ontologies, and descr is a description in natural language of such a sense. As matching terms could correspond to ontology concepts, attributes, or instances, three lists of possible senses are associated with each translated label t: S t concept , S t attribute , S t instance . Notice that to perform cross-language sense translations, the external resources are limited to those resources that have multilingual information like EuroWordNet.The multilingual retrieval of a word sense (synset) in EuroWordNet is done by means of the InterlingualIndex (ILI) that serves as a link among the different wordnets. For example, when a synset, such as "bar" with the meaning "the professional position," is retrieved from the English wordnet, its synset ID is mapped through the ILI to the synsets IDs of the same concept in the different language-dependent wordnets (German, Spanish, etc.) that describe the same concept but contain the word description in its specific language. A similar retrieval process is used in the case of multilingual ontologies but using the references between concepts and labels as offered by the standard rdfs:comment and rdfs:label properties.Coming back to our example, in Fig. The ranking method we use to compare structures relies on an equivalence probability measure between two candidate structures, as proposed in • Stylistic evaluation. The current version of LabelTranslator does not support an automated stylistic evaluation. This task was manually carried out by an expert in the domain. The translations proposed were consistent in all cases, according to the context of the ontology.Task 5. Ontology update.The ontology is updated with the resulting linguistic data, which are stored in the LIR model, a separate module adopted by the LabelTranslator NeOn plugin for organizing and relating linguistic information within the same language and across languages to domain ontologies. Figure However, it is worth mentioning that the applicability of these guidelines has also been proved in one of the use cases of the NeOn project (Food and Agriculture Organization of the UN). In order to evaluate the quality of the translations obtained by our system, different experiments were designed. The experiments were carried out by comparing the translations provided by an expert (gold standard) with the translations provided by the ranking algorithm used in LabelTranslator. The ontology corpus used for the evaluation was selected from the set of the KnowledgeWebConclusions
In this chapter, we have presented the methodological guidelines that we propose to help ontology practitioners in the localization activity. These guidelines assume that users have some knowledge on ontology localization. However, the guidelines are presented so that nonexperts can understand them. To the best of our knowledge, the study presented here is the first attempt to offer guidelines for the localization of ontologies. These guidelines have not been formally evaluated. Nevertheless, we have validated their applicability using them in one of the ontologies used in the SEEMP project. We cannot assure that the guidelines will be valid in all localization scenarios, but further validation of the guidelines will be possible in future ontology localization projects with different settings. The localization guidelines also meet the sufficient conditions of any methodological guidelines for localizing an ontology to different natural languages. Specifically, the ontology localization guidelines are:• Grounded on existing practices because they have been defined by combining tasks of existing methodological guidelines • Collaborative because they contemplate the participation and consensus of different actors distributed geographically • Open because they do not limit the types of ontologies or the specific ontology terms (classes, object, or datatype properties) to be considered in localization nor the resources that should be employed in the actual translation • Usable because they are clearly documented and their use does not involve a great effortThe applicability of the ontology localization methodology has been proved in the SEEMP project where this methodology has been used for the localization of the "occupation" ontology, by means of using the guidelines proposed in this chapter. In this project, we have proven that it is feasible to perform a manual localization using basic guidelines instead of a tool-focused approach.Chapter 9 Ontology (Network) Evaluation Marta Motivation
Ontology (network) evaluation plays a key role in ensuring the quality of ontology networks, and it is employed within various ontology engineering scenarios. The main scenario is that of ontology development, namely the process during which the ontology is built. The goal in this case is to assess the quality and correctness of the obtained ontology. The process of ontology development can be achieved through different methods and the evaluation of the obtained ontology changes accordingly. For example, an ontology could be obtained through automatic extraction from representative data sources such as text Alternatively, the ontology development phase could also involve an ontology evolution activity where a base ontology is extended, either manually or through automatic means, in order to cover new domain terminology or to correspond to new application requirements Additionally to ontology development, another scenario where ontology evaluation plays an important role is that of ontology selection. With the recent advances in the area of the Semantic Web, in particular the proliferation of online available ontologies and semantic search engines such as WatsonA final usage scenario is during the ontology modularization process that leads to a network of interconnected ontology modules (Chap. 10), whose quality is iteratively assessed in order to decide whether the modularization has reached the expected results.In this chapter, we further explore ontology (network) evaluation by providing a definition (Sect. 9.2), methodological guidelines (Sect. 9.3), and concrete examples (Sect. 9.4).Definitions and Filling Card
Ontology evaluation is defined as the activity of checking the technical quality of an ontology against a frame of reference (Sua ´rez-Figueroa and Go ´mez-Pe ´rez 2008). Intuitively, whenever an evaluation is performed for a certain ontology (or alignment) aspect (e.g., modeling correctness), the process is always guided by the evaluator's understanding of what is best and what is worse. In some cases, these boundaries (which we refer to as frame of reference) are clearly defined and tangible (e.g., a reference ontology, a reference alignment), but in other cases, they are weakly defined and may be different from one person to another, or even across evaluation sessions. The NeOn Glossary distinguishes two types of ontology evaluations depending on the frame of reference used:• Ontology validation is the ontology evaluation activity that compares the meaning of the ontology definitions against the intended model of the world that itOntology Network Evaluation
Definition
Evaluation of Ontology Networks refers to the activity of checking the technical quality of the ontology network against a frame of reference.Goal
The goal is to compare the ontology network with the specification requirements and gold standards (if available) by taking into account evaluation criteria and applying various evaluation approaches, yielding evaluation results and advices on how to improve the ontology network.Input
A set of ontologies with interconnection links (network).Output
• Evaluation results in the form of quantitative and qualitative measures, and informal advices on the possible ontology network modifications. • A ranked list of ontologies.Who
• Domain experts, users, ontology developers and practitioners from the ontology development team. • Applications which automatically evaluate and reuse ontologies.When
• This activity should be carried out in parallel with the ontology network development and evolution, and after parts of the ontology network are (at least partially, as prototypes) implemented. • It also plays an important role during ontology selection and modularization. Fig. The filling card shown in Fig. Ontology Network Evaluation Workflow and Guidelines
In this section, we describe the NeOn methodological guidelines for carrying out the ontology network evaluation activity. Besides prescribing a methodology, our aim is also to provide a brief overview of the various evaluation methods and techniques that can be used in each step of the methodology.We propose a component-based evaluation approach where each element of the network (e.g., ontologies and alignments between ontology pairs) is evaluated as a stand-alone individual and then the findings of these evaluations are summed up (Fig. Task 1. Selecting individual components of the ontology network. In a first instance, the ontology development team identifies the elements of the network that need to be evaluated including individual ontologies (1) which ontology network elements are critical for the overall network and (2) which of these elements can actually be evaluated. The latter means that there must exist some frame of reference against which these individual components can be, at least in principle, evaluated. As we discussed before, the frame of reference is not necessarily tangible, but can be some idea of the perfect model, or canon, defined by the human evaluator for the particular evaluation task. Examples of frames of references will be given at Task 3.Task 2. Selecting an evaluation goal and approach. For evaluating individual ontologies, the team needs to decide the goal of the evaluation and select an appropriate evaluation approach (as summarized in Table • Domain coverage -Does the ontology cover a topic domain? The extent to which an ontology covers a considered domain is an important factor to be considered both during the development and the selection of an ontology. The evaluation approaches employed to achieve this goal imply the comparison of the ontology to frames of references such as a gold standard ontology • Suitability for an application/task -Is the ontology suitable to use for a specific application/task? Analyzing the degree of interlinking between an ontology and other ontologies (e.g., in terms of reused terms or ontology imports) as well as relying on social rating systems are two key approaches to achieve this goal.Task 3. Identifying a frame of reference and evaluation metric. While in Task 2 the ontology development team decides on the key goal(s) of the evaluation and potential approaches, in Task 3, the team needs to select the concrete ingredients of the evaluation, consisting of:• A frame of reference -What are we comparing against? The frame of reference denotes a set of representative resources that sets a baseline value against which the ontology should be compared. • Evaluation metric(s) -How to measure the features of the ontology that will be compared? Example evaluation metrics are precision and recall, cost-based evaluation metrics, measures of similarity between an ontology or a mapping, and a corpus (domain knowledge), and lexical metrics. Table As exemplified in Table Similarly to • Gold standard: The frame of reference is defined by a baseline ontology or some other kind of structured representation of the problem domain for which an appropriate ontology is needed. A gold standard is often used when the goal of  reviewer with respect to the ontology Additionally, and based on the way in which human evaluators assess ontology quality features (by comparison with their mental idea of the perfect model or canon for these features), we have identified the next three nontangible frames of references as ideal models of topologies, languages, and ontology-construction methodologies, which constitute the boundaries within which comparisons are based when performing the evaluations: (a) the ontology with the optimal topology, (b) the potentially most powerful and expressive ontology language, and (c) the perfect set of steps to follow and requirements to fulfill in order to achieve the best modeled ontology. All these canons or ideal models of topologies, languages, and methodologies are weakly defined since they may vary across evaluations and across the evaluators who defined them.• Topology-based: The frame of reference is defined by the minimum or maximum possible values of the topology evaluation metrics among ontologies within the network, or among ontology entities within the same ontology. Topology metrics automatically assess ontology quality features as well as adoption and use features, by measuring the interlinking structure of ontologies across the network Task 4. Applying the selected evaluation approach. Applying the selected evaluation approach requires a proper setup for the evaluation experiments and implementation of software tools to compute the evaluation metrics, and/or engage the human experts in stimulating sessions to collect their evaluations. We advise ontology developers to refer to the relevant scientific publications cited in this chapter for example evaluation setups and best practices. Evaluation approaches that rely on human judgment Task 5. Combining and presenting individual evaluation results. This task highlights the weakest spots in the ontology network by considering individual evaluation results and how they affect the rest of the network. The evaluation results derived for individual components are combined to reach a global understanding of the network's quality. The final task is to present the results of the evaluation in an appropriate form for possible repair (corrections, additions), improvements, and future evolution of the ontology network.Examples of Ontology Evaluation
Since ontology network evaluation is not a widespread activity as yet, in this section, we present examples of various ontology evaluation studies and show how their stages map to the tasks prescribed by our guidelines. The examples cover all the key evaluation goals described in Task 2: domain coverage (Sect. 9.4.3), quality of modeling (Sects. 9.4.1 and 9.4.2), suitability for an application (Sects. 9.4.3 and 9.4.4), and adoption (Sect. 9.4.5).Evaluation of an Individual Ontology
In this example, we describe the evaluation of YAGO [Task 2] Since the evaluation was performed in an ontology development scenario, the authors' goal was to assess the quality of modeling of YAGO, namely its precision with respect to the data sets from where it has been derived. The approach was that of evaluating the precision by using human expert opinion.[Task 3] To evaluate the precision of an ontology, its facts have to be compared to some ground truths. Since there is no computer-processable ground truth of suitable extent to be used as a frame of reference, the authors relied on manual evaluations against Wikipedia content, which was the frame of reference.[Task 4] During the evaluation, human judges rated as "correct," "incorrect," or "don't know" facts that were randomly selected from YAGO. Since common sense often does not suffice to judge the correctness of the YAGO facts, a snippet of the corresponding Wikipedia page was also presented to the judges. Thus, the evaluation compared YAGO against the ground truth of Wikipedia (i.e., it does not deal with the problem of Wikipedia containing some false information). Thirteen judges evaluated a total of 5,200 facts (ground relations between YAGO entities).[Task 5] The authors use a tabular format (Table This tabular presentation helps identifying the least precise relations and fosters the analysis of such cases. It can be concluded, for example, that a key source of error are inconsistencies of the underlying sources. For example, for the relation bornOnDate, most false facts stem from erroneous Wikipedia categories (e.g., persons born in 1802 are in the 1805 Births Wikipedia category). For facts with literals (such as hasHeight), many errors stem from a nonstandard format of the numbers (e.g., height is considered 1.6 km, just because the infobox says 1,632 m instead of 1.632 m). Occasionally, the data in Wikipedia was updated between the time of extraction and the time of the evaluation. This explains many errors for frequently changing properties such as hasGDPPPP and hasGini.Pattern-Based Ontology Evaluation
In this section, we show how ontology design patterns, specifically content design patterns (CPs), are used to evaluate an ontology. The example does not cover the complete evaluation of the ontology, but presents one specific case where a CP assisted in finding potential problems and additionally suggested a solution. The example is set within the fishery domain, and the evaluated ontology is version 0.3 of the "fishing areas" ontology, modeling the division of water areas into divisions and subdivisions. An example is the FAO major fishing area 51, Western Indian Ocean, and its subareas numbered from 1 to 8, where 1 corresponds to the Red Sea and 2 to the Persian Gulf, but where the subdivisions of these subareas are only numerically identified. [Task 2] The goal of the evaluation was assessing the quality of modeling, and the chosen approach was manual evaluation by an ontology pattern expert.[Task 3] The expert used the pattern catalog available in the ontology design pattern portal[Task 4] The ontology used a locally defined, transitive, "part-of" relation to model the division of subareas and further levels of divisions and subdivisions, thus using the same modeling approach as the "part-of" content pattern. This modeling solution, however, is not suitable for certain contexts, because, when using reasoning, it is not possible to distinguish between the direct and the indirect subparts of an area. For example, if the hierarchical structure of the partitioning of the areas should be reconstructed, for example, for browsing the ontology in a graphical interface, or when answering "what are the divisions of the Red Sea?," only the direct subareas of the Red Sea are of interest rather than all the inferable parts.The "componency pattern" provides a modeling alternative using two inverse object properties: "hasComponent" and "isComponentOf." These are nontransitive properties that can be used in combination with the "part-of pattern" to both register general partitioning but also the nontransitive property of a "proper part," i.e., a direct component of something. When using these two patterns as "gold standards" for modeling, the ontology evaluator can discover the potential problem of a missing nontransitive property to distinguish the different "levels" of area decomposition and propose an appropriate solution.Multiple Evaluations of an Ontology
An example of how various types of evaluations shed light on different aspects of an ontology is provided in [Task 2] In this ontology development scenario, the evaluations had several complementary goals. First, the authors aimed to assess whether the extracted ontology would be a good starting point for building an ontology and relied on an expert evaluation approach for this (shown as evaluation 2 in Fig. [Task 3] The authors made use of the following frames of references and metrics. For evaluation 2, the frame of reference consisted in the expert's knowledge of the domain as he was asked to review and rate the extracted concepts as either correct or spurious or new. A precision value was then computed as a ratio of the correct and new concepts over all extracted concepts. For evaluation 3, the authors used the gold standard ontology as a frame of reference and computed metrics such as lexical overlap (LO -the ratio of overlapping concepts), ontological improvement (OI -the ratio of new concepts that were not in the gold standard but were domain relevant), and ontological loss (OL -the ratio of gold standard concepts which were not extracted). For evaluation 4, the application ontology was used as a frame of reference and compared to the extracted ontology using the metrics defined for evaluation 3.[Task 4] Task 4 consisted in the evaluation performed by the domain expert as well as the computation of the various ontology comparison metrics.[Task 5] The authors sum up the results of the various evaluations in tabular form and perform a subsequent analysis of these results. For example, Table Task-Based Ontology Evaluation
The authors of [Task 2] The goal is to understand the suitability for a task, and the approach consists in exploiting ontologies to support web search and measuring the improvement in terms of search precision obtained in an experimental setting.[Task 3] The frame of reference is defined by the performance scores obtained in a web search task with an original version of the ontology. The metrics used measure ontology features important for certain search tasks (e.g., FFF, EXF).[Task 4] The experimental setup consists of relying on two groups of users to perform web search using WebOdIR within four different domains (two search tasks per domain, i.e., eight tasks in total). WebOdIR exploited a set of ontologies for one group and the extended version of the same ontologies for the second group. The performance score of the search task is computed and compared across the two versions of the ontologies as well as correlated with the computed values of the newly introduced metrics.[Task 5] The authors present these correlations in both tabular and graphical form and conclude on the influence of ontology features on various search tasks. For example, they found that more instances and object properties improve fact finding, while the addition of disjoint and equivalent concepts is beneficial for explanatory and comprehensive search tasks.Evaluating Ontology Adoption and Use
The work of Cantador and colleagues [Task 2] Two main evaluation goals are considered when selecting the optimal ontology: (a) the domain coverage and (b) the adoption and use of the ontology.[Task 3] To evaluate domain coverage, authors select a gold standard as a frame of reference. This gold standard is a representation of the domain of interest and is semiautomatically generated by the user with the support of the tool. To generate it, the user (a) introduces an initial set of terms or selects a textual source from which a set of terms representing the domain of interest can be extracted, (b) complements this set of terms by selecting additional terms from a ranked list, automatically generated by the system by considering previous user-generated gold standards, and (c) extends this set of terms by selecting suggested hypernym, hyponym, and synonym relations from WordNet. To evaluate the adoption and use of the ontologies, this work relies on an assessment by humans' frame of reference. Users share their own experiences by evaluating the used ontologies according to five criteria: correctness, readability, flexibility, level of formality (highly informal, semi-informal, semiformal, and rigorously formal), and type of model (upper-level, core-ontology, domain-ontology, task-ontology, and application-ontology).[Task 4] The tool evaluates the ontologies in two phases. First, the ontologies are evaluated according to their domain coverage by comparing them against the semiautomatically generated gold standard using lexical and taxonomical similarity measures. Second, the ontologies with sufficient domain coverage are assessed on their level of adoption and use with the help of a collaborative filtering algorithm [Task 5] The representation of the results differs for the two types of evaluations. For domain coverage, the tool presents a ranked list of ontologies including their individual scores for the lexical and taxonomical evaluation measures, as well as a combined evaluation score. After the adoption and usage evaluation, the list of ontologies is reranked, and the collaborative ontology evaluation score is added to the previous scores. In addition, the system allows the user to provide her own judgment of the ontology so that her assessment can be exploited for future ontology evaluations and selections.Relevant NeOn Toolkit Plugins
Given the complexity of the ontology evaluation task in terms of the variety of approaches and metrics, the NeOn Toolkit does not provide an evaluation plugin per se. However, various plugins exist that can support different evaluation approaches. We provide a brief description of these plugins here.The RaDON pluginThe XDTools pluginThe Watson for knowledge reuseSummary
Ontology evaluation is an important and complex ontology engineering activity. Its complexity stems both by its applicability in a variety of scenarios (Sect. 9.1) as well as the abundant number of existing approaches and metrics. In this chapter, we aimed at providing practitioners with the right balance of generic guidelines and specific techniques that they could use from the wide landscape of works in this area (Sect. 9.2). We hope that the five diverse evaluation examples in Sect. 9.3 will serve as useful material for exemplifying the proposed guidelines.Although ontology networks contain both ontologies and their links in terms of alignments, we have mostly focused on ontology evaluation. Readers interested in ontology alignment evaluation should also consult Chap. 12. Finally, Chaps. 10 and 11 describe other ontology engineering activities that can benefit from ontology evaluation, namely ontology modularization and evolution.Chapter 10
Modularizing OntologiesMathieu d'Aquin
Abstract As large monolithic ontologies are difficult to handle and maintain, the activity of modularizing an ontology consists in identifying components (modules) of this ontology that can be considered separately while they are interlinked with other modules. The end benefit of modularizing an ontology can be, depending on the particular application or scenario, (a) to improve performance by enabling the distribution or targeted processing, (b) to facilitate the development and maintenance of the ontology by dividing it in loosely coupled, self-contained components or (c) to facilitate the reuse of parts of the ontology. In this chapter, we present a brief introduction to the field of ontology modularization. We detail the approach taken as a guideline to modularize existing ontologies and the tools available in order to carry out this activity.Motivation
In complex domains such as medicine, ontologies can contain thousands of concepts. Examples of such large ontologies are the NCI (National Cancer Institute) Thesaurus 1 with about 27,500 and the Gene Ontology 2 with about 22,000 concepts. However, problems with large monolithical ontologies in terms of reusability, scalability, and maintenance have led to an increasing interest in techniques for dividing ontologies into sets of cohesive, self-contained modules; for extracting modules from ontologies relevant to a sub-domain or a task; as well as for M. d'Aquin (*) Knowledge Media Institute (KMi), The Open University, Walton Hall, Milton Keynes, MK7 6AA, UK e-mail: m.daquin@open.ac.uk combining and manipulating ontology modules. We observe however that there is no universal way to modularize an ontology and that the choice of a particular technique or approach should be guided by the requirements of the application or scenario relying on modularization.In particular, ontologies that contain thousands of concepts cannot be created and maintained by a single person. The broad coverage of such large ontologies normally requires a team of experts. In many cases, these experts will be located in different organizations and will work on the same ontology in parallel. In other situations, large ontologies are mostly created to provide a standard model of a domain to be used by developers of individual solutions within that domain. While existing large ontologies often cover a complete domain, the providers of individual solutions are often only interested in a specific part of the overall domain.Also, the nature of ontologies as reference models for a domain requires a high degree of quality of the respective model. Representing a consensus model, it is also important to have proposed models validated by different experts. In the case of large ontologies, it is often difficult, if not impossible, to understand the model as a whole.On a technical level, very large ontologies cause serious scalability problems. The complexity of reasoning about ontologies is well known to be critical even for smaller ontologies. In the presence of ontologies like the NCI Thesaurus, not only reasoning engines but also modelling and visualization tools reach their limits. Currently, there is no modelling tool that can provide convenient modelling support for ontologies of the size of the NCI Thesaurus.All these problems are a result of the fact that a large ontology is treated as a single monolithic model. Most problems would disappear if the overall model consists of a set of coherent modules about a certain sub-topic that can be used independently of the other modules while still containing information about its relation to these other modules.In the next sections, we describe a general guideline to the modularization of ontologies and tools that can be used to support this activity. We identify three approaches which can be involved in realizing the modularization of an ontology: ontology partitioning, ontology module extraction and ontology module composition.Ontology Modularization
We consider an ontology O as a set of axioms (sub-class, equivalence, instantiation, etc.) and the signature Sig(O) of an ontology O as the set of entity names occurring in the axioms of O, that is, its vocabulary. As described in the NeOn Glossary (Sua ´rez-Figueroa 2010), ontology modularization refers to the activity of identifying one or more modules in an ontology. A module is considered to be a significant and self-contained sub-part of an ontology. Therefore, a module M i (O) of an ontology O is also a set of axioms (an ontology), with the minimal constraint that Sig(M i (O)) Sig(O). Note that, while it may often be desirable, it is not always the case that M i (O) O.Ontology Partitioning
The activity of partitioning an ontology consists of splitting up the set of axioms into a set of modules {M 1 , Á Á Á, Mk} such that each M i is an ontology, and the union of all modules is semantically equivalent to the original ontology O (see Fig. The method of MacCartney et al. ( The method of Cuenca A tool that produces sparsely connected modules of reduced size was presented in Later in this chapter, we describe a method for ontology partitioning based on enforcing good properties in the dependency graph between the resulting modules. Ontology Module Extraction
Ontology module extraction consists in reducing an ontology to the sub-part, the module, that covers a particular sub-vocabulary. This activity has been called segmentation in Such a technique has been integrated in the PROMPT tool The mechanism presented in In Inspired from the previously described techniques, d One important issue related to ontology module extraction is that different scenarios and applications require different ways to modularize ontologies Very similar ideas to the one described in d Ontology Module Composition
In In the same line of ideas, but in a more formalized and sophisticated way, Note finally that the OWL toolsA General Approach to Modularizing Ontologies
As we mentioned in Sect. 10.1, the goal of ontology modularization is to obtain a module or a set of modules from an ontology, which fit the requirements of a particular application or a particular scenario. Especially due to the large number of different techniques that can be used and combined to achieve these goals, there is a need for methodological guidelines to help ontology developers in selecting and applying the appropriate techniques for modularization, depending on the goal of modularization.Note that, as opposed to a single, monolithic ontology, an ontology network is essentially a modular ontology, made of components (the individual ontologies) interacting with each other in a particular context. The approach presented here is applied on individual ontologies (possibly networked) to create either networks of ontologies or elements for networks of ontologies.Generalizing and clarifying the description above, we specify the definition of ontology modularization, as provided by the NeOn Glossary (Sua ´rez-Figueroa 2010), as the activity that takes as an input an ontology and that has for goal to identify a set of modules for this ontology, effectively creating a modular version of it, for the purpose of supporting maintenance and reuse (see Fig. Figure Commonly considered benefits (and thus drivers) of ontology modularization are:• Improving performance by enabling the distribution of reasoning or by exploiting only the relevant modules of a large ontology (see Identifying the purpose of modularization is essential for the next tasks, in particular to select the appropriate modularization technique and criteria to maximize the expected benefit of modularization. Task 2. Selecting a Modularization Approach As explained at the beginning of this chapter, there are two main approaches to obtain modules from ontologies: ontology partitioning and ontology module extraction. It is generally easy to decide which one to choose according to the modularization purpose:• Whenever the purpose relates to the entire ontology (i.e. improving maintenance, and in some cases performance), a partitioning approach should be considered. • Whenever the purpose relates to extracting specific parts of an ontology (e.g. to customize it or reuse it partially), module extraction should be considered.Ontology Modularization
Definition
Ontology Modularization refers to the activity of identifying one or more modules in an ontology with the purpose of supporting reuse or maintenance.Goal
The modularization activity offers a way to cut-down potentially large ontologies into smaller, more manageable modules.Input Output
An ontology. A module or a set of modules from the input ontology. In practice, ontology modules are themselves ontologies.Who
Ontology engineer (ontology development team), curator of the ontology, preferably with the help of domain experts.When
To facilitate ontology reuse, as part of the re-engineering process, as part of a restructuring activity.Fig. 10.3 Ontology modularization filling card
Of course, this needs to be considered in the context of the overall iterative process that constitutes ontology modularization. In general, when the purpose is to obtain a set of modules to cover the entire ontology, in a first iteration, partitioning  should be considered. In subsequent iterations, intermediary modules might need to be further partitioned, or specific modules be extracted.Task 3. Defining Modularization Criteria
The modularization criteria define the basic characteristics that the resulting modules should have, that is, what should go into a module. In d Task 4. Selecting a Base Modularization Technique
As mentioned in previous sections, there is a great variety of techniques and tools for ontology modularization. In d Task 6. Combining Results
As mentioned earlier, we favour an iterative process where the adequate modules are produced by refining and combining the results obtained with various parameters, techniques and approaches. Therefore, at every iteration, everytime a new (set of) module(s) is produced, it is necessary to integrate it -that is, to combine it -with the modules that were produced at previous iterations. The way to combine depends on the criteria for modularization and on the modules already produced. Two possibilities are:• If some modules were too small or not logically complete and the current iteration produced complementary modules, then the results should be merged. • If modules from a previous iteration were too big because the employed technique did not consider some of the criteria, and a new technique is applied that implements the missing criteria, then the common part from the results of both iteration should be considered.Operators for combining modules should be employed here to derive new modules from the results of partitioning or extraction techniques, or from different iterations for the process. The three common operators should be applied in the following situations:• Intersection: when two or more modules have been produced that are complementary in the sense that they are too broad and should be reduced in relation with each other • Union: when two or more modules have been produced that are complementary in the sense that they are too narrow and should be integrated with each other • Difference: when two or more modules have been produced that are complementary in the sense that one should be narrowed down so that it does not overlap with the otherTask 7. Evaluating Modularization
The evaluation of the result of the modularization (meaning the complete set of generated modules to be included in the modular ontology) is a crucial part of the iterative process. Indeed, it depends on this evaluation whether a new iteration is necessary, applying a new set of criteria and a new technique, or if the current (set of) modules are satisfactory, considering the application scenario. There are two ways in which the modularization could be evaluated:• By checking the criteria: Evaluating whether the criteria defined for modularization have been realized as expected by the modularization technique is useful both for checking if the results match the requirements of the application and for establishing a new set of criteria in case another iteration is required. • By testing against the purpose of modularization: If the defined criteria have all been realized, it is important to check whether or not the obtained modularization actually realizes the expected improvement compared to the original ontology. For example, if the goal was to facilitate the maintenance of the ontology, the ontology engineers and domain experts should check whether the structure of the new, modular ontology has been created in a sensible way according to this purpose. Another example could be when the goal is to better support an application; in these cases, further guidelines about how to perform an application-based ontology evaluation can be found in Chap. 9.There can be three outcomes for this task. It can establish by evaluation that:• The modularization is satisfactory, so that the created modules can be finalized and deployed (Task 8). • The modularization is incomplete, so that a new iteration should be carried on, using another set of criteria and another technique to produce complementary results. • The modularization is improper, so that a new iteration is required, re-considering the set of criteria and the technique to employ in order to produce modules that better match the purpose of modularization.Note that in different iterations, only the purpose of modularization cannot change. In particular, even if the approach (extraction or partitioning) generally does not change, it is not hard to imagine scenarios in which a partitioning technique is first applied, followed by extraction procedures on the previously created modules, as showed by the example in Sect. 10.4.Task 8. Finalizing Modularization Once the produced modularization is judged satisfactory, an additional step can be required for it to be deployed and exploited in an application. For example, it is usually necessary to revise the identifiers of each of the modules so that they follow the conventions employed in the target application, to re-establish links between modules, or simply to deploy the resulting modules in a way that it is made accessible in the target application and the editorial workflow.Example
We consider the scenario where a large monolithic ontology has been developed in the past, and this needs to be modularized in order to facilitate its maintenance. The purpose of the modularization has therefore been clearly identified (Task 1). In this case, it is clear that what is required is to produce a set of modules that together cover the entire ontology. Thus, in Task 2, the partitioning approach is selected. Considering that the purpose is to facilitate maintenance, the major criteria (Task 3) to take into account are:• The sizes of the modules, which should be small enough to be easily manageable but not too small so that the ontology curator does not have to handle too many different modules for a particular management task • The relations between modules, which should favour a well-structured organization in the dependency of the modules Considering both criteria above, it is decided to apply the NeOn Toolkit plugin for ontology partitioning (see Sect. 10.5), which works on the dependency graph of modules and intends to provide good structures for this dependency graph (Task 4). The only parameter for this technique is the minimum size of a module (Task 5), which is chosen according to the size of the initial ontology. The resulting partition is described in Fig. The goal of the second iteration is to extract from one of the modules produced previously, the elements related to one particular topic. Thus, we chose to follow the extraction approach (Task 2). The criteria here are mainly that the extracted module should contain ontological elements relevant to this particular topic (Task 3). A specific ontology module extraction technique is selected for this (Task 4) and used to generate relevant modules on the basis of a set of core terms defining the topic (Task 5). The result is depicted in Fig. Tool Support
The abstract example presented above provides an illustration of the overall activity of modularizing an existing ontology, using the iterative method we propose, based on different modularization approaches, and combining results from different techniques. Ideally, the tools necessary to achieve this activity of modularizing should be integrated within the same ontology engineering environment in which the ontologies are developed. Here, we present the tools integrated in the NeOn Toolkit in order to realize ontology partitioning, ontology module extraction and ontology module composition. Together with the NeOn Toolkit, these tools represent an integrated environment for creating and manipulating ontology modules.Ontology Partitioning
Our method for ontology partitioning is based on basic requirements concerning the resulting modularization and its structure. We consider that the result of the partitioning process should not only be a bag of modules but should also provide the relations between them in terms of dependency. In addition, some good properties for this structure should be enforced in order to facilitate the manipulation and maintenance of the modularization.As our approach is based on the dependency structure of modules, we need to define this relation of dependency. We consider a module M 1 to be dependent on a module M 2 if there is at least one entity in M 1 whose definition or description depends on at least one entity in M 2 . The definition or the description of an entity From this definition, we can see that if a module M 1 depends on a module M 2 , it means that M 1 should import M 2 . The main particularity of our approach is that we want the dependency structure of the resulting modularization to have good properties in order to be efficient in facilitating further engineering of the obtained modular ontology. In other terms, as shown in Fig. 1. Rule 1 (no cycle): There should not be any cycle in the dependency graph of the resulting modularization. The rationale for this rule is that we are trying to reproduce the natural situation where modules would be reused. Creating bidirectional interdependencies between reused modules is a bad practice as it introduces additional difficulties in case of an update of one of the modules or when distributing modules In addition, in order to ensure not only that the structure of the modularization respects good properties but also that individual modules are easy to manage and to handle, we add two rules on the characteristics of each module:1. Rule 3 (size of the modules): A module should not be smaller than a given threshold. Indeed, initial experiments have shown that applying only the two rules above can result in very small modules. Too small modules can be hard to manage, as it can result in having to consider too many different modules for a given task (e.g. update) Having the above rules defined, our algorithm for partitioning ontologies is reasonably straightforward. It basically consists in starting from an initial modularization with as many modules as entities in the ontology. From this initial modularization, the algorithm iteratively enforces rules 1 and 2, merging modules when necessary. At the end of this step, a modularization that respects rules 1, 2 and 4 is obtained. The last task consists in merging modules that are too small according to the given threshold, ensuring that this merging ends up in modules that respect both rules 3 and 4.Figure Ontology Module Extraction
In d 1. Users have different, more or less well-defined ideas about what module extraction should do, varying from very elementary cases (e.g. extract a branch) to complex, abstract requirements (should extract everything that helps in interpreting a particular entity). Hence, each of the scenarios we encountered would require a different approach for module extraction. 2. Users want to keep in control of the way the module is created. It is required to support the parameterization of the module extraction for the user to be able to really 'choose' what goes into the module.For these reasons, we implemented a plugin for the NeOn Toolkit to realize module extraction, providing an interactive and iterative approach to this activity. This plugin integrates a number of different 'operators' for module extraction, most of them being relatively elementary: based on an initial set of entities, extract the super-/sub-classes, entities they depend or that depend on them, common super-/ sub-classes, sub-/super-properties, all classes of instances, or all instances of classes. The interface for this plugin (Fig. In addition, the plugin provides straightforward functions to facilitate the selection of the entities to consider for module extraction. This includes restricting the visualization to classes, properties or individuals and searching for entities matching a specific string. Once a module is created, it can simply be saved as part of the current ontology project and become itself processable as an ontology (module) to be composed or partitioned using the other modularization plugins.Ontology Module Composition
A simple module algebra (including operators for Intersection, Union and Difference of module) is implemented in a dedicated plugin, which is realized as a new NeOn Toolkit view. As shown in Fig. Finally, the user can specify whether the application of the operators should be sensitive to differences in the namespace. If not, the operators only consider local names. This is for example relevant for the Difference operator applied to two versions of the same ontology -as often, the namespace changes from one version to another (and thus all elements in the ontology), a difference based on the fully qualified names would not be very meaningful.Conclusion
In this chapter, we motivated and gave an overview of the activity of ontology modularization. We described a general approach for modularizing ontologies and the tools that have been developed for the NeOn Toolkit ontology engineering environment to support this approach. However, even with the provided tool and methodological support, modularizing an ontology is still a very time-consuming task, not only because of the expensive computation it requires but also because of the expertise and experience needed from the ontology engineer to obtain the desired result (which is very often very hard to establish). We described a simple 'abstract' example of ontology modularization. Further to this work, the empirical analysis of existing modular ontologies and of the process of modularizing existing ontologies could give us further insight into the broad notion of ontology modularity.Chapter 11
Ontology Evolution
Rau ´l Palma, Fouad Zablith, Peter Haase, and Oscar Corcho Abstract Ontologies are dynamic entities that evolve over time. There are several challenges associated with the management of ontology dynamics, from the adequate control of ontology changes to the identification and administration of ontology versions. Moreover, ontologies are increasingly becoming part of a network of complex relationships and dependencies, where they reuse and extend other ontologies, have associated metadata in order to ease sharing and reuse, are used to integrate heterogeneous knowledge bases, etc. Under these circumstances, a change in an ontology does not only affect the ontology itself but may also have consequences in all its related artifacts. In this chapter, we propose methodological guidelines for carrying out the ontology evolution activity. We target different scenarios, supporting users in the process of ontology evolution from a generic perspective and on how to use tools that semiautomatically assist them in discovering, evaluating, and integrating domain changes to evolve ontologies. To illustrate their applicability, we describe how such guidelines have been used in real example applications.Motivation
Ontologies are fundamental building blocks of the Semantic Web and are often used as the knowledge backbones of advanced information systems. As such, the growing use and application of ontologies in many different areas during the last years has led to an increasing interest of both researchers and industry in the construction of ontologies and the reuse of existing ones. Reusing existing ontologies instead of creating new ones from scratch has many benefits: it lowers the time and cost of developing new ontologies, avoids duplicate efforts, eases interoperability, etc. As a consequence, complex networks of ontologies are being created where each ontology may depend on several others and may also be related to other artifacts (e.g., individuals, mappings, applications, and metadata).Nevertheless, this situation also brings about new issues. Ontologies (like many other system components) are dynamic entities. An ontology, defined as a formal, explicit specification of a shared conceptualization The management of ontology dynamics raises many challenges such as the identification and administration of different ontology versions or the flow control of ontology changes (i.e., when and how an ontology can change). Moreover, dealing with ontology changes involves the execution of many related tasks. Most of these tasks are already identified in the context of the ontology evolution process, defined in While it seems necessary to apply the ontology evolution activity consistently for most ontology-based systems, it is often a time-consuming and knowledgeintensive activity, as it requires a knowledge engineer to identify the need for change, perform appropriate changes on the base ontology, and manage its various versions. While existing evolution frameworks normally include a description of the life cycle, this description is neither meant nor suited to replace guidelines. Therefore, we propose here methodological guidelines for supporting ontology developers during the evolution of the ontologies and for supporting them in exploiting tools to facilitate the evolution of their ontologies.It is worth noting that both ontology evolution and ontology versioning deal with the management of ontology changes. However, they differ in their focus: ontology evolution focuses on the modification of an ontology, possibly preserving its consistency, whereas ontology versioning focuses on creating and managing different versions of the ontology.We argue that in order to provide a comprehensive support for ontology evolution, targeted at users and ontology engineers, we need two types of guidelines: one that guides users in the process of ontology evolution from a generic perspective and another that provides guidelines on how to use tools that semiautomatically assist users in discovering, evaluating, and integrating domain changes to evolve ontologies.The remainder of this chapter is organized as follows: First, we introduce highlevel guidelines for carrying out the ontology evolution activity. This would give an overall picture of the required tasks and possible options to handle each one, supported by an example in the fishery domain of FAO. Second, we provide guidelines for how to support users in exploiting and customizing tools that support users in evolving ontologies from external domain data using semiautomatic techniques. This is also supported by an applied example in the academic domain.Guidelines for Ontology Evolution
In this section, we present the guidelines set out to help ontology developers in the ontology evolution activity. Such guidelines have been created in the context of the NeOn Methodology for building ontology networks. This methodology takes into account the existence of multiple ontologies in ontology networks, the collaborative ontology development, the dynamic dimension, and the reuse and reengineering of knowledge-aware resources.According to the NeOn Glossary of Processes and Activities (Sua ´rez-Figueroa and Go ´mez-Pe ´rez 2008), ontology evolution refers to the activity of facilitating the modification of an ontology by preserving its consistency; it can be seen as a consequence of different activities during the development of the ontology. Thus, in the framework of the NeOn Methodology we propose the filling card for the ontology evolution, presented in Fig. Ontology Evolution Tasks
Ontology Evolution
Definition
Ontologies evolution refers to the activity of facilitating the modification of an ontology by preserving its consistency; it can be seen as a consequence of different activities during the development of the ontology.Goal
The goal of ontology evolution is to provide a defined process (potenially with tool support) to perform updates and changes to one or multiple ontologies.Input Output
An ontology in a consistant state.A ontology in a consistant state with the proposed changes implemented.Who When
Normally it occurs after the ontology has been deployed and needs to be updated. Changes during the initial creation would be part of the ontology engineering process.All ontology engineers that have to perform changes /updates to a deployed ontology.Fig. 11.1 Ontology evolution filling card
Task 1 Requesting a Change This is the initial task in the evolution of an ontology. In order for ontology evolution to have the desired outcome, it is important that the input ontology is in a consistent state. If the ontology is not in a consistent state, it has to be repaired first, using one of the different ontology diagnosis and repair tools (e.g., RaDON, see Chap. 17) or techniques before starting the evolution process. Note that we require the input ontology to be in a consistent state because dealing with an inconsistent ontology may produce unexpected results. For instance, the propagation of changes may produce inconsistencies in related artifacts. This requirement is also in accordance to existing ontology evolution approaches (e.g., Once changes are discovered or requested, they have to be represented in a formal and explicit way. Typically, a change ontology is used to model proposed/requested changes (e.g., In contrast to previous approaches in the literature, in NeOn, a layered approach for the representation of ontology changes was proposed In case there are multiple change requests for an ontology, the requested changes have to be prioritized. In order to determine which change should be implemented first, one can rely on the status of the person requesting the change, or have an ontology engineer review the requested changes and rank them according to urgency. It is also important that dependencies are considered when ranking the requested changes. It could be that changes are dependent on each other or even contradict each other.Finally, this task may include the use of a well-defined process (a workflow) for coordinating change proposals (see Tool Support in the NeOn Toolkit • RaDON plugin is an ontology diagnosis and repair tool that can be used before starting the evolution activity, i.e., before applying changes. • Tools supporting the request/discovery of changes:-The workflow feature supports the process that coordinates the proposal of changes in a collaborative environment. It supports a top-down/explicit discovery method, i.e., when changes are requested by users/developers. -The Evolva plugin supports the discovery of changes from external data sources (e.g., text, folksonomies, or RSS feeds). Changes are integrated and evaluated by relying on background knowledge such as online ontologies. In the next section, we present in details the guidelines for how to exploit such tool to apply the identified changes on the ontology and produce a new ontology version.Task 2 Planning the Change
In this task, the change request is analyzed, and it is determined why the change needs to be made and which part of the ontology is affected by the change.For that purpose, one uses impact analysis, where all potential consequences (side effects) of a change are identified along with an estimation of what needs to be modified to accomplish a change The previous analysis is also helpful to estimate the cost of evolution. Based on this cost, the ontology engineer can decide whether or not to propagate a change to a dependent artifact As a result of the analysis performed during this task, the ontology engineer may decide to implement the change, or if the change has many side effects or if the cost of implementation is too high, he may defer the change request to a later time or not implement it at all.Once the ontology developer team has decided which changes will be implemented and how they have to be implemented, the next phase of ontology evolution, namely change implementation, is entered.Tool Support in the NeOn Toolkit • The NeOn Toolkit provides simple support when deciding whether to make a change or not. In particular, when a user wants to delete an ontology element, the list of related axioms (the side effect) is shown to the editor, which permits him to verify the cost of implementing the change.Task 3 Implementing the Change Implementing the changes is of varying difficulty, depending on the impact of the requested change. While some change can be as easy as adding or removing a subclass, other changes can require complex operations and restructuring of the ontology.One of the first and foremost important features is change logging, which allows to track which changes have been made, and also allows for an easy undo, in case something goes wrong. The change log can also be published to inform people using the ontology on the updates.If the requested change turns out to be too difficult to be implemented, the ontology may need to be restructured first, before the actual desired change can be implemented One important issue to take into consideration when implementing a change is the management of inconsistencies that this change may introduce in the ontology. In case an inconsistency occurs, it has to be decided how to address it. While some approaches try to keep the ontology in consistent state at all cost by even disallowing changes introducing inconsistencies, others claim that the inconsistencies are inevitable and hence we have to deal with them. Regardless of the approach, the inconsistencies have to be identified and resolved, possibly using some tools as it was mentioned in the introduction. In the literature, this activity has been introduced in Furthermore, another important issue that has to be addressed during the implementation of the change(s) is the management of the ontology version. After the ontology changes, the ontology engineer should decide whether the resulting ontology constitutes a new version of the ontology and hence it should have a different version information. Some recommendations on the use of URIs can be found. For instance, in Finally, as aforementioned, the change(s) have to be propagated to all the ontology related artifacts (if the ontology engineer decided to do it in the previous task based on the analysis of the cost and impact). In Tool Support in the NeOn Toolkit • NeOn Toolkit ontology editor allows the manual application of changes to ontologies. • The change capturing plugin supports the logging of changes automatically from the NeOn ontology editor. It also supports the application of logs generated by other systems. Additionally, it is also in charge of propagating changes to the distributed copies of the same ontology. • RaDON plugin can be used for the management of inconsistencies.Task 4 Verification and Validation
Before the ontology is considered evolved completely, the last step deals with assessing questions whether the right ontology is built and whether it is built in the right way. During this assessment, usually not only the ontology originally modified is verified in isolation, but in general, this activity can include the verification of other artifacts related to the ontology (as mentioned above) to ensure that they were not changed in a wrong way or they have an unexpected behavior. The verification and validation step can include the following activities:• Formal verification, such as state machines and temporal logics, to derive useful properties of the system under study • Testing by users or automatically to verify whether the system behaves as expected • Debugging for localizing and repairing errors found during the verification or testing (usually performed by an ontology engineer) (for example Haase et al. ( Additionally, this task may include curation activities (e.g., approve/reject) derived from the well-defined process (e.g., workflow) that coordinates the change proposals (see Tool Support in the NeOn Toolkit • The Cicero plugin supports the justification of changes.• The workflow feature supports the refining of activities (see Fig. • The Evolva plugin checks the relevance of a change with respect to an ontology by relying on the analysis of ontological contexts and a set of identified relevance patterns supported by a confidence-based ranking Working with Networked Ontologies
The NeOn project deals with networks of ontologies and networked ontologies Example
To describe the proposed guidelines for the ontology evolution activity in a more practical way, in this section we illustrate how to perform this activity by describing an experiment conducted in collaboration with a team of FAO ontology editors in charge of the maintenance of ontologies in the fishery domain. The editors performed collaboratively a set of typical changes and actions to a stable version of one fishery ontology in order to reach a new stable version. In this scenario, a central server kept a shared copy of the ontology and the related changes. In the remainder of this section, we describe only the most relevant points. A detailed and complete description of the experiment is presented in In this scenario, different ontology editors, with different roles (Subject Experts, and Validator), were working collaboratively in the implementation of the changes and hence it was not necessary to prioritize them (prioritization of multiple changes).Each of the proposed changes was represented as an individual of the change ontology proposed in Furthermore, in this scenario, the ontology editors were following a well-defined process (workflow) for the coordination of the change proposals. As a consequence, during this task the system created for any new change proposal, the appropriate workflow action automatically (insert, update, delete). Task 2 Planning the Change For this experiment, it was necessary to implement the requested changes regardless of the side effects. Therefore, it did not perform any analysis of the impact or cost. In fact, the idea of the experiment was to assess the efficiency of the system to support the development of an ontology in a collaborative scenario, not the time or cost of implementing a change.Task 3 Implementing the Change For this task, no restructuring of the change(s) was necessary, because on the one hand the changes were not too difficult to implement due to the ontology structure, and on the other hand, the cost of implementing was not an issue.Additionally, for this task, the system (change capturing plugin of NeOn Toolkit) took care of logging automatically all of the proposed changes (change logging), maintaining the chronological history of the events.In this experiment, the change(s) did not introduce any inconsistencies in the ontology. However, in case it would be necessary to manage inconsistencies, the RaDON plugin for NeOn Toolkit could have been used to detect and fix them.As we introduced at the beginning of this section, for this experiment, the ontology and related changes were centralized in a server. Furthermore, the ontology used for the experiment was not related to other artifacts at the moment. Hence, it was not necessary any propagation of changes.Task 4 Verification and Validation
During this task, the ontology editors analyzed every change to ensure that the resulting ontology was as expected using the visualization plugins of the NeOn Toolkit.Additionally, this task was one of the most important of the experiment as it included all the curation activities derived from the workflow that coordinates the proposal of changes. Hence, in this task, an ontology validator was in charge of accepting and rejecting changes as necessary by using the appropriate workflow plugins of the NeOn Toolkit. Finally, at the moment of the experiment, there was no support for the justification of changes.Guidelines for Exploiting Tools in Ontology Evolution
In this section, we propose a methodological guideline for supporting users in identifying new and relevant domain changes from external data sources. Such guidelines aim to facilitate the process on evolving ontologies to reflect the latest changes in certain domains by analyzing various data sources. This guideline complements the tool-based support provided by the Evolva ontology evolution framework (discussed next), with concrete guidance on how to realize the various tasks of the evolution activity, using semiautomatic techniques in an efficient way.The Evolva Framework
The Evolva ontology evolution framework While the goal of the Evolva framework is to reduce, as much as possible, human intervention within the evolution process, user input is required at the level of evolution management and for fine-tuning of various parts of the framework. The role of the user is needed to properly parameterize the components, select the right sources of information and of background knowledge, validate the results of various steps, and, generally, guide the evolution process to obtain high-quality results. These tasks are not trivial, as they depend a lot on the particular ontology to be evolved, the domain covered, the applications relying on the ontology, and the reasons for its evolution. The experience of the knowledge engineer and his/her knowledge of the ontology and of the exploitable sources of information are therefore essential.Tasks
The tasks for performing a semiautomatic ontology evolution can be seen in the workflow shown in Fig. Task 2 Set the Data Sources and Extraction Parameters
Depending on the domain, domain experts should prepare the data sources that contain relevant information to the ontology context. Such data sources could be in the form of text documents, folksonomies, databases, or even other ontologies. Based on the decision of the ontology development team to evolve the ontology either in terms of schema, individuals, or both, the extraction should be customized accordingly. For example, in the case of schema evolution, the user may choose to extract concepts for the data sources, without dealing with individuals. While in the case of individuals, the evolution process could omit the extraction of schema elements. Choosing between schema and individuals evolution could be biased by the ontology functionalities and domain nature, i.e., when many ontologydependent components exist (e.g., various applications or other aligned ontologies), evolving the ontology schema may be costly, and the ontology development team may choose to perform this operation less frequently. While in environments where ontology components are easily controllable and where a lot of new information is generated leading to a frequent generation of new concepts, schema evolution would be required.Task 3 Validate Extracted Data
After extracting knowledge elements from the data sources, noise and irrelevant entities should be excluded. The user is supported by manual and automated validation techniques with customizable parameters. For the manual validation, the domain expert would serve as one of the best quality checkers as he/she is the most knowledgeable about the ontology context. This task is completed after checking that all the data are valid to be processed further by the system.Task 4 Setup Relation Discovery and Quality Check
The role of the user, after the data validation task, is to prepare the automated relation discovery process. The relation discovery process links the validated data to the ontology. This requires the user to select the various types of background knowledge sources to be used. The choice of background knowledge is directly dependent on the type of domain the ontology represents. If the domain were specialized, the user would choose domain-specific background knowledge sources (e.g., specialized thesauri). This would improve the quality of relations and increase the system precision. While if the domain is generic, using online ontologies or generic thesauri would perform well. In addition to the selection of sources, the user should fine-tune the parameters of the relation discovery process, such as the settings related to the automatic relevance checking, or specify the maximum depth to explore. In addition to the supplied automatic quality checking methods, for example, in terms or relevance, domain experts should additionally check the quality of relations, before using them later in the system.Task 5 Generate Ontology Changes and New Ontology Version
Based on the approved relations in the previous task, ontology changes are generated and applied on the new ontology version. Users should specify where to apply the changes, i.e., directly on the initial ontology or on a detached copy. The choice of where to perform changes depends on the environment and the ontology development team approach. The team should be aware that applying changes on the initial ontology would directly affect the dependent components. If this is not feasible, or designers prefer to keep the initial ontology intact while reviewing the changes, creating a detached ontology version would be more appropriate.Task 6 Validate New Ontology and Manage Changes
The user should control the changes performed on the new ontology version. With the new evolved ontology, problems such as inconsistencies and duplication are likely to emerge. Users in this task specify the checking methods to be applied on the new ontology version using reasoners, for example, in addition to manually control the recorded ontology changes.Task 7 Deploy New Ontology Version
Once the new version is approved, users should control the propagation of the new ontology version to the dependent components. Links to the previous ontology version should be checked and whether the new ontology has been successfully saved and accessible.Example
In this part, we highlight an example of ontology evolution scenario using the Evolva plugin for the NeOn Toolkit, following the guidelines presented in the previous section. We run our example in an environment where the NeOn Toolkit and Evolva pluginConsider the case of evolving the latest version of the SWRC ontologyAfter preparing the ontology and identifying the part of ontology to evolve, we move to select the data sources containing relevant information with a potentially added value to our ontology (Task 2). This is implemented in Data Sources step of We load the Leverhulme text documents and run the extraction and validation process. A list of extracted concepts is returned, with Evolva automatically identifying existing terms in the ontology and terms that fall below a length threshold. If the automatic validation performs poorly overall, it is possible for users to fine-tune the parameters and rerun the validation process again. In addition to the automatic validation, users have the ability to go through the list of concepts and manually select terms they find irrelevant (Task 3), implemented in the Data Validation step in Fig. After the data validation process and approving relevant data, we move to Task 5 of setting up the relation discovery process with the right background knowledge, sources, and parameters. The SWRC ontology domain is, to some extent, a generic academic purpose ontology. Thus, related information can be easily found through online ontologies in which a lot of academic domain ontologies can be found, as well as through WordNet, the generic thesaurus. Thus, we choose to perform the Evolva automatically harvests the chosen background knowledge sources and identifies how extracted concepts should be integrated in the ontology. If needed, Evolva also provides the option to discover relations between new concepts, before being integrated in the ontology. This has been implemented in the Relation Discovery step in Fig. After the relations linking new concepts to existing concepts in the ontology, Evolva relies on online ontologies from where the relation is identified to assess the relevance of the relation with respect to the ontology. Using identified relevance patterns, with pattern-specific confidence, relations are returned ranked to the user with the highly relevant relations placed on top Once all relations are approved and relevant, they are used to generate the ontology changes (last step in Fig. Our next task is to validate the new ontology version and manage the new changes that the ontology has been subject to (Task 6). Evolva relies on the change logging plugin After approving the final ontology version, we deploy it by double-checking the links to the previous ontology version that are automatically created by Evolva (Task 7). We also check that the ontology has been saved correctly, and that it is still accessible by doing some checks such as running queries and validating the results.Conclusion
Ontology evolution is a tedious and time-consuming task. To successfully keep the ontology up to date with domain changes, ontology engineers should be supplied with the right guidelines and tool usage to make this task easier. For that, we presented in this chapter guidelines for ontology evolution covering two aspects: a high-level ontology evolution process and tool-oriented guidelines to semiautomatically identify, evaluate, and apply domain changes to ontologies.The first aspect describes the tasks involved in the ontology evolution process from a generic perspective and discusses guidelines in possible ways to achieve each task. The second aspect aims to facilitate the process of identifying ontology changes from external domain data, checking their quality, and integrating them in the ontology, by using semiautomatic techniques. The guidelines in this case include how to use and parameterize the involved tools to achieve the optimal new ontology version.The two aspects work together to enable ontology engineers to understand the complete picture and tasks involved in ontology evolution, to successfully move from an existing ontology state to a new one with the appropriate representation of domain changes that arise.Chapter 12
Methodological Guidelines for Matching Ontologies
Je ´ro ˆme Euzenat and Chan Le Duc Abstract Finding alignments between ontologies is a very important operation for ontology engineering. It allows for establishing links between ontologies, either to integrate them in an application or to relate developed ontologies to context. It is even more critical for networked ontologies. Incorrect alignments may lead to unwanted consequences throughout the whole network, and incomplete alignments may fail to provide the expected consequences. Yet, there is no well-established methodology available for matching ontologies. We propose methodological guidelines that build on previously disconnected results and experiences.Motivation
Ontology matching is the activity of establishing correspondences between ontologies. Correspondences express relationships supposed to hold between entities in ontologies, for instance, that a 'district' in one ontology is the same as a 'kreis' in another one or that 'fishery' in an ontology is a subclass of 'company' in another one. An alignment may be used to link an ontology with its background, i.e. set it in a more general context: This is typically what is achieved by providing an alignment with an upper-level ontology. An alignment can also be used to link an ontology with its previous versions or alternative ontologies in other applications.We use interchangeably the terms matching operation (focussing on the input and result), matching task (focussing on the goal and the insertion of the task in a wider context) and matching activity (focussing on the internal processing).The ontology matching process may be summarised as in Fig. Ontology matching is a very important operation in modern ontology engineering because of the networked environment in which ontologies are engineered and supposed to work. Methodologically, it is worthwhile to express relations between ontologies because this allows (1) for working with small and self-sufficient modules instead of monolithic ontologies, (2) for expressing the links between two versions of the same ontology and thus to upgrade data from one ontology to another or (3) for putting back an ontology in the context of an upper-level ontology, allowing it to play better with other ontologies. Networked ontologies are sets of ontologies together with alignments relating the entities of these ontologies. These ontologies may be related because they are complementary, two independent domain ontologies, e.g. sales and tyres, refinement, a domain ontology specialising a top-level ontology, or supplementary, a version replacing another version or two ontologies about the same domain. In networked ontologies, alignments are as important as the ontologies themselves because relationships between ontologies are the basis of networks.Hence, methodological guidance for ontology matching is particularly required and needs to be supported for helping ontology engineers to develop semantic applications. Contrary to ontology building which is an open-ended (design) activity, ontology matching is an inductive activity bounded by the ontology to be matched. Hence, it requires a more focussed methodology.Yet, very little support exists for such an activity at the methodological or at the tool level. Even in the database field, where similar but simpler problems have existed for years, there is no consensus methodology on how schema matching can be conducted. This chapter provides guidance for matching ontologies based on existing partial guidelines and overall experience collected so far in the field.We do not consider ontology matching as an independent activity. On the contrary, we consider it as related to ontology management: When ontologies evolve, alignments must follow this evolution. Moreover, as proposed in the work of As such, alignments can be manipulated to better suit the needs of users. We consider this ontology alignment life cycle and further investigate the methodological guidelines for supporting it. In the spirit of NeOn (see Chap. 1), these guidelines put the emphasis on networks of ontologies as well as reusing ontologies and alignments.In what follows, we first introduce synthetic descriptions of the ontology matching activity (Sect. 12.2). Then, we discuss the issue of the format in which alignments have to be delivered in order to support reusable matching (Sect. 12.3) before considering step by step the proposed methodological guidelines (Sect. 12.4). Then we present support offered by tools for the proposed methodological guidelines (Sect. 12.5). Finally, examples are given (Sect. 12.6) before concluding.Ontology Matching Filling Cards
We present below two different ontology matching activities. These depend on the time at which ontology matching is supposed to take place. If ontology matching is supposed to occur at design time, then its goal is to match two ontologies for connecting them in a network; if it is to occur at runtime, then the goal of the activity is to generate a matching process that achieves ontology matching at runtime.This distinction between runtime and design time ontology matching is very important in practice because the output of the two operations is not the same. At design time, the resulting alignment is used for relating the different ontologies which will be used at runtime, for instance, for transforming queries. At runtime, ontology matching is used for finding alignments between ontologies which were not known at design time. This could be for composing semantic web services using different ontologies, for instance. When ontology matching is performed at design time (see Fig. However, functionally, these two operations can also be seen as the same since they, in practice, generate an ontology matching process which is executed at different moments. Hence, the guidelines that we apply are the same in both cases because it consists of choosing software components which are applied at different time. Matching two ontologies.
Two ontologies to be matched.When designing ontologies. In networked ontology applications, this activity can occur at any time.Ontology engineers, who form the ontology development team (ODT), in collaboration with users and domain experts.An alignment between these two ontologies, which may have been further transformed into a processable element, e.g., query mediator, merged ontologies. Alignments and Formats
Although formats should not be a main concern for methodology, it is here very important because the input and output of most of the tasks is an alignment. Hence, choosing a common alignment representation makes tasks interoperable and allows for better sharing and reusing the product of the ontology matching activity.Ontology alignments are sets of relationships between ontology entities. Alignments may be expressed in various languages. For instance, the two relations mentioned in the introduction can be expressed in OWL However, application-specific output is not particularly interoperable. It is not easy to transform a database view into OWL axioms or SKOS statements into ODEDialect Run time ontology matching
Definition
Ontology matching (in run time) is the activity which finds relationships between ontologies.Goal
Designing a process for matching two ontologies.The specification of a process for matching two ontologies.Semantic application designers.
When developing applications requiring run time matching.Input Output Who When
The characteristics of the ontologies to match and the context in which this matching operation will occur. expressed in OWL, its only possible use is to 'merge' two OWL ontologies. It cannot easily be used to import data from one ontology to another or to translate queries. Moreover, such formats are not easy to share and retrieve (see Sect. 12.4.7) or to manipulate (see Sect. 12.4.6), e.g. for merging the results of several matchers if they do not use a format that supports such manipulations. Hence, in order to avoid early commitment to a particular type of usage, it is to be preferred to keep the alignments in a declarative language. Such a language allows for manipulating and composing alignments as well as for generating the required representation (OWL, SKOS and others) when necessary.Using a neutral and declarative representation Detailed Guidelines
Ontology matching has been the focus of a lot of attention in the recent years. However, little work has been carried out on the methodological support for finding alignments. We provide here the outline for such methodological guidelines. It can be summarised by the workflow of Fig. Identifying Ontologies and Characterising Needs
The first task in finding alignments is to identify the ontologies to be matched and to characterise the need. Indeed, the type of required alignment will be different if the It goes step by step through characterising the problem, selecting existing alignments, selecting appropriate matchers, running the matchers, evaluating the results and correcting the choices made before (matchers, parameters), documenting and publishing good results and finally using them goal is to merge two ontologies in a knowledge-based system or to add yet another data source to a query mediator. In the former case, the alignment will have to be strictly correct, otherwise the system may draw incorrect inferences, but the relationships can be diverse: subsumption and disjointness assertions can be very useful. In the latter case, lack of completeness is not a problem since other sources may return the missing answers, but relations other than equivalence are not straightforwardly used in query mediation. This first task is similar to Activity 3 of the work of It is also useful to characterise the kind of ontologies: Are they labelled in the same natural language? What is their expressiveness? Are individuals related to the ontologies available?Characterising the situation in which matching will be performed should not be neglected. It will determine the choice of matchers or alignments as well as the parameters to care for. • Are instances available at match time? • Is matching performed under time constraints? • Has matching to be performed automatically?• Must the alignment be correct? • Must the alignment be complete? • What type of operation (merging ontologies, transforming queries) is to be performed?These characteristics of the situation are requirements for the ontology matching process. There has been research attempting to refine such requirements. Finding Existing Alignments
Finding existing alignments which satisfy the need of the application is the second task. Alignments may be found on the web or through specialised directories.Reusing existing alignments should be privileged because of the cost of generating such alignments. For that purpose, the task of sharing (see Sect. 12.4.7) prepares alignment retrieving.Ideally, alignments should come with annotations characterising their level of trustworthiness, the purpose for which they have been built and the type of relations they use.These alignments must concern the ontologies to be matched, and they have to satisfy the constraints related to the alignment established in Sect. 12.4.1. In particular, correctness and completeness are criteria to use for selecting among various alignments. These criteria may be assessed manually, on a sample, or can be inferred through the properties of their generation methods. In particular, one can use metadata attached to such alignments. They can reveal the method used for matching the ontologies (in particular, if these are automatic or manually generated alignments), they can cover manual assessments about the alignment (people publishing them can annotate the alignments to tell what they are good for) or they may contain indications of their intended use which can be matched with that of the current situation.So, practically, selecting an alignment requires:• Finding alignment repositories • Finding those alignments between the ontologies to match • Assessing the capacity of these alignments to address the needs previously identified, either based on metadata, or on the content of the alignments • Deciding for one alignment based on this assessment If apparently suitable alignments are available, the user can directly go to the validation task (Sect. 12.4.5). Otherwise, it is necessary to create a new alignment from the ontologies, as is explained in Sect. 12.4.3.Selecting a Matcher
In order to build a new alignment, a suitable matcher has to be found. Many matchers have been developed over the years, and they provide different results on different types of data sets and matching contexts. Hence, the criteria elicited in the characterisation phase (Sect. 12.4.1) are also used for selecting the most appropriate matcher.There have been several studies about how to choose a matcher depending on the characteristics of the ontologies and those of the expected alignments. They are worth taking into account. The work of The OntoMas system The problem of such methods is that they require extensive information about available matchers which is not always available or always accurate when the assessment comes from the matcher developers. An important source of information is the result of the various matcher evaluation campaigns that have been run. The most important one is the Ontology Alignment Evaluation Initiative (OAEI) campaignsSo, in practice, choosing a matcher can be achieved by:• Finding available matchers • Assessing their capacity to generate alignments that fill the identified requirements by reading their documentation or comparing their performances in similar tasks during evaluations • Deciding for one matcher based on this assessment Other works try to automate this task, or the choice of matcher parameters, on the fly Matching Ontologies
The next task consists of running the matcher against the ontologies and collecting the resulting alignment. It may seem like the simplest task, methodologically speaking, because matchers have been designed exactly for this purpose.But the user should not hesitate to run the matcher several times or to run several matchers, trying different sets of parameters and different thresholds. It is also useful to process matching incrementally by curating the returned alignment and feeding it again to the matcher for improving it.In fact, all the procedure that can be applied at the enhancing phase (see Sect. 12.4.6) can also be directly applied during the matching phase without any prior evaluation.Hence, this task can be further decomposed into a more complex sub-workflow (see Evaluating Alignments
Once an alignment has been obtained, it is necessary to perform a final screening and validation. Evaluation can be applied on alignments that have been retrieved as well. This task corresponds to the evaluation task of Fig. Evaluation consists of assessing properties of the obtained alignment. It can be performed either manually or automatically. Manual evaluation can be achieved by running a dry test of the final application or by asking an independent expert to assess the quality of the alignment and perform some manual assessment. For that purpose, graphical tools which allow to navigate quickly both in the alignment and in the ontologies are invaluable.An often overlooked functionality of matching algorithms is their ability to give explanation for the provided alignments. Explanations can be obtained by interacting with the matcher or by accessing metadata about a stored alignment.Automated quantitative evaluation can be performed by using techniques for evaluating alignments used in matcher evaluation campaigns such as OAEI 1 or SEALS 2 . These would require to extract samples from the results and computing measures like precision and recall which would provide an approximation of correctness and completeness.There is no definitive answer as to what is a good result for evaluation. The evaluation must be performed so as to assess evaluation criteria. The characterisation of the problem (Sect. 12.4.1) aims at defining such success criteria. For some applications, high recall is required, while for some others, recall is not important. Moreover, the meaning of 'high' is not the same for all applications: A critical application which can break if some correspondence is missing will require 100% recall while a non critical application may be satisfied with 98%. The sub-workflow of fine-tuning matchers (all tasks but matching are optional). After matching, it is possible to apply automatically some alignment manipulation that can trim the alignment under a threshold, check and restore the consistency of an alignment or compose the alignment with another alignment. The result of these manipulations can be fed back as input to the matching operation or can be the final result of the workflow. Alternatively, it is possible to modify the parameters of the matcher and to run it again. These operations can be triggered manually or automaticallyIf the evaluation results are positive, i.e. the alignment satisfies these success criteria, then the obtained alignment can go through the next task, storing and sharing (Sect. 12.4.7); otherwise, the alignment can be improved (Sect. 12.4.6) before being input to the matcher and/or another matcher and/or parameters can be chosen.Enhancing Alignments
Enhancement can be obtained either through manual modification of the alignment, e.g. with the help of an alignment editor, or the application of refinement procedures, e.g. selecting correspondences by applying thresholds. This enhancing task can lead to:• The selection of another matcher, as in Fig. Among these procedures, the most straightforward one consists of trimming the alignment under some thresholds. There are many different ways to apply automatic thresholds It may also restore consistency when the resulting alignment has been detected inconsistent in the evaluation (Sect. 12.4.5). By consistency checking, we do not necessarily mean logical consistency checking, but rather that the result does not violate particular constraints which may be:• Acyclicity • Syntactic anti-patterns Alignments obtained from various sources, such as other matchers or alignment libraries, may be composed in a single alignment through various operators: composition, meet, join and union.Storing and Sharing
An extra task is to save and share the obtained alignment in a declarative format and to give it proper annotations to record its provenance and purpose. This task is very often overlooked but it is vital if one wants to find alignments in the corresponding task (Sect. 12.4.2): carefully annotating alignments will help others to reuse them. This task corresponds to the communication task of Fig. When an alignment is deemed worth publishing, then it can be annotated, stored and communicated to other parties interested in such an alignment.Annotations of alignments should record the information that is useful for the 'finding existing alignment' task (Sect. 12.4.2). In particular, what is the purpose of this alignment, what is the assessment of its quality? Below is a sample of metadata associated with an alignment in the Alignment API: Storing an alignment requires some type of persistent storage. This is usually achieved through the use of a database management system, but a web site based on a file system may be sufficient. However, alignments must be properly indexed to retrieve them when necessary on various characteristics (one ontology, pairs of ontologies, arity, etc.). Indexing can be direct through a URI identifying alignments or indirect through queries looking for alignments based on their metadata. In general, it is preferable that both access modes be available.Finally, these alignments may be shared by interested communities. For that purpose, they should be accessible on the web through HTML interfaces or web services.There are several software supporting sharing alignments on the web. The Alignment serverRendering Alignments
Finally, the alignment is transformed into another form or interpreted for performing actions like mediation or merging.This task corresponds to the exploitation task of Fig. Rendering may deliver the alignment as such in RDF in order to be processed by an interpreter such as a query mediator. But it also can be transformed, as discussed in Sect. 12.3, into OWL axioms, SKOS relations or sets of owl:sameAs statements.The dotted arrow on Fig. Support for Matching Ontologies
We think that methodological guidelines are more useful and better accepted if they are supported by tools rather than delivered as rules to be applied. So far, existing support is available in the alignment manipulation part rather than the requirement analysis part.Independent Tools
Some tools offer alignment manipulation that can be used for alignment enhancement (Sect. 12.4.6).Foam The Alignment server, associated with the Alignment APIMost of the software developed for editing alignments are candidates for design time matching. The same alignment editor can be used for manipulating more precisely the obtained alignments. They should provide a convenient display of the currently edited alignments and the opportunity to discard, modify or add correspondences. Ideally, each design time function should be available from an alignment editor. Since ontologies and alignments can be very large, it is very challenging to offer intuitive alignment editing support. There exists such editor prototype such as OnaGuiIntegrated Tools
Model management has been promoted in databases for dealing with data integration in a generic way. It offers a high-level view to the operations applied to databases and their relations. RondoIntegrated tools integrate alignment management to ontology management. The Web Service Modeling Toolkit (WSMT) Prote ´ge ´is an ontology edition environment which offers design time support for matching. In particular, it features PromptThe NeOn Toolkit Alignment Plugin
The NeOnNeOn supports ontology alignments in both the NeOn Toolkit and the Cupboard ontology server.The NeOn Toolkit Alignment plugin works in two modes: an offline mode in which the user can work locally on the alignments. The user can run the matchers which are embedded in the toolkit against ontologies in the NeOn Toolkit and manipulate alignments which are in the local environment. Figure The online mode connects the NeOn Toolkit to an alignment server allowing to share ontologies and to apply these operations on alignments stored on the server. Both online and offline modes provide the functions of the Alignment API: retrieving alignments, matching ontologies, trimming alignments under various thresholds, storing them in permanent stores and rendering them in numerous output formats. These operations support the whole alignment life cycle (Fig. The Alignment plugin allows one to automatically compute and manage ontology alignments. More precisely, it offers the following functionalities:• Find alignments between ontologies or those available on the server • Match ontologies • Trim alignments by applying thresholds to existing alignments • Retrieve and render alignments in a particular format • Upload and store an alignment permanently on the server Alignments stored in the server can be further shared through the Cupboard ontology server. It allows for indexing alignments available from alignment servers. Hence, these alignments can be available to each Cupboard user to be stored in her cupboard and, as for ontologies, be rated and annotated. Cupboard provides direct access to alignments as well as indirect access to the Alignment server to generate new alignments when they are missing.Examples
In this section, we consider a user having to connect an ontology designed for drug and prescription to existing ontologies outside. These examples are closely related to the application presented in Chap. 20.Identifying Needs
More precisely, the newly proposed Semantic Nomenclature ontology (presented in Chap. 20), designed from schemas of pharmacological firm databases, has to be matched to ontologies available on the web. This could help searching for literature about concerned drugs or exporting drug interactions as linked data for other applications to take them into account.The requirements for this matching activity allow it to be performed offline, without time constraints, so the use of the NeOn Toolkit and user supervision is perfectly suited. The ontologies, having been developed independently and for different purposes, are not expected to match exactly. Correct correspondences are expected, completeness is secondary. The type of operation to be performed with the resulting alignments is data export (for exposing linked data) and query translation (for connecting to the literature).Identifying Ontologies
Watson • The LDIS Drug ontology More ontologies on this topic are available, and a comparison can be found in Herrero Ca ´rcel and Pariente ( The ontologies are relatively homogeneous being in English (with some Spanish comments in Semantic Nomenclature) and OWL. They have comparable sizes with the notable exception of UMLS.Finding Existing Alignments
Finding available alignments may be achieved by using an alignment server. In the present case, there is no alignment available between these ontologies.Selecting a Matcher
The user then proceeds by selecting a matcher suited to match these ontologies. In this case, given that ontologies are about a very close and normalised domain, they are written in the same natural language; the user may select very simple matchers based on the strings naming entities. There are several matchers available, either under the NeOn Toolkit or the Alignment server; the best way is to try them and to see the results (see Sect. 12.6.5).In a second iteration, tests have been performed with more elaborate matchers such as a simple use of WordNet which would use synonyms to match terms and distance in the hypernym graph. Or it can use the Aroma matcher which will attempt at determining association rules between concepts before extracting an alignment between them Matching
The simple StringDistAlignment method with different string distances is run, and results are displayed in Table The user first ran the method with Levenshtein measure (edit distance) and SMOA measure which tries to better interpret the way people label things, e.g. by using syntactic variations. The threshold has been put to .75 so as to avoid considering far-reaching similarity between strings. Later, a threshold of .85 has been applied in order to further ensure correctness (because a higher threshold will eliminate unlikely matches).Evaluating
There is no automatic way to evaluate these results. They have to be manually looked into by the user to assess their quality (they can be displayed by alignments editors).Concerning the drug ontology, the small returns with the Levenshtein distance are obviously correct. The use of SMOA provides mostly new correct matches, such as interacts/hasInteraction. The only non-fully correct matches are the matching of DrugInteraction and OtherInteraction to interaction. Using SMOA with a .75 threshold provides reasonable results. Some more matches, such as isIndicated/has_indication_text, could have been found, but not many. The small number of matches can be explained as follows: Semantic Nomenclature is more oriented towards the drug production and commerce processes, while the drug ontology is targeting the consumption process.Concerning UMLSSN and RxNorm, Levenshtein was better than SMOA which was returning quite a lot of unwanted matches, such as isProducedBy/produces or Clinical_Finding/Clinical_Drug. After a closer examination, there is no real reason to find more correspondences than those provided by Levenshtein, so the user may want to use these. Especially with UMLSSN, it seems that the labels have been chosen so that they correspond to those of UMLS so they exactly match.Using the more elaborate matchers has confirmed this. They have only returned plausible but not necessarily valid correspondences, such as Physical_Entity/ Physical_Object.Enhancing
Enhancement may be achieved by two means: either by manual edition of the resulting alignment or by running a new matcher, using new parameters or applying different threshold to the results. This is what has been done by using different thresholds and testing the more elaborate matchers, i.e. starting back to Sect. 12.6.4. In the end, once the SMOA alignment with the drug ontology has been found acceptable with respect to other results, this alignment is manually edited and selected.Both means can be interleaved: It is possible to edit an alignment and to use it as further input for a matcher.Storing and Sharing
Once an alignment of sufficient quality is established, especially if it has been curated by hand, it must be better documented, for instance, by adding metadata explaining how it has been obtained, who has curated it and what is the reached confidence in each correspondence. This is illustrated in Sect. 12.4.7. Then, it can be uploaded to an Alignment server so that it would be visible to other people (in the previous step of Sect. 12.6.3).12.6.9 Rendering Finally, the obtained alignments have to be used. We have considered that the data expressed in the Semantic Nomenclature ontology could be converted in the drug ontology so as to communicate critical information about interaction. This may be achieved either by generating an XSLT transformation applying to the data expressed in XML for obtaining the interactions under the drug ontology or a more elaborate process may take advantage of the alignment to generate links between instances of both ontologies.On the other side, if RxNorm or UMLSSN is used to query bibliographical databases, the alignments may be used for translating queries expressed with respect to the Semantic Nomenclature into queries expressed in the two other ontologies and eventually evaluate them in parallel.Conclusions
Establishing relations between ontology entities is part of modern ontology engineering and a very important activity for networked ontology engineering. This activity remains difficult though there are many solutions for carrying it out. We proposed methodological guidelines for ontology matching which integrates with the alignment life cycle and can cooperate with ontology engineering methodologies. In particular, we paid a particular attention to alignment sharing and reuse. These guidelines are based on research work on particular tasks: Some of these have been investigated in depth and others have not. Similarly, some tools cover parts of these guidelines, but none is able to support them entirely.Hence, more work is necessary to achieve a fully instrumented ontology matching methodological support, and no doubt it will raise some demands for improvement in the proposed methodological guidelines.Further Readings
There are few methodological accounts of ontology matching. Part III
The NeOn Toolkit Chapter 13Overview of the NeOn Toolkit Michael Erdmann and Walter Waterfeld Abstract The NeOn Toolkit is one of the major results of the NeOn project. It is a state-of-the-art, open-source, multiplatform ontology engineering environment, which provides comprehensive support for the ontology engineering life cycle of networked ontologies. It is based on an open and modular plugin architecture that allows adding additional plugins realizing more advanced features supporting more complex ontology engineering activities. A substantial number of plugins have been developed within and outside the NeOn consortium and are available at the NeOn Toolkit homepage. The NeOn Toolkit supports the Web Ontology Language OWL 2, the ontology language specified by the W3C, and features basic editing and visualization functionality. Its user interface, especially the presentation of class restrictions, makes the NeOn Toolkit accessible to users that do not have long experience with ontologies but instead know the object-oriented modeling paradigm. In the chapter, we will present the feature set of the NeOn Toolkit and how to use it. A second part explains some architecture and implementation background and how new plugins can be integrated into the common platform.Introduction to the NeOn Toolkit
The NeOn Toolkit is a state-of-the-art, open-source, multiplatform ontology engineering environment, which provides comprehensive support for the ontology engineering life cycle of networked ontologies (see Chap. 1).M. Erdmann (*) ontoprise GmbH, An der RaumFabrik 33a, 76227 Karlsruhe, Germany e-mail: michael.erdmann@ontoprise.de W. Waterfeld Software AG, Uhlandstraße 12, 64297 Darmstadt, Germany e-mail: Walter. The NeOn Toolkit supports the Web Ontology Language OWL 2, the ontology language specified by the World Wide Web Consortium This chapter consists of two main parts. In the first part, we will discuss the basic features of the NeOn Toolkit and describe how to use them. Since the NeOn Toolkit is an open and extensible platform, we will look under the hood of the toolkit and we will discuss the building blocks and standards that are used to define the toolkit and which can be exploited by developers to create additional functionalities for the NeOn Toolkit in the form of plugins. Thus, we mean here developers of ontology functionalities and not developers of ontologies, where the latter we consider as users of the NeOn Toolkit.The NeOn Toolkit: For Users
In this section, we demonstrate the features of the core NeOn Toolkit and how to use them.General Features
The Screen Layout
The NeOn Toolkit operates on ontologies The workspace and project metaphor is visualized in the NeOn Toolkit by the layout of the basic OWL perspective. A perspective is an Eclipse mechanism to describe the composition and layout of different views in a window. A view is a subwindow displaying certain information and/or allowing user input. The OWL perspective initially contains three main views, which represent the basic functionality of the NeOn Toolkit, as shown in Fig. • Ontology navigator: This view in the top left of the screen displays all ontology projects of the current workspace. Each project can hold multiple ontologies, and each ontology contains folders for classes, properties (object, data, and annotation properties), and data types. Each of these folders displays a hierarchical presentation of the classes, properties, and data types of the respective ontology. • Individuals: When you select a class in the Ontology Navigator, a list of all its individuals is listed in this view. • Entity properties: The NeOn Toolkit displays details of each selected entity in this view, which takes the most space of the screen and is located on the righthand side. The content of this view adapts, depending on the type of the currently selected entity (ontology, class, property, individual, etc.). Since most entities (can) have a lot of relevant information, which would not necessarily fit on a single screen, the entity property view uses several tabs (at the bottom of the view) where users can switch between different aspects of the entity. The basic operations users perform with the NeOn Toolkit are (1) the creation of new objects (projects, ontologies, and other entities) and ( For manipulating the properties of an entity, users typically will use the Entity Properties View. Since OWL ontologies consist of axioms, users also essentially interact with the NeOn Toolkit on the axiom level. The tabs of the Entity Properties View contain forms for different aspects of each entity where users can add new properties or alter or delete existing ones. In later sections, we will give some more details for the most commonly used forms.Loading and Saving
In order to load or save ontologies (outside of the workspace), the NeOn Toolkit provides import and export features, which are available from the context menu of ontology nodes in the Ontology Navigator tree and from the File menu. The NeOn Toolkit supports several serialization formats for OWL ontologies:• OWL/RDF -the official OWL 2 W3C recommendation in RDF/XML • OWLX -an OWL 2-XML presentation according to the OWL 2 recommendation • OWL 2 -the functional syntax of the OWL 2 recommendation • OMN -the Manchester syntax for OWL 2 Entity Label Modes
The labels that are displayed for entities in the different fields of the user interface depend on the selected Entity Label Mode. The toolbar icon labeled "ns"• Complete URI: displays the complete URI of an entity (e.g., "http://www.fao. org/aims/geopolitical.owl#group") including its local name. • Local name: displays only the local name of an entity (e.g., "group"). This makes the ontology a lot more readable, but since the local names are not necessarily unique, there is potential for conflicts.• QName: displays the namespace prefix and local name of an entity (e.g., "geo: group"). This is the recommended setting and is especially useful when multiple ontologies are used or different namespaces are in use. In case a default namespace is defined for an ontology, no prefix is shown for this namespace. • Language: If the ontology contains labels (rdfs:label) for entities, this option can display the human readable labels in a specified language, thus providing multilanguage support. Note that all languages are available, which are specified in the preferences.Manchester Syntax
Most fields in the Entity Property View can take single entity names, URIs, QNames, or local names. Others can hold complex class expressions, for example, the superclasses of a class can be either a simple class (referenced by its ID) or an anonymous class specified as a complex class expression. The NeOn Toolkit supports the Manchester syntax for formulating these expressions. The Manchester syntax is a user-friendly compact syntax for the ontology language OWL Of course, users can also create even more complex classes by nesting multiple class expressions within each other. For example, the following formula in Manchester syntax describes the set of people who have at least one child that has some children that are only men, that is, grandparents that only have grandsons:Person and hasChild some (Person and (hasChild only Man) and (hasChild some Person))Navigation
The fields in the Entity Properties View often contain references to other entities, for example, the domain of a property or the superclass of a class. These references can also be nested within more complex expressions. Oftentimes, it is desirable and useful to jump directly to the referenced entities to further inspect the model. The NeOn Toolkit provides a handy navigation utility which supports this task. By pressing the Control (Ctrl) key and hovering over the name of an entity, the text becomes highlighted in blue and gets underlined (see Fig. Search and Finding References
Another useful feature of the NeOn Toolkit is its Search facility. Ontologies can be quite large in size, and to this purpose, the NeOn Toolkit provides a helpful search dialog (see Fig. After hitting the Search button, the results will be displayed in the Search View. It presents a list of all entities matching the keyword, organized according to the ontology (and project) they were found in. By double-clicking a line in the Search View, it will jump to the respective entity (see Fig. All nodes in the Ontology Navigator provide a context menu action to Find References. When selecting this item, the NeOn Toolkit will find all axioms that Autocompletion
The NeOn Toolkit also features a useful autocompletion function, which is available in all text boxes of the Entity Properties View while in edit mode. Autocompletion is triggered by clicking CTRL + Space and starts automatically after a second of idle time. Since expressions in Manchester syntax can be complex, the NeOn Toolkit also tries to limit its proposals to applicable entity names, for example, only class names or only property names. In Fig. The Help Facility
The built-in Help system of the NeOn Toolkit is quite elaborate, and a full user documentation is available from within the Toolkit. It is accessible via the Help Additionally, many wizards or dialogs provide context-sensitive Help by clicking the question-mark icon in the lower left-hand corner. This will guide users to appropriate pages in the documentation.OWL Entities in the NeOn Toolkit User Interface
By selecting a node in the Ontology Navigator or another view, users can set the focus of the NeOn Toolkit on the associated entity. As the focus changes, the content of the Entity Properties View updates to show the details of the newly selected entity. Depending on the type of entity Annotations
The OWL 2 W3C recommendation offers the opportunity to add annotations to all kinds of entities. The NeOn Toolkit supports this feature with a single GUI element, the Annotations Tab, for all entities (see Fig. Source View
Another useful feature (for advanced users) is the ability to view an entity in some form of textual serialization. The NeOn Toolkit offers this functionality via the Source View Tab for each entity type. Ontologies can be displayed in several serialization formats, for example, RDF/XML, functional-style syntax, or Manchester syntax. For other entities, the NeOn Toolkit displays the Manchester syntax of the frame representing the entity and, additionally, a list of all axioms relevant for this entity. The tab uses syntax highlighting to make the display easier to read (see Fig. Project
The Entity Properties View for projects includes an Aggregated Statistics tab, in addition to a general tab that displays some metadata about the current project. In this tab, the NeOn Toolkit presents overall statistics about the ontologies in the project, for example, the total number of classes, properties, or axioms. Like the search, the statistics is based on asserted knowledge only.OWL Ontology
The main tab for ontologies is the Imports and Namespaces tab, which lists all ontologies imported by this one (via owl:imports) and the defined namespaces. A screenshot can be seen in Fig. The NeOn Toolkit supports ontologies which are loaded from remote locations, for example, from the web. The Location field of the Ontology Entity Properties View shows whether the ontology is stored locally or loaded from the web. OWL Class
Generally speaking, OWL classes can be considered as sets of individuals that share similar characteristics. These classes are organized in hierarchies, of which owl: Thing is the root class. OWL classes are described through so-called class descriptions. The class hierarchy is displayed in the Ontology Navigator. Clicking on a class shows its instances in the Individual View and all properties that have this class in its domain in a special Domain View. The number of individuals that belong to a class is also displayed in the navigator together with the class label, for example, in Fig. The Entity Properties View for classes provides two additional tabs, besides the Annotations and Source View tab. The Taxonomy tab lets the users view and modify the super-and subclasses of the current class, as well as equivalent and disjoint classes. A special feature of the NeOn Toolkit, not present in other ontology editors, is the special treatment of OWL restrictions. On a separate Class Restrictions tab (see Fig. • owl:allValuesFrom (ALL) • owl:someValuesFrom (SOME) The Entity Properties Views for all three kinds of OWL properties are quite similar. In the following, we will only describe the Entity Properties View for object properties since its features are essentially a superset of the other properties.The Domain and Range tab (see Fig. OWL 2 and RDFS properties can be hierarchically organized. This information is collected in the Taxonomy tab. It displays Super-and Sub-Properties, as well as Equivalent Properties. Object properties also can have Inverse Properties or property chains (the composition of a sequence of properties) as subproperties. Thus, it is possible, for example, to specify that the property hasUncle is a subproperty of the concatenation of hasParent and hasBrother.OWL Individual
The Entity Properties View for individuals contains the general tabs for Annotations and the Source View and two tabs that allow users to display and define properties for an individual. The Properties tab (see Fig. Extending the NeOn Toolkit's Feature Set
In the previous section, we have described the features that are built in the NeOn Toolkit core. Of course, the core only contains the essential functionalities of the NeOn Toolkit, and many more features One of the prominent functionality that is available via easy-to-install features is reasoning. Thus, it is possible to cope with different reasoning functionality and with different reasoner realizations. For example, the reasoner plugin provides important reasoning functionality based on the Pellet2 and the HermiT3 reasoner (see Chap. 17).The mechanism to install new features while working with the NeOn Toolkit refers to a special location in the web, the so-called NeOn update site. This update site is maintained by the NeOn FoundationThe NeOn Toolkit: For Developers
The NeOn Toolkit has an open and modular architecture, which it inherits from its underlying platform, Eclipse. Eclipse is a rich development environment, which is widely adopted in the programming world and also perfectly fits the modeling paradigm for ontologies. It provides developers a framework to easily create, publish, and integrate new features into the NeOn Toolkit. Eclipse is open-source, and the NeOn Toolkit also is published under the same open Eclipse Public LicenseThe openness of the platform and the reliance on open standards was a major driver in the design and development of the NeOn Toolkit. Besides the open Eclipse platformArchitecture
The architecture of the NeOn Toolkit must cover the complete ontology functionality. This includes the coverage of tools for the whole ontology life cycle, and it must enable all ontology engineering activities. The NeOn Toolkit focuses on the development part of ontology functionality. However, for ontology-based applications, the distinction between development time and runtime is not as clear as in conventional applications, for example, schematic information like classes are often also changed and modified at runtime, which is impossible for conventional applications. Therefore, the architecture of the NeOn Toolkit also includes runtime components. The architecture must also allow the easy integration of additional ontology engineering functionalities in a highly modular fashion Thus, we defined for the NeOn Toolkit a generic architecture with a layering approach. The layering resembles increasing abstraction layers for ontology functionalities. The layers, however, also organize the data and control flow between the components of each layer. The components of higher layers invoke components of lower layers but not vice versa.Based on these principles, the NeOn Toolkit architecture consists of three layers (see Fig. • Infrastructure services: These are the basic ontology services contained in all versions of the NeOn Toolkit. The OWL API implementation is the most important one. • Engineering components: The main ontology functionality is contained in the engineering plugins provided by the NeOn Toolkit core and by additional plugins. • Front-end components: They contain the user interfaces for the engineering plugins. They are similarly extendible like the engineering components.This layering and more details are described in the NeOn Toolkit design documents APIs and Realization
Eclipse Platform
For the realization of the architecture, we use the Eclipse platform. Eclipse, which has a strong record as a software development environment, provides rich functionality for the development of plugins for the toolkit. Additionally, Eclipse provides, via its extension point mechanism, a simple way to extend its functionality for other types of development assets. For the NeOn Toolkit, this mechanism has been used extensively to realize the basic tools for ontology development.These can now be used to realize more specific ontology tools. The advantages of this extension point mechanism are twofold: First, it allows an easy realization of a basic ontology modeling tool because many of Eclipse's existing functionalities can be reused; second, the development and integration of additional, more specific ontology tools is almost seamless because the functionality can be easily plugged in as an extension of the core NeOn Toolkit.OSGi
OSGi ( In NeOn, we have leveraged the implicit capabilities of NeOn plugins as OSGi bundles for publishing ontology engineering plugins as web services. This is possible by offering a web service container based on an OSGi server. Into such a web service container, all needed NeOn ontology engineering functionalities can be deployed. For a specific ontology engineering plugin, only a web service wrapper has to be generated and also deployed. Thus, any ontology functionality, originally only available in the NeOn Toolkit, is now available as a web service. This is of course not possible for front-end plugins. However, due to the separation of engineering plugins and front-end plugins of the NeOn architecture, the NeOn Toolkit provides the means to realize this capability for any functionality defined in the engineering layer.OWL API
Over the past few years, the W3C OWL working group has continuously extended and redefined the specification of the web ontology language, initially dubbed OWL 1. While originally the NeOn Toolkit had its own realization of an ontology API, following the release of the OWL 2 specification, it switched to the (Manchester) OWL APICreate Your Own Plugin
To facilitate the deployment of new plugins, we make use of the Eclipse Update mechanism which allows for deploying and updating new features. Features are a concept of Eclipse to represent a unit of useful and deployable functionalities. The role of features is to allow providers to make collections of plugins that logically go together.Developers interested in adding functionalities to the NeOn Toolkit can find more information on the NeOn Toolkit wiki, in the developer's cornerAs a central entry point for information about available plugins, we maintain a plugin wikiFor the NeOn Toolkit, we have created a dedicated NeOn Update Site referenced in the NeOn Toolkit core. After a quality assurance procedure, newly available plugins are uploaded to the update site and thus become immediately accessible to all users of the Toolkit. When developing a plugin for NeOn, a developer can even set up his/her own update site, following the instructions from Eclipse.orgConclusions
In this chapter, we have described the main features of the core NeOn Toolkit and illustrated how the core functionality can be extended by downloading additional plugins. The Toolkit is an open platform to which anybody can contribute, and a number of resources are available for users and developers interested in the NeOn Toolkit:• http://neon-toolkit.org: From here, users can download the latest release of the toolkit. Plugins are documented here, and developers find important information to get started. • http://neon-project.org: This site contains information about the EU-funded NeOn Integrated Project with a lot of documents describing results of our research on networked ontologies, most of which has also been translated in functional code in the form of plugins for the NeOn Toolkit. • http://www.neon-project.org/nw/NeOn_Movies: This contains a collection of tutorial videos explaining the main functionalities of the NeOn Toolkit plugins. • http://www.neon-toolkit.org/mailman/listinfo/: This web page provides information about two mailing lists. One is intended for developers of NeOn plugins to discuss implementation questions. The other is meant for users of the technology where we provide tips and answer questions with respect to the Toolkit and its plugins. • Some participants of the NeOn Project have recently founded the NeOn Technology Foundation Inc Chapter 14Scheduling Ontology Engineering Projects Using gOntt
Mari Carmen Sua ´rez-Figueroa, Asuncio ´n Go ´mez-Pe ´rez, and Oscar Mun ˜oz-Garcı ´a Abstract In order to manage properly ontology development projects in complex settings and to apply correctly the NeOn Methodology, it is crucial to have knowledge of the entire ontology development life cycle before starting the development projects. The ontology project plan and scheduling helps the ontology development team to have this knowledge and to monitor the project execution.To facilitate the planning and scheduling of ontology development projects, the NeOn Toolkit plugin called gOntt has been developed. gOntt is a tool that supports the scheduling of ontology network development projects and helps to execute them. In addition, prescriptive methodological guidelines for scheduling ontology development projects using gOntt are provided.Introduction
One of the crucial aspects within engineering processes is the issue of planning and scheduling development projects. These two terms are often thought of as synonymous; however, they are not. While planningBearing in mind that ontologies are part of software products and that sometimes ontologies are considered a kind of software, experiences and practices in software M.C. Sua ´rez-Figueroa (*) • A. Go ´mez-Pe ´rez • O. Mun ˜oz-Garcı ´a Ontology Engineering Group, Facultad de Informa ´tica, Universidad Polite ´cnica de Madrid, Campus de Montegancedo sn., 28660 Boadilla del Monte, Madrid, Spain e-mail: mcsuarez@fi.upm.es; asun@fi.upm.es; omunozgarcia@gmail.com engineering can be adopted and adjusted in the ontology engineering community. For this reason and with the aim of achieving in ontology engineering a similar degree of maturity to that of the software engineering field, we take as basis software engineering works to provide ontology engineers with help in planning and scheduling ontology development projects.In software engineering, every development project has a life cycle To properly manage software development projects, it is crucial to have knowledge of the entire software development life cycle The project schedule is a calendar that links the tasks to be performed with the resources to support their performance. One of the most common forms of representing schedules is to use a Gantt chart Ontologies are used for making knowledge explicit and allowing it to be shared. One of the keys when building ontologies as in the case of software products is to plan and schedule the ontology development. However, in ontology engineering, planning and scheduling are still in their early stages. Only METHONTOLOGY To cover this lack of methods and tools for planning and scheduling, this chapter describes (a) the gOntt plugin, a tool that supports the scheduling of ontology network development projects and helps to execute them, and (b) prescriptive methodological guidelines for scheduling ontology development projects using gOntt.Scheduling Ontology Development Projects
Scheduling, as defined in the NeOn Glossary of Processes and Activities mentioned in Chap. 2, refers to the activity of identifying the different processes and activities to be performed during ontology development, their arrangement, and the time and resources needed for their completion. Thus, this activity includes as an important task the establishment of the ontology network life cycle, that is, the specific ordered sequence of processes and activities that ontology developers carry out during the life of the ontology network.The goal of scheduling is to organize the different processes and activities in time, that is, to state a concrete programming or scheduling that guides the ontology network development, including processes and activities, their order and time, as well as human resource restrictions.To establish the concrete schedule for the ontology network development, four important questions have to be answered:1. How to organize an ontology development project into phases, or in other words, which ontology network life cycle model is the most appropriate for the ontology network development? 2. Which particular processes and activities should be carried out in the ontology network development? 3. Which order and dependencies exist among processes and activities? 4. What amount of resources (human and time) is needed and available for the development of the ontology network?The first three questions are related to the establishment of the ontology network life cycle, and their responses would result in a general plan for the ontology network development. The fourth question is related to the inclusion of time and human resources restrictions for each process and activity included in the plan, and its response would result in the concrete schedule for the ontology network development. An estimate on how many people should be involved in the ontology network development can be obtained using the ONTOCOM model As in the case of software engineering projects, an ontology engineer cannot start the ontology development project scheduling without having first identified the ontology requirements. In addition, the scheduling activity needs as input the types of potential knowledge resources (ontological resources, non-ontological resources, and/or ontology design patterns) to be reused during the development.The filling card for the scheduling activity, presented in Fig. gOntt: NeOn Toolkit Plugin for the Scheduling Activity
To support the scheduling of ontology development projects, the NeOn Toolkit provides a plugin called gOntt. This plugin is conceptually based on the following ingredients that are explained in Chap. 2: (a) the scenarios of the NeOn Methodology, (b) the set of ontology network life cycle models, and (c) the NeOn Glossary of Processes and Activities. The gOntt plugin has the following main objectives: 1. To support ontology developers in the decision of which ontology network life cycle model is the most appropriate for building their ontologies 2. To help ontology developers in the decision of which concrete processes and activities should be carried out in the ontology network development and in which order 3. To instantiate the life cycle model selected and to create a particular life cycle for the ontology development with the processes and activities needed, including time restrictions on them 4. To inform ontology developers about how to carry out a particular process or activity through the NeOn methodological guidelines and a reference to the concrete NeOn plugins to be used -that is, to help ontology developers in the ontology project execution.According to the aforementioned objectives, gOntt functionalities can be divided into two main groups: functionalities for scheduling ontology development projects and functionalities for helping in the execution of ontology development projects.The functionalities for scheduling an ontology network development are:• To create particular schedules from scratch, by allowing the ontology developer to include processes, activities, phases, and relationships, along with restrictions between them. Such processes and activities could either come from the NeOn Glossary of Processes and Activities or be new ones proposed by the developer. • To create particular schedules in a guided way. gOntt creates preliminary plans for the ontology development with a simple two-step wizard. The ontology developer uses the wizard to answer a set of simple and intuitive questions that implicitly allow him to select the ontology life cycle model and the processes and activities to be carried out.-gOntt internally uses a set of heuristics based on methodological foundations and scheduling templates• To create, modify, and delete gOntt projects.• To export and import gOntt projects in an interchange format based on XML (files with .got extension).• To provide graphical and textual visualizations of gOntt projects.• To rename, reorder, and delete processes, activities, and phases from a gOntt project. • To change the scope of a given process or activity (e.g., to change a given activity to a different phase). • To create, modify, and delete connections between activities, between processes, and between activities and processes. If a connection exists between two elements, the latter cannot start until the former is completed. These connections can have two different meanings:-Logical dependencies: when it is required that one activity is carried out before another because of the nature of the activities (e.g., diagnosis before repair in ontology validation) -Temporal dependencies: when an activity should be performed after another because of project requirements (e.g., ontology reuse and non-ontological reuse can be carried out in parallel because they have no restrictions between them but, in some cases, there are not enough human resources to perform the activities in parallel, and so they should be set to perform in sequence)• To include and modify the duration and the starting date of the processes, activities, and phases. • To check gOntt projects with respect to logical and temporal constraints.• To provide usual editing capabilities such as copy, paste, undo, and redo.The functionalities for helping in the execution of ontology development projects are:• To provide the developer with some methodological guidelines for the processes and activities identified in the NeOn Methodology, and thus -To display a filling card, which includes the process or activity definition, its goal, inputs and outputs, performer of the process or activity, and time of the performance. Figure Figure To create a new gOntt project, one of the following NeOn Toolkit new wizards must be launched (see Fig. Once the scheduling has been created for a given project, ontology developers can access to the gOntt project by clicking on the "scheduling" item in the Ontology Navigator as Fig. Guidelines for Scheduling Ontology Development Project Using gOntt
The workflow for carrying out the scheduling of ontology development projects using gOntt is presented in this section. Each of the tasks in the workflow proposed includes prescriptive methodological guidelines. The tasks for carrying out the scheduling activity are shown in Fig. Task 2. Selecting the set of scenarios. The goal of this task is to select the set of scenarios to be followed during the ontology network development. Users, domain experts, and the ontology development team carry out this task, taking as input both the ontology requirement specification document (ORSD) and the set of potential knowledge resources to be used during the development.To help ontology developers in this task, gOntt presents the set of natural language questions displayed in Fig. With the responses to these questions, the set of heuristics based on methodological foundations and the set of scheduling templates (Sua ´rez-Figueroa 2010), Fig. Task 3. Updating the initial plan. The goal of this task is to modify (if necessary) the initial plan presented by gOntt. Users, domain experts, and the ontology development team carry out this task, taking as input both the ontology requirement specification document (ORSD) and the set of potential knowledge resources to be used during the development.Ontology developers can modify the initial plan in the following ways: (a) by including or deleting processes, activities, and model phases and (b) by changing order and dependencies among processes and activities. Task 4. Establishing resource restrictions and assignments. The goal of this task is to include information about temporal scheduling and human resource assignments in the life cycle obtained in Task 3. Users, domain experts, and the ontology development team carry out this task, taking as input both the ontology requirement specification document (ORSD) and the set of potential knowledge resources to be used during the development.Conclusions
In order to manage properly ontology development projects in complex settings and to apply the NeOn Methodology correctly, it is crucial to have knowledge of the entire ontology development life cycle before starting development. The ontology project plan defines the tasks to be executed, the time when the tasks will be executed, and the dependencies between tasks. The project plan is the only way, as can be shown in other disciplines, to commit people to the project and to show how the work will be performed. It also aids ontology engineers in monitoring project execution and assessing the impact of a particular delay in the planned tasks.With these notions in mind, this chapter presented (a) the gOntt plugin, a tool that supports the scheduling of ontology networks developments as well as their execution, and (b) prescriptive methodological guidelines for scheduling ontology development projects using gOntt.In relation to the work presented in this chapter, an integration of gOntt and the guidelines proposed with the ONTOCOM model Introduction
Being an Eclipse RCPThe RCP takes the burden of integrating plugins from the user interface perspective, e.g., by adding menus and toolbar buttons, populating the lists of views and perspectives, or adding new types of items that can be created via wizards. However, it is usually up to the developer to facilitate conceptual integration of her plugin by characterizing the goals of its features. The ways for doing so in the Eclipse platforms are little more than giving appropriate names and assigning categories to the UI contributions provided by their plugins. Performing this task can be tricky if the platform is supported by a large community, whose each member develops a plugin not knowing what others are doing. As a result, developers may arbitrarily add whatever categories, items, and labels they see fit for their plugins regardless of the rest. For example, two developers can create multiple categories for views, give them unique identifiers but label them both as Visualization independently on one another. As a result, end users will see two Visualization categories grouping different UI elements. Again, one developer could name a category after the plugin providing the corresponding UI elements, while another could name it after an arbitrarily named task supported by her plugin. In other words, when a contributor develops a plugin for the NeOn Toolkit, as well as for most plugin-based frameworks, she projects her own interpretation of the implicit metamodel of the user interface. Moreover, the uncontrolled proliferation of features An instance of the scenarios described above is shown in Fig. Kali-ma is a NeOn Toolkit plugin that aids developers and end users alike in creating a conceptually harmonized view on other known NeOn Toolkit plugins (and, more in general, tools that support the life cycle of ontologies). Kali-ma implements a user interface paradigm alternative to the Eclipse Workbench (and which can be switched with the latter in real time). This interface groups all UI contributions and access methods by the plugins issuing them and, with relatively little development effort, the plugins themselves by categories best representing the goals they are targeted at. It also adds a set of collaborationoriented functionalities for end users, such as a metadata search feature, a whiteboard for executing dynamic plugin assemblies, and dedicated real-time chat support for ontology projects.The remainder of this chapter provides an insight on the plugin as a whole, its functionalities, and the rationale behind them. Section 15.2 guides the reader through the plugin features and is structured so that the reader can concentrate on the section for end users (Sect. 15.2.1) or the one for developers (Sect. 15.2.2), depending on the reader's role. Developers are however advised to read both subsections in order to gain an understanding on the effects of their Kali-ma extensions on the interaction experience. Section 15.2.3 focuses on the underlying software architecture and how it combines standard components in Java with others in OWL (namely an extended version of the C-ODO Light ontology described in Chap. 4), thus being of interest for software engineers and ontology specialists alike.Kali-ma Plugin Features
By the end of this section, the reader will have learned about the functionalities exposed by the Kali-ma plugin for facilitating interaction with and configuration of software components in the NeOn Toolkit. An insight is also provided, as well as documented, as to which steps the user needs to perform in order to activate and interact with these functionalities.Although the Kali-ma plugin is oriented toward providing alternate modalities for end users to interact with the functionalities provided by the NeOn Toolkit, the rule body and several other aspects by which these modalities are provided are customizable. Some of such features are configurable at runtime by end users, while others are available by applying simple extensions to plugins by their respective developers. By this distinction, the remainder of this section is structured so as to allow a neat separation between functionalities that refer to end users for direct consumption and functionalities that refer to developers for their plugins to provide alternate interaction paths. In particular, the next section will also focus on what features can be configured by end users prior to launching the Kali-ma plugin on a running NeOn Toolkit platform.Functionalities for End Users
When the Kali-ma plugin is activated, a desktop-integrated graphical user interface (GUI), called Dashboard, replaces the traditional Eclipse Workbench-based NeOn Toolkit interface. The constituents of this user interface, an example of which is shown in Fig. Kali-ma provides a number of functionalities aimed at end users and aids them in the configuration of, and rapid access to, selected sets of tools apt for completing certain classes of tasks. These are as follows:• Tool organization and selection based on preferred criteria.• Quick plugin access that groups most functionalities of a plugin into a single widget. • Profile management for bookmarking sets of plugins and associating them with ontology projects, thereby managing profiles. • Project-based real-time chat that allows remote collaborating parties to share metadata of a common ontology project. • Advanced search for ontology data and metadata.• Pipeline assembly, for broadcasting the output of a plugin to other listening plugins in order to accomplish complex tasks. • Assistant, for obtaining real-time guidance.Preliminary Configuration
As with most NeOn Toolkit plugins, Kali-ma is configurable in several aspects concerning its way to handle interaction with the framework. While it does make sense to customize some of these aspects only once the Kali-ma dashboard has been activated, other features require prior configuration, as they affect the way dashboard elements are constructed. This section discusses the latter set of features and the steps to follow for configuring them.Kali-ma comes with a "safe" default setup, in that all the plugin functionalities can be activated with no alteration of the default settings, granted an available internet connection. The only exception is the chat functionality, which requires the user to set the hostname of a Jabber/XMPP chat server where she has an account already registered.All the settings of the Kali-ma plugin are grouped under a single Kali-ma entry in the NeOn Toolkit Preferences category. Remember that the Preferences panel can be accessed in different ways, depending on the operating system used. For example, Windows users will find it in the Window top menu, while OS X users will find it in the NeOn Toolkit top menu.Due to their intrinsic heterogeneity, the configuration parameters are in turn grouped into four categories:1. Appearance is the category of customizable cosmetic aspects of the Kali-ma user interface.• Open profiles docked is an optional override for the docking options of each plugin widget in a user profile. When this option is checked, if the user opens a Kali-ma user profile, all of its plugin widgets will be minimized to the Kali-ma dock on startup, even if set otherwise in the profile itself. This option is preferable for users who wish to start with a dashboard as clear as possible. • Widget background policy determines what background color should be used for each plugin widget. Depending on the setting, the color can be either the one used for a category that classifies the plugin or one set by the user for that specific plugin.2. Network deals with how Kali-ma exploits online resources. Currently, all the settings in this category are related to the built-in XMPP chat service.• XMPP Host and Port locate the resource where the XMPP messaging service is provided, e.g., for GTalk use Host talk.google.com and Port 5222. • XMPP Service name, the identifier of the XMPP service on the host, if different from the host name, e.g., jabber.org. • Multiuser chat service, the identifier of the Multiuser Chat (MUC) service on the host, e.g., conference.jabber.org. Although not all XMPP-based services come with this functionality, this is required for the Kali-ma chat to work.3. Reasoning enables the user to configure the parameters by which Kali-ma should locate and classify ontology design tools. These settings can have a significant impact on startup performance, but their default values are relatively safe on that respect. Note that changes to this configuration will only take effect the next time the Kali-ma dashboard is launched.• Plugin address book location is the physical URI of the ontology that indicates where the OWL descriptions of each plugin should be fetched from. Its default value is a plugin registry maintained by the Ontology Design Patterns portal• Perform online update denotes when Kali-ma should check for updates to the online plugin address book. Available options are "Each run," "Only on next run," and "Never." Note that if the address book has not been fetched yet (e.g., on the first run of Kali-ma ever), the update will be performed even if the "Never" option is set. • Cache plugin classification indicates whether Kali-ma should materialize all inferences about plugins and store them into a local cache ontology. Because inferencing is a lengthy and highly CPU-intensive task, it is recommended to set this option unless major changes in the plugin registry occur. Note that this option only indicates whether the cache should be built, not whether it should be used: it will always be used if present. To force-rebuild the cache, the user can clear all the local data by clicking the Clear now button. This button is grayed out if there are no such local data.4. Toolkit integration manages the way Kali-ma handles the standard NeOn Toolkit user interface along with its own. Users will configure these parameters according to their will to be provided with both interfaces altogether.• Stick dashboard to main window. If this option is set, the Kali-ma UI will appear on top of the standard NeOn Toolkit window, and its behavior will mimic the one of that window. Thus, when the NTK window is minimized, hidden, or maximized, so will be the Kali-ma widgets. Note that the Kali-ma dashboard is not modal; therefore, the NTK UI components in the background can still be interacted with. • Main window behavior allows the user to set how the main NTK window should appear or disappear when the Kali-ma dashboard is activated or deactivated. The user can opt for the main window to be hidden or minimized or neither. This option is only available when the "Stick dashboard to main window" option is unchecked.Example 15.1. This and all the examples in this chapter are based on a runthrough scenario extracted from the case study described in Chap. 20. The Semantic Nomenclature of pharmaceutical products was carried out using the NeOn Methodology and related software support. Therefore, in order to use Kali-ma to carry out the activities specified in this methodology, an engineer will select Processes and activities from the Reasoning ! Criterion for tool classification configuration panel.Activating the Dashboard
Unlike most other NeOn Toolkit plugins, which support specific tasks in the engineering of networked ontologies and are therefore integrated with the platform, Kali-ma provides a GUI that runs in parallel with the standard one. For this reason, Kali-ma integration is limited to the preferences panel and the commands for activating its own user interface, called the dashboard. These commands are located:• In the Launch Dashboard menu entry in the Kali-ma top menu • In the NeOn Toolkit top bar as the Launch Dashboard button (an open perspective is required for displaying the button)When one of these two actions is performed, the reasoning and plugin discovery tasks for preparing the dashboard are started as a background job. In particular, the following actions are performed:1. The local tool descriptions and cache ontology are checked. If neither is present, or the online update parameter is set, plugin descriptions are fetched from the locations indicated in the online registry. 2. If variations between the local plugin ontology and the online registry are detected, the user is notified about these changes and prompted to choose whether to apply them or not. If changes are applied, any local cache is invalidated. 3. Plugins are classified by the designated criterion in one of the following ways:• If a valid local cache is present, it is queried directly.• If no valid cache is present but Kali-ma is configured to build one, it will first do so then query the cache it just built. This task is highly CPU intensive but will not have to be performed again as long as the cache remains valid. • If no valid cache is present and Kali-ma is not configured to build one, it will use a reasoner to classify plugins. This task is CPU intensive and will have to be run on every dashboard startup unless a cache is built.4. The Kali-ma dashboard is activated and displayed in its default state. The NeOn Toolkit main window is hidden from view if set to do so.Example 15.2. The project manager of the Semantic Nomenclature case study creates a new NeOn Toolkit ontology project called "SemanticNomenclature" and shares it with engineers using a version control tool such as CVS or Subversion. When the Dashboard is activated using the Launch Dashboard button, Kali-ma becomes aware of this project and can store profiles and configurations in its directory.Recall that the dashboard is an aggregate of basic user interface components called widgets, whose look-and-feel exploits the capabilities offered by the GUI toolkit of the host operating system. Every widget identifies a functionality, or set of functionalities, in the NeOn Toolkit. Widgets can be grouped in two major categories: native widgets denote built-in interaction-oriented functionalities offered by the Kali-ma plugin itself and are always available regardless of what tools are installed on the platform; plugin widgets are representatives for plugins that are installed on the system, and they offer quick access to the functionalities available due to these plugins being installed. Widgets belonging to this latter category are available upon user request when the corresponding plugin is installed on the NeOn Toolkit platform, no matter what the canonical interaction paths to access them.Organizing the Plugin Space
The heart of the Kali-ma approach for organizing the NeOn Toolkit as a functionality provider resides in the classification of its plugins by a unique, design-centered criterion that is nonetheless customizable. Therefore, its core functionality is to present end users with an overview of the plugins that are available in their running instance of the NTK and to help them select the one(s) whose coverage best suits the tasks that need to be performed.The C-ODO organizer is the widget used for presenting this aggregate overview of plugins. This widget is named after C-ODO Light, the design ontology that is the base for all the classification criteria adopted by default in Kali-ma. Recall that an overview of the goal, rationale, and architecture of the C-ODO Light ontology network was given in Chap. 5.The C-ODO organizer is the tool browser provided by Kali-ma. Users are free to choose from time to time, whether they wish to explore the plugin space as a tree or as a graph, by switching between the Tree View and the Wheel View tabs. The Tree View is organized as a simple Category ! Plugin two-level tree; i.e., by expanding a category it is possible to view all and only the plugins that fall under that category. This also implies that a plugin that encompasses more than one category will appear as a child of multiple nodes in the taxonomy. The Wheel View, so called after the shape adopted by the category set, provides the same information in a graph. Although it takes up more space than the Tree View, it displays more useful information altogether. When a category is selected in the Wheel View, all and only the plugins under that category are displayed as in the Tree View. However, for each shown plugin, an edge appears for every other category it falls under.The categories used for classifying plugins have a variable dependency on C-ODO Light, yet they are all based on this ontology for modeling the notion of an ontology design tool. The criterion used for identifying these categories can be selected from the Reasoning panel of the Kali-ma preferences (entry "Criterion for tool classification") prior to launching the plugin. The available criteria are as follows:1. Custom design functionalities. These denote specific tasks and operations involved in the design of networked ontologies. They are arbitrarily defined by plugin developers, so the set of design functionalities can be highly fine grained, depending on the choices of developers. "Create project," "Cast vote," or "Delete annotation" are examples of such design functionalities. This criterion is enabled by selecting "implements (Design Functionality)" from the Reasoning preferences. End users should expect a sparse classification, with many categories each with a limited number of plugins, yet with high redundancy across multiple categories, roughly one for each functionality implemented in that plugin. 2. NeOn Methodology refers to the fixed set of activities that are part of the NeOn Methodology canon as defined in Chap. 2. For this criterion, the categories are established a priori, and whether a plugin supports an activity in the methodology, this reflects the rationale used for selecting such plugins in gOntt (cf. Chap. 14). This criterion is enabled by selecting "supports activity (Activity)" from the Reasoning preferences. 3. Ontology design aspects is a limited, fixed set of generic design functionalities that aggregate the most common aspects of designing networked ontologies in a collaborative environment. The categories are set and very limited in order to provide dense classification of design tools. Also, it is the only case where the categories to which plugins belong are not explicitly defined but are instead obtained by inferencing over other features defined by the developers, namely the types of knowledge their plugins consume and produce. This criterion is enabled by selecting "has aspect (Design Aspect)" from the Reasoning preferences.Both the Tree View and the Wheel View in the C-ODO organizer can be filtered by means of the funnel-shaped icon opposite the tabs. The filtering feature is due to the fact that the ABoxes describing ontology design tools, as well as their registries, are not bundled with the actual tools. In fact, they do not reside locally on the host platform in general but are instead exposed on the web. Moreover, they are not necessarily limited to NeOn Toolkit plugins but can span across several frameworks and architectural paradigms, such as plugins for other platforms, stand-alone applications, web applications, and web services.Thus, three filters are available and can be cascaded: "Show only NeOn Toolkit plugins" will exclude all those design tools that, according to their ontological descriptions, do not qualify as plugins for the NeOn Toolkit. "Show only installed tools" will apply the previous filter and skim all the NeOn Toolkit plugins that are known to exist but are not detected as installed on the host platform. Finally, "Hide empty categories" will remove all the nodes representing categories to which no design tools are known to belong, regardless of the status of the other filters.Example 15.3. The Semantic Nomenclature project manager has to select plugins for the implementation phase of the use case. The C-ODO organizer Tree View shows all the activities in the NeOn Methodology that come with software support. The ODEMapster plugin is selected (by double-clicking) from the "Non-Ontological Resource Reuse" activity, the OWLDoc plugin from the "Ontology Documentation" activity, the Watson plugin from the "Ontology Reuse" activity, and the RaDON plugin from the "Ontology Validation" activity. To create a schedule for all the activities to be performed in the phase, the gOntt plugin widget is also selected from the "Scheduling" activity. When each plugin is selected, its corresponding widget is displayed.Interaction with Plugins
The standard mechanism by which a plugin is integrated with the Eclipse Rich Client Platform is by implementing extension points. An extension point allows a plugin to provide a contribution to the hosting platform, both on the functional level and on the user interface levelThe current version of the Kali-ma plugin allows users to run NeOn Toolkit plugins through the following stand-alone access methods:1. Views are single panels within the Eclipse workbench that serve as containers for arbitrary user interface controls. Multiple views can be aggregated in container objects, called Folders, which are essentially tabbed panes where each tab allows displaying one view at a time within the same folder. Views are usually associated to single-use cases, such as displaying the results of a SPARQL query, and can be manually moved across folders. 2. Perspectives are named composite panels that combine a group of folders and views in a predefined fashion. View combinations are usually associated to entire functionalities, which can be performed by interacting with the user interface elements in each view. Single views can only be shown within a perspective, and the NeOn Toolkit provides a default perspective for authoring OWL ontologies. 3. New Wizards are paged dialogs for guided creation operations. The list of available NewWizards in a system can be accessed from the "New" item in the "File" menu. Examples of this access method allow users to create ontology development projects, ontologies, and gOntt schedules. While we cannot rule out cases where new resources have to be created from existing ones (e.g., ontologies need to be created within an existing project), many New Wizards are associated to stand-alone use cases for creating new resources from scratch.Figure Profile Management
A selection of plugins to be displayed as widgets in the Kali-ma dashboard could be of much more use than simply assisting a single user during a single engineering session. If an open dashboard were just a volatile object that had to be manually rebuilt from scratch every time the NeOn Toolkit is restarted, not only would it be awkward to share in a collaborative context (which is assumed to be recurrent in NeOn-compliant ontology engineering), it would also discourage users and project managers from adopting Kali-ma to support medium-and long-term phases in an ontology engineering project.In order to counter these preposterous potential shortcomings, Kali-ma offers a profile management functionality, which is concretely available as a native widget by its own right. The Profile manager widget, depicted in Fig. A dashboard profile is essentially a named sorted set of plugins that can be serialized as an XML element and lives in the scope of both NeOn Toolkit workspaces and single ontology projects. Having performed a selection of plugins, all of which have a corresponding widget open in the Kali-ma dashboard, the user is able to retain this selection of plugins for sharing or future reuse. To do so, it is sufficient to type a name for the new profile in the top area of the widget and click the "Save widgets to Profile" button in the bottom area. This done, the current set of plugins is stored locally in the kalima_profiles.xml file in the workspace metadata directory for the Kali-ma plugin. Profiles can be listed, renamed, or deleted and one at a time can be set as active and displayed on screen by opening Although dashboard profiles exist by their own right in a given NeOn Toolkit workspace, it is possible to bind them to one or more ontology development projects. This operation is also available as a context menu action, and its effects are visible on the second column of the table in the center of the widget, which displays the names of the ontology projects to which a profile is bound to. Binding a profile to one or more ontology projects results in saving a copy of that profile in another kalima_profiles.xml file, this time placed in the project directory. This action implies the ability to carry profiles along with a single project when it is exported to another system, as it is a common practice to share entire projects in Eclipse environments.Example 15.4. The Semantic Nomenclature project manager wishes to share the tools for the Implementation phase selected earlier with all the ontology engineers who are set to perform each activity. A profile named "Implementation phase" is created and bound to the "SemanticNomenclature" ontology development project in the NeOn Toolkit. Because all participants are synchronized on this project, they will all get a copy of the new profile the next time they update their working copy of the project.Dashboard Control and Docking
To counter the risks of ending up with a screen overcrowded by widgets, Kali-ma comes with an additional interface element called the Dock. As its name suggests, the Dock is conceptually inspired by a consolidated praxis in modern operating systems, which provide a user interface feature for quickly switching between applications. In our interpretation, the Kali-ma Dock provides a compact user interface for holding references to elements of the dashboard that are not of immediate interest, yet it still makes sense to hold in the current view of the system. For example, the user may want to remember having selected a certain plugin but does not need to access it in that particular instant. Every widget that supports docking comes with a toolbar button that, when clicked, instructs the dashboard controller to hide that widget and add a corresponding entry in the Kali-ma Dock. A dock entry is a very simple interface element that serves a placeholder for a docked widget. Each entry consists of a label with the plugin identifier and an arrow button for restoring the docked widget to its original position.The Dock widget itself responds to the same screen overcrowding issue that holds for plugin widgets and other dashboard widgets; therefore, it is not visible on screen at all times. The Dock hides itself every time the last docked widget is restored (i.e., there are no more dock entries) and becomes visible again once a widget is docked (i.e., a dock entry is added). This is due to the fact that, at this stage, the Dock serves the sole purpose of holding references to widgets that are hidden from view. This behavior may vary as further functionalities are added to the Kali-ma Dock in the future.Project-Based Real-Time Chat
Several phases of the articulated ontology life cycle management process are conceived with user collaboration in mind, and as such should they be carried out Kali-ma includes a lightweight real-time chat system to support synchronous communication in an environment where users can instantly share references to resources in a common ontology project. Through the Chat widget, a single user can join one or more dedicated virtual chat rooms, each named after an ontology project she has in common with other users. Additionally, for each project, it is possible to send the identifiers of any OWL entity loaded within that project with just a few keystrokes.Example 15.5. The project manager and engineers that share the "SemanticNomenclature" project and have the same XMPP Chat configuration in the Kali-ma preferences will all be presented with an option to join the "SemanticNomenclature" chat room and discuss their engineering activities there.As with other optional widgets, the Kali-ma chat interface can be activated by means of the Dock widget by simply clicking the balloon-shaped icon on its toolbar. In the default panel of this widget, it is sufficient for a user to type in her credentials (set by the chat server administrator), freely choose an alternate label, or alias, and log into the chat server. With this done, a combo box will display the list of available chat rooms, each named after an ontology project in her NeOn Toolkit workspace. Multiple chat rooms, one per project, can be joined at once, and a chat room will be seamlessly created on the fly if it has not yet been configured by another user. A user may send any free text message by simply typing it in a chat room window. However, if a reference to an OWL class, property, or individual needs to be broadcast to other users sharing the same project, it is sufficient to start typing in part of its name (not necessarily a prefix) and invoke the autocompletion key combination (usually Ctrl + Space) to select from a list of matching entities that exist within that project. Multiple OWL entity references can be broadcast in a single message by invoking autocompletion.Any party is free to host a chat server compatible with Kali-ma. The plugin uses the open standard instant messaging protocol XMPP (Extensible Messaging and Presence Protocol)Obtaining Help
Kali-ma provides its own real-time help system, aimed at displaying appropriate justification of each node appearing in the C-ODO Organizer taxonomy, and in doing so, to take advantage of any metadata present in the ontologies describing tools and classification criteria.Real-time guidance is provided through the Helper widget. The Helper is essentially a lightweight web browser capable of rendering HTML. However, it also reacts to local events within the dashboard, such as a particular widget being focused or a node being selected in the C-ODO organizer. While help messages related to native functionalities are hardcoded, those deriving from metadata such as OWL annotations derive from elements of the ontological component of Kali-ma, which also include remote tool descriptions. For instance, when a node is selected that represents a design aspect, NeOn Methodology activity, functionality, or design tool, the Helper widget displays the rdfs:comment annotation for the corresponding OWL individual.Functionalities for Plugin Developers
One goal of Kali-ma is to reorganize the plugin space under a single, shared criterion that can apply to the majority of plugins. To that end, it provides a set of functionalities to aid developers in describing the features of their plugins so that Kali-ma can elaborate on them and construct a single, harmonic view. These functionalities belong to the following categories:• Plugin description management guides users throughout the creation of the ontology that describes how a plugin contributes to the life cycle management of ontologies. • The interoperability API allows developers to launch and customize a Dashboard programmatically from the code of any plugin.Plugin Description Management
As will be presented in Sect. 15.2.3.2, the Kali-ma infrastructure includes a semantic layer involving components that are invariant in the domain of collaborative ontology engineering, as is the C-ODO Light network, and others that can be customized and adapted to new and refined taxonomies and criteria, such as the rules for categorizing the tool space. Standing amid these two levels are the realworld entities, i.e., the ABoxes where actual ontology design tools are instantiated and facts are provided for them. Kali-ma has no built-in or prior knowledge of which design tools exist, whether C-ODO Light-based ontologies describing them are provided and what physical URIs should be dereferenced for locating these descriptions. It does, however, provide a mechanism for locating such ontologies from a single, configurable source. Coupled with this mechanism, we are offering an online service for semiautomatic construction of C-ODO Light-based plugin descriptions. The next section details the key functional characteristics of both features mentioned above.Plugin Description Generator
Knowledge of the ontology tool population is not delegated to a single online repository. It is the plugin provider's call to author pieces of structured knowledge concerning their own products; thus, it is reasonable to expect them to remain depositaries of this knowledge, while at the same time sharing it in an open environment such as Linked Data. Concurrently, it was felt convenient to have a system for aggregating references to these ontologies at disposal, rather than crawling the whole Semantic Web.In an effort to meet both demands, an interactive tool for constructing these OWL tool manifests was devised as a service to be available anytime, anywhere. A working prototype of this service was released as C-ODO-o-matic (simply dubbed Codomatic throughout the remainder of the chapter)A sample of running Codomatic code generator for the Cicero argumentation plugin The list boxes that follow this field in the figure allow providers to include functional specifications of their tools: through these interface objects, it is possible to select an arbitrary number of knowledge types that the tool is known to consume as input or produce as output, as well as the design functionalities and NeOn processes and activities that it supports. For all fields but the NeOn processes and activities one, an additional text box is available, where the provider can arbitrarily instantiate new knowledge types and design functionalities, if the existing ones are felt to fall short of accuracy or completeness in describing the tool in question. However, while new design functionalities can immediately be exploited when classifying a set of tools with respect to them, new knowledge types cannot contribute to the rules for inferring supported design aspects, unless the providers include additional defined classes that are restricted on the hasInputType or hasOutputType properties for their new knowledge types.The aforementioned statement supports the claim that by no means is Codomatic intended to serve as a replacement for a full-fledged OWL editor. The service is intended for the creation of minimal OWL manifests based on C-ODO Light, and yet it leaves room for extension and refinement. Providers can use the NeOn Toolkit OWL editor to add annotations for newly declared knowledge types and functionalities and relate them to existing ones where need be, as well as define additional rules for inferring supported design aspects from knowledge type statements.The "Generate code" button triggers an asynchronous remote procedure call to a servlet that encapsulates submitted data and uses the same OWL API as Kali-ma's to output the corresponding ontology, whose source code is posted to the text area below the button. This code includes all the necessary ontology imports and is intended to be copied verbatim to an RDF document, which should then be uploaded to a location of the provider's choice. Codomatic does not pose restrictions to tool providers as to what physical locations should be used for their newly generated ontologies, nor does it store submitted base URIs or any other information used for generating the OWL code. References to physical locations can be submitted through the corresponding plugin pages on the NeOn Toolkit Wiki, as documented in its plugin development and submission guideInteroperability API
In addition to supporting collaboration and interaction between end users, Kali-ma as a plugin comes with additional developer features that allow other ontology plugins to interoperate either with each other or with Kali-ma. There are two distinct methods of allowing programmatic interoperability, which is achieved through simple direct intervention on the plugin code. These two methods cover separate interoperability aspects and can be implemented independently. They are:1. Construction of Pipeline assemblies within widgets, for executing plugin functionalities without switching to the plugin user interface for that plugin 2. External Dashboard control, for manipulating the contents of the Dashboard Interoperability between plugins is achieved by construction of pipeline assemblies, which are dynamic software structures where the output of one component can be concatenated to one or more other components in the assembly in order to execute complex computational tasks. For instance, a design pattern selection service exposed by the eXtreme Design plugin (described in Chap. 3) could reuse the output axioms of a search issued using the Watson plugin (described in Chap. 7) in order to perform query expansion for broader pattern selection. Because the process has no strict coupling at build time, such a scenario can be realized without either plugin knowing a priori which other plugin it should expect its input from, or which one should accept its output.The other supported interoperability aspect is Dashboard control, i.e., the programmatic manipulation of the Kali-ma user interface. This allows other developers to construct custom dashboard configurations specific for the engineering activity supported by another tool. Among NeOn Toolkit plugins, the gOntt tool for project scheduling supports Kali-ma dashboard interoperability, as it is possible from within a gOntt schedule to launch a Kali-ma dashboard containing widgets for all registered NeOn Toolkit plugins that support a given process, activity, or phase in that schedule. This support is among the features showcased by the gOntt plugin description in Chap. 14.To reach either level of interoperability, a plugin must implement a simple Java API exposed by Kali-ma itself. A developer who wishes a plugin functionality to be directly called via its dashboard widget will simply have to implement an Eclipse extension point, which is mapped to a simple Java interface, both provided by Kalima. The developer will simply have to wrap a call to a plugin functionality into a Java class that implements this interface and annotate the single public method with the types of the parameters expected to be consumed and produced by that functionality.The Dashboard control API is also simple and straightforward. It is enough for a developer to invoke any static method of the DashboardLauncher class exposed by the Kali-ma API, and a dashboard will be launched, containing widgets for all the available plugins whose identifiers were passed as parameters. This implementation may occur in a separate plugin, without any intervention on the original plugin code.Architectural Design
The software architecture of the Kali-ma plugin, used for performing semantic reorganization of the tool space, incorporates both procedural and logical components. That is, although the plugin is essentially a Java program (or, to be more precise, a set of OSGi bundles) like most other plugins, some functionalities are not entirely encoded as procedures in the plugin code but instead rely on formal semantics that describe their behavior. Although the entire knowledge needed for managing the tool space is maintained in its original OWL formalism, this is treated in a similar fashion as runtime software libraries. Ontologies that describe the domain model, plugin space, and classification criteria are dynamically aggregated and linked at runtime.The sections that follow provide an insight on the software architecture of Kalima. After a quick overview on the next section, Sect. 15.2.3.2 describes the actual ontology network used by the tool. Section 15.2.3.3 describes the software modules that handle and reason upon the ontology network in order to classify NeOn Toolkit plugins. Finally, Sect. 15.2.3.4 provides a quick insight as to how the result is presented to the user.Basic Software Architecture
The heterogeneous representation of the Kali-ma components, as well as the openness to possibly reusing the procedural components in engineering fields other than ontologies, imply a layered infrastructure of the tool. This infrastructure can be seen as split into three major components as depicted in Fig. Ontological Component
The ontological component, encoded in its entirety in OWL, is at the lowest level of the stack. It is itself a layered subsystem, as the dependencies between its modules are acyclic. The component as a whole can be seen as a large networked ontology, although only the essential logical infrastructure is hardwired, whereas expert ontology engineers can define categorization rules without an exhaustive knowledge of the tool space, while leaving plugin contributors the liberty to author descriptions for their tools and host them wherever they see fit.  can handle, and so on. These are not hardwired in the built-in portion of the ontological component and can be located anywhere on the web. Recall from Sect. 15.2.2.2 that an online service is available for the automatic generation of these tool descriptions.Reasoning Component
In avoidance of the unwise practice of allowing the presentation component to handle the knowledge base straight away, Kali-ma implements a dedicated subsystem for extracting relevant knowledge. The ontological component provides such knowledge that the reasoning component wraps into a Java object model, which can then be accessed from the Dashboard controller in the presentation subsystem. This lower-level component in the Kali-ma software architecture, and the intermediate layer in the whole infrastructure, provides a software counterpart to the ontological component.The reasoning component comprises the following modules:• The Kali-ma object model represents parts of the C-ODO Light network, along with attached ontologies with additional categorization rules, in the form of Java types. This model includes interfaces for design tool, knowledge type, NeOn activity and design aspect OWL classes, and for generic annotated entities, whose RDFS label and comment annotations are deemed significant in the context of Kali-ma (i.e., they are presented to end users). • The description visitor is responsible for instantiating the object model mentioned above from the ABoxes supplied by C-ODO Light-based plugin descriptions and classification rule ontologies. This module includes monitorable operations for initializing OWL managers and DL reasoners (both supplied by external packages), loading them with fixed and user-defined ontologies and querying them. This system can be configured to manage a cache; thus, it does not necessarily query the DL reasoner on each Kali-ma run. • A model registry is where the instantiated object model is stored and kept track of. It stores wrapped OWL individuals and relationships between them and allows changes to the model to be monitored through its own event system. The model registry is ephemeral and does not need to be serialized, as it can be completely rebuilt at runtime from the ontological component in reasonable time.Through the components of this subsystem, Kali-ma becomes aware of what NeOn Toolkit plugins are known and/or installed in the running system, what are the relevant relations in ontology design, and which of them are supported by collected plugins. The Kali-ma application logic has no prior knowledge of such relationships.Presentation Component
The top-level component of the Kali-ma infrastructure, called presentation component, implements both the user interface and its controller in the Model-View-Controller (MVC) paradigm Introduction
A key component of the Semantic Web is provided by the large number of ontologies available online. Given such large-scale availability of ontologies, ontology reuse is becoming more common, and tools, such as the Watson plugin for the NeOn Toolkit To address this issue, we have developed a novel tool for visualizing and navigating ontologies, called KC-Viz, which has been realized as a plugin for the NeOn Toolkit. KC-Viz exploits automatically created ontology summaries, based on the idea of key concepts In this chapter, we present a description of the main functionalities provided by KC-Viz, and we show how it attempts to address some of the limitations of current tools for ontology engineering.Limitations of Top-Down Approaches to Navigating Ontologies
Ontology Sensemaking
Throughout this chapter, we will use as an illustrative example a version (v2.4) of the SmartProducts ontology 1 , which is being developed in the course of the EUfunded SmartProducts project 2 . The aim of this ontology is to support the specification of smart devices, able to engage proactively in cooperative problem solving with other devices.As shown in Fig. Like most other ontology engineering environments available today, the NeOn Toolkit provides an "Ontology Navigator" window, which supports ontology navigation using the classic top-down file system model, where clicking on a folder reveals its contents. In the case of an ontology engineering tool, the folder metaphor is used "to open up" a class, to reveal its sub-classes. As discussed in In this chapter, we will indeed focus on this category of tasks, which we will refer to informally as ontology sensemaking tasks. More specifically, we will use the term "sensemaking" to refer to the construction of a mental model of an ontology, which encompasses the ontology as a whole and is sufficient for a user to make a decision (for example) on whether an ontology is suitable for a particular application or whether it covers certain areas of interest to the required extent, with respect to user-specific criteria. In sum, the emphasis here will be less on supporting tasks which require understanding a particular detail of the ontology than on supporting tasks which require developing a "global" model of an ontology, at a certain level of abstraction. In addition, although KC-Viz also support the visualization of non-taxonomic (i.e., domain) relations, here we will focus the discussion almost exclusively on the navigation and visualization of taxonomies, on the basis that developing an understanding of the overall taxonomic structure of an ontology is an essential part of the sensemaking process.Example: Using the Ontology Navigator for Sensemaking
Figure • Understanding the overall size and shape of the ontology. By "size" here we mean, given a node in the ontologyUnderstanding the shape of an ontology (or part of it) also means to understand whether it is balanced, indicating that all parts of the (sub-)ontology in question have been developed to a similar extent, or unbalanced, possibly indicating that some parts of the (sub-)ontology are less developed than others. • Identifying the main components of the ontology and the typical exemplars of these components. For instance, from Fig. At this point, the reader may argue that what is needed is simply to explore the structure in more depth, by clicking on the top four classes, to open up the next level of detail. Figure The brief and informal analysis shown here is consistent with the findings uncovered in more extensive empirical studies, such as • Poor efficiency and effectiveness. To open up the display shown in Fig. • No abstraction or saliency mechanisms. The system has no way to automatically hide nodes which are deemed not important (i.e., salient) according to some criterion, and conversely, it is not able to bring to the attention of the user highly important nodes, again with respect to some user criterion.It is also important to emphasize that such problems are less associated with the file system browsing metaphor than with the generic top-down navigation approach. For instance, ontology engineering toolkits such as TopBraid ComposerKC-Viz is an ontology visualization and navigation system, which has been designed to address the issues highlighted here, by providing a rich set of navigation and visualization mechanisms, which include flexible zooming into and hiding of specific parts of an ontology, the ability to identify the most important concepts in an ontology, according to empirically validated criteria, as well as a plethora of other mechanisms to facilitate sensemaking and exploration of ontologies. As already mentioned, a key aspect of KC-Viz is its reliance on a key concepts extraction algorithm, which allows KC-Viz to produce the kind of ontology summaries that human experts are able to produce. Hence, in what follows we will first describe the key concept extraction algorithm used by KC-Viz, before providing an overview of its functionalities.Key Concept Extraction
Informally, key concepts can be seen as the best descriptors of an ontology, i.e., information-rich concepts, which are most effective in summarizing what an ontology is about. In Overview of KC-Viz
Initial Visualization of an Ontology with KC-Viz
Normally, a KC-Viz session• The network of ontologies contains four top-level classes (i.e., classes directly linked to owl:Thing) -however, only two of them are displayed (Abstract and TemporalThing) in the initial summary. • The ontology contains a lot of information about food -e.g., the tree under class FoodOrDrinkItem contains 46 classes. • Key distinctions include time (TemporalThing) and space (PhysicalEnti-tyInSpace). However, there are also temporal things, which are not physical entities in space. We can deduce this because the visualization tells us that TemporalThing has 108 sub-classes, while PhysicalEntityInSpace has 89. • Class Abstract has a lot of sub-classes ( More importantly, this initial visualization provides a much better structure for further exploration, than the rigid top-down navigation, which we illustrated in Sect. 16.2. In particular, on the basis of this initial snapshot, we can identify key "gaps" that we need to fill, in order to get a complete picture of the ontology. For instance, we may want to explore: • The sub-tree under class Abstract to get a better understanding of this part of the ontology. As discussed in Sect. 16.2, because of the relatively poor structure of this part of the ontology, better control of the navigation process than that provided by the Ontology Navigator will be needed, in order to be able to explore this part of the ontology effectively. • The sub-tree under FoodOrDrinkItem, as this is clearly a rich part of the ontology.• The sub-tree under PhysicalEntityInSpace, which appears to encompass both location-related notions and device-related ones (Assembly). • Which sub-classes of TemporalThing are not also sub-classes of PhysicalEntityInSpace. • What kinds of agents are modeled by this ontology.• Why products are not physical entities in space.• Others.In sum, the claim here is that the key concept extraction algorithm used by KC-Viz, together with the degree of control that we get over it (size of summaries and whether or not to consider imported axioms), allows the effective generation of initial ontology snapshots, which helps the user in forming an initial idea of what an ontology is about. In what follows, we show how the flexible support for exploration provided by KC-Viz capitalizes on this initial summary to facilitate effective ontology navigation and sensemaking.Exploring Ontologies with KC-Viz
Let us consider our first task: to get a better understanding of the sub-tree under class Abstract. We have already seen that a rigid top-down approach does not work very well here, in particular because class Abstract contains many direct subclasses. So, let us try exploring with KC-Viz.If we click right on a class displayed in KC-Viz, in this case, Abstract, we obtain a menu which includes options for inspecting, expanding, and hiding a class. If we select "Expand," the menu shown in Fig. • Whether to explore following taxonomic relations, other relations (through domain and range), or any combination of these. • Whether or not to make use of the ontology summarization algorithm, which in this case will be applied only to the sub-tree of class Abstract. • Whether or not to limit the range of the expansion -e.g., by expanding only to 1 or 2 levels. • Whether to display the resulting visualization in a new window ("Hide"), or whether to add the resulting nodes to the current windows. In the latter case, some degree of control is given to the user with respect to the redrawing algorithm, by allowing her to decide whether or not to limit the freedom of the graph layout algorithm to rearrange existing nodes. This is particularly useful in those situations where expansion is meant to add only a few nodes, and the user does not want the layout to be unnecessarily modified -e.g., because she has already manually rearranged the nodes according to her own preferences.As shown in Fig. Figure Analogously, we can also explore the other key constituents of the SmartProducts network of ontologies, by careful expansion of the sub-trees we wish to explore. For instance, again by choosing expansion by key concepts, we can find out more about the structure of the sub-tree under FoodOrDrinkItem, as shown in Fig. Other Functionalities Provided by KC-Viz
While the flexible expansion mechanism is the key facility provided by KC-Viz to support flexible exploration of ontology trees, a number of other functionalities are for the most common operations and also enables her to switch to a more efficient (but sub-optimal) algorithm when dealing with very large ontologies. Echoing the findings reported in the paper by • Poor efficiency and effectiveness. The exploration sequence shown in It is also interesting to assess the functionalities provided by KC-Viz with respect to the seven visualization task types proposed in • Overview. This is one of the key functionalities provided by KC-Viz. In contrast with other approaches -e.g., CropCircles • History. KC-Viz supports undo/redo actions at both macro and micro level, to allow users to go back to, replay, or undo earlier operations. • Extract. The key mechanism for extracting parts of an ontology is through the Expand menu, allowing flexible extraction of nodes at various levels in the hierarchy, in accordance with the key concept extraction algorithm, and following taxonomic and/or domain or range relationships.Related Work
Surveys on ontology visualization methods, like The indented list category covers tree-centric views of the ontology, similar to the one provided by the Ontology Navigator in the NeOn Toolkit, which was shown in Methods like IsaVizTreeMap CropCircles The group of techniques categorized as "context + focus and distortion" is based on "the notion of distorting the view of the presented graph in order to combine context and focus. The node on focus is usually the central one and the rest of the nodes are presented around it, reduced in size until they reach a point that they are no longer visible" Finally, Katifori et al. also discuss a class of systems called "Information Landscapes," which provide a 3D, landscape-oriented alternative to zoomable visualizations. Hence, the remarks we made above about the latter category of systems apply to information landscapes as well.Conclusions and Future Work
In this chapter, we have presented KC-Viz, an innovative approach to visualizing and navigating ontologies, which exploits a powerful ontology summarization algorithm, KCE, to introduce effective abstraction mechanisms in the ontology exploration and sensemaking processes. Crucially, KC-Viz maximizes the value of the foundational functionality afforded by KCE, by providing a flexible set of options to zoom in or hide specific parts of an ontology, history browsing mechanisms, flexible graphical layout formatting, and integration with other components of the NeOn Toolkit.Our next task will be to evaluate KC-Viz formally, by comparing the performance in sensemaking tasks of users equipped with KC-Viz versus other users, to try and determine whether there is objective evidence that KC-Viz improves both the efficiency and the effectiveness of a sensemaking task.We also plan to improve the range of functionalities provided by KC-Viz, in particular by opening up the key concept extraction algorithm to the users, to allow them to decide which criteria to prioritize in the generation of ontology summaries. Also, better explanation facilities are needed, as in some cases it is not easy to understand why a particular concept is deemed "important" by KC-Viz, while another one is not.In conclusion, it can be argued that, with a few exceptions, the ontology engineering community has historically overlooked the importance of HCI issues and has failed to provide user interfaces that can truly support users effectively, as highlighted by Many ontology languages have been developed, ranging from simple languages such as the Resource Description Framework (RDF) and the RDF Vocabulary Description Language, that is, RDF Schema (RDFS), to expressive languages such as the Web Ontology Language OWL. Ontologies specified in these languages allow for deductive reasoning: drawing conclusions based on facts and axioms in a knowledge base. Often used reasoning tasks are checking the consistency of a knowledge base, materialising inferences that can be drawn from the codified knowledge in an ontology and answering queries over the knowledge base.To support the life cycle of networked ontologies, we have developed three plugins which provide reasoning services in the NeOn Toolkit:• The reasoning plugin, which provides standard reasoning tasks on OWL 2 ontologies. Using the reasoner plugin, a user can get all the facts that can be inferred from specified facts and axioms and check if an ontology is consistent. • The RaDON plugin, which provides functionalities for diagnosing and resolving inconsistencies in networked ontologies. That is, in case the reasoner found an ontology to be inconsistent, it is difficult for a user to figure out the cause of the inconsistency, as the reasoner only provided a yes/no result. Within the NeOn Toolkit, the RaDON plugins enable a user to investigate the cause for an inconsistency. In addition, RaDON features algorithms which automatically resolve inconsistencies by removing those axioms which cause the inconsistency. • The query plugin, which allows users to pose queries in the SPARQL query language (Prud'hommeaux and Seaborne 2008; We cover preliminaries in Sect. 17.2, describe the approach for investigating and resolving inconsistencies in Sect. 17.3, explain the main functionalities the plugins provide in Sect. 17.4, summarise the usage of the plugins in Sect. 17.5 and finally conclude with Sect. 17.6.Preliminaries
In the following, we introduce basic notions used throughout the rest of the chapter.RDF and RDFS
RDF serves as foundational data model for the Semantic Web. RDF is a graphstructured data model for encoding semi-structured data. RDF data consists of RDF triples: statements of subject-predicate-object. The formal semantics of RDF is specified in Ontology languages, such as RDFS and OWL, provide vocabulary for describing classes and properties. Constructs of RDFS, such as subclass and sub-property relations, as well as constructs of the more expressive ontology language OWL are covered in the next section.Description Logics
Description logics (DL) Diagnosing and Repairing Inconsistent Networked Ontologies
Relationship Between Inconsistency and Incoherence
The relationship between incoherence and inconsistency is not simple. First, the fact that an ontology is inconsistent does not necessarily imply that it is incoherent, and vice versa. There exist different combinations of inconsistency and incoherence, as illustrated in Fig. From the definitions of incoherence above, we know that incoherence can occur in the terminology level only. When dealing with inconsistency, we can differentiate terminology axioms and assertional axioms. We have the following categorisation of different kinds of reason for inconsistent ontologies.• Inconsistency due to terminology axioms: In this case, we only consider inconsistency in TBoxes. Figure On the other hand, the inconsistency in the example in Fig. The first kind of inconsistency is only related to terminology axioms. In this case, the unit of change is a concept (either atomic or complex). Therefore, some revision approaches which take the individual names as the unit of change, such as the one proposed in From this discussion, we observe that the causes for incoherence and inconsistency are manifold, and their interdependencies are complex. Incoherence is always caused by conflicts in the terminology. It may or may not affect the consistency of the overall ontology. Inconsistency may arise due to conflicts in the ABox, in the TBox or a combination of both ABox and TBox.Debugging Inconsistent/Incoherent Ontologies
Current DL reasoners, such as RACER, can detect logical incoherence and return unsatisfiable concepts in an OWL ontology. However, they do not support the diagnosis and incoherence resolution at all. To explain logical incoherence, it is important to debug relevant axioms which are responsible for the contradiction.Definition 17.2. A MUPS of T with respect to A is the minimal sub-TBox of T in which A is unsatisfiable. We will abbreviate the set of MUPS of T with respect to a concept name A by mupsðT ; AÞ. Let us consider an example from where A, B and C are atomic concept names and A i (i ¼ 1,. . .,7) are defined concept names, and r and s are atomic roles. In this example, the unsatisfiable concept names are A1, A3, A6, A7 and MUPS of T with respect to A i (i ¼ 1, 3, 6, 7) are:MUPS are useful for relating sets of axioms to the unsatisfiability of specific concepts, but they can also be used to calculate a minimal incoherence-preserving sub-TBox, which relates sets of axioms to the incoherence of a TBox in general and is defined as follows.Definition 17.3. A MIPS of T is the minimal sub-TBox of T which is incoherent. The set of MIPS for a TBox T is abbreviated with mipsðT Þ. For T in the above example, we get 3 MIPS:To debug an inconsistent ontology, the notion of a minimal inconsistent subontology is proposed in Definition 17.4. A minimal inconsistent sub-ontology (MIS) O 0 of O is an inconsistent sub-ontology of O that has not any proper subset O 00 such that O 00 is also inconsistent.There are many algorithms for debugging incoherent DL-based ontologies, which can be classified into two approaches: a glass-box approach and a blackbox approach.A glass-box approach is based on the reasoning algorithm of a DL. The advantage of a glass-box approach is that it can find all MUPS of an incoherent ontology by a single run of a modified reasoner. Most of the glass-box algorithms are obtained as extension of tableau-based algorithms for checking satisfiability of a DL-based ontology. The first tableau-based algorithm for debugging of terminologies of an ontology is proposed in A black-box approach treats a DL reasoner as a 'black-box' or an 'oracle' and uses it to check satisfiability of an ontology. The approach is reasoner-independent, in the sense that the DL reasoner is solely used as an oracle to determine concept satisfiability with respect to an ontology. The disadvantage of this approach is that it needs to call the reasoner an exponential number of times in the worst case; thus, it cannot handle large ontologies. Several algorithms belong to this approach, such as those given in In order to handle large incoherent ontologies, which may naturally exist when ontologies from ontology network are integrated, we propose several optimisations. The first optimisation is based on the syntactic locality-based module defined for OWL DL ontologies in Computing MISs of an inconsistent ontology can be similar to computing MUPSs of an unsatisfiable concept. That is, we can simply adapt the black-box algorithm for computing MUPS to compute MISs. However, such an algorithm cannot scale to large ABoxes. In A General Approach for Resolving Inconsistency and Incoherence in Ontology Evolution
In this subsection, we deal with the problem of resolving inconsistency and incoherence. More specifically, we consider this problem in the context of ontology evolution. In this case, the problem is similar to the belief revision in classical logic.The problem of revision of ontology is described as follows. Suppose we have two ontologies, where O is the original ontology and O 0 is the newly received ontology which contains a set of axioms to be added to O. Even if both O and O 0 are individually consistent and coherent, putting them together may cause inconsistency or incoherence. Therefore, we need to delete or weaken some axioms in O to restore consistency and coherence. Note that when the original ontology is inconsistent or incoherent and the newly received ontology is empty, then the problem of revision of ontology is reduced to the problem of resolving inconsistency and incoherence in a single ontology. Therefore, the approach proposed in this chapter can be also used to deal with a single ontology.Usually, the result of revision is a set of ontologies rather than a unique ontology We first introduce the notion of a disjunctive ontology That is, an ontology revision operator is a function which maps a pair of ontologies to a disjunctive ontology which can consistently infer the newly received ontology. In practice, we may only need one ontology after revision. In this case, we can obtain such an ontology by ranking the ontologies obtained by the revision operator and then selecting the one with highest rank. Ranking of ontologies can either be given by the users or be computed by some measures, such as ranking of test cases and syntactic relevance (see The current work on ontology revision suffers from some problems, to name a few, we have the following ones:• There is much work on the analysis of applicability of AGM postulates for belief change to DLs Another problem which is as important as inconsistency handling is incoherence handling, where an ontology is incoherent if and only if there is an unsatisfiable named concept in its terminology. As analysed in A revision approach which resolves both logical incoherence and inconsistency is missing.We now propose our general approach which resolves incoherence and inconsistency in an integrated way. The approach consists of the process steps shown in Fig. , which is both consistent and coherent. In each of the revision steps, the result may be a disjunctive ontology, since there may exist several alternative ways to resolve the incoherence or inconsistency. However, in each step, a decision is made: Which single ontology should be selected as input for the subsequent step. This decision can be made either by the user or an automated procedure based on a ranking of the results as discussed above.Our general approach does not yet specify how to deal with inconsistency or incoherence. Moreover, for different kinds of inconsistency, we can use different strategies to resolve them. For example, when resolving inconsistency due to terminology axioms, we can take the maximal consistent subsets of the original TBox with respect to the new TBox as the result of revision. Whilst when resolving Fig. Repairing Ontology Mappings
There has been some work on handling inconsistency in distributed ontologies connected via mappings, where a mapping between two ontologies is a set of correspondences between entities in the ontologies. In a distributed system consisting of two ontologies and a mapping between them, correspondences in the mapping can have different interpretations. For example, in distributed description logics (DDL) In Main Functionalities
In the following, we discuss the main functionality provided by the plugins which cover reasoning and query processing with networked ontologies.Reasoning Functionality
The installed reasoner materialises inferences and combines them with the ontology, over which the query evaluation is carried out. Table Diagnosing and Resolving Inconsistencies
Before representing the functionalities of RaDON, some terminologies involved are introduced first: RaDON provides a set of techniques for dealing with inconsistency and incoherence in ontologies. In particular, RaDON supports novel strategies and consistency models for distributed and networked environments.RaDON extends the capabilities of existing reasoners with the functionalities to deal with inconsistency and incoherence. Specifically, the functionalities provided by RaDON include:• Debugging an incoherent or an inconsistent ontology to explain why a concept is unsatisfiable or why the ontology is inconsistent. • Repairing an ontology automatically by computing all possible explanations with respect to all unsatisfiable concepts if the ontology is incoherent, or with respect to the inconsistent ontology if it is inconsistent. • Repairing an ontology manually based on the debugging results. For the manual repair, the user can choose the axioms to be removed for restoring the coherence or consistency. • Repairing a mapping between two ontologies.• Coping with inconsistency based on a paraconsistency-based algorithm.Query Functionality
The query plugin allows for query answering over local ontologies residing in memory in the OWL API by using the ARQ query processor included in the Jena Semantic Web framework Summary of Usage
In the following we show how to operate the plugins.Reasoning Plugin
The reasoning plugin provides common access to reasoners for NeOn Toolkit components. The plugin can be configured in a Preferences view shown in Fig. RaDON Plugin
RaDON provides two plugins to deal with a single ontology or an ontology network. In the plugin of 'Repair a Single Ontology', the following specific functionalities are provided:• Handle incoherence: This functionality corresponds to the button of 'Handle Incoherence' which can be activated if the ontology is incoherent. That is, there is at least one unsatisfiable concept in the ontology. All the minimal unsatisfiability-preserving subsets (MUPS) can be computed for each unsatisfiable concept. • Handle inconsistency: This corresponds to the button of 'Handle Inconsistency' which is activated if the ontology is inconsistent. That is, there is no model for the ontology. All the minimal inconsistent subsets (MIS) can be calculated. • Repair automatically: This corresponds to the button of 'Repair Automatically'.If the button of 'Repair Automatically' is pressed, our algorithm will provide some axioms to be removed to keep the coherence or consistency of the ontology. Specifically, this can be done by computing the minimal incoherence-preserving subsets (MIPS) or MIS, respectively, and then choosing automatically some axioms to be removed. • Repair manually: This corresponds to the button of 'Repair Manually'. If this button is activated, a new dialogue will be shown with the information of MIPS or MIS which are computed based on the found MUPS or MIS, respectively. The user could choose the axioms to be removed by themselves.In the plugin of 'Repair and Diagnose Ontology Network', the similar functionalities in the plugin above are given. The main difference is that this plugin is to repair and diagnose a mapping between two ontologies by assuming the two source ontologies are more reliable than the mapping itself.SPARQL Query Plugin
The query plugin can be invoked using the 'SPARQL Query' context menu, which starts the SPARQL view.Users first load the ontology into the SPARQL query processor. Optionally, users can load an ontology and materialise inferences at the same time. Having loaded an ontology, users can specify a SPARQL query against the loaded ontology. For example, the following SPARQL query lists all instances of type owl:Class. PREFIX owl: <http://www.w3.org/2002/07/owl#> PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> SELECT * WHERE-{ ?s rdf:type owl:Class . } When users click the 'Evaluate' button, the plugin generates a table with the variable bindings. Figure In addition to SPARQL SELECT queries which return a table with variable bindings, the plugin also evaluates CONSTRUCT queries which return an RDF graph. The returned RDF graph can be used to create a new ontology in the NeOn Toolkit. Thus, the SPARQL query allows for the selection of parts of an ontology and further refinement inside the Toolkit.Conclusion
In this chapter we have discussed the functionality and workings of NeOn Toolkit's reasoning and query plugins. Reasoning tasks are important in knowledge engineering, for example, to check for logical consistency of modelled artefacts. The plugins provide users with advanced reasoning and query answering functionality over networked ontologies.Part IV
Case Studies
Chapter 18 Knowledge Management at FAO: A Case Study on Network of Ontologies in Fisheries Caterina Caracciolo, Juan Heguiabehere, Aldo Gangemi, Claudio Baldassarre, Johannes Keizer, and Marc Taconet Abstract In this chapter, we illustrate the work conducted at the Food and Agriculture Organization of the United Nations (FAO) with the creation of a network of ontologies about fisheries, developed with NeOn technologies and methodologies. The network included the main thematic areas needed to talk about fish stocks (often referred to as aquatic resources) and included data sources of various types: reference data for time series, thesauri for document indexing, actual time series, and the reuse of an existing well-known ontology maintained by FAO (the geopolitical ontology). Such a network of ontologies was also used within a prototypical web-based application. After describing the methodologies used to create the network, and its contents and features, we draw some conclusions and highlight the lessons learned during the process. List of Acronyms
Introduction
The Food and Agriculture Organization of the United Nations (FAO) has collected data about food and agriculture since its foundation in 1945. Since the beginning, metadata has been used to annotate, organize, and classify data, as in the case of thesauri used to index documents 1 and reference data for statistics, i.e., the concepts used as dimensions of a piece of statistical data 2 to store and retrieve statistics 3 . However, compared to most modern information systems, one may notice some limitations. First of all, especially for what concerns reference data, much of the actual relationships between the objects that are referenced are kept away from it. For example, there is reference data for species and reference data for fishing areas, but if users want to know something about what species is found in a given fishing area, they should search for this information in other information systems, where data about species distribution is available. The result of this situation is that it is well possible to query the system for time series about "catch of bluefin tuna (Thunnus thynnus, Linnaeus, 1758) in the Indian Ocean," although bluefin tuna is actually found in open waters of tropical and subtropical seas worldwide. Moreover, data is usually collected and stored according to one specific classification system. This implies that the only way to query a database is by using exactly the same classification used for storing the data. Conversion of data according to different classifications is then a time-consuming task that only domain experts may reliably perform. Obviously, the establishment of correspondences between classification systems is a delicate task that falls outside the scope of an information management project, but a modern information management approach should enable easy conversion between one system and the other, any time when the conversion is made available by experts.For these reasons, the data owned by FAO is a good application area to prove and refine the technologies developed within the NeOn project. We concentrated on the domain of fisheries, as it is a good example of domain where the possibility of linking together different information systems is crucial. The Fisheries and Aquaculture Department (FIES) of FAO publishes reports about various aspects of fisheries, including aquatic resources, on a regular basis. Reports are usually based on data contributed by different institutions, often stored in different information systems and encoded in various formats. Therefore substantial data integration effort is usually required. In our use case on fisheries, we worked on the hypothesis that the conversion of data and metadata into a network of ontologies could lead to improved information sharing. The experimental application we designed is a Fisheries Stock Depletion Assessment System (FSDAS) In the rest of this chapter, we present our experience with the making of a network of ontologies and on the application based on that. In Sect. 18.2, we illustrate the domain and data used for the use case. In Sect. 18.3, we describe the methodology followed for the creation of the network. In Sect. 18.4, we present the resulting network of ontologies and highlight its features. In Sect. 18.5, we present the ontology-based system we developed. Finally, in Sect. 18.6, we draw our conclusion and hint at future work.Domains and Data
A fish stock is a subpopulation of particular species of fish with some definable attributes and living in definable marine areas. A simplified notion of fish stock is the one of marine resource: a fish stock considered from a management perspective. We adopted the operational notion of marine resource and concentrated on a few entities considered crucial to talk about it: first of all, marine species, but also water areas (needed to indicate where species are caught) and land areas (to keep track of legal dependency of water areas and vessels). We also considered other entities, such as fisheries commodities and fishing techniques.As for the actual data sets, we considered a relevant subset of the data set available in FAO: a set of statistics on fisheries (mainly catch statistics) and documents about fisheries. The corresponding metadata (i.e., reference data for statistics and thesauri for documents) have been reengineered into a network of ontologies. We also reused and included in the network the FAO geopolitical ontology, used to aggregate information about countries and to operate the FAO Country Profiles portalData Sets Included in the Network of Ontologies
We have considered three distinct types of metadata sources:1. Reference data used to identify the "dimensions" of a piece of statistical data (we focused on catch data) 2. Thesauri used to index documents about fisheries: AGROVOC 6 and ASFA thesaurus 7 3. The geopolitical ontology 8 . an ontology about geopolitical information maintained by FAO referenced by the following dimensions: time (in years), space (land and/or water areas), and the variable representing the observed object (e.g., biological species, vessels, commodities). Figure Reference data is also used in the fisheries fact sheets produced by FIES, where a large amount of information about fisheries, aquaculture, and related subjects (including fishing techniques, fishing areas, fisheries and aquaculture country profiles) is made available to the public in the form of semistructured text. All fisheries fact sheets in FAO are in XML format, structured according to a comprehensive XML schema that includes all elements used in all types of fact sheets. Fact sheets are organized by domains (e.g., cultured species, fishing equipment, fishery, gear type), each corresponding to an element under the root FIGISdoc, the root of any fact sheet (XML document). Domains are fully specified by means of nested elements. Each element includes a description meant for human use.FAO thesauri. We considered two thesauri: AGROVOC and ASFA. AGROVOC is the FAO's corporate thesaurus, covering all domains of interest to FAO, including fisheries. AGROVOC is currently available in 19 languages, and 5 more are under development The FAO geopolitical ontology is a repository of information (e.g., names in various languages, codes, historical changes, geographical coordinates) about geopolitical entities such as countries and groups. It is largely used within FAO and by partners for the purpose of querying information systems using codes for internal storage of data about geopolitical entities Coverage of the Fisheries Network of Ontologies
Once identified the resources to use for our use case, we went on selecting the domain entities needed to ensure appropriate coverage of the network of ontologies and consequent use in the FSDAS. For example, aquatic species and water areas are necessary to define aquatic resources; land areas, together with aquatic species and water areas, are necessary to reference catch statistics. Other entities are useful to integrate our view on the fisheries domain, such as commodities (for production data), fishing gears, and fishing vessels. The user not acquainted with statistics and fisheries should keep in mind that names are important to enable users to search across different data sets, while codes are important for actual storage and retrieval of data.In this section, we concentrate on the most important entities needed to understand and reference fisheries data: aquatic species, water areas, land areas, and aquatic resources.Aquatic species. FAO maintains a list of aquatic species of interest to FAO (both for fisheries and aquaculture), and about which data is collected: the ASFIS (Aquatic Science and Fisheries Information System) list 12 , which currently includes nearly 11,000 items 13 . Each species is provided with a taxonomic code, which reflects a biological point of view (simplified taxonomic classification), used for the purpose of data aggregation along taxonomic lines. Species are also given an ISSCAAP 14 code, whose purpose is aggregation according to groups formed according to a commercial point of view. Species are also given an "inter-agency 3-alpha code", which is a compact way to represent them and is used for data exchange across UN agencies. As for names, only one preferred name in English, Spanish, and French is taken for each species. The English name is available for most of the records, and about one-third of them also have a French and Spanish nameWater areas. Marine and inland waters are divided into a variety of zones and areas, depending on the purpose of the division (e.g., legal jurisdiction, statistical reporting of catch data, environmental assessment) and on the author of the division (e.g., national or international body). For our purpose, the most important organization of water areas is the FAO Division Areas, used by FAO and partners for data collection and statistical reporting. This is a system of 27 major areas, divided into subareas, each divided into divisions, and these finally into subdivisions, covering marine waters as well as inland waters Land areas. Land territories are central to most statistical collections. Fisheries data is no exception since fish catches are either assigned to the country of the flag flown by the fishing vessel or to the country where the vessels lands. Either way, one may not forget territorial information. The names of territories (countries and groups) are established by international agreements. By agreement, at least two types of names of territory are given in each language: long names to be used in official documents and short names to be used in informal communications. A variety of codes are used for land areas. Most remarkably, ISO codesFish stock aka aquatic resources. From a biological point of view, a stock comprises all the individuals of fish in an area, which are part of the same reproductive process. A stock occupies a well-defined spatial range and is independent of other stocks of the same species. When dealing with fishing management, though, it is common to use the notion of aquatic resource, to vaguely defined "stocks", especially for management purposes. Just like a stock, a fishery resource is defined in space and its geographical demarcation and often has a political or jurisdictional connotation (e.g., Moroccan resources, exclusive economic zones (EEZ), or high seas resources).Methodology for the Creation of the Network
The methodologies for creating ontology networks have been widely studied within NeOn (see Chap. 2). In our use case, when creating ontologies, we dealt with various different situations:• Reusing and reengineering non-ontological resources (Scenario 2 of the NeOn Methodology) • Building ontologies from scratch (corresponding to NeOn Methodology Scenario 1) • Reusing ontological resources (Scenario 3) • Reusing ontology design patterns (Scenario 7) • Mapping the ontologies created in order to obtain a networked set of ontologies (Scenario 5)We elicited the specifications for single ontologies, mappings among them, and for the entire network, by extensive conversation with domain experts, in some cases summarized by explicit competency questions. We also analyzed the systems and data sets currently in use, to infer requirements about the expressivity of the ontologies, their coverage and use for data collection, storage, and retrieval. In particular, the scope, purpose, and level of formality of the network were derived from those of the existing systems. In this sense, reaching consensus about the ontologies was not really a problem, while several phases of consistency check, verification, and validation of the requirements we extracted were performed together with domain experts.The ontologies in the network have been built by (1) reengineering non-ontological resources either from relational databases or from informal knowledge organization systems: thesauri, classification schemes, etc.; (2) designing them from scratch; (3) reusing without any change. As for the reengineering of relational databases, we followed the approach proposed in We followed an iterative approach consisting in a phase of domain analysis, a phase of domain modeling, and a phase of data population of those models. A phase of validation made with the collaboration of FAO fisheries and information management experts followed, which triggered a new iteration of modeling and population. Each iteration was carried out in collaboration with domain experts, and sanity checks were performed thereafter.As for the reengineering of knowledge organization systems, we have considered two thesauri: ASFA and AGROVOC. Both had been previously ported to RDF by using heterogeneous techniques: see Note that the different nature of the resources we used as a starting point, thesauri and reference data (from relational databases), forced different modeling styles. In fact, the ontologies resulting from the reengineering of reference data have a precise semantics: if a class exists in the ontology, its extensional interpretation (the set of individuals that have that class as rdf:type) includes exemplifications of the domain concept expressed by the name of that class. For example, the WaterArea class refers to the collection of things that are water areas according to fishery experts; the Species class refers to the collection of things that are taxonomical species in the knowledge of fishery experts.On the contrary, thesauri cannot be assumed to have an extensional semantics. For example, asfa:Catchment_area cannot be directly interpreted as a class of catchment areas but only as a thesaurus concept; in other words, it has a purely intensional semantics. SKOSIn other cases, ontologies have been built from scratch in order to model in a more explicit way concepts that are implicitly used in the other resources that have been reengineered, or because the FSDAS system needed the implementation of some application requirements. Examples include the ontologies about aquatic resources and the ontology for catch records. In those cases, the importance of competency questions Finally, the inclusion in the network of the geopolitical ontology is a case of reuse of an ontological resource. We have only reused data about countries, while excluding other data that can be found in there, such as groups of countries.Ontologies were networked in a variety of ways. First of all, improved versions (either for the model or for the data) of ontologies were connected to one another by means of the attribute owl:priorVersion. Second, some of the ontologies produced were also designed in a modular way, thus creating a partial order graph by means of owl:import statements. Third, mapping data was taken by the reference data, where it was made available in relational form. For example, this happened with ontologies about different commodity types. Fourth, a limited amount of mapping was manually provided by domain experts. Finally, mappings were learned through automatic methods (supervised by experts) and expressed as OWL ontologies that contain the linking axioms between the vocabulary and the data from the linked ontologies. Examples of these cases include mappings between ASFA and some RTMS-based ontologies (e.g., species, aquatic areas, etc.), the linking between countries in the geopolitical ontology and the catch data, and most of the mappings included in the network.The mapping between ASFA and AGROVOC (we already highlighted the differences in their intended semantics) is an interesting case of automatic extraction of data and manual mapping. In that case, we had two options: either to enforce an extensional semantics in ontologies derived from thesauri or to leave the intensional semantics of the whole resource, and impose extensional semantics only on specific thesaurus fragments, when needed. As the former approach has proven to imply a time-consuming and partly arbitrary process, we opted for the second approach, both for ASFA and AGROVOC. The consequence of this choice is that, for any further usage of ASFA or AGROVOC concepts within the fishery ontology network, task-oriented decisions will be required to provide a domain semantics. For example, if the concept asfa:Catchment_area (an individual from the class skos:Concept) is aligned to the class waterarea:Area (a class from the FAO_fishing_area ontology), and the expected application aims at, for example, finding the water areas for catch records of tunas, asfa:Catchment_area should be also represented as an owl:Class by means of a refining rule so that any matching water area (e.g., Mediterranean_sea) extracted from a document indexed by means of ASFA can be represented as an instance of both asfa:Catchment_area (as a class) and waterarea:Area. OWL2The mapping between ASFA and AGROVOC was performed by using string matching techniquesDomain experts performing the evaluation of the suggested mappings were presented in a spreadsheet with three columns, each showing an ASFA concept, the candidate AGROVOC concepts (shown using their preferred English label) as a concept-list embedded in a menu box, and the available SKOS mapping relations to choose from.The results of the ASFA-AGROVOC mapping are finally included in a new ontology that imports SKOS, ASFA, and AGROVOC (Fig. FAO Network of Ontologies
In this section, we describe the FAO network of ontologies we produced and analyze in details its features. All ontologies are available from the FAO websiteThe thematic areas covered by the network are all those introduced in Sect. 18.2 (aquatic species, water areas, land areas, and aquatic resources), plus others (fishing gears and fishery commodities) that we do not describe in this chapter because less central to the notion of fish stock. Figure The case of the ontology of catch record is an interesting one because it organizes metadata (with extensive linking to the dedicated ontologies of reference data) and data, i.e., pieces of statistical data about catch of aquatic species. We considered the data collected by the Northwest Atlantic Fishery Organization (NAFO)The picture shows the schema of a typical catch record, highlighting the types (boxes) of subjects and objects of the properties (arrows) holding between a record and the objects or data involved in that record. The catch record ontologyThe ontologies produced were networked by means of mappings of various natures between their classes and/or individuals. Here we concentrate on those mappings that are exploited by the FSDAS to provide users with richer entry points to the data than those provided by current information systems (as per the limitations mentioned in Sect. 18.1): we do not go in further details with the network of import statements, or prior versions, and concentrate on mappings modeled by using SKOS vocabulary and mappings expressing thematic, domainoriented information.In the following, we list the mappings included in the network, including the reason for having them in there. First, we list the mappings modeled as skos: exactMatch:• Taxonomic -ASFA -Matching between species, to provide aquatic species, mainly described taxonomically with more information about common names• ASFA -AGROVOC -Matching between species names, to exploit the multilinguality of AGROVOC for species names• Fisheries commodities -ASFA -To provide ASFA commodities with the exact classifications Many of these mappings were identified by using the experience (methodologies, tools) gained during the Ontology Alignment Evaluation Initiative (OAEI) Also, some domain properties were identified and new links created (see • ASFA -Fishing gear ISSCFC. Relation: species caught by gear.-To allow explicit grouping of species based on the gears used for their capture • Taxonomic -Fishing gear ISSCFC. Relation: species caught by gear.-To allow explicit grouping of species based on the gears used for their capture • Taxonomic -FAO fishing areas. Relation: species found in FAO water areas.-To allow explicit grouping of species based on the water areas where they are known to be found• Taxonomic -FAO geopolitical ontology. Relation: species in the vicinity of country.-To allow explicit grouping of species based on the countries having a shore with the water areas where the animal is known to be found• Taxonomic -Commodities ISSCFC. Relation: species used for commodity.-To provide a biological view on fisheries commodities• Exclusive economic zones -FAO fishing areas. Relation: EEZ area intersects FAO area.-To provide a biological view on marine political boundaries• Exclusive economic zones -FAO geopolitical ontology. Relation: EEZ area is owned by a country.-To associate water areas and the country ruling over it• Large marine ecosystems -FAO fishing areas. Relation: LME area intersects FAO area.-To have a biological view on FAO reporting areas for statistical purpose• Species ISSCAAP -Commodities ISSCFC HS. Relation: ISSCAAP group of species originates commodity.-To provide correspondence between different classifications of aquatic species according to a commercial point of view• Species ISSCAAP -Taxonomic. Relation: ISSCAAP group of species includes taxonomic entity.-To provide a biological view on a grouping of species based on commercial interest• Aquatic resources -Taxonomic -FAO fishing areas. Relations: aquatic resource consists of aquatic species; aquatic resource lives in FAO area.-To express the composition of a stock in terms of species and its presence in a given FAO water areaAs for how to expose the mappings between ontologies, i.e., if by means of dedicated ontologies or not, we decided on the basis of what is best suited for provenance and later maintenance. We created third entities in all cases where the links are extracted after the creation of the ontologies. While in all other cases, we preferred to leave the linking information inside the ontologies: this happened especially in the case of correspondences between classification systems (e.g., commodities), which have the same provenance as the reference data.Linguistic information. A dedicated linguistic model has been developed within NeOn, the LIR model (see Chap. 4 in this book). The LIR model is a quite sophisticated framework meant to support creation of ontologies that are multilinguality aware, as is the case, for example, of resources such as AGROVOC, which is currently distributed in 19 languages (5 more to be released soon). However, our network of fisheries ontologies showed a limited degree of multilinguality (in most cases, names are available in no more than three languages), which made us adopt a simplified modeling of linguistic information but compatible with the LIR model.URIs. We used hash URIs 34 for all ontologies based on reference data, forming their local part by concatenating key codes used in the relational database 35 . This type of URIs guarantees uniqueness; it is easy to check because the meta code allows one to recognize at a glance if a piece of data was correctly taken from the database and organized in the right class; given the stability of the source databases, the generation of new versions is always compatible with previous versions, and it is therefore of easy maintenance. Finally, this convention ensures uniformity throughout the reference tables, which implies that URIs may be built in the same way independently of the reference data set at hand.The inconvenience of such a type of identifier is that it is very little informative to casual users who are not aware of the database behind. However, this drawback is partially overcome by having rdfs:labels that may be used for display, but still one may argue that better, i.e., more informative and user-friendly URIs may be used. Appropriate considerations should be made on a case-by-case approach 36 . The discussion above shows that no uniform approach to URIs can be taken in this 34 http://www.w3.org/TR/2007/WD-cooluris-20071217/ 35 As in http://www.fao.org/aims/aos/fi/species_taxonomic.owl#ID_31005_2632 36 Compare, for example, the following cases: For biological entities, the only names available (in FAO data) for all entities are the scientific names. In the case of FAO water areas for statistical reporting, the best option seems to be using the code itself (that vary in length and formal composition, which may represent a problem for maintenance) given to each area, as it is the only piece of information that each item adopts. Other entities in the future should be taken directly from the body in charge, as in case of the large marine ecosystems is maintained by the US National Oceanic and Atmospheric Administration (NOAA). The case of exclusive economic zones (EEZ) is more complex, as there is no single accepted way to model and manage this type of data. GIS technology provides a good tool to keep track of EEZ borders, but for our purposes, it is also important that a coding system, if possible standardized, be available. Similar issues apply to the case of vessel and gear types, while the case of commodities is even more controversial, as in domain: sometimes, a human-readable name is the best choice; in other cases, names are not available at all, or if they are, they are simply too long and cumbersome to use. Codes may be preferred; however, they follow a number of different formats, and often, they are revised and changed more often than names. For this reason, we kept numeric identifiers in the URIs, and in so doing, we privileged uniformity, uniqueness, and ease of maintenance over the possibility for a human user to grasp from the URI what it is about. However, when data is to be visualized, rdfs:labels are used instead of URIs.Fisheries Stock Depletion Assessment System
The Fisheries Stock Depletion Assessment System (FSDAS) is a web-based prototype (Fig. Users experience FSDAS as a browsable and queryable web application that returns organized, ranked, linked results, together with direct links to related stored documents or web pages. Figure The query panel (Fig. FSDAS also supports the grouping of aquatic resources by (marine) area or other criteria, such as the fishing gear typically used for their capture. Conclusions and Lessons Learned
In this chapter, we reported on our experience with the application of tools and techniques developed within the NeOn project to the area of fisheries. We have developed a network of ontologies covering the most important thematic areas needed to manage information about aquatic resources (i.e., aquatic species, water areas, and land areas) and massively included mappings between the ontologies in the network. The network also included a data set of statistical data: this is the first attempt to give a semantic modeling to numeric data of that sort. Based on the network, we built a prototypical application, called Fisheries Stock Depletion Assessment System, with the goal to show users how networked ontologies could be used.The FSDAS prototype nicely shows the networked ontologies developed for this work and how they can be used to bridge the information gap currently existing between data sets. In particular, by using as a rich network of ontologies, it allows users to query and look at the data collected, in particular statistical data, according to more points of view than the one used for data storage.The creation of the network was nicely supported by the NeOn Toolkit and its plugins, in particular for the reengineering of non-ontological data (i.e., the reference data for time series) and the ontology mapping. However, in the case of ontology mapping, also a number of NeOn technologies have been used, that were later developed as NeOn plugins. For the reengineering of reference data, we used extensively the NeOn plugin ODEMapster, which relies on ontologies for the extraction of data from relational database. This approach resulted to be very useful in preliminary phases but seems to be less convenient for data maintenance, as any minor change in the original database requires that a new transformation phase takes place.The network of fisheries ontologies was produced within the scope of a large prototypical effort, and it was not used to replace current information systems and data sets. The implication of this state of affairs is that we essentially duplicated data, as data maintenance continued to take place in the original data sources which continued to feed existing information systems. In order for a complete replacement to take place, all information systems accessing the data included in the network of Fig. The work exposed in this chapter also contributed to a better understanding of the issue of converting traditional thesauri into modern formats to be compatible with current approach to web publications. For instance, the conversion of the AGROVOC thesaurus into a concept schema is now finalized, while the conversion of the ASFA thesaurus is about to be completed, also thank to the lessons learned during the making of this work (see considerations about AGROVOC and ASFA semantics in Sect. 18.3).The activity of ontology mapping has had a remarkable follow up in the area of linked data, which has RDF and concept schemas (ontologies) at its very foundations. One of these results is that FAO is currently enhancing the network of ontologies and the conversion work carried out within NeOn to produce a larger data repository of open linked data However, almost all current solutions are stand-alone applications, each with their own model of an electronic invoice.Consequently, many industries suffer from migrating legacy systems to the formats required by the current e-invoicing solutions. This is obviously an entry barrier, especially for small and medium enterprises; large companies suffer less because they can "force" their providers to comply with a particular format, or else, they are out of business. Taking into account that a middle-sized organization processes around 100,000 invoices per year, the potential benefits are self-evident, with an estimated saving of almost 80% with respect to traditional paper invoices Throughout industry, there is a large duplication of effort that could be significantly reduced if companies in the same sectors were willing to share models and infrastructure, a precondition which is made more complex by the competitive environments where they operate. The main limitations therefore include:1. High investment (acquisition and maintenance) for industrial stakeholders to set up their own business IT infrastructure. 2. Difficulty in setting up business partnerships is due to high IT integration costs; this requires integration and communication across heterogeneous infrastructures, with additional investments to be made as each new partner joins such partnership. In practice, this implies the development of ad hoc transformation software between each pair of invoice formats and models potentially participating in economic transactions, which is time-consuming, expensive, and cumbersome. 3. Lack of possibilities to benefit from the fact that business partners of companies in a same sector are often shared and their invoice models could be common.This scenario provides an opportunity for building semantic platforms for data interoperability among different stakeholders for business transactions in the form of invoice exchange. Our main objective is to facilitate interoperable invoice exchange between organizations following different formats and models, thus reducing entry barriers for companies and stimulating widespread adoption of e-invoicing, especially among small-and middle-sized companies.In order to achieve such objective, users of e-invoicing systems need to be provided with (1) expressive, modular, and extensible means to represent invoice data observing both relevant standards and proprietary, local, and even tacit (not explicitly defined by a data model) representations of invoices and (2) usable tools that enable users to define correspondences between their invoice data and such formal models, which can be subsequently exploited for automated exchange of invoices between stakeholders following different representation models and/or formats.In this chapter, we focus on the principled development of networked ontologies to address the first of the abovementioned issues, allowing for automatic transformation of large amounts of invoice data across formats and models.Use Case Description
The main problem of interoperable invoice exchange is heterogeneity. Current technology and methods do not allow building generic solutions which allow the different peers involved in a commercial transaction to automatically process any type of invoice. The range of Enterprise Resource Planning (ERP) systems according to which invoices are emitted and the different formats that exist in the market are so wide that it has been necessary for organizations to take special measures in order to adopt electronic.Nowadays, two main types of measures are applied: (1) to create clusters or sectorial associations that agree to define common invoicing infrastructure in terms of shared ERP platforms, invoice formats and models, and processes and (2) to identify the invoicing infrastructure that is necessary to automate invoice exchange with given stakeholders and build specific ad hoc plugins which implement gateways between the invoicing infrastructure of each peer.Additionally, these two options can be mixed to a certain extent, as in the case of PharmaInnovaHowever, all this process is costly in time and effort, and flexibility is low. For example, if a new member enters the cluster, the agreed invoice model needs to be revised not only within the cluster but also with the clients. This problem also applies to all the possible commercial transactions of pharmaceutical laboratories with suppliers, wholesalers, and pharmacies and, by extension, to those between wholesalers and pharmacies. Furthermore, knowledge about how to process invoice data in order to address the interoperability problem should be provided as directly as possible by subject matter experts (SMEs) in the field, leveraging their expertise on the domain to conduct this process. This allows to minimize the number of errors introduced by engineers with a limited knowledge of the e-invoicing models and formats of specific companies and to reduce the cost of implementing the transformation process by leaving additional engineers out of the loop.However, the knowledge of SMEs about invoice representations is usually constrained to their own e-invoicing systems. Therefore, it becomes necessary to provide them with a shared, formal conceptualization, e.g., an ontological representation of the e-invoicing domain, which SMEs can use to describe their invoice data. The election of an ontological framework for such purpose supports a threefold objective:1. Provide a formal model for the representation of knowledge related with the einvoicing domain, which observes both e-invoicing standards and sectorial specializations 2. Serve as a semantic gateway for invoice transformation during invoice exchange 3. Ensure the consistency of invoices exchanged between heterogeneous systems by leveraging the expressivity of the ontologies for automatic data and object type checks, observations of cardinality constraints, etc.In this use case, we follow a learn-by-example approach where SMEs define correspondences between a sample invoice and the ontologies, which enable them to semantically annotate invoice data. The correspondences defined through such annotations are stored, recording metadata about each individual piece of invoice data annotated by the SME and the ontology entity it corresponds to. Subsequent invoices following the format and model of the sample can be automatically processed using the correspondences identified during the annotation phase, thus supporting their transformation into ontology entities and, from there, into whatever other invoice format and model treated in the same way. Ontology Network Development
In the context of this use case, we have built an ontological framework in the form of expressive, modular, and extensible networked ontologies following the ontology development guidelines provided by the NeOn Methodology (Chap. 2) and, simultaneously, contributing to its development. The resulting invoicing networked ontologies are available as exemplary ontologies at the Ontology Design Patterns• The observed ontology requirements specification • The established ontology development life cycle, as part of the scheduling activity • The most relevant processes and activities that have been performed for the development of the ontologiesOntology Requirements Specification
Ontology requirements have been obtained fundamentally through competency questions, answered by SMEs in a number of business sectors, who covered a broad spectrum of the different roles in the invoicing process, including:• U1. User of the invoicing application who is going to model a new invoice • U2. User who emits invoices • U3. User who receives invoices • U4. User who administrates the invoicing system • U5. Developers of invoicing applicationsThe complete set of competency questions and answers obtained can be found in Go ´mez-Pe ´rez et al. 1. Competency questions about the e-invoicing workflow CQ7: What is necessary to identify the emitter of the invoice? NIF/CIF. CQ9: What is necessary to identify the products in the invoice? Product description. CQ15: What is the address of the emitter of the invoice? The supplier fiscal address. CQ17: What is the status of the invoice X? The status can be imported, emitted, in process, accepted, in creation or disused.2. Business rules applied during invoice exchange (4 CQs). Examples for this group are: CQ20: What is the total discount applied to this invoice? Discounts in payment date. CQ21: Is it possible to apply any special price to this invoice? Yes, if you describe the concept in the description line. CQ22: Is it possible to apply any business rule in this invoice? No, only the rules related to the amount to pay and supplier code. CQ23: What is the unitary price before applying discounts? The net price.3. Information about the roles of e-invoice emitters and receivers (32 CQs).Examples for this group are:CQ29: How much is the total price of the invoice? Price in the specific invoice received. CQ33: When do we have to pay? Date in the specific invoice received. CQ49: Have we sold any other product in invoice X? Products in the specific invoice received. CQ60: Do we have to apply any specific rule in invoice X? Rules in the specific invoice received. Multilingualism needs (2 CQs
). The CQs for this group are:CQ18: What is the language of the invoice? Currently only Spanish. CQ19: Where is the emitter of the invoice from? Spain.6. Time modeling (15 CQs). Examples for this group are:CQ62: When do we have to pay the invoice X? Date that depends on the agreement. CQ63: When are the goods arriving bought in invoice X? On the agreed date. CQ67: What is the expiry date of invoice X? There is no expiry date. CQ75: How many products did we buy during the month? Number of products in the received invoices.Currency representation (8 CQs):
CQ76: In what currency are the receivers paying in invoice X? Euro CQ78: What taxes are applied in the invoice X? IVA, IGIC, or RE. CQ82: How much is the total amount in invoice X? Total line amount -total discounts + taxable amount.8. General and composed competency questions (14 CQs):CQ4: What concepts are mandatory for a wholesaler/provider/laboratory? Two types of information: first regarding the identification of companies (names, addresses, bank accounts, etc.) and second information about the amounts of the products in the invoice and their prices CQ84: Given a set of invoices of different companies, is it possible to identify the common concepts used? Yes. CQ87: Given the information of a company, what products did it buy? Products in the received invoice. CQ91: Given the information of a product, how many units have been sold? Number of units in the received invoice.Among the most relevant findings, interviews with SMEs showed the need of observing the main e-invoicing standards, which were identified as EDIFACT and UBL, and proprietary data models in business partnerships.Ontology Development Life Cycle and Scenarios
for Building the Invoicing Networked OntologiesIn the face of eventual changes in the requirements to fulfill by the invoicing networked ontologies, we have applied an iterative-incremental life cycle model (explained in Chap. 2) for the development of the invoicing networked ontologies.In addition to the activities performed as part of Scenario 1 (e.g., the ontology conceptualization activity), we have mainly combined Scenario 6 of the NeOn Methodology (reusing, merging, and reengineering ontological resources) and Scenario 2 (reusing and reengineering non-ontological resources) from those described in Chap. 2. The ontology specialization and ontology localization activities, respectively, from Scenario 8 and Scenario 9 have also been performed. See Fig. Processes and Activities Performed
The following activities have been carried out for building the invoicing networked ontologies:1. Ontology elicitation. In this support activity, the pharmaceutical domain was analyzed, with a focus on the invoicing life cycle, describing the steps an invoice goes through from the time it is emitted to the moment it is validated by the receiving company. This analysis also includes the actors that participate in the process (laboratories, wholesalers, and providers), and their requirements. 2. Ontology requirements specification. As described in Sect. 19.3.1, this activity is aimed at addressing the requirements that need to be fulfilled by the ontologies in order to effectively support applications implementing the approach described herein. 3. Reuse of existing knowledge resources. The knowledge resources used for creating the invoicing networked ontologies can be organized in the following groups:-Upper-level ontologies and related projects. The motivation for using upperlevel ontologies comes from the need of reuse of the main reference ontology for invoicing. The purpose of this ontology is that it can be instantiated for different sectors of the industry. The first instantiation is for the pharmaceutical sector, laboratories mainly, but it will also be extended for providers of these laboratories or wholesalers. These providers provide from chemical products to energy or clean products, so they need different instantiations of the invoice reference ontology. -Invoicing resources. These resources are mainly technologies for electronic invoicing. The technologies are UBL, EDIFACT, and the PharmaInnova approach. -Projects whose main goal is to integrate the invoice vocabulary into ontologies. These include the ONTOLOG project4. Ontology conceptualization (development of the invoicing networked ontologies).In this step, we conceptualized the resources analyzed in the previous activities. A result of this activity is the ontology design pattern InvoiceDescription of the Invoicing Networked Ontologies
As shown in Fig. The coverage of the e-invoicing domain provided by these ontologies is extensive, with almost 700 classes, around 500 object properties, and 300 data properties (Table IBO has been built through the reuse of a number of business process ontologies, like the enterprise ontology (EO) During ontology reuse and conceptualization, we have used the core functionalities provided by the NeOn ToolkitApplication Description: i2Ont
The range of ERP systems managing invoicing information (SAP, ORACLE, PeopleSoft, Baan, Movex, openXpertya, etc.) and the different languages for exchange of electronic business documents that exist in the market (EDIFACT, UBL, Intermediate Document from SAP, etc.) are extremely diverse. NeOn Toolkit with i2Ont pluginThe solution proposed is grounded on a combination of networked ontologies and a graph-based visualization and navigation paradigm. Networked ontologies provide a formal, semantic backbone between different electronic invoicing formalisms and models, including support for the main invoicing standards, like EDIFACT and UBL, and sectorial approaches like PharmaInnova. The user interface allows for a simple navigation across the relevant invoicing concepts, and the formal invoice model described in the ontology network allows ensuring correctness and completeness of the correspondence between the different electronic invoice representations.Previous approaches to the invoice interoperability problem required implementing specific transformations between the formats and models of each pair of organization exchanging electronic invoices. This was cumbersome and little scalable. On the contrary, i2Ont learns by example, i.e., sample electronic invoices are used to define the mappings between electronic invoice data and ontology concepts. Subsequent electronic invoices received by the system, with a format and model compliant with such sample invoices, are transparently imported as instances of the invoicing ontologies by means of applying the mappings defined during the learning phase. From that point on, invoices are automatically exported to whatever invoice format and model known by the system without needing to implement ad hoc (and costly) transformations.Exploitation Roadmap
The exploitation roadmap of the approach described in this chapter starts from the integration of i2Ont technology with the PharmaInnova platform and its subsequent exploitation by PharmaInnova and its members. For this purpose, a new version of i2Ont has been developed in the form of a web application called PharmaInvoicingPharmaInvoicing presents functional and usability enhancements with respect to i2Ont, focused on improving import and export performance and user interaction throughout the electronic invoicing life cycle. It also includes a back office providing i2Ont's functionalities for SMEs to configure the correspondences (mappings) between their invoices, using a single sample invoice and PharmaInnova's model.Figure The use of PharmaInvoicing is completely transparent from the underlying knowledge representation formalism. Invoices can be imported, exported, accepted, rejected, and electronically signed, just like they are without the application of these technologies, the only difference being the savings in terms of saved money, time, and effort of IT experts in implementing ad hoc software for translating invoice data from one format and model to another. Figure As shown in Fig. Conclusions
In this chapter, we have described how networked ontologies can be used to alleviate classical interoperability problems in electronic invoice exchange by means of (1) providing a formal model for the representation of knowledge related with the e-invoicing domain, which observes both e-invoicing standards and sectorial specializations, (2) serving as a semantic gateway for invoice transformation during invoice exchange, and (3) ensuring the consistency of invoices exchanged between heterogeneous systems by leveraging the expressivity of the ontologies for automatic checks. A detailed description of the developed networked ontologies has been provided, as well as of the application of the NeOn Methodology, which supported such development and simultaneously benefited from this use case as a comprehensive test bed. We have provided a short description of an application (i2Ont) implementing the approach and introduced a subsequent business roadmap for the resulting technology. Future work includes developing extensions of the invoicing networked ontologies, for a more thorough coverage of invoicing formats and models beyond EDIFACT and UBL, and the corresponding extensions of the application in order to benefit from such extensions. to describe drug models. Initiatives such as BioPortal or the OBO Foundry are clear examples of the uptake of biomedical ontologies. It is in this direction where there is a clear need of a solution where different terminologies expressed in ontological form are mapped and connected in a way that the interoperability is ensured.In order to achieve an interoperable nomenclature of drugs, a potential solution has to provide the means to (1) transform the different vocabularies and models into ontologies, (2) put all the ontologies together by creating the necessary mappings between different drug descriptions, and (3) create the infrastructure to query the ontologies using the terms that the different stakeholders are more familiar with.In this chapter, we describe a proof of concept of this interoperable nomenclature built according to the NeOn approach. The chapter is focused on the application of different steps of the NeOn Methodology (Chap. 2) and the use of the NeOn Toolkit (Chap. 13) and some of the plugins recommended by the methodology. It is worth noting that the case study and the NeOn Methodology evolved together during the NeOn project, so the methodology received continuous feedback from real scenarios of usage in order to be eminently practical.The chapter is structured as follows: First, we give a brief overview of the case study followed by a discussion about how we applied the NeOn Methodology to engineer the Semantic Nomenclature ontology network. The resulting ontology network is then presented, along with a brief overview of the application showcase developed to query the network. A conclusion section gives the general considerations of the chapter.Semantic Nomenclature Use Case Description
One of the major problems in achieving a common drug description is the different stakeholders involved. Standardization bodies, governments (transnational, central, regional, or local), international public bodies such as the World Health Organization (WHO)In order to achieve semantic interoperability, we need to:1. Enable the safe, meaningful sharing and combining of pharmaceutical data between heterogeneous systems 2. Enable the consistent use of modern terminology systems and medical knowledge resources 3. Ensure the necessary data quality and consistency to enable rigorous uses of heterogeneous data Addressing all the previous issues needs more than a methodological and technological sound approach. It needs the agreement and collaboration of most of the named stakeholders in the overall life cycle, from the ontologies definition and mapping to the validation, evaluation, and continuous update of the results. This issue is clearly out of the scope of a project such as NeOn. The use case is therefore focused only on the methodological and technological aspects of the solution, offering a proof of concept of the NeOn approach toward the creation of a shared and consistent knowledge base about pharmaceutical products. Being part of the NeOn project, we used the NeOn Methodology and NeOn tools both at conceptual and implementation levels.In order to test the benefits of this approach, the case study focused on a limited number of stakeholders and resources as shown in Fig. Figure • The Digitalis and Integra databases from the Spanish Agency of Medicine and Health Products (AEMPS)The approach followed was to model different resources and apply NeOn in order to align the models. The new models are then populated with real drug data, and the resultant knowledge base can be queried in order to obtain a more complete and interoperable drug description.Applying the NeOn Methodology to Engineer the Ontology Network
Being both part of the NeOn project, the Semantic Nomenclature use case and the NeOn Methodology evolved together. The use case applied the NeOn Methodology (Chap. 2) and gave continuous feedback of the results of its usage. As a result of this process, the Semantic Nomenclature ontology network has been developed within this use case.In this section, we describe a summary of the usage of the NeOn Methodology in the Semantic Nomenclature use case.Ontology Requirements Specification
In the NeOn project, we started the use case definition with the requirements definition activity. Apart from gathering non-functional requirements, related mainly to issues such as scalability or reliability, the main functional requirements were gathered using competency questions (as explained in Chap. 5). These questions were proposed and answered by some of the stakeholders mentioned in the list below, which are the possible users for the ontology:• U1: Pharmacist who is interested in searching for drugs information • U2: BOTPlus technician whose main interest is to complete information of their commercial database with drug data from other nomenclatures • U3: AEMPS expert who analyzes the situation of the information about drugs or updates its contentThe competency questions gathered were enumerated and classified into groups suggesting the initial list of the different concepts mentioned by the user groups. The main groups are concepts about pharmaceutical products, laboratories, and active ingredients. As a matter of example, see below some competency questions about pharmaceutical products:• CQ1. What is the drug commercial name? • CQ13. Which is the drug composition? • CQ17. Which route of administration is used? • CQ18. What is the drug pharmaceutical form?The complete set of competency questions, answers, and requirements obtained can be found in Go ´mez-Pe ´rez et al. Scenario Selection
The NeOn Methodology is scenario-based, meaning that instead of prescribing a rigid workflow, it suggests activities for a variety of scenarios. The NeOn Methodology presents and describes nine common scenarios that may arise during ontology development as described in Chap. 2.Based on the relation between life cycle model and the scenarios, for the first iteration of the ontology development, we have followed Scenarios 1, 2, 3, and 8, while in the second iteration, when the methodology and our initial network was more advanced, we also followed Scenarios 6, 7, and 9. Not all the phases specified on the methodology for the scenarios selected have been addressed in detail, especially in the first iteration.According to the NeOn Methodology, the ontology network life cycle model defines in an abstract way how to develop an ontology network project and how to organize the processes and activities into phases or stages. Due to the incremental development of the ontologies followed in the project, the ontology network life cycle model selected for the use case was the Iterative-Incremental Ontology Network Life Cycle Model. This model organizes the ontology development in a set of iterations (or short mini-projects with a fixed duration). Each iteration is scheduled as a single ontology project using a waterfall model. Requirements specified in the Ontology Requirements Specification can be divided in different subsets, and implements throughout the different iterations.Two main iterations have been selected for the development of the Semantic Nomenclature ontology network. Six-phase Waterfall Model was selected for the both iterations. This model allows the reengineering of ontological resources and non-ontological resources (NORs), which was in the scope of the case study.Ontological Processes and Activities Performed
In this section, we summarize the most interesting processes and activities followed according to the NeOn Methodology and the scenarios selected in the use case. Figure It is worth noticing that the processes and activities followed in both iterations were basically the same. This is due to the fact that the result of the first iteration was a preliminary version of the ontology network that was further elaborated and extended in the second version.• Support activities: The use case started with a survey of the domain. As part of Scenario 1 of the NeOn Methodology, we followed the methodological support activities, such as the Ontology Environmental Study and Ontology Feasibility Study, in parallel to the Ontology Requirements Specification activity (as explained in Sect. 20.3.1). In this phase, we decided that a network of ontologies was the best approach for the use case. It is worth noting that in the second iteration of the case study, we did a second round of the Ontology Requirements Specification activity adding new requirements, especially from hospitals toward the differentiation between commercial and clinical drugs. However, the overall objective and approach of the use case remained unchanged. The second iteration was closer to the incremental approach "produce and deliver" new ontologies to the case study, but the already available ones were not discarded but improved. We did not use the template for ontology requirements specification document (ORSD) provided by the methodology because it was drafted during the last period of the project, but we used a similar approach by gathering requirements, competency questions, etc. In the second iteration, we did a proper scheduling activity by using the tooling support provided by the NeOn Toolkit (the gOntt plugin explained in Chap. 14) in order to generate a plan for the iteration. • Reusing resources: Following Scenarios 2 and 3, we tried in parallel to reuse as much as possible already available ontologies in the domain and model new ontologies from existing non-ontological resources. For the non-ontological resources, we carried out several activities proposed by the NeOn Methodology, such as resource search, resource assessment, or resource selection. The result of this process was the selection of several resources: the ATC classification schemas created manually and populated automatically from the WHO ATC XML version, the Digitalis and BOTPlus ontologies created from the abovementioned databases using the R2O-ODEMapster plugin, and the SPC ontology created manually from the SPC standard specification. • In parallel, we tried to reuse as much as possible already available ontologies in the domain. We followed several activities proposed by the NeOn Methodology, such as ontology search, ontology assessment, ontology comparison, and ontology selection for ontology reuse. The results of Scenarios 2 and 3 can be found in Go ´mez-Pe ´rez et al. ( Semantic Nomenclature Ontology Network
The Semantic Nomenclature ontology network comprises a set of ontologies organized in four levels: the representation ontology (OWL), general ontologies, domain ontologies, and application ontologies. The Semantic Nomenclature (SN) ontology plays a central role in the network (see Fig. The domain level comprises ontologies defining the main notion and concepts of the pharmaceutical domain. At this level, we include ontologies providing a classification of pharmaceutical terms, such as the ATC and SPC ontologies created within the case study, or the RxNorm 10 , the UMLS Metathesaurus 11 , MeSH 12 , OpenGALEN 13 , and NCI 14 , all reused or reengineered from existing ontologies or resources. Some of these ontologies have been just partially mapped to the network as a proof of concept. We did some tests using the alignment plugin in order to perform these mappings. In some cases, the candidate alignments were good enough, although the overall impression was that a lot of manual work had to be done (see Chap. 12 for more details on ontology alignment). New domain ontologies can be added to the network by creating the necessary mappings.At the Application level, we have ontologies representing knowledge of realworld resources used for a specific purpose or application. This is the case of the Digitalis and BOTPlus ontologies, containing governmental and private views of commercial pharmaceutical products in Spain.We have reused existing ontologies for defining common domain elements. After looking at different ontologies, we chose the W3C time ontology for managing dates, parts of the Galen ontology for the definition of units of measurement, and the geographical module from the Simile Ontology.The Semantic Nomenclature ontology network is shown in Fig. The ontology network makes possible the easy interoperability and integration of the distributed resources for the description of pharmaceutical products. Moreover, the ontology network facilitates the aggregation of drug-related information connecting new ontological resources via mappings to the SN ontology. This solution makes possible searching for aggregated information by querying the knowledge base using SN ontology elements or domain elements. Apart from the obvious usage on the semantic interoperability area, these queries allow the different stakeholders to potentially keep up to date their back-office systems or take better decisions based on the aggregated information presented. Semantic Nomenclature Application
The Semantic Nomenclature application is eye catching for the pharmaceutical community as a new nomenclature (compendium) based on semantic web technologies. The application is targeting mainly pharmaceutical-knowledge experts. The main result is focused on the runtime aspects of NeOn, but it relies on the work done at the design time on the ontologies using the NeOn Toolkit, different NeOn plugins, and the NeOn Methodology. Apart from being a test bed of the NeOn runtime services, the goal is to offer a view over a set of networked ontologies, allowing functionalities such as querying, adding new ontologies to the network, rating of ontology elements, or adding new ontology mappings.The Semantic Nomenclature application is supported at the infrastructure level by a knowledge base (KB) populated according to the Semantic Nomenclature ontology network model. This KB contains relevant information about pharmaceutical products and associated knowledge about other types of entities like active ingredients, diseases, laboratories, etc. The Semantic Nomenclature application provides a feature set in different dimensions: (a) accurate query answering mechanism by using a rich web client form that abstracts the end user from the underlying SPARQL construction of queries, (b) access to aggregated data using the Cupboard NeOn service, and (c) collaborative and social functionalities using the Cicero NeOn Service integrated from the web application.There is also a different angle to consider, which is related with the Open Linked Data initiative. Linked DataThe implementation of the application deals with the integration of the ontology network presented before in a user-friendly web application. The implementation has two main different layers: a business processing layer at server side and a presentation layer at the client side. On the one hand, the server is essentially dedicated to data processing and management of the functional process, and its architecture is generic to interact with several software components provided by NeOn or other third parties. On the other hand, the client side is dedicated to the information presentation and the data interaction. The application uses Google Web Toolkit (GWT)The users of the Semantic Nomenclature web prototype are mainly pharmaceutical-knowledge experts with a limited knowledge of ontologies. It is not intended for users with no knowledge about ontologies at all, but they do not need to be ontology experts to get benefits from using the prototype. But the prototype is not closed to those domain actors; it is open to any other kind of users, such as people who want to retrieve semantic-enriched information about pharmaceutical products, personnel from hospitals, governmental agencies, etc. Moreover, the prototype provides functionalities to biomedicine, pharmacy, or health-care researchers to assess about the models of the ontologies used by the prototype, add new models, or discuss with other colleagues. Conclusion
In this chapter, we described how we applied the NeOn Methodology and the NeOn Toolkit and some of its plugins on the development of an ontology network in the scope of the Semantic Nomenclature use case developed within the NeOn project. We explained the current situation and the interoperability problems posed by the existence of multiple stakeholders, the heterogeneity of the different solutions for drug description, and the huge amount of data involved, being this issue at the very core of the semantic interoperability in eHealth effort. A detailed description of the methodological activities carried out in the use case toward the definition of the Semantic Nomenclature ontology network has been presented. As the NeOn Methodology and the use case evolved in parallel, both received continuous feedback from each other during the project life span. We have also briefly presented an overview of the Semantic Nomenclature web application that takes advantage of the underlying ontology network. Index
A
Fig. 2 . 1
Fig. 2 . 3
Fig. 2 . 5
Fig. 3
Fig. 3 . 2
Fig. 3
Fig. 3 . 4
Fig. 3 . 5
Fig. 3
Fig. 3 . 7
Fig. 3 . 8
Fig. 3 .
Fig. 3 .
Fig. 3 .
Fig. 4
Fig. 4 . 2
Fig. 4
Fig. 4
Fig. 4 . 5
Fig. 4
Fig. 5 . 1
Fig. 5 . 4
Fig. 6 . 1
Fig. 6 . 2
Fig. 6 . 3
Fig. 6
Fig. 6 . 6
Fig. 6 . 7
•
Fig. 6
Fig. 6
Fig. 7 . 2
Fig. 7 . 3
Fig. 8 . 1
Fig. 8 . 2
Fig. 8 . 3
Fig. 8 . 4
Fig. 8 . 5
Fig. 8 . 6
Fig. 8 . 7
Fig. 8 . 9
Fig. 9 . 2
Fig. 9 . 3
Fig. 10 . 1
Fig. 10 . 2
Fig. 10 . 4
Fig. 10 . 5
Fig. 10 . 6
Fig. 10 . 7
Fig. 10 . 8
Fig. 10 .
Figure 11 .
Fig. 11 . 2
Fig. 11 . 3
Fig. 11 .
Fig. 11 . 5
Fig. 11 . 6
Fig. 12 . 3
Fig. 12. 4
Fig. 12 . 7
Fig. 13. 1
Fig. 13 . 2
Fig. 13 . 3
Fig. 13 . 5
Fig. 13
Fig. 13 . 8
Fig. 13. 9
Fig. 13 .
Fig. 13 .
Fig. 13 .
Fig. 13 .
Fig. 13 .
Fig. 14 . 1
Fig. 14 . 2
Fig. 14. 3
Fig. 14 .
Fig. 15 . 1
Fig. 15 . 2
Fig. 15 . 3
Fig. 15 . 4
Fig. 15 . 5
Fig. 15. 6
Fig. 16 . 1
Fig. 16 . 3
Fig. 16 . 4
Fig. 16 . 6
Fig. 16. 8
Fig. 16 .
Fig. 17 . 1
Fig. 17. 3
Fig. 18 . 1
Fig. 18 . 2
Fig. 18 . 3
3.
Fig. 18 . 4
Fig. 18 . 5
Fig. 18
Fig. 18 . 8
Fig. 19 . 1
4
Fig. 19. 2
Fig. 19
Fig. 19 . 5
Fig. 19 . 6
Fig. 19 . 7
Fig. 20. 1
Fig. 20. 2
Fig. 20 . 3
Fig. 20 . 4
Table 2
Table 2 .
. Reasoning ODPs are procedures that perform automatic inference. Examples of Reasoning ODPs are so-called normalizations (Vrandec ˇic ´and Sure 2007). Other Reasoning ODPs include common reasoning tasks, such as classification, subsumption, inheritance, materialization, de-anonymizing, etc.Table 3
Table 3
web interface, users will click on songs and get to see the artists that have recorded them and links to information on those recordings Priority HighTable 5
Table 5 .
• The Classification of Italian Education Titles published by the National Institute of Statistics, ISTAT 21 , is a classification scheme modeled with the flattened data model and implemented in a spreadsheet.Table 6
Table 6 .
Example Graphical representation of the example of non-ontological resource Output: designed ontologyTable 6 .
Algorithm 1 Discovering the semantics of the relations -getRelation 5: else if contains(tj,ti) then 6: relation tj.subClassOf.ti 7: else 8: wordnetRelation WordNet(ti, tj) 9: if wordnetRelation ¼ ¼ hyponym then 10: relation ti.subClassOf.tj 11: else if wordnetRelation ¼ ¼ hypernym then 12: relation tj.subClassOf.ti 13: else if wordnetRelation ¼ ¼ meronym then 14: relation ti.partOf.tj 15: else if wordnetRelation ¼ ¼ holonym then 16: relation tj.partOf.ti 17:Table 6
Table 6
Table 6 .
6.6.1.3 Analysis of the Applicability of the MethodTable 6
.7 SEEMP reference ontology statistical dataTable 6
Table 6
Table 7 .
Table 7
Table 7 .
Table 7 .
Table 7
12 Table 7
a http://www.w3.org/2001/sw/BestPractices/OEP/SimplePartWhole/part.owl b http://www.ordnancesurvey.co.uk/oswebsite/ontology/Mereology.owl c http://www.berkeleybop.org/ontologies/obo-all/relationship/relationship.owlTable 7
Table 7
Table 7
Table 7 .
Table 7 .
Table 7 .
knowledge selection: experiments and evaluations. In: 18th international conference on Database and Expert Systems Applications, DEXA 2007, Regensburg, Germany d'Aquin M, Sabou M, Motta E (2008) Reusing knowledge from the semantic web with the Watson Plugin. Demo at International Semantic Web Conference, ISWC 2008, Karlsruhe, GermanyTable 8 .
Table 8 .
Table 9 .
Table 9
Table 9
Table 9
Table 9 .
Table 10
Table 12
Table 12 .
Table 13 .
Table 13
Table 17
Table 19 . 1
Introduction: Ontology Engineering in a Networked Worldhttp://www.fao.org/fishery/enhttp://www.loa-cnr.it/DOLCE.htmlSkill Ontology from the University of Essen, which defines concepts representing the competencies required to describe job position requirements and job applicant skills. Available at http://www.kowien.uni-essen.de/publikationen/konstruktion.pdfNorth American Industry Classification System, which provides industry-sector definitions for Canada, Mexico, and the United States to facilitate uniform economic studies across the boundaries of these countries. Available at http://www.census.gov/epcd/www/naics.htmlStandard Occupational Classification, which classifies workers into occupational categories (23 major groups, 96 minor groups, and 449 occupations). Available at http://www.bls.gov/soc/A phase is a distinct period or stage in a process of development. 10 M.C. Sua ´rez-Figueroa et al.http://mayor2.dia.fi.upm.es/oeg-upm/files/pdf/NeOnGlossary.pdf 2 The NeOn Methodology for Ontology EngineeringIn this book, ontology developers refer to software developers and ontology practitioners involved in the development of ontologies.An example of CQ can be "where is located the device Z? The device Z is at coordinates X, Y".M.C. Sua ´rez-Figueroa et al.http://www.w3.org/TR/owl-ref/ 2 The NeOn Methodology for Ontology EngineeringSee, for example, a list of novel ontology search engines described at: http://esw.w3.org/topic/ TaskForces/CommunityProjects/LinkingOpenData/SemanticWebSearchEngineshttp://swoogle.umbc.edu/http://watson.kmi.open.ac.uk/WatsonWUI/http://sindice.com/ 2 The NeOn Methodology for Ontology EngineeringThe NeOn Methodology for Ontology Engineeringhttp://ontologydesignpatterns.org/http://www.w3.org/2001/sw/BestPractices/ 2 The NeOn Methodology for Ontology Engineeringhttp://www.illc.uva.nl/EuroWordNet/ 24 M.C. Sua ´rez-Figueroa et al.http://www.ontologydesignpatterns.orgEven in cases when ontology requirements are explicitly expressed, e.g., as described in Chap. 5, there are commonly other implicit domain assumptions that need to be addressed at reuse time. In our experience, it is also quite rare that explicit requirements are distributed together with their corresponding ontology.See the FOAF project website: http://www.foaf-project.org/For further details, and a definition of "non-ontological resource", see Chap. 6 of this book.3 Pattern-Based Ontology DesignDOLCE -Project Home Page: http://dolce.semanticweb.orgSee http://www.geneontology.org/See http://www.nlm.nih.gov/research/umls/http://ontologydesignpatterns.org/wiki/Submissions:Information_realizationhttp://ontologydesignpatterns.org/wiki/Submissions:Placehttp://ontologydesignpatterns.org/wiki/Submissions:Situation 3 Pattern-Based Ontology Designhttp://ontologydesignpatterns.org/wiki/Submissions:PartOfhttp://ontologydesignpatterns.org/wiki/Submissions:ComponencyIndependently of the generality at which a CP is singled out, it must contain the central notions that "make rational thinking move" for an expert in a given domain for a given task.Pattern-Based Ontology Design  V.Presutti et al.   http://ontologydesignpatterns.org/wiki/Submissions:ContentOPs http://ontologydesignpatterns.org/wiki/Submissions:Collection Pattern-Based Ontology DesignThe ODP portal main page, http://www.ontologydesignpattern.orgThe ODP Portal pattern registry can be downloaded at: http://ontologydesignpatterns.org/ schemas/registry.owlhttp://www.daml.org/ontologies/http://www.schemaweb.info/http://swoogle.umbc.edu/The NeOn Ontology ModelsThe NeOn Ontology ModelsPlease notice that not all classes and properties are included. The ontology is available for download in several ontology formats at http://omv.ontoware.org/ 70A. Adamou et al.In the remainder of this chapter, when OWL appears without any version information, it refers to OWL1. As opposed, when referring to OWL2, we explicitly note it.OMV extensions are also available at http://omv.ontoware.org 4 The NeOn Ontology Modelshttp://www.neon-toolkit.org/http://oyster2.ontoware.orghttp://www.infoq.com/zones/centrasite/http://bioportal.bioontology.orghttp://cupboard.open.ac.uk:8081/cupboardhttp://watson.kmi.open.ac.uk/http://protegewiki.stanford.edu/wiki/MetaAnalysisThe Semantic Web search engine Watson provides data about the language of ontology labels that shows that around 80% of ontologies have literals only in English (http://watson.kmi.open.ac. uk/blog/2007/11/20/1195580640000.html)http://www.ontologydesignpatterns.org/cp/owl/taskrole.owlhttp://www.ontologydesignpatterns.org/cp/owl/intensionextension.owl 4 The NeOn Ontology Modelshttp://www.ontologydesignpatterns.org/cpont/codo/owl22codo.owlA. Adamou et al.   http://www.ontologydesignpatterns.org/cpont/codo/omv2codo.owlhttp://www.ontologydesignpatterns.org/cpont/codo/doap2codo.owlhttp://trac.usefulinc.com/doaphttp://www.ontologydesignpatterns.org/cpont/codo/accessrights2codo.owlhttp://www.uni-koblenz.de/~bercovici/owl/2008/7/accessRight.owlhttp://www.uni-koblenz.de/~bercovici/owl/2008/7/entity.owlhttp://www.uni-koblenz.de/~schwagereit/owl/agents.owlhttp://www.ontologydesignpatterns.org/cpont/codo/som2codo.owl 4 The NeOn Ontology Modelshttp://www.ontogrid.nethttp://esperonto.net/fundfinderhttp://www.neon-project.org/nw/Ontology-driven_fish_stock_depletion_assessment_systemhttp://www.isoco.com/ontologies/mio/index.htmlhttp://www.seemp.org 94 M.C. Sua ´rez-Figueroa and A. Go ´mez-Pe ´rezOntology Requirements Specificationhttp://www.w3.org/TR/rdf-schema/http://www.wsmo.org/wsml/wsml-syntax 5 Ontology Requirements SpecificationIn software engineering, functional requirements refer to the required behavior of the system, that is, the functionalities that the software system should have, while non-functional requirements refer to implicit expectations about how well the software system should work. That is, these requirements can be seen as aspects about the system or as "non-behavioral" requirementsM.C. Sua ´rez-Figueroa and A. Go ´mez-Pe ´rezhttp://cicero.uni-koblenz.de/wiki/index.php/Main_Page 5 Ontology Requirements Specificationhttp://www.whocc.no/atc_ddd_index/Along this chapter, we use either NOR or non-ontological resource without distinctionhttp://www.fao.org/fi/glossary/default.asphttp://wordnet.princeton.edu/http://www.fao.org/figis/servlet/RefServlet 6 Reusing and Re-engineering Non-ontological Resources for Building Ontologieshttp://www.fao.org/agrovoc/http://www.vanderwal.net/folksonomy.htmlhttp://del.icio.us/ 110 B. Villazo ´n-Terrazas and A. Go ´mez-Pe ´rezReusing and Re-engineering Non-ontological Resources for Building Ontologieshttp://www.rosettanet.org/http://www.edibasics.co.uk/http://www.unspsc.org/http://wordnet.princeton.edu/ 6 Reusing and Re-engineering Non-ontological Resources for Building OntologiesThis document is the outcome of the ontology specification activity (Sua ´rez-Figueroa et al. 2009) (see Chapter 5).A deep analysis of the quality of the resource is out of the scope of this chapter.6 Reusing and Re-engineering Non-ontological Resources for Building OntologiesB. Villazo ´n-Terrazas and A. Go ´mez-Pe ´rezhttp://ontologydesignpatterns.org 6 Reusing and Re-engineering Non-ontological Resources for Building Ontologies 123PR-NOR-CLTX-01 Classification scheme Path enumeration Ontology schema (TBox)PR-NOR-CLTX-02 Classification Scheme Adjacency list Ontology schema (TBox)PR-NOR-CLTX-03 Classification scheme Snowflake Ontology schema (TBox)PR-NOR-CLTX-04 Classification scheme Flattened Ontology schema (TBox)PR-NOR-CLAX-10 Classification scheme Path enumeration Ontology (TBox + ABox)PR-NOR-CLAX-11 Classification scheme Adjacency list Ontology (TBox + ABox)PR-NOR-CLAX-12 Classification scheme Snowflake Ontology (TBox + ABox)PR-NOR-CLAX-13 Classification scheme Flattened Ontology (TBox + ABox)PR-NOR-TSTX-01 Thesaurus Record basedOntology Schema (TBox)PR-NOR-TSTX-02 Thesaurus Relation basedOntology Schema (TBox)PR-NOR-TSAX-10 Thesaurus Record based Ontology (TBox + ABox)PR-NOR-TSAX-11 Thesaurus Relation based Ontology (TBox + ABox)PR-NOR-LXTX-01 Lexicon Record basedOntology schema (TBox)PR-NOR-LXTX-02 Lexicon Relation basedOntology schema (TBox)PR-NOR-LXAX-10 Lexicon Record based Ontology (TBox + ABox)PR-NOR-LXAX-11 Lexicon Relation based Ontology (TBox + ABox) 25 http://ontologydesignpatterns.org 26 Ontology design patterns are included in the ODP portal. The ODP portal is a Semantic Web portal dedicated to ontology design best practices for the Semantic Web, emphasizing particularly ontology design patterns (OPs)http://www.dbpedia.org/Extract, transform, and load (ETL)  of legacy data sources is a process that involves (1) extracting data from the outside resources, (2) transforming data to fit operational needs, and (3) loading data into the end target resourceshttp://www4.fao.org/asfa/asfa.htmhttp://mccarthy.dia.fi.upm.es/ontologies/asfa.owl 6 Reusing and Re-engineering Non-ontological Resources for Building OntologiesB. Villazo ´n-Terrazas and A. Go ´mez-Pe ´rezAttributive adjectives are part of the noun phrase headed by the noun they modify, for example, happy is an attributive adjective in "happy people." In English, the attributive adjective usually precedes the noun in simple phrases but often follows the noun when the adjective is modified or qualified by a phrase acting as an adverb.http://ontologydesignpatterns.org/wiki/Submissions:PartOf 6 Reusing and Re-engineering Non-ontological Resources for Building Ontologieshttp://dbpedia.org/http://owlapi.sourceforge.net/http://www.seemp.org/http://www.cenitmio.es/ 132 B. Villazo ´n-Terrazas and A. Go ´mez-Pe ´rezhttp://ec.europa.eu/eurostat/ramon/http://online.onetcenter.org/http://www.eurodyn.com/ 6 Reusing and Re-engineering Non-ontological Resources for Building Ontologieshttp://www.wsmo.org/wsml/ 6 Reusing and Re-engineering Non-ontological Resources for Building Ontologieshttp://www.iso.org/iso/en/prods-services/iso3166ma/index.html 6 Reusing and Re-engineering Non-ontological Resources for Building Ontologieshttp://www.countriesandcities.com/regions/http://park.org/Regions/http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/http://wordnet.princeton.edu/We consider a module(d'Aquin M et al. 2007b) as a part of the ontology that defines the relevant set of terms for a particular purpose.An ontology statement (or triple) contains the following three components: subject, predicate, and object.7 Ontology Development by ReuseThe rest of the cases are presented in Sua ´rez-Figueroa (2010).http://www.google.es/http://swoogle.umbc.edu/http://watson.kmi.open.ac.uk 7 Ontology Development by ReuseOntology Development by ReuseThe term module has here the pragmatic sense equivalent to the d'Aquin's reference cited in the Introduction.http://hcs.science.uva.nl/projects/NewKACTUS/library/lib/mereology.htmlObtained on September 30,2009, from http://www.internetworldstats.com 2 http://www.co-ode.org/galen/ 3 http://www.opencyc.org/downloads 4 http://www.aktors.org/publications/ontology/Ontology LocalizationIn NLP, context refers to the environment in which a word is used and provides the information needed for figuring out the meaning of homonyms or polysemic words.8 Ontology Localizationhttp://droz.dia.fi.upm.es/hrmontology/ 8 Ontology Localizationhttp://watson.kmi.open.ac.uk/WatsonWUI/The number of context labels used to disambiguate a translated label depends on the ontology domain. However, in our experiments we found that a threshold of three context labels reduce the time of response of the overall system and it is compatible with the range of good responses found by comparing the results with human evaluations.http://knowledgeweb.semanticweb.org/ 8 Ontology Localizationhttp://kmi-web05.open.ac.uk/WatsonWUI/http://sindice.com/Ontology (Network) EvaluationODP stands for Ontology Design Pattern.http://cupboard.open.ac.uk:8081/cupboard-search/ 9 Ontology (Network) Evaluationhttp://www.ontologydesignpatterns.orghttp://www.mygrid.org.uk 9 Ontology (Network) Evaluation 205http://www.neon-toolkit.org/wiki/2.3.1/RaDON 9 Ontology (Network) Evaluationhttp://www.neon-toolkit.org/wiki/2.3.1/XDToolshttp://www.neon-toolkit.org/wiki/2.3.1/Watson_for_Knowledge_Reusehttp://ncit.nci.nih.gov/http://www.geneontology.org/Modularizing OntologiesM. d'Aquinhttp://km.aifb.kit.edu/projects/owltools/http://kaon2.semanticweb.org/M. d'AquinR.Palma et al.   Ontology EvolutionR.Palma et al.   Available at http://aims.fao.org/en/website/Fisheries-ontologies-/sub2#speciesDetails on how to install and run Evolva can be found at: http://evolva.kmi.open.ac.uk/The SWRC ontology can be downloaded from: http://ontoware.org/swrc/swrc/SWRCOWL/ swrc_updated_v0.7.1.owlhttp://www.leverhulme.ac.uk/ 11 Ontology EvolutionJ. Euzenat (*)INRIA & LIG, F-38330 Montbonnot Saint-Martin, France e-mail: Jerome.Euzenat@inria.fr C. Le Duc Universite ´Paris 8, 93200 Saint-Denis, France e-mail: Chan.Leduc@iut.univ-paris8.frJ.Euzenat and C. Le Duc   Methodological Guidelines for Matching Ontologieshttp://oaei.ontologymatching.orghttp://www.seals-project.eu 12 Methodological Guidelines for Matching Ontologieshttp://alignapi.gforge.inria.frhttp://bioportal.bioontology.orghttp://alignapi.gforge.inria.frhttp://sourceforge.net/projects/onagui/http://infolab.stanford.edu/modman/rondo/http://protege.stanford.edu/plugins/prompt/prompt.htmlhttp://www.neon-project.org 12 Methodological Guidelines for Matching Ontologieshttp://lsdis.cs.uga.edu/projects/asdoc/DrugOnt_schema.owlhttp://swpatho.ag-nbi.de/owldata/umlssn.owlhttp://www.nlm.nih.gov/research/umls/rxnorm/ 12 Methodological Guidelines for Matching Ontologieshttp://NeOn-Toolkit.orgA good introduction to the OWL language can be found inOverview of the NeOn Toolkit"ns" here stands for "namespace."The body responsible for the management and distribution of the NeOn Toolkit, http://www. neon-foundation.org/http://www.eclipse.org/legal/epl-v10.html 13 Overview of the NeOn Toolkithttp://www.eclipse.org/http://owlapi.sourcrforge.net 13 Overview of the NeOn Toolkithttp://neon-toolkit.org/wiki/Developer_Cornerhttp://neon-toolkit.org/wiki/Neon_Pluginshttp://wiki.eclipse.org/FAQ_How_do_I_create_an_update_site_%28site.xml%29%3Fhttp://www.neon-foundation.org/ 13 Overview of the NeOn Toolkithttp://www.wordnet-online.com/planning.shtmlhttp://www.wordnet-online.com/scheduling.shtmlScheduling Ontology Engineering Projects Using gOnttSuch scheduling templates show ontology project default plans based on the different and possible combinations among life cycle models, scenarios, and processes and activities.14 Scheduling Ontology Engineering Projects Using gOnttCheat sheets is a new emerging technology within Eclipse V3.0 that is meant to guide a developer through a series of complex tasks to achieve some overall goal. Some tasks can be performed automatically, such as launching the required tools for the user. Other tasks need to be completed manually by the user.gOntt has its own extension point that the rest of NeOn Toolkit plugins should implement (see Sua ´rez-Figueroa et al. 2010 for more detail). 14 Scheduling Ontology Engineering Projects Using gOnttM.C. Sua ´rez-Figueroa et al. (eds.), Ontology Engineering in a Networked World, DOI 10.1007/978-3-642-24794-1_15, # Springer-Verlag Berlin Heidelberg 2012Eclipse Rich Client Platform, the software development toolkit originally written for the Eclipse integrated development environment (IDE).Customizing Your Interaction with Kali-maThe Ontology Design Patterns portal, http://www.ontologydesignpatterns.org 15 Customizing Your Interaction with Kali-mahttp://wiki.eclipse.org/FAQ_What_are_extensions_and_extension_points%3F330A. Adamou and V. PresuttiXMPP, http://xmpp.orgGoogle Talk, http://www.google.com/talkJabber, originator of the initial XMPP design and implementation, http://www.jabber.org334A. Adamou and V. PresuttiAt the time of writing, the service is hosted at http://wit.istc.cnr.it:8080/codomaticA. Adamou and V. Presuttihttp://neon-toolkit.org/wiki/Plugin_HowTo 15 Customizing Your Interaction with Kali-maThis network of ontologies can be downloaded from http://projects.kmi.open.ac.uk/ smartproducts/ontologies/SP_v2_4.ziphttp://www.smartproducts-project.euE. Motta et al.   Visualizing and Navigating Ontologies with KC-VizThis is because the relevant preference in the NeOn Toolkit is set to display all inherited classes, thus allowing us to browse the complete structure of the SmartProducts network of ontologies. Alternatively, we can choose to see only definitions local to the Philips-Test2 ontology, by deselecting the option "Show Imported Axioms."If the node is owl:Thing, then we are talking about the size of the whole ontology, otherwise the size of a particular subtree.16 Visualizing and Navigating Ontologies with KC-Vizhttp://www.topquadrant.com/products/TB_Composer.htmlAll the examples in this paper have been generated using version 2.5 of the NeOn Toolkit and KC-Viz v1.3.0.It is important to point out that while Fig.16.4 and later figures show exactly the concepts returned by KC-Viz, for the sake of readability we have, when appropriate, manually rearranged the layout, to try and minimize the compression caused by the physical size of this document. This is needed primarily because KC-Viz displays assume a landscape orientation, while this article is formatted according to a portrait orientation.Crucially, the option "Ontology summary considers also imported ontology" must be enabled in the KC-Viz preferences, otherwise only a summary of the concepts local to the Philips-Test2 ontology will be generated.16 Visualizing and Navigating Ontologies with KC-Vizhttp://www.w3.org/2001/11/IsaVizhttp://protegewiki.stanford.edu/index.php/OntoViz 16 Visualizing and Navigating Ontologies with KC-Vizhttp://www.ietf.org/rfc/rfc3987.txthttp://www.ietf.org/rfc/rfc3987.txt 17 Reasoning with Networked OntologiesG.Qi and A. Harth   Reasoning with Networked Ontologieshttp://www.fao.org/documents/en/docrep.jspReference data may be considered a specific type of metadata for statistics. Other types of metadata are about data provenance and methodology for data creation.For a list of statistical databases on fisheries maintained by FAO, see http://www.fao.org/fishery/ statistics/enC.Caracciolo et al.   For details about the requirements driving this work in the context of the use case, see Iglesiashttp://www.fao.org/countryprofiles/http://aims.fao.org/website/AGROVOC-Thesaurus/subhttp://www4.fao.org/asfa/asfa.htmhttp://www.fao.org/countryprofiles/geoinfo.asp?lang¼enDetailed information regarding fisheries statistics can be found in the Handbook of Fishery Statistical Standards [HBFSS] by the Coordinating Working Party on Fishery Statistics (CWP).The Coordinating Working Party on Fishery Statistics (CWP) supported by its participating organizations has served since 1960 as the premier international and inter-organization forum for agreeing upon common definitions, classifications, and standards for the collection of fishery statistics.Reference tables are browsable at: http://www.fao.org/fishery/rtms/enIssues related to the treatment of multilinguality in AGROVOC are presented inCaracciolo and  Sini (2007).18 Knowledge Management at FAOhttp://www.fao.org/fishery/collection/asfis/enASIFS also includes taxonomic entities above the species level such as families or orders, on the basis of the data reported to FAO by countries or other governing bodies.International Standard Statistical Classification for Aquatic Animals and Plants (ISSCAAP). 388 C. Caracciolo et al.Member agencies of the CWP have agreed to use these standard species names in statistical publications and questionnaires. However, (a) it has not been possible to assign appropriate names in all three languages to all species items, and (b) these names may not correspond with nationally or regionally used common names.http://www.fao.org/fishery/area/search/enhttp://www.lme.noaa.gov/ISO codes are established by the International Standard Organization (ISO).18 Knowledge Management at FAOInitially, we used the stand-alone version of ODEMapster and then moved to the plugin version for the NeOn Toolkit.http://www.w3.org/TR/skos-reference 18 Knowledge Management at FAOhttp://www.w3.org/TR/owl2-syntaxWe applied matching techniques based on the family of edit-distance functions. We chose the Jaro-Winkler techniquehttp://aims.fao.org/website/NeON/sub2Any aquatic species relevant to fisheries, including some aquatic birds and plants.http://193.43.36.238:8181/fi/website/FIRetrieveAction.do?dom¼ontology&xml¼sectionR.xmlhttp://www.fao.org/fishery/cwp/handbook/M/en 18 Knowledge Management at FAOhttp://www.nafo.int/http://www.ontologydesignpatterns.org/cp/owl/fsdas/catchrecord.owlhttp://www.ontologydesignpatterns.org/cp/owl/observation.owlhttp://www.ontologydesignpatterns.org/cp/owl/place.owlKnowledge Management at FAOhttp://www.ontologydesignpatterns.org/cp/owl/fsdas/waterareasfactsheets.owlhttp://www.ontologydesignpatterns.org/cp/owl/fsdas/speciesfactsheets.owlhttp://oaei.ontologymatching.org/ 398 C. Caracciolo et al.http://aims.fao.org/website/Linked-Open-Data/subhttp://www.pharmainnova.com 19 Electronic Invoice Management in the Pharmaceutical Sectorhttp://ontologydesignpatterns.org/wiki/Ontology:Main 19 Electronic Invoice Management in the Pharmaceutical SectorElectronic Invoice Management in the Pharmaceutical Sectorhttp://ontolog.cim3.net/http://xbrlontology.com/http://ontologydesignpatterns.org/wiki/Submissions:InvoiceAvailable at: http://ontologydesignpatterns.org/wiki/Ontology:Aggregated_Invoice_Ontology and http://www.isoco.com/ontologies/neon/AggregatedInvoiceOntology.owl 19 Electronic Invoice Management in the Pharmaceutical Sectorhttp://www.eil.utoronto.ca/Enterprise-modelling/tovehttp://www.loa-cnr.it/DOLCE.htmlhttp://www.ontologyportal.orghttp://ontolog.cim3.net/cgi-bin/wiki.pl?CctRepresentationhttp://neon-toolkit.orghttp://neon-toolkit.org/wiki/Neon_Pluginshttp://www.neon-project.org/nw/Movie:_i2Ont 19 Electronic Invoice Management in the Pharmaceutical Sectorhttp://www.neon-project.org/nw/Movie:_i2Ont_Web 19 Electronic Invoice Management in the Pharmaceutical Sectorhttp://www.iso.org/iso/catalogue_detail.htm?csnumber¼40784http://www.hl7.orghttp://www.who.inthttp://www.cen.euhttp://www.ihtsdo.orgT.P. Lobo and G.H. Ca ´rcelhttp://www.aemps.es/https://botplusweb.portalfarma.com/http://www.whocc.no/atc_ddd_index/ 20 Integrating Product Information in the Pharmaceutical Sectorhttp://ec.europa.eu/enterprise/sectors/pharmaceuticals/files/eudralex/vol-2/c/spcguidrev1-oct2005_en.pdfIntegrating Product Information in the Pharmaceutical Sectorhttp://linkeddata.org/http://www.drugbank.ca/http://dailymed.nlm.nih.govhttp://diseasome.eu/http://dbpedia.org/http://code.google.com/webtoolkit/http://www.json.org/ 20 Integrating Product Information in the Pharmaceutical Sectorproject, 82, 83, 85, 322, 327, 331, 334  pruning, 23, 163 quality, 202 re-engineering activity, 17 reference, 36 repository, 17 requirement, 94 restructuring activity, 23 reuse, 106, 147, 344 search, 17, 18, 106 selection, 17, 18, 194 semantic nomenclature, 429 shape, 347 size, 347 specialization activity, 24 statement, 10 summary, 351, 360 translation, 163, 263, 275 update, 25, 182 upper-level, 257, 258, 263 validation, 195 verbalization, 76 verification, 196 version, 258 Ontology Alignment Evaluation Initiative (OAEI), 265, 398 Ontology-based system, 385 Ontology design patterns (ODPs),