Preface
The second edition of the Handbook on Ontologies provides an up-to-date comprehensive overview of the field of ontologies that is evolving rather fast. Since the first edition of the handbook that was finished in 2003 and published in 2004, ontologies have achieved an even more important role with respect to the advancement of established information systems, of systems for data and knowledge management, or of systems for collaboration and information sharing as well as for the development of revolutionary fields such as semantic technologies and, more specifically, the Semantic Web. By covering a broad range of aspects that are related to ontologies, i.e. language and engineering aspects, infrastructures and applications, the reader of this handbook may either get a broad, comprehensive picture of the field of ontologies or he may investigate specific aspects of ontologies that are most relevant for her or his work.Major Changes with Respect to the 1st Edition
Between the time we wrote the preface to the 1st edition of the ontology handbook, 5 years ago, and today, a large amount of research work, development and use of ontologies have happened. Therefore, the handbook has undergone major changes from the first to the second revision.At the level of the coarsest granularity, the reader may discover one completely new part (Part III) on -Ontologies. This part now covers the description of some very intriguing ontologies that were not around in 2003. Thereby, this part mirrors the fact that -unlike in 2003 -finding ontologies on the Web is now easy, selecting the right one may be the hard and learning by example what is a good ontology or what is a promising field of application for ontologies is absolutely necessary.Furthermore, the reader may discover that the part on Ontology Infrastructure has been divided into two parts, one on Infrastructure for Ontologies and one on Ontology-Based Infrastructures and Methods. The first extends the scope of ontologies by providing a larger extent of scalability for dealing with ontologies. The second extends the scope of ontologies by presenting refined and new approaches for putting ontologies into different types of software infrastructures and methods. The latter kind does in fact constitute a generic type of application of ontologies, one that is independent of a particular target domain of application.Finally, one may find again core parts of the first edition, such as Part I: Ontology Representation Languages, Part II: Ontology Engineering and Part VI: Ontology-Based Applications.However, within these parts 21 (sic!) of the 36 overall chapters had to be written from scratch. Nearly all the remaining chapters have undergone substantial changes to make them up-to-date. We will not describe these changes in detail, but we want to present to the reader the flow he now finds in the second edition of the ontology handbook.Overview of the 2nd Edition of the Handbook
'Ontology' is still a rather overloaded term, which is used with several different meanings. This handbook does not consider the philosophical notion of an 'ontology' as addressed in philosophy for more than two thousand years by investigating questions like "what exists?". It rather approaches the notion of ontologies from a Computer Science point of view. In Computer Science, ontologies started to become a relevant notion in the 1990s, related mostly to work in Knowledge Acquisition at that time. In this context the basic definition of an ontology was coined as follows: "An ontology is an explicit specification of a conceptualization" (cf. What Is an Ontology? preceeding the main body of the handbook will further elaborate the definition of what an ontology is: Nicola Guarino, In order not to appeal to intuition and to fix the terminology precisely, the introduction will give a very formal approach to the definition of ontologies.The formally well-versed reader may enjoy the formal accuracy of the definition of ontology -at a level of precision that has been sorely lacking so far. However, the more application-oriented reader may focus only on the run-through example given in the introduction and rather warm up with the applications he may find in Part VI: Ontology-Based Applications.Part I: Ontology Representation Languages
The main body of the handbook starts with a part on current representation languages for ontologies in correlation with other aspects, such as data, the Web and rules. The first chapter describes the family of ontology languages described as Description Logics that constitutes the foundation for the majority of ontology work found nowadays. Description Logics is a subset of first-order predicate logics and combines expressiveness with a well-understood logical framework: U. Sattler, F. Baader, and I. Horrocks: Description LogicsThe second chapter describes an approach to ontologies that is derived from work in the area of logics-based databases. Ontologies in F-Logic ties in neatly with existing database frameworks and has found many up-and-running commercial applications.J. Angele, G. Lausen, and M. Kifer: Ontologies in F-Logic
As explained in What is an Ontology? the aspect of sharing is central to the notion of ontologies. The Web is the current, almost pervasive means to share information and also knowledge. A lightweight representation for data and knowledge on the Web is the Resource Description Framework (RDF). Its core objectives and its connection to logical frameworks are explained in the following chapter.J.Z. Pan: Resource Description Framework
Very soon after the definition of RDF and RDF Schema, ontology engineers and users recognized the need for a more expressive ontology representation language. The outcome of a joint development process is explained in Web Ontology Language: OWL. OWL and particularly its flavor of OWL-DL now constitute the language of choice for representing an ontology in the Semantic Web.G. Antoniou and F. van Harmelen: Web Ontology Language: OWL When ontologies were developed to improve the expressiveness of knowledge bases, they were typically used in connection with production rule-based systems. While production rule-based systems have severe disadvantages with regard to manageability because of a lack of declarativeness, new developments since the first edition of the handbook have shown how logical rulesPart II: Ontology Engineering
The second part of the handbook deals with the practical development of ontologies. The first chapter presents a generic model of ontology development that is now state-of-the-art and found in many variations in textbooks on the topic of ontology engineering. In fact, it also introduces the different aspects of ontology engineering found in the remainder of this part of the ontology handbook. Y. Sure, S. Staab, and R. Studer: Ontology Engineering Methodology While the general methodological blueprint reflects concerns that one would also find in a software engineering process, the next chapter focuses on an issue that becomes almost unavoidable for large, realistic ontologies. It shows how to develop ontologies in a distributed setting where most experts cannot afford to assemble often -if at all.H.S. Pinto, C. Tempich, and S. Staab: Ontology Engineering and Evolution in a Distributed World
Using DILIGENTThe sound engineering of ontologies needs sophisticated tools. The following two chapters describe Formal Concept Analysis and OntoClean, tools that both aim at improving the inheritance relationships of specified concepts. The first does so by analysing the correlation between intensions and extensions of concepts, while the second investigates how the variability of a concept is constrained by the intended conceptualization.G. Stumme: Formal Concept Analysis
N. Guarino and C.A. Welty: An Overview of OntoClean
Beyond conceptual relationships, ontology engineers need to express specific concerns: knowledge about knowledge, part-whole-relationships, etc. The chapter on Ontology Design Patterns explains how the idea of software design patterns can be adopted in ontologies to provide an understandable and expressive model.A. Gangemi and V. Presutti: Ontology Design Patterns
Such ontology design patterns may be filled by manual work, but the use of machine learning mechanisms as a tool for suggesting ontological constructs is an increasingly important means. P. Cimiano, A. Mädche, S. Staab, and J. Völker: Ontology Learning When learning an ontology, the induction mechanisms need to distinguish between the -possibly multiple -names of a concept or relation and the concept or relation itself. Thus, it constructs a lexicon. Investigating existing lexica, one finds that these actually contain many more specific hints useful for reuse during ontology construction.G. Hirst: Ontology and the Lexicon
At the end of the ontology engineering process, the resulting ontology needs to be matched against the requirements. Such requirements may be task or domain specific (e.g. high precision when retrieving knowledge); however, there are also general criteria of soundness of ontologies that are analysed by Vrandečić.D. Vrandečić: Ontology Evaluation
The whole process of ontology engineering may be supported by specific tools that allow the management of specification and design documents as well as ontology-specific concerns such as traceability information, patterns, lexica, etc. Though the full support of all these aspects has not been realized by any environment, the current state-of-the-art is elaborated on by Mizoguchi and Kozaki.R. Mizoguchi and K. Kozaki: Ontology Engineering Environments
The part on ontology engineering closes the ultimate issue of concern about ontologies in any kind of application: ontologies are supposed to improve the total cost of operating a system by improving system aspects such as efficiency or quality. However, with regard to the total cost of ownership, one also needs to consider the amount of time and money to be invested in the construction and the maintenance of the ontology.E. Simperl and C. Tempich: Exploring the Economical Aspects of Ontology Engineering
Only if the overall balance between investment and return yields a sufficiently large margin, the employment of an ontology can be taken into consideration. As will be shown in the parts on Ontology-Based Infrastructures and Applications, the improvement of ontology engineering, an increased amount of experience and sound and scalable infrastructure, now contribute successfully to the uptake of ontologies and ontology technologies.Part III: Ontologies
Some important experiences of ontology engineering are captured in structures of existing ontologies and in the use of existing ontologies. Authors have distinguished different types of ontologies at different levels of generality and for different types of purposes (cf. A widely used foundational ontology has been defined with DOLCE:S. Borgo and C. Masolo: Foundational Choices in DOLCE DOLCE is re-used in some domain and application ontologies in order to provide a sound and comprehensively specific, yet extensible framework. Extensibility is a core concern, because it is impossible to formalize domains like Software or Multimedia completely. The next chapter targets concerns about processes and tasks. It reflects the fact that the integration of procedural aspects into static constraints specified in ontologies is gaining importance because of application domains such as Web Services (Chapter "Semantic Web Services").M. Grüninger: Using the PSL Ontology
The final two chapters of this part consider the re-use of knowledge structures toward a more comprehensive formalization in an ontology. Both in biomedicine and in the domain of managing cultural objects, the need for rich structuring of the complex domains have led to a long tradition in defining knowledge structures and to a fruitful field of application for ontologies.N. Shah and M. Musen: Ontologies for Formal Representation of Biological Systems
M. Doerr: Ontologies for Cultural Heritage
Part IV: Infrastructures for Ontologies
The scalability of ontology technology is crucial to its uptake in industrial settings. Thereby, one can find an overwhelming development. As recently as 1994, Benchmarking of systems (cf. Current work targets simple ontological structures in RDF with billions of triples 1 . The first chapter of this part explains state-of-the-art systems for RDF Storage and Retrieval.A. Hertel, J. Broekstra, and H. Stuckenschmidt:RDF Storage and Retrieval Systems
With regard to description logic languages like OWL-DL, one nowadays finds ontology reasoning systems that can also handle 10 5 concepts in ontological concept definitions occurring in practice (cf. also R. Möller and V. Haarslev: Tableau-Based Reasoning
Furthermore, developments in recent years have joined means of optimization used in the fields of logic programming and logic databases (also cf. Chapter "Ontologies in F-Logic") with the field of description logics (Chapter "Description Logics") in order to achieve higher scalability for ontological reasoning with databases (more specifically: A-Boxes).B. Motik: Resolution-Based Reasoning for Ontologies
Beyond infrastructures for single ontologies, this part also presents approaches towards managing multiple ontologies. In order to search for and re-use ontologies it is necessary to index them. Ontology Repositories provide such means for supporting the process for search and re-use. J. Hartmann, R. Palma, and A. Gómez-Pérez: Ontology Repositories On the Semantic Web ontologies, and the entities they contain are supposed to cross-reference to each other to allow for a seamless use of ontologies. If such cross-references do not yet exist, infrastructures for establishing ontology mappings allow for their definition. N.F. Noy: Ontology Mapping 1 Cf. the billion triple challenge at http://challenge.semanticweb.org/Part V: Ontology-Based Infrastructure and Methods
Infrastructures and methods may be extended with ontology technologies to benefit from the agreement on a shared vocabulary and the underlying reasoning technology. These characteristics are frequently given when the domain of applications of the infrastructures and methods is inherently complex, must be kept extensible, and requires the interaction of some user.The first two chapters on ontology-based infrastructures and methods fall into the domain of software. The first contribution identifies different opportunities in the Software Engineering lifecycle where ontologies may play a rolethe 'user' here is the software developer.Part VI: Ontology-Based Applications
Over the last 15 years, ontology-based applications have been spreading and maturing. One now finds ontology-based applications in areas as diverse as customer support and engineering of cars.Coming from knowledge acquisition and knowledge-based systems, a core area of application for ontologies has been knowledge management. But, rather than fully capturing knowledge about a particular domain, the idea of using ontologies in knowledge was that one would agree on a common vocabulary and use it for knowledge shared by formal as well as by informal means, e.g. texts, while making best use of the reasoning technology in the background.A. Abecker and L. van Elst: Ontologies for Knowledge Management
As mentioned earlier (Chapter "Ontologies for Formal Representation of Biological Systems"), biomedical applications have been using ontological structuring for a long time -albeit tending towards less formal structures. There, the most prominent domain of application was data integration for human use. Now, however, the target of ontologies in this domain is the support for computational support of biological data, e.g. in experiments and simulations.R. Stevens and P. Lord: Application of Ontologies in Bioinformatics
Two more chapters target the use of ontologies in portals. The domains of art and cultural heritage have a long interest in comprehensive classifications to manage the many artifacts available, e.g. in museums, and present them to the public.E. Hyvönen: Semantic Portals for Cultural Heritage
The final chapter of the handbook targets the domain of digital libraries and shows how ontology-based recommender systems can facilitate access to such libraries -especially in situations where traditional recommender systems stall because of the so-called "cold start problem", i.e. the disadvantage that it is difficult to provide good recommendations when only little is known about the user of the system.S.E. Middleton, D. De Roure, and N.R. Shadbolt:Ontology-Based Recommender SystemsConclusion
As the size of the handbook is physically limited, this last chapter will conclude the handbook. Many more chapters would be necessary to fully capture the range of ontology technologies and applications. New ontology representation languages have been researched and become input for standardization processes, such as the extension of OWL-DL into OWL-2. New tools are being developed to provide infrastructure for ontologies via Web frontends. New applications pop up almost daily and in unforeseen domains. Hence, this handbook cannot be complete and will not be in the next couple of years. However, we see this as a vital sign for the area of research and development of ontology and ontology technologies and not as a detriment to the intended usefulness of this handbook. We hope that you, the reader, will enjoy its content and make productive use of it.Introduction
The word "ontology" is used with different meanings in different communities. Following Computational ontologies are a means to formally model the structure of a system, i.e., the relevant entities and relations that emerge from its observation, and which are useful to our purposes. An example of such a system can be a company with all its employees and their interrelationships. The ontology engineer analyzes relevant entities 3 and organizes them into concepts and relations, being represented, respectively, by unary and binary predicates. 4 The backbone of an ontology consists of a generalization/specialization hierarchy of concepts, i.e., a taxonomy. Supposing we are interested in aspects related to human resources, then Person, Manager, and Researcher might be relevant concepts, where the first is a superconcept of the latter two. Cooperates-with can be considered a relevant relation holding between persons. A concrete person working in a company would then be an instance of its corresponding concept.In 1993, Gruber originally defined the notion of an ontology as an "explicit specification of a conceptualization" What we call "concepts" in this chapter may be better called "properties" or "categories." Regrettably, "property" is used to denote a binary relation in RDF(S), so we shall avoid using it. Also, Smith made us aware that the notion of "concept" is quite ambiguous which are similar to Gruber's. However, the one from Gruber seems to be the most prevalent and most cited.All these definitions were assuming an informal notion of "conceptualization," which was discussed in detail in • What is a conceptualization? • What is a proper formal, explicit specification? • Why is 'shared ' of importance?It is the task of this chapter to provide a concise view of these aspects in the following sections. It lies in the nature of such a chapter that we have tried to make it more precise and formal than many other useful definitions of ontologies that do exist -but that do not clarify terms to the degree of accuracy that we target here.Accordingly, the reader new to the subject of ontologies may prefer to learn first about applications and examples of ontologies in the latter parts of this book and may decide to return to this opening chapter once he wants to see the common raison d'être behind the different approaches.2 What is a Conceptualization? Despite the complex mental nature of the notion of "conceptualization," Genesereth and Nilsson choose to explain it by using a very simple mathematical representation: an extensional relational structure. Example 2.1 Let us consider human resources management in a large software company with 50,000 people, each one identified by a number (e.g., the social security number, or a similar code) preceded by the letter I. Let us assume that our universe of discourse D contains all these people, and that we are only interested in relations involving people. Our R will contain some unary relations, such as Person, Manager, and Researcher, as well as the binary relations reports-to and cooperates-with. • D = {I000001, ..., I050000, ...} • R = {Person, Manager, Researcher, cooperates-with, reports-to} Relation extensions reflect a specific world. Here, we assume that Person comprises the whole universe D and that Manager and Researcher are strict subsets of D. The binary relations reports-to and cooperates-with are sets of tuples that specify every hierarchical relationship and every collaboration in our company. Some managers and researchers are depicted in Fig. The problem is that the extensional relations belonging to R reflect a specific world state. However, we need to focus on the meaning of the underlying concepts, which are independent of a single world state: for instance, the meaning of cooperates-with lies in the particular way two persons act in the company.In practice, understanding such meaning implies having a rule to decide, observing different behavior patterns, whether or not two persons are cooperating. Suppose that, in our case, for two persons I046758 and I044443 to cooperate means that (1) both declare to have the same goal; (2) both do something to achieve this goal. Then, the meaning of "cooperating" can be defined as a function that, for each global behavioral context involving all our universe, gives us the list of couples who are actually cooperating in that context. The reverse of this function grounds the meaning of a concept in a specific world state. Generalizing this approach, and abstracting from time for the sake of simplicity, we shall say that an intensional relation To formalize this notion of intensional relation, we first have to clarify what a "world" and a "world state" is. We shall define them with reference to the notion of "system," which will be given for granted: since we are dealing with computer representations of real phenomena, a system is simply the given piece of reality we want to model, which, at a given degree of granularity, is "perceived" by an observing agent (typically external to the system itself) by means of an array of "observed variables." Definition 2.2 (World)
With respect to a specific system S we want to model, a world state for S is a maximal observable state of affairs, i.e., a unique assignment of values to all the observable variables that characterize the system. A world is a totally ordered set of world states, corresponding to the system's evolution in time. If we abstract from time for the sake of simplicity, a world state coincides with a world.At this point, we are ready to define the notion of an intensional relation in more formal terms, building on Definition 2.3 (Intensional relation, or conceptual relation)
Let S be an arbitrary system, D an arbitrary set of distinguished elements of S, and W the set of world states for S (also called worlds, or possible worlds). The tuple <D, W > is called a domain space for S, as it intuitively fixes the space of variability of the universe of discourse D with respect to the possible states of S. An intensional relation (or conceptual relation) ρ n of arity n on <D, W > is a total function ρ n : W → 2 D n from the set W into the set of all n-ary (extensional) relations on D.Once we have clarified what a conceptual relation is, we give a representation of a conceptualization in Definition 2. • for all worlds w in W : Person 1 (w) = D • for all worlds w in W : Manager 1 (w) = {..., I034820, ...} • for all worlds w in W : Researcher 1 (w) = {..., I044443, ..., I046758, ...} • reports-to 2 (w 1 ) = {..., (I046758, I034820), (I044443, I034820), , ...} • reports-to 2 (w 2 ) = {..., (I046758, I034820), (I044443, I034820), (I034820, I050000), ...} • reports-to 2 (w 3 ) = ... • cooperates-with 2 (w 1 ) = {..., (I046758, I044443), ...} • cooperates-with 2 (w 2 ) = ...What is a Proper Formal, Explicit Specification?
In practical applications, as well as in human communication, we need to use a language to refer to the elements of a conceptualization: for instance, to express the fact that I046758 cooperates with I044443, we have to introduce a specific symbol (formally, a predicate symbol, say cooperates-with, which, in the user's intention, is intended to represent a certain conceptual relation. We say in this case that our language (let us call it L) commits to a conceptualization. Here emerges the role of ontologies as "explicit specifications of conceptualizations." In principle, we can explicitly specify a conceptualization in two ways: extensionally and intensionally. In our example, an extensional specification of our conceptualization would require listing the extensions of every (conceptual) relation for all possible worlds. However, this is impossible in most cases (e.g., if the universe of discourse D or the set of possible worlds W are infinite) or at least very impractical. In our running example, we are dealing with thousands of employees and their possible cooperations can probably not be fully enumerated. Still, in some cases it makes sense to partially specify a conceptualization in an extensional way, by means of examples, listing the extensions of conceptual relations in correspondence of selected, stereotypical world states. In general, however, a more effective way to specify a conceptualization is to fix a language we want to use to talk of it, and to constrain the interpretations of such a language in an intensional way, by means of suitable axioms (called meaning postulates The axioms for intensionally and explicitly specifying the conceptualization can be given in an informal or formal language L. As explained in the introduction, Committing to a Conceptualization
Let us assume that our language L is (a variant of) a first-order logical language, with a vocabulary V consisting of a set of constant and predicate symbols (we shall not consider function symbols here). We shall introduce the notion of ontological commitment by extending the standard notion of a (extensional) first order structure to that of an intensional first order structure. Definition 3.1 (Extensional first-order structure) Let L be a firstorder logical language with vocabulary V and S = (D, R) an extensional relational structure. An extensional first order structure (also called model for L) is a tuple M = (S, I), where I (called extensional interpretation function) is a total function I : V → D ∪ R that maps each vocabulary symbol of V to either an element of D or an extensional relation belonging to the set R. It should be clear now that the definition of ontological commitment extends the usual (extensional) definition of "meaning" for vocabulary symbols to the intensional case, substituting the notion of model with the notion of conceptualization. Figure Example 3.1 Coming back to our Example 2.1, the vocabulary V coincides with the relation symbols, i.e., V = {Person, Manager, Researcher, reports-to, cooperates-with}. Our ontological commitment consists of mapping the relation symbol Person to the conceptual relation Person 1 and proceeding alike with Manager, Researcher, reports-to, and cooperates-with.Specifying a Conceptualization
As we have seen, the notion of ontological commitment is an extension of the standard notion of model. The latter is an extensional account of meaning, the former is an intensional account of meaning. But what is the relationship between the two? Of course, once we specify the intensional meaning of a vocabulary through its ontological commitment, somehow we also constrain its models. Let us introduce the notion of intended model with respect to a certain ontological commitment for this purpose.Definition 3.3 (Intended models)
Let C = (D, W, ) be a conceptualization, L a first-order logical language with vocabulary V and ontological commitment K = (C, I). A model M = (S, I), with S = (D, R), is called an intended model of L according to K iff 1. For all constant symbols c ∈ V we have I(c) = I(c) 2. There exists a world w ∈ W such that, for each predicate symbol v ∈ V there exists an intensional relation ρ ∈ such that I(v) = ρ and I(v) = ρ(w)The set I K (L) of all models of L that are compatible with K is called the set of intended models of L according to K.Condition 1 above just requires that the mapping of constant symbols to elements of the universe of discourse is identical. Example 2.1 does not introduce any constant symbols. Condition 2 states that there must exist a world such that every predicate symbol is mapped into an intensional relation whose value, for that world, coincides with the extensional interpretation of such symbol. This means that our intended model will be -so to speak -a description of that world. In Example 2.1, for instance, we have that, for w 1 , I(P erson) = {I000001, ..., I050000, ...} = Person 1 (w 1 ) and I(reports-to) = {..., (I046758, I034820), (I044443, I034820), (I034820, I050000) , ...} = reports-to 2 (w 1 ).With the notion of intended models at hand, we can now clarify the role of an ontology, considered as a logical theory designed to account for the intended meaning of the vocabulary used by a logical language. In the following, we also provide an ontology for our running example.Definition 3.4 (Ontology)
Let C be a conceptualization, and L a logical language with vocabulary V and ontological commitment K. An ontology O K for C with vocabulary V and ontological commitment K is a logical theory consisting of a set of formulas of L, designed so that the set of its models approximates as well as possible the set of intended models of L according to K (cf. also Fig. Example 3.2
In the following we build an ontology O consisting of a set of logical formulae. Choosing the Right Domain and Vocabulary
On the basis of the discussion above, we might conclude that an ideal ontology is one whose models exactly coincide (modulo isomorphisms) with the intended ones. Things are not so simple, however: even a "perfect" ontology like that may fail to exactly specify its target conceptualization, if its vocabulary and its domain of discourse are not suitably chosen. The reason for that lies in the distinction between the logical notion of model and the ontological notion of possible world. The former is basically a combination of assignments of abstract relational structures (built over the domain of discourse) to vocabulary elements; the latter is a combination of actual (observed) states of affairs of a certain system. Of course, the number of possible models depends both on the size of the vocabulary and the extension of the domain of discourse, which are chosen more or less arbitrarily, on the basis of what appears to be relevant to talk of. On the contrary, the number of world states depends on the observed variables, even those which -at a first sight -are considered as irrelevant to talk of. With reference to our example, consider the two models where the predicates of our language (whose signature is reported above) are interpreted in such a way that their extensions are those described respectively in Examples 2.1 and 2.2. Each model corresponds to a different pattern of relationships among the people in our company, but, looking at the model itself, nothing tells us what are the world states where a certain pattern of relationships holds. So, for example, it is impossible to discriminate between a conceptualization where cooperates-with means that two persons cooperate when they are just sharing a goal, and another where they need also do something to achieve that goal. In other words, each model, in this example, will "collapse" many different world states. The reason of this is in the very simple vocabulary we have adopted: with just two predicates, we have not enough expressiveness to discriminate between different world states. So, to really capture our conceptualization, we need to extend the vocabulary in order to be able to talk of sharing a goal or achieving a goal, and we have to introduce goals (besides persons) in our domain of discourse. In conclusion, the degree to which an ontology specifies a conceptualization depends (1) on the richness of the domain of discourse; (2) on the richness of the vocabulary chosen;(3) on the axiomatization. In turn, the axiomatization depends on language expressiveness issues as discussed in Sect. 3.4.Language Expressiveness Issues
At one extreme, we have rather informal approaches for the language L that may allow the definitions of terms only, with little or no specification of the Fig. It is difficult to draw a strict line of where the criterion of formal starts on this continuum. In practice, the rightmost category of logical languages is usually considered as formal. Within this rightmost category one typically encounters the trade-off between expressiveness and efficiency when choosing the language L. On the one end, we find higher-order logic, full first-order logic, or modal logic. They are very expressive, but do often not allow for sound and complete reasoning and if they do, reasoning sometimes remains untractable. At the other end, we find less stringent subsets of first-order logic, which typically feature decidable and more efficient reasoners. They can be split in two major paradigms. First, languages from the family of description logics (DL) (cf. chapter "Description Logics"), e.g., OWL-DL (cf. chapter "Web Ontology Language: OWL"), are strict subsets of first-order logic. The second major paradigm comes from the tradition of logic programming (LP) Why is Shared of Importance?
A formal specification of a conceptualization does not need to be a specification of a shared conceptualization. As outlined above, the first definitions of "ontologies" did not consider the aspect of sharing For practical usage of ontologies, it turned out very quickly that without at least such minimal shared ontological commitment from ontology stakeholders, the benefits of having an ontology are limited. The reason is that an ontology formally specifies a domain structure under the limitation that its stakeholder understand the primitive terms in the appropriate way. In other words, the ontology may turn out useless if it is used in a way that runs counter to the shared ontological commitment. In conclusion, any ontology will always be less complete and less formal than it would be desirable in theory. This is why it is important, for those ontologies intended to support large-scale interoperability, to be well-founded, in the sense that the basic primitives they are built on are sufficiently well-chosen and axiomatized to be generally understood.Reference and Meaning
For appropriate usage, ontologies need to fulfill a further function, namely facilitating the communication between the human and the machine -referring to terminology specified in the ontology -or even for facilitating intermachine and inter-human communication. The communication situation can be illustrated using the semiotic triangle of Ogden and Richard All agents, whatever their commitment to an ontology is, find themselves in a communication situation illustrated using the semiotic triangle: The sender of a message may use a word or -more generally -a sign like the string "Person" to stand for a concept the sender has in his own "mind." He uses the sign in order to refer to abstract or concrete things in the world, which may, but need not be, physical objects. The sender also invokes a concept in the mind of an actor receiving this sign. The receiver uses the concept in order to point out the individual or the class of individuals the sign was intended to refer to. Thereby, the interpretation of the sign as a concept as well as its use in a given situation depends heavily on the receiver as well as the overall communication context. Therefore, the meaning triangle is sometimes supplemented with further nodes in order to represent the receiver or the context of communication. We have illustrated the context by an instable arrow from sign to thing that constrains possible acts of reference. Note that the act of reference remains indirect, as it is mediated by the mental concept. Once the concept is invoked, it behaves (so to speak) as a function that, given a particular context (i.e., the world state mentioned in previous sections), returns the things we want to refer to. Moreover, the correspondences between sign, concept, and thing are weak and ambiguous. In many communication circumstances, the usage of signs can erroneously invoke the wrong concepts and represent different entities than intended to.This problem is further aggravated when a multitude of agents exchanges messages in which terms do not have a prescribed meaning. Unavoidably, different agents will arrive at different conclusions about the semantics and the intention of the message.When agents commit to a common ontology they can limit the conclusions possibly associated with the communications of specific signs, because not all relations between existing signs may hold and logical consequences from the usage of signs are implied by the logical theory specifying the ontology. Therefore the set of possible correspondences between signs, concepts and Fig. Discussion
In this chapter we have introduced three core aspects of computational ontologies: conceptualizations, specifications of conceptualizations, and shared ontological commitments. These are very broad categories suitable to investigate many different formalisms and fields of applications.In fact, they are not even the only aspects of ontologies, which may be classified into different types, depending on the way they are used. For instance, the primary purpose of top-level ontologies lies in providing a broad view of the world suitable for many different target domains. Reference ontologies target the structuring of ontologies that are derived from them. The primary purpose of core ontologies derives from the definition of a super domain. Application ontologies are suitable for direct use in reasoning engines or software packages -and this list is not yet complete and will require many more experiences yet to be made.Introduction
The aim of this section is to give a brief introduction to description logics, and to argue why they are well-suited as ontology languages. In the remainder of the chapter we will put some flesh on this skeleton by providing more technical details with respect to the theory of description logics, and their relationship to state of the art ontology languages. More detail on these and other matters related to description logics can be found in Ontologies
There have been many attempts to define what constitutes an ontology, perhaps the best known (at least amongst computer scientists) being due to Gruber: "an ontology is an explicit specification of a conceptualisation" Ontologies are becoming increasingly important in fields such as knowledge management, information integration, cooperative information systems, information retrieval and electronic commerce. One application area which has recently seen an explosion of interest is the so called Semantic Web The effective use of ontologies requires not only a well-designed and welldefined ontology language, but also support from reasoning tools. Reasoning is important both to ensure the quality of an ontology, and in order to exploit the rich structure of ontologies and ontology based information. It can be employed in different phases of the ontology life cycle. During ontology design, it can be used to test whether concepts are non-contradictory and to derive implied relations. In particular, one usually wants to compute the concept hierarchy, i.e. the partial ordering of named concepts based on the subsumption relationship. Information on which concept is a specialization of another, and which concepts are synonyms, can be used in the design phase to test whether the concept definitions in the ontology have the intended consequences or not. This information is also very useful when the ontology is deployed.Since it is not reasonable to assume that all applications will use the same ontology, interoperability and integration of different ontologies is also an important issue. Integration can, for example, be supported as follows: after the knowledge engineer has asserted some inter-ontology relationships, the integrated concept hierarchy is computed and the concepts are checked for consistency. Inconsistent concepts as well as unintended or missing subsumption relationships are thus signs of incorrect or incomplete inter-ontology assertions, which can then be corrected or completed by the knowledge engineer.Finally, reasoning may also be used when the ontology is deployed. As well as using the pre-computed concept hierarchy, one could, for example, use the ontology to determine the consistency of facts stated in annotations, or infer relationships between annotation instances and ontology classes. More precisely, when searching web pages annotated with terms from the ontology, it may be useful to consider not only exact matches, but also matches with respect to more general or more specific terms -where the latter choice depends on the context. However, in the deployment phase, the requirements on the efficiency of reasoning are much more stringent than in the design and integration phases.Before arguing why description logics are good candidates for such an ontology language, we provide a brief introduction to and history of description logics.Description Logics
Description logics (DLs) In this introduction, we only illustrate some typical constructors by an example. Formal definitions are given in Sect. 2. Assume that we want to define the concept of "A man that is married to a doctor and has at least five children, all of whom are professors." This concept can be described with the following concept description:Human ¬Female ∃married.Doctor (≥ 5 hasChild) ∀hasChild.Professor This description employs the Boolean constructors conjunction ( ), which is interpreted as set intersection, and negation (¬), which is interpreted as set complement, as well as the existential restriction constructor (∃R.C), the value restriction constructor (∀R.C), and the number restriction constructor (≥ n R). An individual, say Bob, belongs to ∃married.Doctor if there exists an individual that is married to Bob (i.e. is related to Bob via the married role) and is a doctor (i.e. belongs to the concept Doctor). Similarly, Bob belongs to (≥ 5 hasChild) iff he has at least five children, and he belongs to ∀hasChild.Professor iff all his children (i.e. all individuals related to Bob via the hasChild role) are professors.In addition to this description formalism, DLs are usually equipped with a terminological and an assertional formalism. In its simplest form, terminological axioms can be used to introduce names (abbreviations) for complex descriptions. For example, we could introduce the abbreviation HappyMan for the concept description from above. More expressive terminological formalisms allow the statement of constraints such as ∃hasChild.Human Human, which says that only humans can have human children. A set of terminological axioms is called a TBox. The assertional formalism can be used to state properties of individuals. For example, the assertions HappyMan(BOB), hasChild(BOB, MARY) state that Bob belongs to the concept HappyMan and that Mary is one of his children. A set of such assertions is called an ABox, and the named individuals that occur in ABox assertions are called ABox individuals.Description logic systems provide their users with various inference capabilities that deduce implicit knowledge from the explicitly represented knowledge. The subsumption algorithm determines subconcept-superconcept relationships: C is subsumed by D iff all instances of C are necessarily instances of D, i.e. the first description is always interpreted as a subset of the second description. For example, given the definition of HappyMan from above, HappyMan is subsumed by ∃hasChild.Professor -since instances of HappyMan have at least five children, all of whom are professors, they also have a child that is a professor. The instance algorithm determines instance relationships: the individual i is an instance of the concept description C iff i is always interpreted as an element of C. For example, given the assertions from above and the definition of HappyMan, MARY is an instance of Professor. The consistency algorithm determines whether a knowledge base (consisting of a set of assertions and a set of terminological axioms) is non-contradictory. For example, if we add ¬Professor(MARY) to the two assertions from above, then the knowledge base containing these assertions together with the definition of HappyMan from above is inconsistent.In order to ensure reasonable and predictable behavior of a DL system, these inference problems should at least be decidable for the DL employed by the system, and preferably of low complexity. Consequently, the expressive power of the DL in question must be restricted in an appropriate way. If the imposed restrictions are too severe, however, then the important notions of the application domain can no longer be expressed. Investigating this tradeoff between the expressivity of DLs and the complexity of their inference problems has been one of the most important issues in DL research. Roughly, the research related to this issue can be classified into the following four phases.Phase 1 Phase Phase We are now at the beginning of Phase 4, where industrial strength DL systems employing very expressive DLs and tableau-based algorithms are being developed, with applications like the Semantic Web or knowledge representation and integration in bio-informatics in mind.Description Logics as Ontology Languages
As already mentioned above, high quality ontologies are crucial for many applications, and their construction, integration, and evolution greatly depends on the availability of a well-defined semantics and powerful reasoning tools. Since DLs provide for both, they should be ideal candidates for ontology languages. That much was already clear ten years ago, but at that time there was a fundamental mismatch between the expressive power and the efficiency of reasoning that DL systems provided, and the expressivity and the large knowledge bases that ontologists needed The suitability of DLs as ontology languages has been highlighted by their role as the foundation for several web ontology languages, including OWL, an ontology language standard developed by the W3C Web-Ontology Working Group 2 (see chapter "Web Ontology Language: OWL"). OWL has a syntax based on RDF Schema, but the basis for its design is the expressive DL SHIQ Let us point out some of the features of SHIQ that make this DL expressive enough to be used as an ontology language. Firstly, SHIQ provides number restrictions that are more expressive than the ones introduced above (and employed by earlier DL systems). With the qualified number restrictions available in SHIQ, as well as being able to say that a person has at most two children (without mentioning the properties of these children):(≤ 2 hasChild), one can also specify that there is at most one son and at most one daughter:(≤ 1 hasChild.¬Female) (≤ 1 hasChild.Female).Secondly, SHIQ allows the formulation of complex terminological axioms like "humans have human parents": Human ∃hasParent.Human.Thirdly, SHIQ also allows for inverse roles, transitive roles, and subroles. For example, in addition to hasChild one can also use its inverse hasParent, one can specify that hasAncestor is transitive, and that hasParent is a subrole of hasAncestor.It has been argued in the DL and the ontology community that these features play a central role when describing properties of aggregated objects and when building ontologies Complex roles are often required in ontologies. For example, when describing complex physically composed structures it may be desirable to express the fact that damage to a part of the structure implies damage to the structure as a whole. This feature is particularly important in medical ontologies: it is supported in the Grail DL Concrete domains Nominals are special concept names that are to be interpreted as singleton sets. Using a nominal Turing, we can describe all those computer scientists that have met Turing by CSientist ∃hasMet.Turing. Again, nominals can have dramatic effects on the complexity of a logic The Expressive Description Logic SHIQ
In this section, we present syntax and semantics of the expressive DL SHIQ In contrast to most of the DLs considered in the literature, which concentrate on constructors for defining concepts, the DL SHIQ also allows for rather expressive roles. Of course, these roles can then be used in the definition of concepts.Definition 1 (Syntax and semantics of SHIQ-roles and concepts).
Let R be a set of role names, which is partitioned into a set R + of transitive roles and a set R P of normal roles. The set of all SHIQ-roles is R ∪ {r -| r ∈ R}, where r -is called the inverse of the role r.Let C be a set of concept names. The set of SHIQ-concepts is the smallest set such that So far, we have fixed the syntax and semantics of concepts and roles. Next, we define how they can be used in a SHIQ TBox. Please note that authors sometimes distinguish between a role hierarchy or RBox and a TBox -we do not make this distinction here.Definition 2 (TBox).
A role inclusion axiom is of the form r s, where r, s are SHIQ-roles. A general concept inclusion (GCI) is of the form C D, where C, D are SHIQ-concepts.A finite set of role inclusion axioms and GCIs is called a TBox.An interpretation I is a model of a TBox T if it satisfies all axioms in T , i.e. C I ⊆ D I holds for each C D ∈ T and r I ⊆ s I holds for each r s ∈ T .A concept definition is of the form A ≡ C, where A is a concept name; it can be seen as an abbreviation for the two GCIs A C and C A.In addition to describing the relevant notions of an application domain, a DL knowledge base may also contain knowledge about the properties of specific individuals (or objects) existing in this domain. This done in the assertional part of the knowledge base (ABox). t. T (written C ≡ T D) if they subsume each other.
The ABox A is called consistent with respect to the TBox T iff there exists a model of T and A. The individual a is called an instance of the concept C with respect to the TBox T and the ABox A iff a I ∈ C I holds for all models of I of T and A.By definition, equivalence can be reduced to subsumption. In addition, subsumption can be reduced to satisfiability since C T D iff C ¬D is unsatisfiable w.r.t. T . Satisfiability and the instance problem can be reduced to the consistency problem since C is satisfiable w.r.t. T if the ABox {C(a)} is consistent w.r.t. T , and a is an instance of C w.r.t. T and A if the ABox A ∪ {¬C(a)} is inconsistent w.r.t. T .As mentioned above, most DLs are (decidable) fragments of (first-order) predicate logic ∀x.(A(x) ∧ ∃y.r(x, y) ∧ C(y)) ⇒ (D(x) ∨ ∀y.s(y, x) ⇒ E(y)).
This translation preserves the semantics: we can easily view DL interpretations as predicate logic interpretations, and then prove, e.g. that each model of a concept C w.r.t. a TBox T is a model of the translation of C conjoined with the (universally quantified) translations of T .The reasoning services that can decide the inference problems introduced above can be implemented using various algorithmic techniques, including tableaux-based techniques (see chapter "Tableau-Based Reasoning") and resolution-based techniques (see "Resolution-Based Reasoning for Ontologies").Describing Ontologies in SHIQ
In general, an ontology can be formalised in a DL knowledge base as follows. Firstly, we restrict the possible worlds by introducing restrictions on the allowed interpretations. For example, to express that, in our world, we want to consider humans, which are either muggles or sorcerers, we can use the GCIs Human Muggle Sorcerer and Muggle ¬Sorcerer.Next, to express that humans have exactly two parents and that all parents and children of humans are human, we can use the following GCI:Human ∀hasParent.Human ( 2 hasParent. ) ( 2 hasParent. ) ∀hasParent -.Human,where is an abbreviation for the top concept A ¬A. The next GCI expresses that humans having an ancestor that is a sorcerer are themselves sorcerers:Human ∃hasAncestor.Sorcerer Sorcerer.Secondly, we can define the relevant notions of our application domain using concept definitions. Recall that the concept definition A ≡ C stands for the two GCIs A C and C A. A concept name is called defined if it occurs on the left-hand side of a definition, and primitive otherwise.We want our concept definitions to have definitional impact, i.e. the interpretation of the primitive concept and role names should uniquely determine the interpretation of the defined concept names. For this, the set of concept definitions together with the additional GCIs must satisfy three conditions: 2. There are no cyclic definitions, i.e. no cyclic dependencies between the defined names in the set of concept definitions. In contrast to concept definitions, the GCIs in SHIQ may well have cyclic dependencies between concept names. An example are the above GCIs describing humans.As a simple example of a set of concept definitions satisfying the restrictions from above, we define the concepts grandparent and parent The TBox consisting of the above concept definitions and GCIs, together with the fact that hasAncestor is a transitive superrole of hasParent, implies the following subsumption relationship:Grandparent Sorcerer ∃hasParent -.∃hasParent -.Sorcerer, i.e. grandparents who are sorcerers have a grandchild who is a sorcerer. Though this conclusion may sound reasonable given the assumptions, it requires quite some reasoning to obtain it. In particular, one must use the fact that hasAncestor (and thus also hasAncestor -) is transitive, that hasParent - is the inverse of hasParent, and that we have a GCI that says that children of humans are again humans.To sum up, a SHIQ-TBox can, on the one hand, axiomatize the basic notions of an application domain (the primitive concepts) by GCIs, transitivity statements, and role inclusions, in the sense that these statements restrict the possible interpretations of the basic notions. On the other hand, more complex notions (the defined concepts) can be introduced by concept definitions. Given an interpretation of the basic notions, the concept definitions uniquely determine the interpretation of the defined notions.The taxonomy of such a TBox is then given by the subsumption hierarchy of the defined concepts. It can be computed using a subsumption algorithm for SHIQ (see chapters "Tableau-Based Reasoning" and "Resolution-Based Reasoning for Ontologies"). The knowledge engineer can test whether the TBox captures her intuition by checking the satisfiability of the defined concepts (since it does not make sense to give a complex definition for the empty concept), and by checking whether their place in the taxonomy corresponds to their intuitive place. The taxonomy of our example TBox would contain, for example, the fact that Grandparent is subsumed by Parent which is, in turn, subsumed by Human -if this is not intended, then the knowledge engineer would need to go back and modify the TBox. The expressive power of SHIQ together with the fact that one can "verify" the TBox in the sense mentioned above is the main reason for SHIQ being well-suited as an ontology language In case we have, in addition to our TBox T , also an ABox A, we can first ask a DL reasoner to check whether A is consistent w.r.t. T to make sure that our assertions in A conform with the axioms expressed in T . Consider the following ABox: A = { Human(Harry), Sorcerer Extensions and Variants of SHIQ
The ontology language OWL extends SHIQ with nominals and concrete datatypes; see chapter "Web Ontology Language: OWL." In this section, we discuss the consequences of these extensions on the reasoning problems in SHIQ.Concrete datatypes, as available in OWL, are a very restricted form of concrete domains Animal (age < (father • age)).Extending expressive DLs with concrete domains may easily lead to undecidability Finally, as mentioned above, it is quite straightforward to extend SHIQ, or even SHOIQ, with complex role inclusion axioms. The resulting DL, SROIQ Reasoning Beyond the Standard Inference Problems
As argued in the introduction, standard reasoning services for concepts (such as satisfiability and subsumption algorithms) can be used in different phases of the ontology life cycle. In the design phase, they can test whether concepts are non-contradictory and can derive implied relations between concepts. However, for these services to be applied, one already needs a sufficiently developed TBox. The result of reasoning can then be used to develop the TBox further. Until recently, however, DL systems provided no reasoning support for writing this initial TBox. The development of so-called non-standard inferences in DLs (like computing least common subsumers In the presence of ABoxes, one often wants to ask queries that are more complex than simple instance queries involving only one individual and one concept. So-called conjunctive queries, which are treated in the second subsection, overcome this deficit.Non-standard Inferences
In this subsection, we will sketch how non-standard inferences can support building a DL knowledge base.Assume that the knowledge engineer wants to introduce the definition of a new concept into the TBox. In many cases, she will not develop this new definition from scratch, but rather try to re-use things that are already present in some knowledge base (either the one she is currently building or a previous one). In a chemical process engineering application 1. The knowledge engineer decides on the basic structure of the newly defined concept, and then tries to find already defined concepts that have a similar structure. These concepts can then be modified to obtain the new concept.Instead of directly defining the new concept, the knowledge engineer first
gives examples of objects that belong to the concept to be defined, and then tries to generalize these examples into a concept definition.Both approaches can be supported by the non-standard inferences mentioned above, though this kind of support is not yet provided by any of the existing DL systems. The first approach can be supported by matching concept patterns against concept descriptions. A concept pattern is a concept description that may contain variables that stand for descriptions. A matcher σ of a pattern D onto the description C replaces the variables by concept descriptions such that the resulting concept σ(D) is equivalent to C. For example, assume that the knowledge engineer is looking for concepts concerned with individuals having a son and a daughter sharing some characteristic. This can be expressed by the pattern ∃hasChild.(Male X) ∃hasChild.(Female X).The substitution σ = {X → Tall} shows that this pattern matches the description ∃hasChild.(Male Tall) ∃hasChild.(Female Tall). Note, however, that in some cases the existence of a matcher is not so obvious.The second approach can be supported by algorithms that compute most specific concepts and least common subsumers. Assume that the examples are given as ABox individuals i 1 , . . . , i k . In a first step, these individuals are generalized into concepts by respectively computing the most specific (w.r.t. subsumption) concepts C 1 , . . . , C k in the available DL that have these individuals as instances. In a second step, these concepts are generalized into one concept by computing the least common subsumer of C 1 , . . . , C k , i.e. the least concept description (in the available DL) that subsumes C 1 , . . . , C k . In this context, rewriting of concepts comes into play since the concept descriptions produced by the algorithms for computing least common subsumers may be rather large (and thus not easy to comprehend and modify for the knowledge engineer). Rewriting minimizes the size of these description without changing their meaning by introducing names defined in the TBox.Until now, the results on such non-standard inferences are restricted to DLs that are considerably less expressive than SHIQ. For some of them, they only make sense if used for inexpressive DLs. For example, in DLs that contain the disjunction constructor, the least common subsumer of C 1 , . . . , C k is simply their disjunction, and computing this is of no help to the knowledge engineer. What one would like to obtain as a result of the least common subsumer computation are the structural similarities between the input concepts.Thus, support by non-standard inferences can only be given if one uses DLs of restricted expressive power. However, this also makes sense in the context of ontology engineering. In fact, the users that will require the most support are the naive ones, and it is reasonable to assume that they will not use (or even be offered) the full expressive power of the underlying DL. This two-level approach is already present in tools like Protégé Another way to overcome the gap between DLs of different expressive power is to use the approximation inference Queries
As we have seen in Sect. 3, given an ontology consisting of an ABox and possibly a TBox, we can retrieve instances of concepts from it, thereby explicating knowledge about concept instances in the given ontology. In this sense, we can use concepts as a query language. It has turned out, however, that this is a rather weak query language which does not allow one to query, for example, for humans whose parents are married. Continuing the example from Sect. 3, could express this query as a conjunctive query: q(x) :-Human(x), hasParent(x, y), hasParent(x, z), married(y, z)Conjunctive queries are well-known in the database community and have been suggested as an expressive query language for DLs Conclusion
The emphasis in DL research on a well-defined, logic-based semantics and a thorough investigation of the basic reasoning problems, together with the availability of highly optimized systems for very expressive DLs, makes this family of knowledge representation formalisms an ideal starting point for defining ontology languages. The standard reasoning services such as consistency checking, computation of the taxonomy, testing for unsatisfiable concepts, and instance retrieval, are provided by highly optimised, state-of-the-art DL systems for very expressive DLs. Optimizations of these systems for large ABoxes and the implementation of conjunctive query answering algorithms are active research areas.To be used in practice, the domain expert also needs tools that further support knowledge acquisition (i.e. building ontologies), maintenance (i.e. evolution of ontologies), and integration and inter-operation of ontologies. First steps in this direction have already been taken. For example, Protégé In this chapter we have concentrated on very expressive Description Logics that are the formal basis for the web ontology language OWL. For the sake of completeness, we mention here some recent results on inexpressive DLs that are relevant in the context of ontology applications. Several biomedical ontologies, such as SNOMED Introduction
A conceptual model (or an ontology) is an abstract, declarative description of the information for an application domain. It includes the relevant vocabulary, constraints on the valid states of the information, and the ways to draw inferences from that information.Conceptual modeling has a long history in the area of database systems. It began with the seminal work on the Entity-Relationship (ER) model S. Staab and R. Studer (eds.), Handbook on Ontologies, International Handbooks
Declarative query languages, like SQL, were central to relational database systems from their inception. The most attractive aspect of database query languages is the fact that their queries say which things to find rather than how to find them. Clearly, such languages are much harder to implement than the traditional imperative languages, like C. In the early days, database query languages had to be severely restricted in order to enable reasonably efficient implementations. However, as applications grew in sophistication, computing power increased, and our knowledge of algorithms for query processing expanded, the use of rule-based languages for processing information became more and more attractive. Further push came from the Semantic Web, which increased the awareness of the need for logic-based languages for processing ontologies and other distributed knowledge on the Web. This awareness led W3C to create a working group that was chartered with creation of a rule interchange format (RIF) -a family of standardized languages intended to facilitate the exchange of rule-based applications over the Web. There are several major implementations of F-logic, including FLORID This paper takes a view of F-logic as an ontology modeling language as well as a language for building applications that use these ontologies. The ability to span both sides of the engineering process, ontologies and applications, is a particularly strong aspect of F-logic. This should be contrasted with the current state of semantic Web applications, which specify ontologies declaratively, using the OWL language In the following sections we give an overview of the syntax and the semantics of F-logic by illustrating the main features through a number of simple examples. Then we describe the ways in which F-logic can be implemented. Towards the end we present a real-life ontology-based application of F-logic in the automotive industry.F-Logic by Example
In this paper we use the new syntax for F-logic -a simplified and extended version of the original syntax introduced in A Simple Ontology-Based Application
F-logic is an object-oriented language and ontologies are modeled in this language in an object-oriented style. One starts with class hierarchies, proceeds with type specification, defines the relationships among classes and objects using rules, and finally populates the classes with concrete objects.The first part of the example presented in Fig. Then follows a set of rules, which say what else can be derived from the ontology. The first rule says that if ?X is the father of ?Y and ?Y is a man, then ?Y is a son of ?X. A similar relationship holds for sons and mothers, and for daughters, fathers, and mothers. All variables in a rule are implicitly quantified outside of the rule (and for that reason the quantifiers are dropped). For instance, from the logical point of view the first rule in Fig. The original F-logic explicitly distinguished between functional methods (whose cardinality constraint is {0:1}) and set-valued methods (whose cardinality is {0: * }). It did not use the cardinality constraints syntax, but used instead => to denote functional methods in the schema and =>> for set-valued methods. To specify facts, the symbols -> and ->> were used, respectively. However, experience has shown that this notation is too error prone (it is easy to forget the extra ">") and not sufficiently flexible. For instance, it was hard to specify that some attribute is mandatory, i.e., must always have at least one value. Using the new syntax this is easy enough. For example, the following states that the name of a person must always be known:Here string is a new built-in data type for strings -see Sect. 2.4.Objects and Their Properties
Figure Object Names and Variable Names
Object names and variable names, also called id-terms, are the basic syntactic elements of F-logic. To distinguish object names from variable names, the later are prefixed with the ?-sign. Examples of object names are Abraham, man, daughter, and of variable names are ?X, or ?method. Object names can take several different forms:• Symbol. A symbol is a sequence of characters enclosed in quotation marks (e.g., "ab*-@c"). Alphanumeric strings do not require quotes (e.g., abc123, parent).• A primitive data type. An object that belongs to a primitive data type has the form "..."^^typename (e.g., "12:22:33"^^time, "123"^^integer).Primitive data types are discussed in Sect. 2.4. • A numeric shorthand. Integers, decimals, and floating point numbers have shorthand notation. For instance, "123"^^integer can be written simply as 123, "123.45"^^decimal as 123. Complex id-terms are created from function symbols and other id-terms as usual in predicate logic: couple(Abraham, Sarah), f(?X). An id-term that contains no variable is called a ground id-term.Methods
Application of a method to an object is specified using data-F-atoms. A remarkable feature of F-logic is that methods are also represented as objects and can be handled like regular objects without any special language support. For instance, in Fig. In F-logic, classes are also objects and thus are represented as id-terms. Hence, classes can have methods defined on them, and they can be instances of other classes. As mentioned earlier, methods are also objects and, as such, can belong to classes (or can themselves serve as classes). This can be helpful when one needs to attach meta-information to ontology elements -for example, to annotate them with provenance information:son:Relation[authoredby -> Hans].Furthermore, variables are permitted at all positions in isa-and subclass-F-atoms, so objects, methods, and classes are represented and queried uniformly using the same language facilities. In this way, F-logic naturally supports the meta-information facility. In contrast to other object-oriented languages where an object can be an instance of exactly one most specific class (e.g., ROL Expressing Information About an Object: F-Molecules
F-molecules are used to make several different assertions about the same object in a compact way. For example, the following F-molecule says that Isaac is a man, his father is Abraham, and Jacob and Esau are amongst his sons.Formally speaking, a nested molecule is equivalent to a conjunction of its atomic components.As usual with nesting of expressions, there are precedence rules. In F-logic, molecules are processed left-to-right, and if this is not what one expects then parentheses must be used. For instance, the first clause below says that Isaac is a man and he believes in God, whereas the second clause says that Isaac is a man and that the class man, when treated as an object, believes in God. Signatures
Signature-F-atoms specify the schema of a class; they declare the methods that apply to the various classes, the types of the arguments used by those methods, and the methods' ranges. In addition, cardinality constraints can be specified, as we saw previously. Syntactically, signature atoms are similar to data-F-atoms, but the arrow * => is used instead of -> . Here are some examples of signatures: The first states that the single-valued method father is defined for instances of the class person and the range of that method is the class man. We know that this is a single-valued method because of the cardinality constraint that says that a person can have at most one father. The second signature defines the multi-valued method daughter for the class person. It says that the objects returned by this method belong to the class woman. How do we know that daughter is a multi-valued method? By its cardinality constraint (more precisely, the lack of it). Unless stated otherwise, a method is multi-valued and its default cardinality constraint is {0: * }, which does not really constrain anything. The third signature-F-atom declares the multi-valued method son, which applies to objects of the class man. The method takes arguments of type woman. The result of the method must be an object of class man. Boolean combinations of ranges can also be used. For instance (momentarily changing the theme of our example), we could define teaching assistants to be both students and employees:   F-logic supports method overloading. This means that methods denoted by the same object name may be declared for different classes. Methods may also be overloaded and used with different numbers of parameters, as we saw in examples ( Inheritable and Non-inheritable Methods
Now we are ready to explain the difference between the symbols => and * => in signatures. F-logic distinguishes between inheritable and non-inheritable methods. Inheritable methods are roughly like instance methods in Java. One defines them for a class, they are inherited by subclasses and instances, and they are invoked by applying them to instances of a class. Non-inheritable methods are like static methods in Java. They are defined for classes and make sense only when applied to classes. However, when inheritable methods are inherited to the instances of the classes, they become non-inheritable. For instance, Isaac is an instance of the class man. When the methods father, mother, etc., are inherited by Isaac from man, these methods become non-inheritable, so the following is implied by the semantics of F-logic:Isaac[father{0:1}=>man, mother{0:1} => woman, daughter => woman, son => man].The stars are gone and => is used instead of * => . This is why the query in Fig. F-Molecules Without Properties
If we want to represent an object without giving any properties, we have to attach an empty method-specification list to the object name:This statement asserts that the object Thing exists, but does not state any properties. The query that asks about existence of this objectwill return true, but ?-Thing[foo -> ?].will return false, if the property foo had not been defined. The fact Thing[ ] is different from the fact Thing. This latter says that Thing is a proposition, which implies that the query ?-Thing.(which is different from the query ?-Thing[ ]) would succeed.Predicate Symbols
Experience shows that it is convenient to be able to use predicates alongside objects. In F-logic, predicate symbols are used in the same way as in other deductive languages, such as Datalog. A predicate formula is constructed out of a predicate symbol followed by one or more arguments included in parentheses. Such a formula is called a P-atom. The following are examples of P-atoms. married The last P-atom above is a 0-ary predicate symbol, a proposition. Information expressed by P-atoms can usually be represented by F-atoms, as shown below:Nesting of F-molecules inside P-molecules is permitted and handled similarly to nesting F-molecules. For instance, the P-molecule married(Isaac[father -> Abraham], Rebekah:woman).is equivalent to the following set of P-and F-atoms: married F-logic also supports predicate signatures, but we do not discuss this here. Path Expressions
Path expressions are a standard fixture in most object-oriented languages. In F-logic, a path expression of the form obj.expr denotes the set of objects {a1,a2,...}, such that obj[expr -> {a1,a2,...}] is true. The expression in the path expression can be a simple attribute or a method application. Method expressions can be further applied to the results of path expressions, and in this way longer path expressions can be constructed: obj.expr1.expr2.expr3.Here are some path expressions and the sets of objects they refer to in the example of Fig. The second line illustrates an application of a method in a path expression, and the third line shows a path expression where expressions are applied repeatedly. Since a method can yield a set of results, one might wonder about the meaning of an expression such as this:Abraham.son.sonSince Abraham.son is a set, what does the second attribute, son, apply to? In F-logic, the answer is that it applies to every object denoted by Abraham.son, and the results of all such applications are unioned. Thus, this path expression denotes the set of all Abraham's grandchildren. Path expressions in F-logic were first introduced In imperative object-oriented languages, path expressions provide the only way to navigate object relationships. In F-logic, most general way to navigate through objects is to use F-molecules and combine them with logical connectives and and or. However, the use of path expressions can simplify formulas in many cases. Since a path expression, such as obj.expr, denotes all the objects that can bind to the variable ?X in obj[expr -> ?X], path expressions can help eliminate variables. For instance, Abraham's grandsons can be represented in either of the following ways: These queries have two answers, one per each of Abraham's grandsons. The first query is most concise. The second query combines path expression notation with frame-based notation. It is slightly longer, but both queries use just one variable. The third query does not use path expressions; it is bulkier than the first two, and requires two variables instead of one. It should be clear that by stacking more method applications in one path expression one can eliminate many variables and thus simplify some queries and rules.Built-in Data Types and Methods
The new syntax for F-logic supports a large number of XML Schema data types and the corresponding built-ins. The built-ins are largely the same as in XQuery, but F-logic follows more elegant, object-based conventions.The most important data types are string, integer, decimal, iri, time, dateTime, and duration. By convention, the built-in types and methods start with an underscore. The constants that belong to these types are denoted as "..."^^type. The first three of the built-in types above are self-explanatory. The type iri is for representing International Resource Identifiers (e.g., "http://foo.bar.com/a/b/c#fgh?id=7"^^iri), which generalize URLs and are used to denote objects on the Web. This type provides methods such as schema (http in our case), host (foo.bar.com), path (/a/b/c), and more. The type time represents a time point within one day (for instance, "12:33:56"^^time), and dateTime is a type of arbitrary time points (for instance "2007-06-22T10:23:55+03:00"^^dateTime represents the time point of 10:23:55 on June 22, 2007 with time zone three hours ahead of Greenwich). The type duration represents temporal durations (for instance, "P2Y3DT1H3.4S"^^duration represents the duration of two years, three days, one hour, and 3.4 seconds). The duration-objects can be added to or subtracted from the time and dateTime objects. The following examples illustrate these types and their corresponding built-ins.?-"file:///abc/cde/efg"^^iri[ scheme -> ?P].// ?P = file ?-"mailto:me@foo.com"^^iri[ user -> ?U, host -> ?H]./ / ?U = me, ?H = foo.com ?-"2007-11-22T23:33:55.234"^^dateTime[ hour -> ?Hr]. // ?Hr = 23 ?-"P21Y11M12DT11M55S"^^duration[ year -> ?Yr1].// ?Yr = 21 ?-"21:22:55"^^time[ add("PT2H1M1S"^^duration) -> ?X]./ / ?X = "23:23:56"^^timeAlong with the primitive data types come variables that are restricted to that data type only. For instance, ?L^^iri can unify only with the objects of the primitive data type iri, which is described above. Thus, if we had then the only answer will be ?L = "file:///abc/cde/efg"^^iri. The object NewYork will not be returned because it is not an IRI.Rules
Rules are perhaps the best candidates for building applications around ontologies. We have already seen examples of rules in Fig. ?X[son -> ?Y] :-?Y:man[father -> ?X].Its semantics can be informally explained as follows. Whenever we can find id-terms for the variables ?X and ?Y so that all molecules in the body become either existing or derived facts, then the head of the rule is derived after applying the same substitutions to ?X and ?Y in the head. In case of our rule above, its body is true for ?X=Abraham and ?Y=Isaac or ?Y=Ismael; or for ?X=Isaac and either ?Y=Jacob or ?Y=Esau. This is due to the facts that Isaac:man[father -> Abraham], Ishmael:man[father -> Abraham], Jacob: man[father - ?O[typeError -> ?M] :-?O[?M => ?T], ?O[?M -> ?V], not ?V:?T.This rule says that a type error exists if there is a data molecule ?O[?M -> ?V] that does not conform to the signature ?O[?M => ?T] for the method ?M. If a type error is found, ?O is bound to the object where the type error exists, and ?M to the offending method.Note that here both the signature and the data molecule can be derived rather than explicitly given. Also, due to signature inheritance, it is enough to use ?O[?M => ?T] instead of ?C[?M * => ?T], for some superclass ?C of ?O. This is because if c[m * => t] and o:c are true for some o, c, m, t, then o[m => t] is derived by inheritance.Rules can be recursive. For example, given a genealogy (a parenthood relationship), we may want to specify ancestry information as follows:?X[ancestor -> ?Y] :-?X[parent -> ?Y]. ?X[ancestor -> ?Y] :-?X[ancestor -> ?Z], ?Z[parent -> ?Y].A more complex case is when we want to combine ancestry information with the information about the number of generations by which an ancestor is removed from the subject person.?X[generation(?Y) -> 1] :-?X[parent -> ?Y]. ?X[generation(?Y) -> ?N] :-?X[generation(?Z) -> ?N1], ?Z[parent -> ?Y], ?N is ?N1+1.Note that the generation method is, in general, multi-valued, since there can be multiple genealogical lines connecting a pair of individuals.The genealogy example gives us an opportunity to illustrate the powerful facility of aggregate functions supported by most F-logic based systems:?X[shortestAncestryLine(?Y) -> ?N] :-?N = min{?L|?X[generation(?Y) -> ?L]}.This rule says that the length of the shortest ancestry line between any pair of individuals, ?X and ?Y, is some number ?N that is computed as the smallest ?L such that ?X[generation(?Y) -> ?L] is true.Scoped Inference: Modularization and Integration
The concept of scoped inference The basic idea is that a knowledge base is a collection of scopes of inference or modules. Each module is a collection of rules and facts. The notion of a rule is extended as follows. As before, it is a statement of the form Head :-Body. The head literal is a predicate or an F-molecule -still no change here. The notion of a body of a rule is also unchanged -a Boolean combination of predicates and F-molecules. However, now these predicates and molecules can optionally be labeled with module references like this: pred-ormolecule@module-name. Note that only the formulas that occur in a rule body can have references to modules. The formulas in the head cannot.A rule of the form Head :-Body that belongs in a particular module, M, defines Head for that module. A subformula of the form L@N inside Body is a query to module N, asking whether L is implied by the knowledge base that resides in module N. For instance, some data source, gendata, may provide information about parents of various individuals. One may not be able to (or may not want to) insert new rules into that data source in order to preserve the integrity of the data. However, it is possible to create a different module, say mygenealogy, put rules there, and reference the information in the data source gendata: ?X[ancestor -> ?Y] :-?X[parent -> ?Y]@gendata. ?X[ancestor -> ?Y] :-?X[parent -> ?Z]@gendata, ?Z[ancestor -> ?Y].Here, the molecules of the form ...[ancestor -> ...] are defined in the same module where our two rules belong, i.e., mygenealogy. The literals of the form ...[parent -> ...]@gendata are queries to the module gendata where the parenthood information resides. Some other module might query both of these modules, but the answers might be different. For instance, the queries ?-?X[parent -> ?Y]@gendata. ?-?X[parent -> ?Y]@mygenealogy.will likely return different answers, since the parent attribute is not defined in mygenealogy. Thus, the first query will return all it knows about the parenthood relationship among individuals, while the second query will return nothing. Likewise, ?-?X[ancestor -> ?Y]@gendata. ?-?X[ancestor -> ?Y]@mygenealogy.will return different answers: the ancestor attribute is not defined in module gendata so the first query will return nothing, while the second will return the transitive closure of the parent attribute (which mygenealogy imports from gendata using the above rules).Module names can be arbitrary strings. Some systems even allow module names to be arbitrary terms. However, in case of public ontologies, module names most often are URIs.Modules can be created in different ways. First, a new module can be created on-the-fly and rules can be added to it at run time. For instance, in FLORA-2, the following query will create the module mygenealogy and drop the above rules into it:?-newmodule{mygenealogy}, insertrule{((?X[ancestor -> ?Y] :-?X[parent -> ?Y]@gendata), (?X[ancestor -> ?Y] :-?X[parent -> ?Z]@gendata, ?Z[ancestor -> ?Y]) )@mygenealogy }.Another method is to create a file containing rules and facts, and then load it into a module. For example, in the FLORA-2 system this is done as follows:?-load(myfile>>mygenealogy).Finally, a file can be designated to specifically contain rules for a particular module using the declaration of the form :-module mygenealogy.at the beginning of the file.Besides modularization, the concept of a module is a potent vehicle for integration of and reasoning about ontologies that reside at different sources.If one just unions the rules and the facts found at the sources of interest, as implied by the import mechanism of the OWL language, the rules may contradict each other or have subtle and unintended interactions. In contrast, if different sources are treated as separate modules, one can differentiate among the information residing at these sources and specify the appropriate integration rules. These rules may give preference to some sources, partially or completely disregard information supplied by others, or clearly flag conflicting information.F-logic modules can be imported into other modules. This allows one to construct ontologies in a hierarchical way. For instance, an upper level ontology may be defined in one module and a domain-specific ontology, defined in another module, might inherit concept definitions from the upper-level ontology. This can be conveniently specified by the following statement included at the top of the domain ontology::-importmodule myupperlevelontology.The effect of this statement is that every concept and method defined in the upper ontology can be used in the domain ontology without the need for the @myupperlevelontology designator.The reader is referred to Inheritance
In frame-based systems, inheritance comes in two forms: structural and behavioral. Structural inheritance means that declarations of structure in a class are also inherited by subclasses. Behavioral inheritance deals with default method definitions. The F-logic Forum group has decided to include structural inheritance as a core feature, but made behavioral inheritance optional.Structural inheritance is simpler, and we look at it first. Consider the signature declarations such as Since man::person holds, it should follow that a man's father is also a man: should logically follow from the above. Structural inheritance is monotonic in the sense that types inherited from superclasses are never overwritten, but are accumulated instead. In the following example, class workingStudent is defined as subclass of the classes worker and student. A worker is paid by at least one company and a student may be paid by research institutions. As structural inheritance is monotonic, it follows that whenever a working student is paid, the money must come from an institution which is a company and a research institution as well.Behavioral inheritance is much more complex because it can be overwritten by explicit or derived information specified for subclasses. For instance, most humans have their heart on the left and so it is reasonable to specify this as a default. In F-logic, defaults for classes are specified using the * -> arrow style:But dextrocardiacs have their hearts on the right, which is expressed as Suppose John Doe is a dextrocardiac: JohnDoe:dextrocardiac. He inherits heartPosition * -> left from the class person and heartPosition* -> right from the class dextrocardiac. Although this is a contradiction, the class dextrocardiac is more specific to John Doe than the class person, so inheritance from dextrocardiac overrides inheritance from person.Unlike structural inheritance, behavioral inheritance is nonmonotonic. This means that inferences that were made from a set of facts and rules, S, may no longer hold from a bigger set S ⊇ S. To see this, let us return to our biblical genealogy. We know Abraham:person, and there has been no indication that he was a dextrocardiac. According to the common understanding of inference by inheritance, it should follow thatIt also follows that, for example, man[heartPosition* -> left]. (Properties are inherited to individual class instances as non-inheritable methods and to subclasses as inheritable ones. This is why we use the -> style arrow for Abraham and * -> for man.)Suppose now that new information says that Abraham was a dextrocardiac. The rules of inheritance then tell us that inheritance from dextrocardiac overrides inheritance from person so Abraham[heartPosition -> left] must become false while Abraham[heartPosition -> right] must become true. In other words, a larger set of premises no longer entails the old conclusion that Abraham's heart is on the left.In F-logic, class hierarchies can be defined by rules, so it is not possible to "eyeball" an ontology and tell which classes are subclasses of other classes and which inheritance overrides what. This is further complicated by the fact that class hierarchies may depend on negative facts derived through default negation. The complete treatment of the semantics for inheritance in F-logic is beyond the scope of this paper. Several semantics have been proposed Implementations of F-Logic
Implementations of F-logic based systems can be roughly divided into two categories: those that are based on native object-oriented deductive engines and those that use relational deductive engines. The engines can be further divided into bottom-up and top-down engines.FLORID implements F-logic using a dedicated bottom-up deductive engine, which handles objects directly through an object manager. In that sense, it is similar to object-oriented databases. In contrast, FLORA-2 and OntoBroker TM use relational engines, which do not support objects directly. Instead, both systems translate F-logic formulas into statements that use predicates (relations) instead of F-logic molecules, and then execute them using relational deductive engines. FLORA-2's target engine is XSB -a Prologlike inference engine with numerous enhancements, which make XSB into a more declarative and logically complete system than the usual Prolog implementations. XSB's inference mode is top-down with a number of bottomup-like extensions. OntoBroker TM uses its own relational deductive engine. Its main inference mode is bottom-up, but it includes several enhancements inspired by top-down inference, such as dynamic filtering The translation from F-logic into the relational syntax used by FLORA-2 and OntoBroker TM was defined in (1) First, molecular expressions are replaced by equivalent conjunctions of atomic molecules. We illustrated this process in earlier sections. (2) Next, these atomic expressions are represented by first-order predicates.(3) The resulting set of rules is augmented with additional "closure rules" to capture the specific semantics of F-logic. Some rules are needed to express statements such as the transitivity of the subclass relationship; other rules implement the semantics of inheritance and other features.Table The following are examples of some of the closure rules added in stage (3) of the translation process:// closure rules for ?X :: ?Y sub(?X, ?Z) :-sub(?X, ?Y) and sub(?Y, ?Z).  In systems that support non-monotonic inheritance, additional rules are included An Industrial Application: Configuration of Test Cars
We will now describe a project in the automotive industry where ontologies have two main purposes: (1) representing and sharing knowledge to optimize business processes for testing cars and (2) integration of live data into this optimization process. A car manufacturer has a fleet of test cars. The cars are frequently reconfigured and tested with the new configuration. Reconfiguration could mean changing the engine, the gear, the electric system, etc. When parts are replaced, many of their interdependencies must be taken into account. In many cases these dependencies are known only by certain human experts and require significant amount of communication between different departments in the manufacturer's plant, between the manufacturer and suppliers, and between different suppliers. Often test cars are misconfigured and do not work. Thus, if dependencies can be checked by a computer without building misconfigured cars, manufacturer's costs can be significantly lowered.The same advantage applies to the development of new cars, collaboration between the manufacturer and suppliers, and the built-to-order process.Besides describing human knowledge about various domains, ontologies serve as mediators between data sources Creation of the ontology for the auto manufacturer is supported by OntoBroker TM and OntoStudio TM Without the rules, an ontology can describe only simple relationships between concepts, like a part being a component of another part, or being connected to another part. More complex relationships require the use of rules and constraints. The following are examples of such rules and constraints:Rule 1 : If a part is assigned to a sub-system then sub-parts are also assigned to the same sub-system.Taken separately, each of the above rules or constraints is simple. However, it is not the complexity of individual rules that determines the difficulty of the problem, but the overwhelming amount of such rules and constraints. The rules can interfere with each other and make the task of configuring a correct test car complex and error prone. On the other hand the simplicity of the individual rules indicates that such ontologies can be created and maintained by domain experts, like mechanical engineers in our case.F-Logic and the Semantic Web
F-logic has strong following in the Semantic Web area. Although it was not chosen as a standard for ontology representation, the most frequent ontological constructs map straightforwardly into F-logic. For example, Bruijn and Heymans F-logic plays even more important role in the area of Semantic Web Services, where the need to go beyond ontologies is most evident. It is one of the core ideas underlying the Web Services Modeling Ontology (WSMO) Conclusions
The focus of this paper is the use of F-logic as a language for representing ontologies and building intelligent applications on top. F-logic is a frame-based logic language, which supports object-oriented style of application development. A remarkable feature of F-logic is the ability to reason about objects and their schema naturally and without the need for special language features.A number of implementations of F-logic exist -both commercial and opensource academic systems (OntoBroker TM , FLORID, FLORA-2). There is now vast experience with using F-logic for building intelligent information systems that rely on ontologies for extensibility and interoperability. Many practical applications, such as Introduction: Heading for the Semantic Web
In Realising the Full Potential of the Web The Web's provision to allow people to write online content for other people is an appeal that has changed the computer world. This same feature that is responsible for fostering the first goal of the Semantic Web, however, hinders the second objective. Much of the content on the existing Web, the socalled syntactic Web, is human but not machine readable. Furthermore, there is great variance in the quality, timeliness and relevance The vision of the Semantic Web is to augment the syntactic Web so that resources are more easily interpreted by programs (or 'intelligent agents'). The enhancements will be achieved through the semantic markups which are machine-understandable annotations associated with Web resources.Encoding semantic markups will necessitate the Semantic Web adopting an annotation language. To this end, the W3C (World Wide Web Consortium)  community has developed a recommendation called resource description framework (RDF) Example 1. Annotating Web Resources in RDF
As shown in Fig. We invite the reader to note that the above RDF annotations are different from HTML Annotations alone do not establish the semantics of what is being markedup. For example, the annotations presented in Fig. Annotation and Meaning
The vision of the Semantic Web is to make Web resources (not just HTML pages, but a wide range of Web accessible data and services) more understandable to machines. Machine-understandable annotations are, therefore, introduced to describe the content and functions of Web resources.RDF
RDF Fig. 2. RDF statements
annotated resource is elp:Ganesh, which is annotated with three properties ex:mytitle, ex:mycreator and ex:mypublisher. Note that : b1 is a blank node identifier.Given that RDF alone does not specify the intended meaning for Web resources, how do we provide meaning to Web resources through annotations? The meaning comes either from pre-agreed informal semantics, e.g. from Dublin Core, or from ontologies.Dublin Core
One way of giving meaning to annotations is to provide some pre-agreed informal semantics for a set of information properties. For example, the Dublin Core Metadata Element Set If we replace the properties ex:mytitle, ex:mycreator and ex:mypublisher used in Fig. The limitation of the 'pre-agreed informal semantics' approach is its inflexibility, i.e. only a limited range of pre-agreed information properties can be expressed.Ontology
An alternative approach is to use ontologies to specify the meaning of Web resources. Ontology is a term borrowed from philosophy that refers to the science of describing the kinds of entities in the world and how they are @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> @prefix dc: <http://purl.org/dc/elements/1.1/> @prefix elp: <http://example.org/Animal#> elp:Ganesh dc:title "A resource called Ganesh" ; dc:creator "Pat Gregory" ; dc:publisher : b1 . : b1 elp:name "Elephant United" . Fig. The ontology approach is more flexible than the pre-agreed informal semantics approach because users can customise vocabulary and constraints in ontologies. For example, applications in different domains can use different ontologies. Typically, ontologies can be used to specify the meaning of Web resources (through annotations) by asserting resources as instances of some important concepts and/or asserting resources relating to resources by some important properties defined in ontologies.Ontologies can be expressed in Description Logics. An ontology usually corresponds to a TBox in Description Logics (see chapter "Description Logics"). Vocabulary in an ontology can be expressed by named concepts and roles, and concept definitions can be expressed by equivalence introductions. Background assumptions can be represented by general concept and role axioms. Sometimes, an ontology corresponds to a DL knowledge base. For example, in the OWL Web ontology language to be introduced in chapter "Web Ontology Language: OWL," an ontology also contains instances of important concepts and relationships among these instances, which can be represented by DL assertions. In the rest of the chapter, we will introduce RDF Schema (RDFS), an ontological schema language, and a novel modification of RDF(S) as a semantic foundation for many of the latest Description Logics-based SW ontology languages, including OWL-DL and OWL 1.1.RDFS: A Web Ontological Schema Language
Following W3C's 'one small step at a time' strategy, RDFS can be seen as a first try to support expressing simple ontologies with RDF syntax. In RDFS, predefined Web resources rdfs:Class, rdfs:Resource and rdf:Property can be used to define classes (concepts), resources and properties (roles), respectively.Unlike Dublin Core, RDFS does not predefine information properties but a set of meta-properties that can be used to represent background assumptions in ontologies:• rdf:type: the instance-of relationship • rdfs:subClassOf: the property that models the subsumption hierarchy between classes • rdfs:subPropertyOf: the property that models the subsumption hierarchy between properties  • rdfs:domain: the property that constrains all instances of a particular property to describe instances of a particular class • rdfs:range: the property that constrains all instances of a particular property to have values that are instances of a particular class RDFS statements are simply RDF triples; viz. RDFS provides no syntactic restrictions on RDF triples. Figure At a glance, RDFS is a simple ontological schema langauge that supports only class and property hierarchies, as well as domain and range constraints for properties. According to the RDF Model Theory (RDF MT) to be explained in Sect. 3.2, however, it is more complicated than that (see Proposition 1 on page 79).RDF(S) Datatyping
RDF(S) provides a specification of datatypes and data values; accordingly, it allows the use of datatypes defined by any external type systems, e.g. the XML Schema type system, which conform to this specification.Definition 1. (Datatype)
A datatype d is characterised by a lexical space, L(d), which is a non-empty set of Unicode strings; a value space, V (d), which is a non-empty set, and a total mapping L2V (d) from the lexical space to the value space.For example, boolean is a datatype with value space {true, f alse}, lexical space {"T", "F","1","0"} and lexical-to-value mapping {"T" → true, "F" → false, "1" → true, "0" → false}. Definition 2. (Typed and Plain Literals) Typed literals are of the form "v"ˆˆu, where v is a Unicode string, called the lexical form of the typed literal, and u is a URI reference of a datatype. Plain literals have a lexical form and optionally a language tag as defined by The denotation of a typed literal is the value mapped from its enclosed Unicode string by the lexical-to-value mapping of the datatype associated with its enclosed datatype URIref. For example, "1"ˆˆxsd:boolean is a typed literal that represents the boolean value true, while "1"ˆˆxsd:integer represents the integer 1. Plain literals, e.g. "1", are considered to denote themselves The associations between datatype URI references (e.g. xsd:boolean) and datatypes (e.g. boolean) can be provided by datatype maps defined as follows. A datatype map may include some built-in XML Schema datatypes (as seen in Example 2), while other built-in XML Schema datatypes are problematic and thus unsuitable for various reasons. For example, xsd:ENTITIES is a list-value datatype that does not fit the RDF datatype model. RDF Model Theory
RDF MT provides semantics not only for RDFS ontologies, but also for RDF triples. RDF MT is built on simple interpretations. To simplify presentations, in this chapter we do not cover blank nodes, which are identified by local identifiers instead of URIrefs.  Finally, the semantics of RDFS statements written in RDF triples is given in terms of RDFS-Interpretations. Definition 6. (RDFS-Interpretation) Given rdfV, a set of URI references V and the set rdfsV, called the RDFS vocabulary, of URI references in the rdfs: namespace, an RDFS-interpretation I of V is an RDF-interpretation of V ∪ rdfV ∪ rdfsV which introduces: and satisfies all the RDFS axiomatic statements. According to Definition 7, LV is a subset of IR; i.e. literal values are resources. Condition 1 ensures that the class extension of rdfs:Literal is LV. Condition 2) asserts that RDF(S) datatypes are classes, condition 2) ensures that there is a resource d for datatype d in M d , condition 2) ensures that the class rdfs:Datatype contains the datatypes used in any satisfying M dinterpretation, and condition 2) explains why the range of IL is IR rather than LV (because, for "s"ˆˆu, if s ∈ L(IS(u)), then IL("s"ˆˆu) ∈ LV). Condition 3 requires that RDF(S) datatypes are sub-classes of rdfs:Literal.If the datatypes in the datatype map M d impose disjointness conditions on their value spaces, it is possible for an RDF graph to have no RDFS M dinterpretation which satisfies it, i.e. there exists a datatype clash. For example, : x rdf:type xsd:string. : x rdf:type xsd:decimal.would constitute a datatype clash because the value spaces of xsd:string and xsd:decimal are disjoint. In RDF(S), an ill-typed literal does not in itself constitute a datatype clash, cf. Condition 2) in Definition 7, but a graph which entails that an ill-typed literal has rdf:type rdfs:Literal would be inconsistent.Having described the semantics, we now briefly discuss reasoning in RDF(S). Entailment is the key inference problem in RDF(S), which can be defined on the basis of interpretations. Indeed, cRDF is impossible to express contradictions if we do not consider datatypes. Mismatch between RDF(S) and OWL-DL
This section describes the relation between RDF(S) and OWL-DL, which is a key sub-language of the standard (W3C recommendation) Web Ontology Langauge. One key question is whether it is possible to use an RDF(S) inference engine to do OWL-DL reasoning, or vice versa. The short answer is no, and this section explains why.The OWL recommendation actually consists of three languages of increasing expressive power: OWL-Lite, OWL-DL and OWL-Full. OWL-Lite and OWL-DL are basically very expressive description logics (DLs). OWL-Full provides the same set of constructors as OWL-DL, but allows them to be used in an unconstrained way (in the style of RDF). OWL-Full is undecidable, because it combines the OWL expressivity with the meta-modelling architecture of RDF(S) This section discusses both the syntactic and semantic mismatches between RDF(S) and OWL-DL. From the syntax aspect, OWL-DL heavily restricts the syntax of RDF(S), viz. some RDF(S) annotations are not recognisable by OWL-DL agents, since they are syntactically ill formed. The RDF/XML syntax form of an OWL-DL ontology is valid, iff it can be translated (according to the mapping rules provided in From the semantics aspect, OWL-DL has an RDF MT-style semantics, in which (including built-in) classes and properties are treated as objects (or resources) in the domain. In order to make it equivalent to the direct semantics of OWL-DL OWL-Full seems to be a bridge between RDF(S) and OWL-DL; however, there exist at least three known issues that the RDF-style semantics for OWL-Full needs to solve, and a proven solution has yet to be given. The first issue is about entailment The second issue is about contradiction classes : c owl:onProperty rdf:type; owl:allValuesFrom : d. : d owl:complementOf : e. : e owl:oneOf : l : l rdf:first : c; rdf:rest rdf:nil.The above triples require that rdf:type relates members of the class : c to anything but : c. It is impossible for one to determine the membership of : c. If an object is an instance of : c, then it is not; but if it is not then it is -this is a contradiction class. Note that it is not a valid OWL-DL class, as OWL-DL disallows using rdf:type as an object property. With naive comprehension principles, resources of contradiction classes would be added to all possible OWL-Full interpretations, which thus have ill-defined class memberships. To avoid the issue, the comprehension principles must also consider avoiding contradiction classes. Unsurprisingly, devising such comprehension principles took a considerable amount of effort The third issue is about the size of the universe In OWL-DL, classes are not objects, so the answer is 'yes': The only object in the domain is the interpretation of elp:Ganesh, the elp:Elephant class thus has one instance, i.e. the interpretation of elp:Ganesh, and the elp:Plant class has no instances. In OWL-Full, since classes are also objects, besides elp:Ganesh, the classes elp:Elephant and elp:Plant should both be mapped to the only one object in the universe. This is not possible because the interpretation of elp:Ganesh is an instance of elp:Elephant, but not an instance of elp:Plant; hence, elp:Elephant and elp:Plant should be different, i.e. there should be at least two objects in the universe. As the above axioms are valid OWL-DL axioms, this example shows that OWL-Full disagrees with OWL-DL on valid OWL-DL ontologies. To partially address this issue, the OWL specification weakens the relations between OWL-DL and OWL-Full by claiming (with a sketched proof) that, given two OWL-DL ontologies O1 and O2, O1 entails O2 w.r.t. the OWL-DL semantics implies that O1 entails O2 w.r.t. the OWL-Full semantics. Furthermore, this example shows that the interpretation of OWL-Full has different features than the interpretation of standard first order logic (FOL) model theoretic semantics. This raises the question as to whether it is possible to layer FOL languages on top of RDF(S).It should be noted that for some the above presentation of the three issues might be a little too negative about the situation w.r.t. OWL-Full and OWL-DL: the first two issues are difficulties that have, in theory, been claimed to be solved by the use of comprehension principles and restrictions on the syntactic form of OWL-DL's RDF serialisation. From this perspective, the main side effect of comprehension principles is that all OWL-Full models have infinite domains; hence, any OWL-DL ontologies that have only finite models are necessarily inconsistent when treated as OWL-Full ontologies. This leads to the third issue and demonstrates why, in the OWL specification, the relations between OWL-Full and OWL-DL is weakened.RDFS-FA: Connecting RDF(S) and OWL-DL
In this section, we introduce RDFS-FA (RDFS with Fixed layered metamodelling architecture), as a sub-language of RDF(S), to restore the desired connection between RDF(S) and OWL-DL. RDFS-FA addresses the following characteristics of RDF(S):• RDF triples have built-in semantics.• Classes and properties, including built-in classes and properties of RDF(S) and its subsequent languages such as OWL, are treated as objects (or resources) in the domain. • There are no restrictions on the use of built-in vocabularies.Intuitively, RDFS-FA provides a UML like meta-modelling architecture. Let us recall that RDFS has a non-layered meta-modelling architecture; resources in RDFS can be classes, objects and properties at the same time, viz. classes and their instances (as well as relationships between the instances) are the same layer. RDFS-FA, instead, divides up the universe of discourse into a series of strata (or layers). The built-in modelling primitives of RDFS are separated into different strata of RDFS-FA, and the semantics of modelling primitives depend on the stratum they belong to. Theoretically there can be a large number of strata in the meta-modelling architecture; in practice, four strata (as shown in Fig. In RDFS-FA, classes cannot be objects and vice versa;Furthermore, RDFS-FA allows users to specify classes and properties in specified strata. For example, the class inclusion axiom elp:Elephant fa:subClassOf2 elp:Animal .requires that both elp:Elephant and elp:Animal are class URIrefs in stratum 1.We conclude this section by showing the interoperability between RDFS-FA and OWL-DL. It is much easier to layer OWL-DL, syntactically and semantically, on top of RDFS-FA than on top of RDF(S). In particular, there is a one-to-one bidirectional mapping (see It has be shown Related Work
As earlier works Horst Patel-Shneider et al. • In the context approach, the names for classes, properties and individuals are not distinct and are interpreted depending on the context; i.e. they are interpreted by class interpretation functions, property interpretation functions and individual interpretation functions, respectively. Intuitively speaking, this approach provides a 'two-layered' meta-modelling architecture, i.e. the instance layer and class layer. OWL FA provides a 'multilayered' meta-modelling architecture. At a quick glance, the 'two-layered' and the 'multi-layered' meta-modelling architectures should be similar; however, the example we show later in this section indicates that they are quite different.• The HiLog approach is closer to the spirit of OWL Full meta-modelling. It has a 'two-step' interpretation function for classes, which first maps symbols to resources in the domain and then maps these resources to a set of resources in the domain. Intuitively speaking, this approach provides a 'one-layered' meta-modelling architecture, in the sense that classes and individuals are both interpreted as resources in the domain. Note that it is dificult/impossible to map classes in the 'one-layered' meta-modelling architecture to the 'multi-layered' meta-modelling architectures such as that of MOF. We now use an example in Let us conclude this section by briefly comparing the three approaches. In terms of syntax, the contextual and Hilog approaches seem to be more elegant in that they do not have to change the syntax of OWL DL, while the FA approach introduces strata numbers to facilitate the 'multi-layered' meta-modelling architecture. In terms of semantics, it seems that the FA approach is closer to the Hilog approach (according to the above example). It is an interesting peace of future work to investigate more detailed differences between the Hilog approach and the FA approach. In terms of computability, the FA approach is closer to the contextual approach in that we can reduce the reasoning services (such as knowledge base satisfiability) to existing DL reasoning services. Finally, the contextual approach and the Hilog approach have not covered datatypes yet, while the FA approach covers datatypes. In order to support datatypes in the contextual approach, some extra syntax may be needed for OWL DL, otherwise it is difficult to distinguish the contexts. For example, in ∃R.E, E can be either a class or a datatype. It is not clear how to support datatypes in the Hilog approach yet.Other existing approaches either limit the extension of RDF(S) to only a property-related subset of OWL with a weaker semantics proposed by ter Horst ( Conclusion
In this chapter, we have presented RDF. RDF is a standard syntax for Semantic Web annotations and languages. RDF Schema is an ontological schema language that supports only class and property hierarchies, as well as domain and range constraints for properties. RDF(S) has a key role in supporting such compatibility by providing a common basis on which more expressive SW languages can be built. Recent research, however, has shown that there exist syntactic and semantic mismatch between RDF(S) and OWL-DL. Accordingly, this chapter includes a novel modification of RDF(S), called RDFS-FA, which provides a solid semantic foundation for many of the latest Description Logic-based SW ontology languages, and imposes no limitation on its extension to more expressive Description Logics (such as OWL-DL, OWL2-DL and OWL-Eu In chapter "RDF Storage and Retrieval Systems," we will further describe entailment and querying over RDF(S) ontologies. As for RDFS-FA, reasoning in RDFS-FA and its OWL extension, OWL-FA, is discussed in Requirements for Ontology Languages
Ontology languages allow users to write explicit, formal conceptualizations of domains models. The main requirements are: (a) a well-defined syntax; (b) a well-defined semantics; (c) efficient reasoning support; (d) sufficient expressive power; and (e) convenience of expression.The importance of a well-defined syntax is clear, and known from the area of programming languages; it is a necessary condition for machine-processing of information. OWL builds upon RDF and RDFS and has the same kind of syntax.Formal semantics describes precisely the meaning of knowledge. "Precisely" here means that the semantics does not refer to subjective intuitions, nor is it open to different interpretations by different persons (or machines). The importance of formal semantics is well-established in the domain of mathematical logic, among others.S. Staab and R. Studer (eds.), Handbook on Ontologies, International Handbooks
One use of formal semantics is to allow humans to reason about the knowledge. For ontological knowledge we may reason about:• Class membership: If x is an instance of a class C, and C is a subclass of D, then we can infer that x is an instance of D. Formal semantics and reasoning support is usually provided by mapping an ontology language to a known logical formalism, and by using automated reasoners that already exist for those formalisms. OWL is (partially) mapped on a description logic, and makes use of existing reasoners such as FaCT and RACER.Limitations of the Expressive Power of RDF Schema
RDF and RDFS allow the representation of some ontological knowledge. The main modelling primitives of RDF/RDFS concern the organization of vocabularies in typed hierarchies: subclass and subproperty relationships, domain and range restrictions, and instances of classes. However a number of other features are missing. Here we list a few:• Local scope of properties: rdfs:range defines the range of a property, say eats, for all classes. Thus in RDF Schema we cannot declare range restrictions that apply to some classes only. For example, we cannot say that cows eat only plants, while other animals may eat meat, too. • Disjointness of classes: Sometimes we wish to say that classes are disjoint.For example, male and female are disjoint. But in RDF Schema we can only state subclass relationships, e.g. female is a subclass of person. • Boolean combinations of classes: Sometimes we wish to build new classes by combining other classes using union, intersection and complement. For example, we may wish to define the class person to be the disjoint union of the classes male and female. RDF Schema does not allow such definitions.• Cardinality restrictions: Sometimes we wish to place restrictions on how many distinct values a property may or must take. For example, we would like to say that a person has exactly two parents, and that a course is taught by at least one lecturer. Again such restrictions are impossible to express in RDF Schema. • Special characteristics of properties: Sometimes it is useful to say that a property is transitive (like "greater than"), unique (like "is mother of"), or the inverse of another property (like "eats" and "is eaten by").So we need an ontology language that is richer than RDF Schema, a language that offers these features and more. In designing such a language one should be aware of the tradeoff between expressive power and efficient reasoning support. Generally speaking, the richer the language is, the more inefficient the reasoning support becomes, often crossing the border of non-computability. Thus we need a compromise, a language that can be supported by reasonably efficient reasoners, while being sufficiently expressive to express large classes of ontologies and knowledge.Compatibility of OWL with RDF/RDFS
Ideally, OWL would be an extension of RDF Schema, in the sense that OWL would use the RDF meaning of classes and properties (rdfs:Class, rdfs:subClassOf, etc.), and would add language primitives to support the richer expressiveness identified above.Unfortunately, the desire to simply extend RDF Schema clashes with the trade-off between expressive power and efficient reasoning mentioned before. RDF Schema has some very powerful modelling primitives, such as the rdfs:Class (the class of all classes) and rdf:Property (the class of all properties). These primitives are very expressive, and will lead to uncontrollable computational properties if the logic is extended with the expressive primitives identified above.Three Species of OWL
All this has lead to a set of requirements that may seem incompatible: efficient reasoning support and convenience of expression for a language as powerful as a combination of RDF Schema with a full logic.Indeed, these requirements have prompted W3C's Web Ontology Working Group to define OWL as three different sublanguages, each of which is geared towards fulfilling different aspects of these incompatible full set of requirements:• OWL Full: The entire language is called OWL Full, and uses all the OWL languages primitives (which we will discuss later in this chapter). It also allows to combine these primitives in arbitrary ways with RDF and RDF Schema. This includes the possibility (also present in RDF) to change the meaning of the pre-defined (RDF or OWL) primitives, by applying the language primitives to each other. The advantage of OWL Full is that it is fully upward compatible with RDF, both syntactically and semantically: any legal RDF document is also a legal OWL Full document, and any valid RDF/RDF Schema conclusion is also a valid OWL Full conclusion. As a disadvantage, the language has become so powerful as to be undecidable, dashing any hope of guarantees on complete and efficient reasoning. • OWL DL: In order to regain computational efficiency, OWL DL (short for: Description Logic) is a sublanguage of OWL Full which restricts the way in which the constructors from OWL and RDF can be used. We will give details later, but roughly this amounts to disallowing application of OWL's constructor's to each other, and thus ensuring that the language corresponds to a well studied description logic. The advantage of this is that it permits efficient reasoning support.The disadvantage is that we loose full compatibility with RDF: an RDF document will in general have to be extended in some ways and restricted in others before it is a legal OWL DL document. Conversely, every legal OWL DL document is still a legal RDF document. • OWL Lite: An even further restriction limits OWL DL to a subset of the language constructors. For example, OWL Lite excludes enumerated classes, disjointness statements and arbitrary cardinality (among others). The advantage of this is a language that is both easier to grasp (for users) and easier to implement (for tool builders).The disadvantage is of course a restricted expressivity.Ontology developers adopting OWL should consider which sublanguage best suits their needs. The choice between OWL Lite and OWL DL depends on the extent to which users require the more-expressive constructs provided by OWL DL and OWL Full. The choice between OWL DL and OWL Full mainly depends on the extent to which users require the meta-modeling facilities of RDF Schema (e.g. defining classes of classes, or attaching properties to classes). When using OWL Full as compared to OWL DL, reasoning support is less predictable since complete OWL Full implementations will be impossible.There are strict notions of upward compatibility between these three sublanguages:• Every legal OWL Lite ontology is a legal OWL DL ontology  ObjectProperty are all specialisations of their RDF counterparts. Figure The original hope in the design of OWL was that there would be a downward compatibility with corresponding re-use of software across the various layers. However, the advantage of full downward compatibility for OWL (that any OWL aware processor will also provide correct interpretations of any RDF Schema document) is only achieved for OWL Full, at the cost of computational intractability.The OWL Language
Syntax
OWL builds on RDF and RDF Schema, and uses RDF's XML syntax. Since this is the primary syntax for OWL, we will use it here, but it will soon become clear that RDF/XML does not provide a very readable syntax. Because of this, other syntactic forms for OWL have also been defined:• An XML-based syntax which does not follow the RDF conventions. This makes this syntax already significantly easier to read by humans. • An abstract syntax which is used in the language specification document.This syntax is much more compact and readable then either the XML syntax or the RDF/XML syntax.• A graphical syntax based on the conventions of the UML language (Universal Modelling Language). Since UML is widely used, this will be an easy way for people to get familiar with OWL.Header
OWL documents are usually called OWL ontologies, and are RDF documents. So the root element of a OWL ontology is an rdf:RDF element which also specifies a number of namespaces. For example: An OWL ontology may start with a collection of assertions for housekeeping purposes. These assertions are grouped under an owl:Ontology element which contains comments, version control and inclusion of other ontologies. For example: <owl:Ontology rdf:about=""> <rdfs:comment>An example OWL ontology</rdfs:comment> <owl:priorVersion rdf:resource="http://www.mydomain.org/uni-ns-old"/> <owl:imports rdf:resource="http://www.mydomain.org/persons"/> <rdfs:label>University Ontology</rdfs:label> </owl:Ontology>The only one of these assertions which has any consequences for the logical meaning of the ontology is owl:imports : this lists other ontologies whose content is assumed to be part of the current ontology. Notice that while namespaces are used for disambiguation purposes, imported ontologies provide definitions that can be used. Usually there will be an import element for each used namespace, but it is possible to import additional ontologies, for example ontologies that provide definitions without introducing any new names.Also note that owl:imports is a transitive property: if ontology A imports ontology B, and ontology B imports ontology C, then ontology A also imports ontology C.Class Elements
Classes are defined using a owl:Class element. <owl:Class rdf:ID="associateProfessor"> <rdfs:subClassOf rdf:resource="#academicStaffMember"/> </owl:Class>We can also say that this class is disjoint from the professor and assistantProfessor classes using owl:disjointWith elements. These elements can be included in the definition above, or can be added by referring to the id using rdf:about. This mechanism is inherited from RDF.<owl:Class rdf:about="#associateProfessor"> <owl:disjointWith rdf:resource="#professor"/> <owl:disjointWith rdf:resource="#assistantProfessor"/> </owl:Class> Equivalence of classes can be defined using a owl:equivalentClass element:<owl:Class rdf:ID="faculty"> <owl:equivalentClass rdf:resource="#academicStaffMember"/> </owl:Class> Finally, there are two predefined classes, owl:Thing and owl:Nothing . The former is the most general class which contains everything (everything is a thing), the latter is the empty class. Thus every class is a subclass of owl:Thing and a superclass of owl:Nothing; in addition, a class may be equivalent to owl:Thing or owl:Nothing.Property Elements
In OWL there are two kinds of properties: User-defined data types will usually be collected in an XML schema, and then used in an OWL ontology.Here is an example of a property, expressed using owl:ObjectProperty <owl:ObjectProperty rdf:ID="isTaughtBy"> <rdfs:domain rdf:resource="#course"/> <rdfs:range rdf:resource="#academicStaffMember"/> <rdfs:subPropertyOf rdf:resource="#involves"/> </owl:ObjectProperty>More than one domain and range may be declared. In this case the intersection of the domains, respectively ranges, is taken.OWL allows us to relate "inverse properties", via owl:inverseOf . A typical example is isTaughtBy and teaches.<owl:ObjectProperty rdf:ID="teaches"> <rdfs:range rdf:resource="#course"/> <rdfs:domain rdf:resource="#academicStaffMember"/> <owl:inverseOf rdf:resource="#isTaughtBy"/> </owl:ObjectProperty> Actually domain and range can be inherited from the inverse property (interchange domain with range).Equivalence of properties can be defined using owl:equivalentProperty.<owl:ObjectProperty rdf:ID="lecturesIn"> <owl:equivalentProperty rdf:resource="#teaches"/> </owl:ObjectProperty>Property Restrictions
With rdfs:subClassOf we can specify a class C to be subclass of another class C ; then every instance of C is also an instance of C . Now suppose we wish to declare, instead, that the class C satisfies certain conditions, that is, all instances of C satisfy the conditions. Obviously it is equivalent to saying that C is subclass of a class C , where C collects all objects that satisfy the conditions. That is exactly how it is done in OWL, as we will show. Note that, in general, C can remain anonymous, as we will explain below.The following element requires first year courses to be taught by professors only (according to a questionable view, older and more senior academics are better at teaching).<owl:Class rdf:about="#firstYearCourse"> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#isTaughtBy"/> <owl:allValuesFrom rdf:resource="#Professor"/> </owl:Restriction> </rdfs:subClassOf> </owl:Class> owl:allValuesFrom is used to specify the class of possible values the property specified by owl:onProperty can take (in other words: all values of the property must come from this class). In our example, only professors are allowed as values of the property isTaughtBy.We can declare that mathematics courses are taught by staff member with ID 949358 as follows:<owl:Class rdf:about="#mathCourse"> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#isTaughtBy"/> <owl:hasValue rdf:resource="#949352"/> </owl:Restriction> </rdfs:subClassOf> </owl:Class> owl:hasValue states a specific value that the property, specified by owl: onProperty must have. And we can declare that all academic staff members must teach at least one undergraduate course as follows:<owl:Class rdf:about="#academicStaffMember"> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#teaches"/> <owl:someValuesFrom rdf:resource="#undergraduateCourse"/> </owl:Restriction> </rdfs:subClassOf> </owl:Class> Let us compare owl:allValuesFrom and owl:someValuesFrom . The example using the former requires every person who teaches an instance of the class, a first year subject, to be a professor. In terms of logic we have a universal quantification.The example using the latter requires that there exists an undergraduate course that is taught by an instance of the class, an academic staff member. It is still possible that the same academic teaches postgraduate courses, in addition. In terms of logic we have an existential quantification.In general, an owl:Restriction element contains a owl:onProperty element, and one or more restriction declarations. One type of restriction declarations are those that define restrictions on the kinds of values the property can take: owl:allValuesFrom, owl:hasValue and owl:someValuesFrom. Another type of restrictions are cardinality restrictions, expressed by owl:minCardinality or owl:maxCardinality . For example, we can require every course to be taught by at least someone.<owl:Class rdf:about="#course"> <rdfs:subClassOf> <owl:Restriction> <owl:onProperty rdf:resource="#isTaughtBy"/> <owl:minCardinality rdf:datatype="&xsd;nonNegativeInteger"> 1 </owl:minCardinality> </owl:Restriction> </rdfs:subClassOf> </owl:Class> Notice that we had to specify that the literal "1" is to be interpreted as a nonNegativeInteger (instead of, say, a string), and that we used the xsd namespace declaration made in the header element to refer to the XML Schema document.Similarly, one can declare an upper limit on the number of class elements (for instance, that a department must have at most thirty members) using owl:maxCardinality.It is possible to specify a precise number. For example, a PhD student must have exactly two supervisors. This can be achieved by using the same number in owl:minCardinality and owl:maxCardinality. For convenience, OWL offers also owl:cardinality .We conclude by noting that owl:Restriction defines an anonymous class which has no id, is not defined by owl:Class and has only a local scope: it can only be used in the one place where the restriction appears. When we talk about classes, please bare in mind the twofold meaning: classes that are defined by owl:Class with an id, and local anonymous classes as collections of objects that satisfy certain restriction conditions, or as combinations of other classes, as we will see shortly. The latter are sometimes called class expressions.Special Properties
Some properties of property elements can be defined directly:• owl:TransitiveProperty defines a transitive property, such as "has better grade than", "is taller than", "is ancestor of", etc. • owl:SymmetricProperty defines a symmetric property, such as "has same grade as", "is sibling of", etc. • owl:FunctionalProperty defines a property that has at most one unique value for each object, such as "age", "height", "directSupervisor", etc. • owl:InverseFunctionalProperty defines a property for which two different objects cannot have the same value, for example the property "isTheSocialSecurityNumberfor" (a social security number is assigned to one person only).An example of the syntactic form of the above is:<owl:ObjectProperty rdf:ID="hasSameGradeAs"> <rdf:type rdf:resource="&owl;TransitiveProperty" /> <rdf:type rdf:resource="&owl;SymmetricProperty" /> <rdfs:domain rdf:resource="#student" /> <rdfs:range rdf:resource="#student" /> </owl:ObjectProperty>Boolean Combinations
It is possible to talk about Boolean combinations (union, intersection, complement) of classes (be it defined by owl:Class or by class expressions). For example, we can say that courses and staff members are disjoint as follows:<owl:Class rdf:about="#course"> <rdfs:subClassOf> <owl:Restriction> <owl:complementOf rdf:resource="#staffMember"/> </owl:Restriction> </rdfs:subClassOf> </owl:Class>The owl:complementOf says that every course is an instance of the complement of staff members, that is, no course is a staff member. Note that this statement could also have been expressed using owl:disjointWith.The union of classes is built using owl:unionOf .<owl:Class rdf:ID="peopleAtUni"> <owl:unionOf rdf:parseType="Collection"> <owl:Class rdf:about="#staffMember"/> <owl:Class rdf:about="#student"/> </owl:unionOf> </owl:Class>The rdf:parseType attribute is a shorthand for an explicit syntax for building list with <rdf:first> and <rdf:rest> tags. Such lists are required because the built-in containers of RDF have a serious limitation: there is no way to close them, i.e. to say "these are all the members of the container". This is because, while one graph may describe some of the members, there is no way to exclude the possibility that there is another graph somewhere that describes additional members. The list syntax provides exactly this facility, but is very verbose, which motivates the rdf:parseType shorthand notation.Note that this does not say that the new class is a subclass of the union, but rather that the new class is equal to the union. In other words, we have stated an equivalence of classes. Also, we did not specify that the two classes must be disjoint: it is possible that a staff member is also a student.Intersection is stated with owl:intersectionOf.<owl:Class rdf:ID="facultyInCS"> <owl:intersectionOf rdf:parseType="Collection"> <owl:Class rdf:about="#faculty"/> <owl:Restriction> <owl:onProperty rdf:resource="#belongsTo"/> <owl:hasValue rdf:resource="#CSDepartment"/> </owl:Restriction> </owl:intersectionOf> </owl:Class> Note that we have built the intersection of two classes, one of which was defined anonymously: the class of all objects belonging to the CS department. This class is intersected with faculty to give us the faculty in the CS department. Further we note that Boolean combinations can be nested arbitrarily. The following example defines administrative staff to be those staff members that are neither faculty nor technical support staff.<owl:Class rdf:ID="adminStaff"> <owl:intersectionOf rdf:parseType="Collection"> <owl:Class rdf:about="\#staffMember"/> <owl:Class> <owl:complementOf> <owl:Class> <owl:unionOf rdf:parseType="Collection"> <owl:Class rdf:about="\#faculty"/> <owl:Class rdf:about="\#techSupportStaff"/> </owl:unionOf> </owl:Class> </owl:complementOf> </owl:Class> </owl:intersectionOf> </owl:Class>Enumerations
An enumeration is a owl:oneOf element, and is used to define a class by listing all its elements. <owl:Class rdf:ID="daysOfTheWeek"> <owl:oneOf rdf:parseType="Collection"> <owl:Thing rdf:about="\#Monday"/> <owl:Thing rdf:about="\#Tuesday"/> <owl:Thing rdf:about="\#Wednesday"/> <owl:Thing rdf:about="\#Thursday"/> <owl:Thing rdf:about="\#Friday"/> <owl:Thing rdf:about="\#Saturday"/> <owl:Thing rdf:about="\#Sunday"/> </owl:oneOf> </owl:Class>Instances
Instances of classes are declared as in RDF. For example: <rdf:Description rdf:ID="949352"> <rdf:type rdf:resource="#academicStaffMember"/> </rdf:Description> or equivalently:We can also provide further details, such as: <academicStaffMember rdf:ID="949352"> <uni:age rdf:datatype="&xsd;integer"> 39 </uni:age> </academicStaffMember> Unlike typical database systems, OWL does not adopt the unique names assumption, thus: just because two instances have a different name (or: ID), that does not imply that they are indeed different individuals. For example, if we state that each course is taught by at most one one staff member: <owl:ObjectProperty rdf:ID="isTaughtBy"> <rdf:type rdf:resource="&owl;FunctionalProperty" /> </owl:ObjectProperty> and we subsequently state that a given course is taught by two staff members:<course rdf:about="CIT1111"> <isTaughtBy rdf:resource="#949318"/> <isTaughtBy rdf:resource="#949352"/> </course> this does not cause an OWL reasoner to flag an error. After all, the system could validly infer that the resources "949318" and "949352" are apparently equal. To ensure that different individuals are indeed recognised as such, we must explicitly assert their inequality: <lecturer rdf:ID="949318"> <owl:differentFrom rdf:resource="#949352"> </lecturer> Because such inequality statements occur frequently, and the required number of such statements would explode if we wanted to state the inequality of a large number of individuals, OWL provides a shorthand notation to assert the pairwise inequality of all individuals in a given list using owl:AllDifferent: <owl:AllDifferent> <owl:distinctMembers rdf:parseType="Collection"> <lecturer rdf:about="#949318"/> <lecturer rdf:about="#949352"/> <lecturer rdf:about="949111"/> </owl:distinctMembers> </owl:AllDifferent> Note that owl:distinctMembers can only be used in combination with owl:AllDifferent.Datatypes
Although XML Schema provides a mechanism to construct user-defined datatypes (e.g. the datatype of adultAge as all integers greater than 18, or the datatype of all strings starting with a number), such derived datatypes cannot be used in OWL. In fact, not even all of the many the built-in XML Schema datatypes can be used in OWL. The OWL reference document lists all the XML Schema datatypes that can be used, but these include the most frequently used types such as string, integer, boolean, time and date.Versioning Information
We have already seen the owl:priorVersion statement as part of the header information to indicate earlier versions of the current ontology. This information has not formal model-theoretic semantics but can be exploited by humans readers and programs alike for the purposes of ontology management.Besides owl:priorVersion, OWL has three more statements to indicate further informal versioning information. None of these carry any formal meaning:• An owl:versionInfo statement generally contains a string giving information about the current version, for example RCS/CVS keywords. • An owl:backwardCompatibleWith statement contains a reference to another ontology. This identifies the specified ontology as a prior version of the containing ontology, and further indicates that it is backward compatible with it. In particular, this indicates that all identifiers from the previous version have the same intended interpretations in the new version. Thus, it is a hint to document authors that they can safely change their documents to commit to the new version (by simply updating namespace declarations and owl:imports statements to refer to the URL of the new version).• An owl:incompatibleWith on the other hand indicates that the containing ontology is a later version of the referenced ontology, but is not backward compatible with it. Essentially, this is for use by ontology authors who want to be explicit that documents cannot upgrade to use the new version without checking whether changes are required.Layering of OWL
Now that we have discussed all the language constructors of OWL, we can completely specify which features of the language can be used in which sublanguage (OWL Full, DL and Lite):OWL Full
In OWL Full, all the language constructors can be used in any combination as long as the result is legal RDF.OWL DL
In order to exploit the formal underpinnings and computational tractability of Description Logics, the following constraints must be obeyed in an OWL DL ontology:• Vocabulary partitioning: Any resource is allowed to be only either a class, a datatype, a datatype property, an object property, an individual, a data value or part of the built-in vocabulary, and not more than one of these. This means that, for example, a class cannot be at the same time an individual, or that a property cannot have some values from a datatype and some values from a class (this would make it both a datatype property and an object property). • Explicit typing: Not only must all resources be partitioned (as prescribed in the previous constraint), but this partitioning must be stated explicitly. For example, if an ontology contains the following:<owl:Class rdf:ID="C1"> <rdfs:subClassOf rdf:resource="#C2" /> </owl:Class> this already entails that C2 is a class (by virtue of the range specification of rdfs:subClassOf). Nevertheless, an OWL DL ontology must explicitly state this information: <owl:Class rdf:ID="C2"/>• Property separation: By virtue of the first constraint, the set of object properties and datatype properties are disjoint. This implies that inverse properties, and functional, inverse functional and symmetric characteristics can never be specified for datatype properties.• No transitive cardinality restrictions: No cardinality restrictions may be placed on transitive properties (or their superproperties, which are of course also transitive, by implication). • Restricted anonymous classes: Anonymous classes are only allowed in the domain and range of owl:equivalentClass and owl:disjointWith, and in the range (not the domain) of rdfs:subClassOf.OWL Lite
An OWL ontology must be an OWL DL ontology, and must further satisfy the following constraints:• The constructors owl:oneOf, owl:disjointWith, owl:unionOf, owl:complementOf and owl:hasValue are not allowed • Cardinality statements (both minimal, maximal and exact cardinality) can only be made on the values 0 or 1, and no longer on arbitrary non-negative integers.• owl:equivalentClass Statements can no longer be made between anonymous classes, but only between class identifiers (Figs. OWL DL has a formal model-theoretic semantics ≤ n U Fig. Fig. 3. OWL DL axioms and facts
The decidability of the logic ensures that sound and complete DL reasoners can be built to check the consistency of an OWL ontology, i.e. verify whether there are any logical contradictions in the ontology axioms. Furthermore, reasoners can be used to derive inferences from the asserted information, e.g. infer whether a particular concept in an ontology is a subconcept of another, or whether a particular individual in an ontology belongs to a specific class.At the time of this writing, an effort is underway to define another sublanguage, sometimes referred to as RDFS+ and other times as OWL Very Lite, which is intended to be a much simpler version that provides only simple reasoning extensions to RDFS to allow for very efficient scalability.OWL DLP
As explained above, OWL is based on Description Logic. Since this is a fragment of FOL, it inherits many of the properties of that logic. In particular, it inherits the open-world assumption and the non-unique name assumption. This has lead to the definition of an interesting sublanguage of OWL DL, named OWL DLP. OWL DLP is not part of the official W3C OWL-species layering, but is nevertheless sufficiently interesting to deserve some discussion here.The open-world assumption says that we cannot conclude some statement x to be false simply because we cannot show x to be true. After all, our axioms may be simply non-committal on the status of x. In other words, we may not deduce falsity from a the absence of truth. The opposite assumption (closed-world assumption, CWA) would allow to derive falsity from the inability to derive truth. The choice between open world semantics and closed world semantics is a recurring issue in the design of web-based KR languages. Although in general the Web would seem more suited to open-world reasoning (and indeed both RDFS and OWL adopt an open-world semantics), there are many use-cases where a closed-world semantics is appropriate: students in a class, customers of a company, cities in a country are all examples of closed sets: if a student is not listed as enrolled, we can safely assume she is not enrolled. Although useful in many cases, there is currently no practical mechanism in RDFS or OWL to state that a given set of individuals (or facts) is "closed". The only (limited) possibility is to empose an owl:maxCardinality constraint on a role.The unique name assumption has been discussed above: if we encounter two individuals with different names, we can safely assume they are indeed different individuals. Typically, database systems assume a single, unique name for each individual. Again, on the web this assumption would be too strong. In a world as large as the web, many individuals are known under multiple names ("Jim Hendler", "James Hendler", "Prof. J. Hendler", "the co-chair of the OWL Working Group", etc.). When encountering two such different names, we should safely assume that they may or may not designate the same individual, until further reasoning decides the issue one way or the other. OWL contains a simple device to state that all individuals in an enumerated set are known to be different (i.e. that they are not just different names for some of the same individuals), but this language construct (owl :AllDifferent) requires the explicit enumeration of these names, which can be either impractical, or even impossible in principle.Hence, for both the CWA and the UNA, ontologies are sometimes in need of one, and sometimes in need of the other. This conundrum was nicely resolved in Fortunately, DLP is still large enough that it can be used for useful representation and reasoning tasks. It allows the use of such OWL constructors as class and property equivalence, equality and inequality between individuals, These constructors do not only allow useful expressivity for many practical cases, while guaranteeing correct interchange between OWL reasoners independent of their CWA and UNA, they also allow for a translation into efficiently implementable reasoning techniques based on databases and logic programs.As is already clear from the above two points, RDFS and OWL do not allow any form default reasoning, even though many years of KR applications have shown this to be a very useful device for dealing with incomplete knowledge. This would be particularly important in a world as large as the Web, where not all properties of all objects will be explicitly known, but must often be inferred by default until shown otherwise. However, a lack of concensus in the KR community on how to best formalise defaults has prevented such features to be included in the Semantic Web standardised representation languages.Finally, a point often raised is that the large and open world of the Web will almost certainly need some forms of uncertainty and fuzziness. Again, lack of concensus has prevented such language features to be included, although it would seem clear that they will ultimately be needed in some form or other, either in the representation or in the inference mechanisms.Summary
OWL is the proposed standard for Web ontologies. It allows us to describe the semantics of knowledge in a machine-accessible way. OWL builds upon RDF and RDF Schema: (XML-based) RDF syntax is used; instances are defined using RDF descriptions; and most RDFS modelling primitives are used. Formal semantics and reasoning support is provided through the mapping of OWL on logics. Predicate logic and description logics have been used for this purpose. While OWL is sufficiently rich to be used in practice, extensions are in the making. They will provide further logical features, including rules.Introduction
The Web Ontology Language OWL, as introduced in chapter "Web Ontology Language: OWL," is the language recommended by the World Wide Web consortium (W3C) for expressing ontologies for the Semantic Web. OWL is based on Description Logics, see chapter "Description Logics," and as such is based on first-order predicate logic as underlying knowledge representation and reasoning paradigm.Throughout the advent of the Semantic Web, however, F-Logic as an alternative approach for expressing ontologies has been based on rules, more precisely on the logic programming paradigm, see chapter "Ontologies in F-Logic." Due to this, and also due to the importance of rule-based systems in industrial practice, it is natural to ask how OWL and rule-based knowledgerepresentation and reasoning can be combined.Achieving a conceptually clear integration of OWL and rules, however, is not a trivial task. The reason for the difficulties lies in the fact that OWL -based on first-order predicate logic -adheres to the openworld assumption, while rules generally follow the closed-world assumption. Consequently, the semantics of OWL and rules differs considerably, and achieving a meaningful, intuitive, and formally clear combined semantics is not straightforward.Research on the topic of ontologies and rules has thus spawned into several different directions. In this chapter, we provide an overview of the state of the art, by discussing briefly some of the approaches which we consider to be most promising or interesting. Since we want to be brief and to the point, our selection is naturally subjective, but we made an effort to include references to many of the recent publications on the topic.In Sect. 2, we present the Semantic Web Rules Language SWRL, a rule extension of OWL DL which adheres to the open-world paradigm, and is thus entirely in the spirit of the OWL DL language. We also introduce the decidable fragment of SWRL known as DL-Safe rules. SWRL adds to the expressive power of OWL by allowing the modelling of certain axioms which lie outside the capability of OWL DL.In Sect. 3, we change perspective and consider rule fragments of OWL DL. In particular, we present its naive Horn fragment DLP -description logic programs -and a more sophisticated fragment called Horn-SHIQ, which encompasses DLP.In Sect. 4, we briefly address hybrid approaches, i.e. theories and systems which combine OWL with some existing rules language, and thus combine the open-and the closed-world assumption.We conclude in Sect. 5.Throughout the chapter, we will employ the syntax for Description Logics as introduced in chapter "Description Logics."SWRL and DL-Safe Rules
Attempts to combine some sort of rules with a description logic go back at least as far as the Classic system In general, when considering combination formalisms, the upper bound is the unrestricted union of the two systems. In essence, the Semantic Web Rule Language (SWRL)Decidability can be regained with the imposition of a safety condition on SWRL rules. Essentially, the possible values of (explicit) variables in SWRL rules are restricted to named individuals only which confines the direct effects of such rules to the ABox. This safety condition is known as "DL-Safety" and such SWRL rules are generally called "DL-Safe SWRL rules" or "DL-Safe rules". Not only are DL-Safe rules (in combination with OWL-DL) decidable, but reasonable implementations are emerging.SWRL was first proposed under the name OWL Rules Language Definition of SWRL
SWRL contains OWL DL as a proper part, that is, all OWL DL axioms are SWRL axioms. Additionally, a SWRL knowledge base may contain a set of rules, which consist of an antecedent (body) and a consequent (head) which themselves are sets of SWRL atoms.A SWRL atom may be of the following• Unary atoms:where C is an arbitrary OWL DL class expression D(dataArg1) where D is a datatype URI or an enumerated value range • Binary atoms: P (arg1, arg2) where P is an object property Q(arg1, dataArg1) where Q is a datatype property arg1 = arg1 equality, or "sameAs" arg2 = arg2 inequality, or "differentFrom"Where arguments are of the form:arg1 | arg2 these are either individuals denoting URIs or individual ranging variables dataArg1 these are data literals or data value ranging variables A SWRL rule is of the form:where atoms 1 through n form the antecedent (i.e. body) and atoms n + 1 through m form the consequent (i.e. the head).The semantics of SWRL are traditionally given via an extension of the "direct" model theory for OWL DL. In the table below, X and Y are meta-linguistic variables that range over constants (i.e. names of individuals) or object variables (i.e. x, y, and z). A represents an arbitrary atomic concept, C and D represent arbitrary class expressions, a and b represent arbitrary constants, and P represents an arbitrary role:It is clear that, except for transitivity axioms, SH can be encoded into first order logic using only two distinct variables (i.e. x and y) and transitivity only needs one additional variable (i.e. z). To extend this translation to a SWRL extension of SH, we need an arbitrary supply of variables, that is, at least enough for each distinct variable in the rule with the largest number of distinct variables in a given rule set. To help distinguish translations of SH axioms from translations of SWRL rules, we shall use capital letters from {X, Y, Z} with subscripts when necessary.Letare all the SWRL variables in the atoms ψ 1 . . . ψ m . Then we can extend the translation function, π, as follows (note that the metavariables X and Y now also range over SWRL variables):Where a ψ is of the form A(X ) or P (X , Y)One immediate consequence of the translation is that it is obvious that SWRL rules can entirely replace the role axioms of SH:∀x, y, z(P (x, y) ∧ P (y, z)Clearly, replacing the variables in the first order translation of the SH axioms with their uppercased versions results in the first order translation of the SWRL rule. Thus SH+SWRL is trivially reducible to ALC+SWRL. Of course, concept inclusion axioms may be encoded as SWRL rules analogously to how property inclusions are (at least, if we allow concept expressions as the functor of atoms; otherwise, we need at least some definitions). If, as the SWRL submission does, we permit SWRL rules with empty antecedents, then we can assimilate the ABox axioms (i.e. type, property, and (in)equality assertions) as well.These reductions, as observed in While we know of no user study comparing the usability of explicit rules to Description Logic style variable free axioms (and we believe that this would be a very difficult study to conduct), there is no shortage of claims that one style or the other is more usable, transparent, or easy to learn. For example, some researchers working on controlled natural languages (CNLs) for OWL argue that it is better to produce a CNL sentence corresponding to the SWRL rule (including explicit variables!) for transitivity instead of using the more succinct "Property P is transitive". At least one trade off seems clear: Description Logic "variable free" style is less cluttered with (clearly redundant!) variables, whereas SWRL style is probably initially more comfortable for people familiar with rule languages such as Prolog or with first order logic or with query languages like SQL. The lack of "visual noise" for standard Description Logic axioms (i.e. concept and property subsumption, transitivity, inverse, etc.) seems decisive except for explication purposes. Similarly, the operator style syntax of Description Logics makes it easier to manage complex nested expressions. This suggests that there may be user advantages to continuing to adopt more expressive role constructors (such as the expanded role composition operator in OWL 2).The undecidability of SHOIN +SWRL can easily be seen by the fact that it is possible to encode the transitivity of an otherwise simple role using a SWRL rule. Transitive roles in number restrictions are a well known source of undecidability for SHIQ (e.g. see We have seen that we can see SWRL as a very straightforward generalisation of description logics. The addition of arbitrary variables to conditionals brings in a lot of expressive power. However, SWRL is a somewhat strange fragment of first order logic. It lacks, at least, n-ary predications and function symbols, but it nevertheless is only semi-decidable. Of course, pure Prolog is another case of an undecidable fragment of first order logic, but there the question of "Why not move to full first order logic?" has a well known answer: Prolog has a proof procedure which gives rise to a powerful, reasonably efficient in the common case, and understandable performance model for computations. In fact various compromises (such as omitting the occurs check) have well understood performance (and soundness/completeness) tradeoffs. Unfortunately, there is no such body of knowledge about SWRL yet.Definition of DL-Safe SWRL
The free syntax of SWRL (e.g. conjunctions in the head and arbitrary concept expressions as atoms) helps emphasis the similarities of SWRL rules with Description Logic axioms, but it obscures the relation between SWRL rules and rules systems based on Horn clauses, such as Datalog. However, it is easy enough to transform an arbitrary SWRL knowledge base (consisting of a rules part and a Description Logic part) into a form where all the SWRL rules consequents contain only a single conjunct and all the atoms are atomic. (Essentially, definitions are added to the description logic part that replace each complex class atom with a fresh atomic atom, and conjunctive consequences are eliminated via the Lloyd-Topor transformations It is clear from the translation that the DL-Safe version of the SWRL rule is much weaker than the unrestricted version. Consider the following simple example (adapted from If we interpret < burn1, leg1 > : locatedIn (10) burn1 : LegInjury Clearly, with DL-Safety, rules can still interact with Description Logic axioms, but only through new ground facts. It is because we infer (10) (via ( Built-ins
This more programmatic feel is enhanced by the presence of built-in atoms, that is, atoms with a fixed, predefined interpretation. The SWRL submission includes built-ins for value comparison, mathematics, and string manipulation among others. There are several issues with built-ins with perhaps the most prominent being how they are to be interpreted if their variables are under-instantiated when being evaluated (that is, whether they should be interpreted as arbitrary constraints or more procedurally, e.g. "throwing" an error when an binding is of an inappropriate type or must be drawn arbitrarily from the domain rather than determined by the knowledge base). However, SWRL built-ins seem fairly popular: at least they are reported as desired, as a supplement to OWL's datatype facility. In spite of this clamor, it is unclear what the right semantics of built-ins (or even just data property atoms) should be, in part due to the fact that the most natural reading of data property atoms (and built-ins) is as general constraints, whereas many users talk about built-ins as if they were procedural attachments.Rule Fragments of OWL
Description Logic Programs
DLP are a naive Horn fragment of OWL. They inherit their semantics from OWL, thus adhering to the open world assumption. At the same time, DLPs can be transformed syntactically into Logic Programming syntax, and thus provide a kind of basic interoperability between OWL and Logic Programming. Let it be noted, though, that DLP is not a common fragment of OWL and Logic Programming, because the two semantics normally considered are different. Nevertheless, the two semantics have a clearly understood and strong relationship, about which we will talk later in Sect. 3.2.Compared to OWL DL, DLP is a rather primitive ontology language. It strictly contains, however, the OWL DL fragment of RDFS (see chapter "Resource Description Framework.") and has the pleasing property of having polynomial data complexity. It is thus one of the tractable OWL fragmentsDefinition of DLP
Originally, DLP was presented as fragment of OWL. We present DLP in an alternative and more constructive way, which exposes the modeling capabilities and the limitations of it. It can be considered a kind of normal form for DLP.We need to fix terminology first. We understand DLP as a semantic fragment of OWL, i.e. we abstract (for the time being) from a concrete syntax: Every OWL statement which is semantically equivalent -in the sense of first order logic -to a (finite) set of function-free Horn clauses constitutes a valid DLP statement. • ABox:where the following apply.-For DLP we allow Left j to be of the forms C, {o 1 , . . . , o n }, ⊥ or , and Right to be of the forms C or . -For DLP IC we allow Left j to be of the forms C, {o 1 , . . . , o n }, ⊥, or , and Right to be of the form C, , or ⊥. -For DLP ICE we allow Left j to be of the forms C, {o 1 , . . . , o n }, ⊥, or , and Right to be of the form C, , ⊥, or {o}. -For the DLP + versions we furthermore allow Right to be of the form ∃R (-) .{a}. The superscript (-) shall indicate that an inverse symbol may occur in these places. Note that (by a common abuse of notation) we allow any of k, m i , n to be zero. For k = 0 the left hand side becomes . Note also that we could have disallowed ⊥ on the left and on the right, since in either case the statement becomes void. Likewise, it would suffice to require n = 0 in all cases, since universal quantifiers on the right are expressable using existentials on the left. Disallowing the existential quantifiers on the left (while keeping universals on the right) is also possible, but at the expense of the introduction of an abundance of new concept names. As an example, note that ∃R.C ∃Q.D E would have to be translated into the set of statements {C 1 D 1 E, C ∀R -.C 1 , D ∀Q -.D 1 }, where C 1 and D 1 are new concept names. Our representation is more compact.Using any of the established syntaxes of the OWL language, an OWL axiom is said to be in DLP if its translation into Description Logic syntax results in a finite set of statements of the above mentioned form.An Example
We give a small example ontology which displays the modeling expressivity of DLP:For the TBox, we model the following sentences(1) Every man or woman is an adult (2) A grown-up is a human who is an adult (3) A woman who has somebody as a child, is a mother (4) An orphan is the child of humans who are dead (5) A lonely child has no siblings (6) AIFB researchers are employed by the University of Karlsruhe They can be written in DL syntax as follows -the axioms actually constitute an ALCIO TBox (see chapter "Description Logics"). We note that for (5) we require DLP IC, while for (6) we require DLP + . As an example for an RBox, we use the following. We therefore consider it to be part of the ABox. To be precise, the original statement actually is (syntactically) not in OWL Lite, but the equivalent set of three ABox statements is. The statement Ian = Horrocks requires DLP ICE. Note also that class inclusions cannot in general be replaced by equivalences. For example, the statement Adult Man Woman is not in DLP.Relation to Logic Programming
A DLP knowledge base can be translated into Horn logic, and the latter can be expressed using Logic Programming syntax. To be more precise, DLP translates syntactically into Datalog. Let us first continue our example, giving the knowledge base in Datalog form.The TBox is as follows.Adult(y) ← Man(y)
(1)Human(y) ← GrownUp(y) ( Mother(y) ← childOf(x, y) ∧ Woman(y)Translating the RBox yields the following statements.Translated as such, DLP knowledge bases can also be evaluated under Datalog semantics, which differs from the OWL semantics. The two semantics, however, coincide on the set of inferred instances of named classes, i.e. ABox reasoning in DLP can be done using Datalog semantics. The formal relationship between the two semantics is as follows.Theorem 1. Let K be a DLP knowledge base and let K be the translation of K into Datalog syntax. Let C be a named class and a be a named individual. Then K |= OWL C(a) under the OWL semantics iff K |= Datalog C(a) under the Datalog semantics. Horn-SHIQ
Horn-SHIQ is another Horn fragment of OWL DL, which encompasses DLP. To be precise, Horn-SHIQ has a feature which lies outside OWL DL, namely the use of qualified number restrictions. Horn-SHIQ has the pleasing property that it is of tractable (i.e. polynomial) data complexity The original definition of Horn-SHIQ, due to Definition of Horn-SHIQ
The following definition is taken from We say that a SHIQ axiom C D is Horn if the concept expression ¬C D has the form C + 1 as defined by the context-free grammar in Table It is easily seen by referring to the definition on page 120, that DLP IC is indeed contained in Horn-SHIQ. Just note that the use of {o 1 , . . . , o n } on the Left is removed by extensionally reducing the ABox. Intuitively speaking, Horn-SHIQ adds the free use of role restrictions to DLP, as, e.g. existential restriction can be used freely. An Example
As an example for a Horn-SHIQ knowledge base, consider the ontology in Table Relation to Logic Programming
Horn-SHIQ can be translated into Datalog syntax, i.e. it can be understood as a rule fragment of SHIQ. We do not have the space to detail the underlying KAON2 translation algorithms, and refer to chapter "Resolution-Based Reasoning for Ontologies" and Theorem 2. Let K be a Horn-SHIQ knowledge base and let D(K) be the transformation of K into Datalog syntax resulting from the application of the KAON2 transformation algorithms. Then the following hold. In order to exemplify the mentioned transformation, we give a translation of the ontology from Table The rules in the lower part of Table The resulting program now allows us to conclude several ABox statements. For example, we can derive that "parent(Elaine)" and that "Sir Lancelot ≈ Lancelot du Lac".  Hybrid Approaches
In Sect. 2, we discussed the SWRL rule extension of OWL, which basically follows the design principles of OWL DL by adhering to the open world assumption and by being semantically based on first-order predicate logic. In Sect. 3, we discussed rule languages which are fragments of OWL DL, and thus inherit its semantics.We have focussed on these perspectives for several reasons:• They align with OWL DL semantically in a very natural way • They are supported by some of the most prominent and powerful OWL reasoning engines • They appear to be least disputed as to their importance for ontology modeling At the same time, there is a multitude of proposals for hybrid systems which comprise both classical OWL reasoning and traditional rule-based approaches like logic programming in different variants. The quest for such hybrid solutions is currently still ongoing, but is not yet close to a conclusion. We briefly discuss the two approaches which we consider to be most mature at this stage.The first of these is called Hybrid MKNF knowledge bases due to The second approach is based on an integration of OWL DL reasoning with Answer Set Programming, which has been realised as the dlvhex system Many additional approaches are currently being investigated and proposed, ranging from tightly integrated ones to loosely coupled systems. We list recent references as a starting point for the interested reader Conclusions
Our discussion of ontologies and rules is centered on two specific paradigms of "ontology" formalisms and rule formalisms: Description Logics and logic programming (though we have only lightly touched on issues with various forms of non-monotonicity and similar core features of logic programming systems). While arguably these are both very prominent, one might say dominant, formalisms in the ontology engineering communities today -and there is an enormous amount of work on their combination as we have seen -historically, this was not always the case. For example, if we consider expert systems of the 1970s and 1980s such as Mycin Integrating logic programming and business rules, much less description logics, has proven to be challenging. This is rather surprising given the obvious parallels between Prolog and OPS5 style rules and the natural thought that use of forward chaining inference methods -or more specifically variants of the Rete algorithm -is irrelevant to the semantics of the rules. It seems very natural to think that a (simple) relational database with some (simple) event-condition-action (ECA) rules is equivalent to that relational database with some Datalog rules. However, a database administrator may have reason to prefer that the ECA rule evaluation modified the database. In fact, that may have been a key feature of those rules: consider a situation where one wished to validate certain inputs but only at input time, that is, subsequent operations are licensed to violate the validation criteria. In such contexts, the notion of "assert" and "retract" have representational significance. If you add, in the action language, the ability to execute code fragments written in a programming language, it seems clear that there is a fundamental divergence. Unfortunately, unlike with Description Logics and logic programming, we do not currently have a well established common semantic framework (i.e. modal theory) ready to hand to aid us with integration.Introduction
Ontologies constitute valuable assets that are slowly, but continuously gaining recognition and use throughout a set of disciplines -as becomes visible in Part C of this book. Ontologies frequently being a complex asset, their creation and management does neither come by coincidence nor does it come for free. Rather, the objectives pursued with their development as well as the development itself must be critically assessed by the organization or -rarely -the individual who is pushing for their creation and maintenance. The discipline that investigates the principles, methods and tools for initiating, developing and maintaining ontologies is "ontology engineering" which is the topic of this part of the handbook. "Ontology engineering methodology" as a part of ontology engineering deals with the process and methodological aspects of ontology engineering, i.e. with the issues of how to provide guidelines and advice to (potential) developers of ontologies.It is the purpose of this chapter to introduce a rather generic ontology engineering methodology to the reader and to indicate where this methodology links to more specific topics discussed mostly, but obviously not completely, in the remainder of part B of this handbook. Such as software engineering methodologies cannot be described in isolation from actual software engineering activities, the purpose of ontology engineering methodologies can only be understood in the context of actual ontology engineering experiences.The methodology presented here, has been derived from several case studies of building and using ontologies in the realm of knowledge management. Knowledge management deals with the thorough and systematic management of knowledge within an enterprise and between several cooperating enterprises. Knowledge management is a major issue for human resource management, enterprise organization and enterprise culture -nevertheless, information technology (IT) constitutes a crucial enabler for many aspects of knowledge management and ontologies frequently turn out to be valuable assets for knowledge management in order to target core knowledge management issues such as search, information integration, or mapping of knowledge assets. As a consequence, knowledge management is an inherently interdisciplinary subject and ontologies used for knowledge management play a central role, but at the same time they are by no means the single factor to determine success or failure of the overall system. Thus, we may derive our rationale that the objective of knowledge management constitutes a typical, yet comprehensive blueprint for issues that arise when developing complex ontologies. Therefore, we have chosen the knowledge management setting described below in order to report on a generic ontology engineering methodology.IT-supported KM solutions are frequently built around some kind of organizational memory There exist various proposals for methodologies that support the systematic introduction of KM solutions into enterprises and with it the construction of ontologies. A classical approach for introducing knowledge management systems -including ontologies -is CommonKADS that puts emphasis on an early feasibility study as well as on constructing several models that capture different kinds of knowledge needed for realizing a KM solution Re-engineering earlier approaches, we found that methodologies must distinguish two processes in order to achieve a clear identification of issues The generic methodology presented here has been developed and applied in the EU project On-To-KnowledgeImplementation and Launch of KM Applications
To implement and launch a KM application, one has to consider different processes (cf. Fig. Human issues (HI) and the related cultural environment of organizations heavily influence the acceptance of KM. It is often mentioned in discussions that the success of KM -and especially KM applications -strongly depends on the acceptance by the involved people. As a consequence, "quick wins" are recommended for the initial phase of implementing any KM strategy. The aim is to quickly convince people that KM is useful for them and adds value to their daily work.Software engineering (SE) for knowledge management applications has to accompany the other processes. The software requirements coming from the knowledge processes need to be reflected in the planning and management of the overall system design and implementation.In the following sections we will now focus on the Knowledge Meta Process as the core process of ontology engineering and we will mention some crosslinks to the other processes as well as to more specific ontology engineering issues.Knowledge Meta Process
The Knowledge Meta Process (cf. Fig. Feasibility Study
Any knowledge management system may function properly only if it is seamlessly integrated in the organization in which it is operational. Many factors other than technology determine success or failure of such a system. To analyze these factors, we initially start with a feasibility study Considering ontology engineering specifically, there is a need to consider the return on investment of developing ontologies as an asset. So far, the accounting of ontology as value assets has not been undertaken to our knowledge. Methods of accounting other intangible assets, such as Kickoff
In the kickoff phase the actual development of the ontology begins. Similar to requirements engineering and as proposed by Valuable knowledge sources may include text documents or available relational data. The knowledge contained in such data sources, and particularly in text, may be unlocked by means of ontology learning methods (cf. chapter "Ontology and the Lexicon"). A specific techniques, which is sometimes used for ontology learning, is the analysis of concept properties allowing for the derivation of hierarchical relationships by means of formal concept analysis (cf. chapter "Formal Concept Analysis").The outcome of this phase is (beside the ontology requirement specification document (ORSD)) a semi-formal description of the ontology, i.e. a graph of named nodes and (un-)named, (un-)directed edges, both of which may be linked with further descriptive text, e.g. in form of mind maps (cf. Refinement
During the kick-off and refinement phase one might distinguish in general two concurrent approaches for modeling, in particular for refining the semi-formal ontology description by considering relevant knowledge sources: top-down and bottom-up. In a top-down-approach for modeling the domain one starts by modeling concepts and relationships on a very generic level. Subsequently these items are refined. This approach is typically done manually and leads to a high-quality engineered ontology. Available top-level ontologies (cf. chapter "Foundational Choices in DOLCE") may here be reused and serve as a starting point to develop new ontologies. In our example scenario we encountered a middle-out approach, i.e. to identify the most important concepts which will then be used to obtain the remainder of the hierarchy by generalization and specialization. However, with the support of an automatic document analysis (cf. chapter "Ontology and the Lexicon"), a typical bottom-up-approach may be applied. There, relevant concepts are extracted semi-automatically from available documents. Based on the assumption that most concepts and conceptual structures of the domain as well the company terminology are described in documents, applying knowledge acquisition from text for ontology design helps building ontologies automatically.To formalize the initial semi-formal description of the ontology into the target ontology, ontology engineers firstly form a taxonomy out of the semiformal description of the ontology and add relations other than the "is-a" relation which forms the taxonomical structure. The ontology engineer adds different types of relations as analyzed, e.g. in the competency questions to the taxonomic hierarchy. However, this step is cyclic in itself, meaning that the ontology engineer now may start to interview domain experts again and use the already formalized ontology as a base for discussions. It might be helpful to visualize the taxonomic hierarchy and give the domain experts the task to add attributes to concepts and to draw relations between concepts (e.g. we presented them the taxonomy in form of a mind map as mentioned in the previous section). The ontology engineer should extensively document the additions and remarks to make ontological commitments made during the design explicit. The application of design patterns for ontologies (cf. chapter "Ontology Design Patterns") may greatly improve the efficiency and effectiveness of the process as well as the quality of the ontology.The outcome of this phase is the "target ontology", that needs to be evaluated in the next step. The major decision that needs to be taken to finalize this step is whether the target ontology fulfills the requirements captured in the previous kickoff phase. Typically an ontology engineer compares the initial requirements with the current status of the ontology. This decision will typically be based on the personal experience of ontology engineers. As a good rule of thumb we discovered that the first ontology should provide enough "flesh" to build a prototypical application. This application should be able to serve as a first prototype system for evaluation.Evaluation
We distinguish between three different types of evaluation: (1) technologyfocussed evaluation, (2) user-focussed evaluation and (3) ontology-focused evaluation.Our evaluation framework for technology-focussed evaluation consists of two main aspects: (1) the evaluation of properties of ontologies generated by development tools, (2) the evaluation of the technology properties, i.e. tools and applications which includes the evaluation of the evaluation tool properties themselves. In an overview these aspects are structured as follows: (1) Ontology properties (e.g. language conformity (Syntax), consistency (Semantics)) and (2) technology properties (e.g. interoperability, turn around ability, scalability etc.).The framework shown above concentrates on the technical aspects of ontologies and related ontologies. However, the aspect of user-focussed evaluation remains open. The most important point from our perspective is to evaluate whether users are satisfied by the KM application. More specific, whether an ontology based application is at least as good as already existing applications that solve similar tasks.Beside the above mentioned process oriented and pragmatic evaluation methods, one also need to formally evaluate ontologies. One of the most prominent approaches here is the OntoClean approach (cf. chapter "An Overview of OntoClean"), which is based on philosophical notions. Another well-known approach (cf. chapter "Ontology Engineering Environments") takes into account the normalization of an ontology. Applying such approaches helps avoiding common modelling errors and leads to more correct ontologies.The outcome of this phase is an evaluated ontology, ready for the roll-out into a productive system. However, based on our own experiences we expect in most cases several iterations of "Evaluation-Refinement-Evaluation" until the outcome supports the decision to roll-out the application. The major decision that needs to be taken for finalizing this phase is whether the evaluated ontology fulfills all evaluation criteria relevant for the envisaged application of the ontology.Application and Evolution
The application of ontologies in productive systems, or, more specifically, the usage of ontology based systems, is being described in the following Sect. 4 that illustrates the knowledge process.The evolution of ontologies is primarily an organizational process. There have to be rules to the update, insert and delete processes of ontologies (cf. A current topic for research and practice is the use of evolutionary knowledge management technologies that frequently build on Web2.0 technology and that decentralize the responsibility of knowledge management processes and meta processes to the individuals in the (virtual) organization with a corresponding need to decentralize ontology engineering (cf. The outcome of an evolution cycle is an evolved ontology, i.e. typically another version of it. The major decision to be taken is when to initiate another evolution cycle for the ontology.Knowledge Process
Once a KM application is fully implemented in an organization, knowledge processes essentially circle around the following steps (cf. Fig. • Knowledge creation and/or import of documents and meta data, i.e. contents need to be created or converted such that they fit the conventions of the company, e.g. to the knowledge management infrastructure of the organization.• then knowledge items have to be captured in order to elucidate importance or interlinkage, e.g. the linkage to conventionalized vocabulary of the company by the creation of relational metadata. • retrieval of and access to knowledge satisfies the "simple" requests for knowledge by the knowledge worker; • typically, however, the knowledge worker will not only recall knowledge items, but she will process it for further use in her context. We now give an example of the Knowledge Meta Process instantiation of a skills management case study at Swiss Life (cf. Feasibility Study
For identifying factors which can be central for the success or failure of the ontology development and usage we made a requirement analysis of the existing skills management environment and evaluated the needs for a new skills management system. We identified mainly the human resources department and the management level of all other departments as actors and stakeholders for the skills management. After finding the actors and stakeholders in the skills management area, we named the ontology experts for each department, which are preferably from the associated training group of each department.Kickoff
The departments private insurance, human resources and IT constitute three different domains that were the starting point for an initial prototype.The task was to develop a skills ontology for the departments containing three trees, viz. for each department one. The three trees should be combined under one root with cross-links in between. The root node is the abstract concept "skills" (which means in German "Kenntnisse/Faehigkeiten") and is the starting point to navigate through the skills tree from the top.During the kickoff phase two workshops with three domain expertsDuring this stage a lot of "concept islands" were developed, which were isolated sets of related terms. These islands are subdomains of the corresponding domain and are self-contained parts like "operating systems" as sub domain in the IT domain. After developing these concept islands it was necessary to combine them into a single tree. This was a more difficult part than assembling the islands, because the islands were interlaced and for some islands it was possible to add them to more than one other island, which implies awkward skills trees that contain inconsistencies after merging. For each department one skills tree was built in separate workshops. A problem that came up very early was the question where to draw the line between concepts and instances. E.g. is the programming language Java instantiated by "jdk1.3" or is "jdk1.3" so generic that it still belongs to the concept-hierarchy? Another problem was the size of the ontology. What is the best depth and width of each skills tree? Our solution was, that it depends on the domain and should be determined by the domain expert.As result of the kick-off phase we obtained the semi-formal ontology descriptions for the three skills trees, which were ready to be formalized and integrated into a single skills ontology. At this stage the skills trees reached a maturity that the combination of them caused no major changes for the single skills trees.Refinement
During the refinement phase we formalized and integrated the semi-formal ontology descriptions into a single coherent skills ontology. An important aspect during the formalization was (1) to give the skills proper names that uniquely identify each skill and (2) to decide on the hierarchical structure of the skills. We discussed two different approaches for the hierarchical ordering: we discovered that categorization of skills is typically not based on an is-a-taxonomy, but on a much weaker hasSubtopic relationship that has implications for the inheritance of attached relations and attributes. However, for our first prototype this distinction made no difference due to missing crosstaxonomical relationships. But, according to In a second refinement cycle we added one more relation type, an "associative relation" between concepts. They express relations outside the hierarchic skills tree, e.g. a relation between "HTML" and "JSP", which occur not in the same tree, but correspond with each other, because they are based on the same content. "HTML" is in the tree "mark-up languages", while the tree "scripting languages" contains "JSP". This is based on the basic characteristics and the history of both concepts, which changed over time. But in reality they have a close relationship, which can be expressed with the associative relation.The other task in this phase was to integrate the three skills ontologies into one skills ontology and eliminate inconsistencies in the domain ontology parts and between them. Because the domain ontologies were developed separately, the merger of them caused some overlaps, which had to be resolved. This happened for example in the computer science part of the skills trees, where the departments IT and private insurance have the same concepts like "Trofit" (which is a Swiss Life specific application). Both departments use this concept, but each uses a different view. The IT from the development and the private insurance from the users view. Additionally the personal skills of any employee are graded according to a generic scale of four levels: basic knowledge, practical experience, competency, and top specialist. The employees will grade their own skills themselves. As known from personal contacts to other companies (e.g. Credit Suisse, ABB and IBM), such an approach proved to produce highly reliable information.As a result at the end of the refinement phase the "target skills ontology" consisted of about 700 concepts, which could be used by the employees to express their skill profile.Application and Evolution
The evaluation of the prototype and the underlying ontology was unfortunately skipped due to internal restructuring at Swiss Life which led to a closing down of the whole case study.Still, we considered the following aspects for the evolution of our skills management application: The competencies needed from employees are a moving target. Therefore the ontologies need to be constantly evaluated and maintained by experts from the human resource department. New skills might be suggested by the experts themselves, but mainly by employees. Suggestions include both, the new skill itself as well as the position in the skills tree where it should be placed. While employees are suggesting only new skills, the experts decide which skills should change in name and/or position in the skills tree and, additionally, decide which skill will be deleted. This was seen as necessary to keep the ontology consistent and to avoid that, e.g. similar if not the same concept appear even in the same branch. For each ontology (and domain) there should exist a designated ontology manager who decides if and how the suggested skill is integrated.Related Work on Methodologies
A first overview on methodologies for ontology engineering can be found in CommonKADS The Enterprise Ontology TOVE METHONTOLOGY More recently, the DILIGENT methodology has been developed that addresses the decentralized engineering of ontologies Conclusion
The described methodology was developed and applied in the On-To-Knowledge project and influenced work, e.g. in the SEKT and the NEON projects. One of the core contributions of the methodology that could not be shown here is the linkage of available tool support with case studies by showing when and how to use tools during the process of developing and running ontology based applications in the case studies (cf. Lessons learned during setting up and employing the methodology in the On-To-Knowledge case studies include: (1) different processes drive KM projects, but "Human Issues" might dominate other ones (as already outlined by Davenport In this chapter we have shown a process oriented methodology for introducing and maintaining ontology based knowledge management systems. Core to the methodology are Knowledge Processes and Knowledge Meta Processes. While Knowledge Meta Processes support the setting up of an ontology based application, Knowledge Processes support its usage. Still, there are many open issues to solve, e.g. how to handle a distributed process of emerging and aligned ontologies that is likely to be the scenario in the semantic web.Introduction and Motivation
Ontologies are used to improve the quality of communication between computers, between humans and computers as well as between humans. Therefore an ontology should result from an agreement between its different stakeholders and this agreement must be reached in a comprehensive ontology engineering process. There are several mature methodologies that have been proposed to structure this process and thus to facilitate it (cf. chapter "Ontology Engineering Methodology" and 1. Decentralization: Existing methodologies do not take into account that even a medium size group of stakeholders of an ontology is often quite distributed and does not necessarily meet often or easily. These methodologies approach ontology engineering in the same style that knowledge-based systems were approached in the past: while the user group of a resulting ontology may be large, its development is performed by a comparatively small group of (1) domain experts who represent the user community and(2) ontology engineers who help structuring that knowledge.In contrast, we have observed that ontology-based applications tend to be built and used in a more widely distributed fashion. By distributed we mean, not only geographically dispersed, but also involving a large number of interested parties from different organizations, with different areas of expertise and competence, different kinds of users with different requirements, etc. For instance, the Gene Ontology (GO), as reported in its web page,There have been very few approaches that have touched upon the issue of adaptation to individual purposes In contrast, we often see the need for interleaving ontology construction and use These issues arise naturally for many ontologies, e.g. Therefore, to account for some of the differences between classical knowledge engineering and ontology engineering methodologies derived from there, we thus have started to develop DILIGENT, a methodology for:1. DIstributed 2. Loosely-controlled, and 3. evolvInG Engineering of oNTologies that is able to 4. support non-expert ontology builders While developing DILIGENT, we also had to consider two general methodological objectives:First, we wanted to provide guidance to the knowledge engineer, the ontology engineer and the non-expert ontology builders that was as fine-grained as possible to make the sequence of tasks as concrete and re-producible by novices as possible.Second, we needed to check DILIGENT by some concrete case studies to show that it can live up to its promises. Clearly, it is very difficult to near impossible to match any methodology, which constitutes an abstraction of many processes, onto an instantiated process in detail. Nevertheless without a reasonable substantiation of the proposed steps in concrete case studies a proposal like DILIGENT would remain vacuous.We will therefore describe DILIGENT in detail and some experiences where it was shown how it maps onto comprehensive case studies. Nevertheless, it will not be possible to describe the finest grain size of DILIGENT. At the finest grain size of methodological support, we have proposed an argumentation framework, an argumentation ontology, technical support and several case studies to investigate only these aspects. Including all these investigations in depth as required by a sound scientific presentation would have doubled the size of this paper, hence we only refer to this work here In the following, we present our ontology engineering methodology, DILI-GENT. In Sect. 2 we give an overview of how we have proceeded to design and validate DILIGENT. In Sect. 3 we describe DILIGENT elaborating the hierarchical task structure in detail. In Sect. 4, we briefly describe how we have applied DILIGENT in some comprehensive case studies, i.e. a distributed knowledge management scenario supported by an ontology-based peer-to-peer knowledge sharing platform and supported by wikis. Eventually, we compare with related work in Sect. 5 and conclude.Developing the DILIGENT Ontology Engineering Methodology
In order to arrive at a sound Ontology Engineering (OE) methodology we have proceeded in five steps to develop DILIGENT.Around 2000, ontology engineering efforts with a clear distributed, looselycontrolled and dynamic flavor were taking place. For instance SUMO Second, the first step in DILIGENT consists of the construction of a core ontology (cf. Sect. 3). In this step DILIGENT does not introduce any special or new requirements for the core ontology when compared to the ones dealt with by existing methodologies (cf. Sect. 1). Therefore, with regard to this step, we have decided not to develop a new methodology, but to borrow from existing work. We expect that any mature methodology can be used. In our case studies, we have exploited the OTK-methodology (chapter "Ontology Engineering Methodology").Third, in order to validate the combined methodology we proceed in two fronts. On the one hand, we analyzed its potential for the past and ongoing development process of the biological taxonomy of living species. When we analyze its evolution since 1735 one can realize that it completely follows the 5-step DILIGENT process, as briefly described in Sect. 3. On the other hand, we conducted a lab experiment case study to specifically investigate whether some argumentation structures dominate the progress in the ontology engineering task and should therefore be accounted for in a fine-grained methodology. Our experiments Fourth, we started a real-life, cross-organizational case study in the tourism industry. We reported about its initial state supporting means in Fifth, by the sum of these initial process templates,The DILIGENT Methodology
In order to give the necessary context for the detailed process description as described in Sect. 3.2 we start by summarizing the overall DILIGENT process model.The DILIGENT process There are different kinds of participants in the DILIGENT process: (1) domain experts, that know about the domain that is targeted (2) ontology engineers, that know how to build ontologies (3) knowledge engineers, that know how to build knowledge or information systems based on ontologies, and (4) users, that use the ontology resulting from the process in their systems for their own uses. The participants directly involved in building the ontology, may or may not use the ontology. However, most ontology users will typically not build or modify the given ontology. DILIGENT supports trained ontology engineers as well as typical users of information systems likewise. The ontology engineers perform the defined activities with more accuracy and awareness of the process, while the non-ontology-engineering-expert users will tend to follow them implicitly guided by the provided tools. At some points of the process there is a subset of participants that plays a special role and has added responsibilities: the board. As in the other steps of the process, the composition of the board is not fixed, that is members can enter or leave, although it should have a more stable composition than that of the participants involved in the DILIGENT process. This board is responsible for the shared ontology: in the beginning it builds the initial version of the ontology, in the iterations that follow it is responsible for the evolution of the shared ontology.General Process
The process comprises five main activities: (1) build, (2) local adaptation, (3) analysis, (4) revision, (5) local update, Fig. Once the initial ontology is made available, users can start using it and locally adapting it for their own purposes. Typically, due to new business requirements or user and organization changes, their local ontologies evolve. In DILIGENT there are two kinds of ontologies: The shared ontology and local ontologies. The shared ontology is available to all users and cannot be changed directly except by the board. Users are free to change, in their local environments, a copy of the shared ontology. The ontology resulting from the changes of a user is the user local ontology.A board of ontology stakeholders analyzes the local ontologies and the users' requests and tries to identify similarities in their ontologies. At this point it is not intended to merge all local ontologies. Instead, changes to local ontologies will be analyzed by the board in order to decide which changes introduced or requested by the users should be introduced in the shared ontology. Therefore, a crucial activity of the board is deciding which changes are going to be introduced in the next version. A balanced decision that takes into account the different needs of user's evolving requirements has to be found.The board should regularly revise the shared ontology, so that the parts of the local ontologies overlapping the domain of the shared ontology do not diverge too far from it. Therefore, the board should have a well-balanced and representative participation of the different kinds of participants involved in the process, which includes ontology engineers, domain experts and users. Of course, these are roles that may overlap.Once a new version of the shared ontology is released, users can update their own local ontologies to better use the knowledge represented in the new version. The last four stages of the process are performed in a cyclic manner: when a new common ontology is available a new round starts again.There are evidences that this process template can be used in different areas and therefore understanding and better supporting it is important. For instance, the taxonomy of life on earth has been evolving since 1735 following a DILIGENT like 5-step process. It was initially proposed by Linnaeus (build ) based on phenetics (observable features). Considering the "most general" level, initially, two kingdoms were proposed: animals and plants. As more and more detailed knowledge about them was discovered, new kingdoms were proposed by its users and introduced by the boards controlling them once some consensus was reached. For instance, when microorganisms were discovered the moving ones were classified in the animals kingdom and the colored (non-moving) ones in the plants kingdom (local adaptation). A few of them were classified in both kingdoms. Users were locally adapting the taxonomy for their own purposes. To more easily identify organisms in both classes, Haeckel (1894) proposed a new kingdom to more easily identify them, the Protista kingdom. This change was introduced by the board (analysis and revision). This kingdom still exists today (locally update) and is used to gather all organisms that do not belong to one of the other kingdoms. The major force driving the reorganization of the taxonomy over time has been the identification of important classifying features and gathering all beings sharing a given value for that feature into that class. The parallel between DILIGENT template process and the development of the taxonomy of life on earth is far more deep than described here. For other examples see DILIGENT Process Stages
In order to facilitate the application of DILIGENT ontology engineering processes and provide guidance to its participants in real settings, DILIGENT had to be more detailed. For this purpose, we have analyzed the different process stages in detail. For each stage we have identified (1) major roles, (2) input, (3) decisions, (4) actions, (5) available tools, and (6) output information. One should stress that this elaboration is rather a recipe or check list than an algorithm or integrated tool set. In different contexts it may have to be adapted or further refined to fit particular needs and settings. Tools may need to be integrated or customized to match requirements of the application context. In Fig. Build
As mentioned before, DILIGENT focuses on distributed ontology development and ontology evolution, but borrows from established methodologies (chapter "Ontology Engineering Methodology" and Roles: Usually, there are three roles: knowledge engineer, ontology engineer and domain expert. The domain expert provides both knowledge and ontology engineers with the required domain knowledge and knowledge sources.  The knowledge engineer creates a conceptual model of the domain from the knowledge extracted from these sources. The ontology engineer generates a machine readable ontology from the conceptual model. Quite often the knowledge engineer and ontology engineer are roles performed by the same person.Additionally to these classical roles we also propose the involvement of users. At this stage, usually the actors involved as users are also involved in the process in one of the other more classical roles. Most of those involved in the build stage are initial board members. Input: Since this stage borrows from traditional OE the usual predevelopment activities are performed. Given our analysis of existing methodologies Decisions: The usual decisions of a classical OE process need to be taken. In contrast to common OE methodologies we do not require completeness of the ontology at this stage. It is particularly important that the ontology is clear and easily understandable by possible users.Actions: As in classical OE development, common core activities are conceptualization, formalization, and implementation. Output: The result is an ontology with the main concepts of the domain. Once an initial ontology is (1) built and released, users will start to adapt it locally for their own purposes.Local Adaptation
This is a use and personalization stage, therefore users use and adapt the released ontology to their own needs. The idea is for users to understand the shared ontology, use it in the context of their applications, eventually find some problems in the shared ontology for their particular applications that require customization on their local ontologies, and accordingly modify these to best suit their needs. All changes should be justified with arguments. Their changes will only apply to their local copies and not to the shared ontology that was made available to all users. In ideal settings, users can also have access to other users' ontologies, when customizing the shared ontology (either under the same framework or from external sources) therefore reuse of ontologies may also be performed. 7 One should stress that all traditional OE activities are usually performed by the users at this stage, such as knowledge acquisition, conceptualization, formalization, evaluation, integration, etc. Once in a while a new shared ontology is made available to users.Roles: The actors involved in the local adaptation step are users of the ontology. However, they usually do not have an OE background. They use the ontology, e.g. to retrieve documents which are related to certain topics modeled in the ontology or more structured data like the projects an employee was involved in.Input: Besides the common shared ontology, in the local adaptation step the information available in the local information space is used. This can be existing databases, ontologies or folder structures and documents. Moreover, external knowledge sources or ontologies can also be reused as well as other user's ontologies.Decisions: The users decide which changes they want to make to their local ontology, hence they must decide if and where new concepts are needed and which relations a concept should have. They should provide reasons for their changes.Actions: To achieve the desired output the user performs different groups of actions namely: Analyze the shared ontology; Change and integrate the shared and local ontologies; and Use the shared ontology. The last two actions of the process step are performed in a cyclic manner until a new shared ontology is available and the entire process step starts again.One important issue is the fact that this stage can either be performed immediately after a build or after a local update stages. In both cases, the shared ontology is available: in the first case, it is the only ontology users have had so far, in the second they have already their own local ontologies that were somewhat connected (or not) to the shared ontology. Users then start adapting the shared ontology to their own purposes. Although these two situations are not different from a conceptual point of view, from a practical point of view they are different since in the second case users usually are not going to simply discard their local ontologies and build them again so that they can be connected to the new version of the shared ontology. Therefore, it is important to assure that there can be a smooth transition.We now describe in detail each one of the proposed actions:The Analysis of shared ontology usually involves (2) Understand shared ontology and (3) Identify similarities between own and shared conceptualization. An ontology should represent a shared conceptualization of part of the world. At this point the analysis is mainly the identification of similarities and mismatches between the available shared ontology and either the conceptual model of the domain users have in their minds or the local ontologies they already developed in previous iterations of the process.(2) Understand the shared ontology The user must learn where the different concepts are located in the ontology and how they are interrelated. The ontology can be very complex, thus understanding the ontology depends mainly on its visualization, and good naming conventions.(3) Identify similarities between own and shared conceptualization Following the comprehension of the ontology, the user can realize the similarities and differences between the own and shared conceptualizations Change and integration of shared and local ontologies usually involves (4) Map equivalent conceptualizations of different actors (4) Map equivalent conceptualizations of different actors: After the identification of similarities they should be made explicit, otherwise the system will not be able to make use of these findings in later stages. This is particularly important when the user is identifying similarities between his local concepts and the new concepts in the shared ontology. Different implementations may add specialized adds-on. Mappings have the advantage, that they leave the original local structures unchanged. Of course users may also decide to change their local structures in favor of the common structure. In this case the changes must be traceable, so that the user can retain his old version, whenever he wants.(5) Identify mismatches in conceptualizations: The techniques to identify similarities can also be applied in the subsequent step to support the user in identifying missing conceptualizations. Depending on the scenario, the user might have access to other users' ontologies and use their local adaptations as further input to identify missing concepts in his own conceptual model.(6) Change conceptualization: After identifying missing or unwanted conceptualizations the user must be enabled to introduce them. This is a customization phase and of course, evaluation is also performed here. Users should assure that their changes are adequate both from a domain and a representation point of view. Since later on the board analyzes the changes performed or requested by the users, users must provide reasons for each change and/or request for change, so that the board can understand them. To support the user in providing reasons, the argumentation framework focuses the user on the relevant arguments, Ontology use typically involves that users Output: One output of the process step is a locally changed ontology which better reflects the user's needs. Each change is supported by arguments explaining the reasons for a change. At this point changes are not propagated to the shared ontology. Moreover, users can send requests for changes directly to the control board, which should also be duly justified. Only in the analysis step the board gathers all ontology changes and requests and their corresponding arguments to be able to evolve the common shared ontology in a user driven revision step.Analysis
In this stage, the board analyzes incoming requests and observations of changes. Roles: In the analysis stage we can distinguish three roles played by board members: (1) The domain expert decides which changes to the common ontology are relevant from the domain point of view and which are relevant for smaller communities only. (2) Representatives from the users explain different requirements from the usability perspective. Decisions: The board must decide which changes to introduce into the new shared ontology at the conceptual level. Metrics to support this decision are (1) the number of users who introduced a change in proportion to all users who made changes. Actions: To achieve the desired output the board takes different actions namely We now describe in detail each one of the proposed actions: (8) Gather locally updated ontologies and corresponding arguments: Depending on the deployed application the gathering of the locally updated ontologies can be more or less difficult. It is important that the board has access to the local changes from users and their corresponding arguments to be able to analyze them. It may also be interesting not only to analyze the current users' ontologies, but also its evolution. However, with an increasing number of participants this in-depth analysis may be unfeasible. Since usually analysis takes place at the conceptual level, reverse engineering is usually an important technique to get the conceptual model from the formalized model (9) Analyze introduced changes: In this action the board tries to identify the parts of the shared ontology which should be modified. As the number of change requests may be large and also contradictory, first the board must identify the different areas in which changes took place. Within analysis the board should bear in mind that changes of concepts should be analyzed before changes of relations and these before changes of axioms. Good indicators for changes relevant to the users are (1) overlapping changes and (2) their frequency. Furthermore, the board should analyze (3) the queries made to the ontology. This should help to find out which parts of the ontology are more often used. Since actors instantiate the ontology locally, (4) the number of instances for the different proposed changes can also be used to determine the relevance of certain adaptations.(10) Decide on changes to be made: Having analyzed the changes and having grouped them according to the different parts of the ontology they belong to, the board has to identify the most relevant changes, that is identify changes presumably relevant for a significant share of all actors. Based on the provided arguments the board must decide which changes should be introduced. Depending on the quality of the arguments the board itself might argue about different changes. For instance, the board may decide to introduce a new concept that better abstracts several specific concepts introduced by users, and connect it to the several specific ones. Therefore, the final decisions entail some form of evaluation from a domain and a usage point of view.Output: The outcome of this action is a reduced and structured list of changes that are to be implemented in the shared ontology that were agreed by the board. Arguments should be provided for each one of them. All changes which should not be introduced into the shared ontology are filtered. Arguments justifying the decisions to leave them out should also be provided. At this stage it is not required to decide on the final modeling of the shared ontology.Revision
The revision and analysis stages are closely related. While in the previous stage the new requirements for the shared ontology are identified, in this stage they are formalized and implemented. In the end the new version of the shared ontology is distributed to its users.Roles: The ontology engineers from the board judge the changes from an ontological perspective more exactly at a formalization level. Some changes may be relevant for the common ontology, but may not be correctly formulated by the users. The domain experts from the board should judge and decide wether new concepts/relations should be introduced into the common ontology even though they were not requested by the users Input: The input for the revision phase is a list of changes at a conceptual level which should be included into the ontology and the arguments underlying them.Decisions: The main decisions in the revision phase are formal ones. All intended changes identified during the analysis phase should be included into the common ontology. In the revision phase the ontology engineer decides how the requested changes should be formalized. Evaluation of the decisions is performed by comparing the changes on the conceptual level with the final formal decisions. The differences between the original formalization by the users and the final formalization in the shared ontology should be kept to a minimal basis.Actions: To achieve the desired output the members of the board, mainly its ontology engineers, perform different actions namely We now describe in detail each one of the proposed actions: (11) Formalization of the decided changes: As in classical OE development, the requested changes must be formalized with respect to the expressivity of the ontology representation language. Before their actual implementation, the agreed changes should be analyzed from a knowledge representation point of view. This evaluation is somehow similar to the one performed when reusing an ontology according to classical reuse methodologies. The goal is to determine how the changes identified in the previous step should be formalized. Once this is done, the actual changes are formalized and the quality of the resulting ontology is again assured through evaluation. All required activities are addressed by classical OE methodologies.(12) Aggregation of arguments: As arguments play a major roll in the decision process we expect that the changes which are eventually included into the common ontology are supported by good arguments. One of the reasons for keeping track of the arguments is to enable users to better understand why certain decisions have been made. Therefore, the board should summarize and aggregate understandable, pedagogical and the most convincing arguments underlying each change. The user should be able to retrieve them.(13) Documentation: With the help of the arguments, the introduced changes are already well documented. However, we assume that some arguments may only be understandable by the domain experts and not users. Hence, we expect that the changes should be documented to a certain level.(14) Distribution of the ontology to all actors: Analogously to stage (1) the shared ontology must be distributed to the different participants. Depending on the overall system architecture different methods can be applied here. Moreover, the board should assure version and release management.Output: The new version of the shared ontology together with its arguments and documentation is the result of this stage. This documentation is essential for users to understand the new shared ontology when a new cycle begins.Local Update
In the local update stage the new shared ontology is released and put to use by its users. They decide which changes they will adopt. Part of this stage is similar to local adaptation: users must get familiar with the new version and identify which parts of their local ontologies they will discard in favor of the new shared ontology and which ones they will retain.Roles: The local update phase involves only users. They perform different actions to include the new common ontology into their local system before they start a new round of local adaptation.Input: The new formalized shared ontology is the input for this step. We also require as input the documentation and arguments justifying those changes. For a better understanding the user should be allowed to request a delta to the original version.Decisions: The user must decide which changes he will introduce locally. This depends on the differences between the own and the new shared conceptualization. The user does not need to update his entire ontology. This stage interferes a lot with the next local adaptation stage. We do not exclude the possibility of conflicts and/or ambiguity between local and shared ontologies, which may entail reduced precision if the ontology is being used in IR applications. 10  Actions: To achieve the desired output the user takes different actions namely Analysis of the new shared ontology; and Integration of new shared version with current user's local one.After the local update, the iteration continues with local adaptation. During the next analysis step the board reviews which changes were actually accepted by the users.We now describe in detail each one of the proposed actions: Analysis of the new shared ontology: The goal is to understand the new shared ontology. The user scans for the changes introduced by the board that are relevant for his use, and controls whether his change proposals were implemented. He must further identify wether the benefits of updating to the new version outweight its effort. Issues to be analyzed include: concepts introduced by other users, consistency of new shared version with local version, maintenance of interoperability with other users.Integration of new shared version with current user's local one: In this action the user reuses or not the new version of the shared ontology. If the new shared ontology is not of use the system should allow the user to retain the outdated version. In this case the user will have to perform (16) Inclusion of the updated version: The system must support the user to easily integrate the new version into his local system. It must be guaranteed that all annotations made for the old version of the ontology are available in the new version. It may require restructuring and adaptation of instantiations to stay in line with the new model.(17) Update of local adaptations which are not included in the common ontology: The update of the local ontology can lead to different kinds of conflict. Changes proposed by the user may indeed have found their way into the common ontology. Hence, the user should be enabled to use from now on 10 Ideally one should be able to blacken out the ambiguous parts like in multilevel databases. This has not been transferred to OE yet.the shared model instead of his own identical model. Furthermore, the board might have included a change based on arguments the user was bringing forward, but has drawn different conclusions. Here the user can decide wether he prefers the shared interpretation.Other options may emerge in the course of further case studies.Output: Ideally the output of the local update phase is an updated local ontology which includes all changes made to the shared ontology. However, since not all users may want to completely change to the new version, we do not require the users to adopt all changes proposed by the board. So, the output is not mandatory since the actors could change the new ontology back to the old one in the local adaptation stage.Applying DILIGENT in Case Studies
In this section we describe briefly how we specifically investigated how a distributed, loosely controlled and evolving ontology engineering process following DILIGENTcould be implemented. For more detailed descriptions refer to the relevant bibliography referred in each subsection.The IBIT Case Study
The first running case study took place under the SWAP project. In this project, the challenges were how the process template could be implemented in a multi-organizational setting with non-expert ontology engineering users, and which finer grained support could be provided to these users.In the SWAP project, the IBIT case study was in the tourism domain of the Balearic Islands. The needs of the tourism industry there, which accounts for 80% of the islands' economy, are best described by the term "coopetition". On the one hand the different organizations compete for customers against each other. On the other hand, they must cooperate in order to provide high quality for regional issues like infrastructure, facilities, clean environment, or safety -that are critical for them to be able to compete against other tourism destinations. To collaborate on regional issues a number of organizations now collect and share information about indicators reflecting the impact of growing population and tourist fluxes in the islands, their environment, and their infrastructures. Moreover, these indicators can be used to make predictions and help planning. For instance, organizations that require Quality & Hospitality Management use the information to better plan, e.g. their marketing campaigns. As another example, the governmental agency IBIT, Due to the different working areas and goals of the collaborating organizations, it proved impossible to build a centralized knowledge management system or even a centralized ontology satisfying all user requirements. The users emphasized the need for local control over their ontologies. They asked explicitly for a system without a central server, where knowledge sharing was integrated into the normal work, but where different kinds of information, like files, emails, bookmarks and addresses could be shared with others. To this end the SWAP consortium -including us at University of Karlsruhe, IBIT, Free Univ. Amsterdam, Meta4, and Empolis -developed the SWAP generic P2P platform and built a concrete application on top that allowed the satisfaction of the information sharing needs just elaborated using local ontologies, which were linked to a shared ontology. A case study was set up. The main goals were the evaluation of the DILIGENT process and the developed peerto-peer platform. The case study lasted for 3 months. Moreover, a set of tools were also specifically developed Regarding the methodology we had four hypothesis: (1) DILIGENT supports collaborative development of a shared ontology; (2) ontologies in use need to evolve; (3) non-ontology engineering experts can participate in ontology engineering processes, and (4) the organizational structure DILIGENT suggests fits the organizational setting found in the IBIT case study, a peerto-peer setting.The first round of our OE process started with the distribution of the three modules of the common ontology to all users. In both rounds, users -during the local adaptation stage -and the board -in the revision stage -could perform ontology change operations (concepts/relations/instances). Most frequently the concept hierarchy was changed.The first month of the case study, corresponded to the first round of the DILIGENT process. One organization with seven peers participated. This organization can be classified as a rather loose one. In the first round we had seven users, six of which had no OE background. In general, the users added concepts to the shared ontology to represent the topics of their core working area. They did not share all their local information, but selected the documents which they thought would be interesting for the group. In the interviews they commented, that they would share more files at a later stage, when they would feel more confident with the system. In this organization most of the users were very active and did local adaptations to best serve their own needs. They also add access to other user's ontologies. Moreover, the board received by e-mail requests to modify the shared ontology. The first round of the process resulted in seven adapted ontologies.In Analysis, the board consisted of two ontology engineers and two domain experts/users, the same that were involved in the build stage. The local adaptations from seven users were collected. Additionally the board had access to their folder structures. All changes introduced were motivated by the users' requests and changes. They all made sense and were not contradictory on the conceptual level. Then, the new shared ontology was distributed.In Local update all users decided to use the new shared ontology as it covered more domain knowledge and they found their requests integrated to it. As a result of this stage the new shared ontology was commonly used and the users' folders were aligned with the new shared ontology.In the second round the case study was extended to four organizations with 21 peers. The users participating in the first round had more experience and were still active. One of the new organizations was very hierarchical. None of the new 14 users had OE experience. The experienced users started with the result of the local update stage, while the new users received only the new shared ontology. All users shared the local information which they thought relevant for the group. The new users behaved in a similar way as the users in the first stage and did not share many folders, as they wanted to gain confidence in the system first. The experienced users, however, published more information, and adapted the local ontologies accordingly. The second local adaptation stage resulted in 14 adapted ontologies. The rest of the users did not make changes. Although, some did not change the shared ontology directly, they submitted change requests to their supervisor, thus they delegated the modeling task. The supervisor then communicated the requests to the board.In Analysis, in this round the board consisted of one domain expert and two ontology engineers. Additionally two users were invited to answer questions to clarify the changes they introduced. The 21 local ontologies of the users were the input to the second round. This time the board had to perform reverse engineering on the formal local ontologies from users in order to get their conceptual models. As in the first round the board included all change requests from users. Again, as in the first round, only very few concepts in the common ontology were never used. All conceptual requests could be modelled in the ontology, providing the next version.The case study ended after the distribution of the new shared ontology. We collected feedback from the users w.r.t. to their impressions on the new version. They emphasized that the new version represented their requirements at that time. The users commented that they appreciated being involved in the development process, although they recognized that they were not experienced in ontology engineering. They did not object to the modeling decisions of the board and understood the reasons for the differences between their change requests and the final modeling.However, updating to the new version was still a problem, since some instances of the ontology might have to be newly annotated to the new concepts of the shared ontology. In our case, documents needed a new classification. This problem can be partly overcome with the help of technology For more detailed descriptions on this project refer to The Judges Case Study
The Judges case study took place under the SEKT project. It aimed at providing an intelligent Frequently Asked Questions system, Iuriservice, that offers help to newly appointed judges in Spain. Although judges had a strong and thorough education and became experts in their domain, they still often seek the help of senior judges or tutors regarding procedural questions. The system focuses on such procedural knowledge, which is often neglected, as it is very hard to externalize. Examples for procedural questions are: How should I organize a round of recognition of suspects if there are no people available? Which are the actual functions and competences of the judge as compared to those of the secretaries? In this regard, the design of legal ontologies requires not only to represent the legal, normative language of written documents (decisions, judgments, rulings, partitions, etc.) but also those chunks of professional knowledge from the daily practice at courts. One of the main features of this professional legal knowledge is that it is context-sensitive. In this sense, it implies: (1) the ability to discriminate among related but different situations; (2) the practical attitude or disposition to rule, judge or make a decision; (3) the ability to relate new and past experiences of cases; (4) the ability to share and discuss these experiences with the group of peers.In this case, the argumentation framework developed under the DILI-GENTmethodology, together with a wiki system proved an invaluable tool that promoted discussion and allowed finding good solutions for the problems newly appointed judges faced.For more detailed descriptions on this project refer to Related Work
In the past, there have been OE case studies involving dispersed teams, such as (KA) 2 ontology Holsapple et al. There are a number of technical solutions to tackle problems of remote collaboration, e.g. ontology editing with mutual exclusion Conclusion
Decentralization can take different forms. One can have more loose or more hierarchical organizations. We observed and supported both kinds of organizations. Therefore, the first finding is the fact that this process can be adapted both to hierarchical and to more loose organizations. DILIGENT processes cover both traditional OE processes and more Semantic Web-oriented OE processes, that is with strong decentralization and partial autonomy requirements.The process helped non-OE-expert users to conceptualize, specialize and refine their domain. The agreement met with the formalized ontologies was high, as shown by people willing to change their folder structures to better use the improved domain conceptualization. In spite of the technical challenges, user feedback was very positive.The DILIGENT process proved to be a natural way to have different people from different organizations collaborate and change the shared ontology. The set-up phase for DILIGENT was rather fast, and users could profit from their own proposals (local adaptations) immediately. The result was much closer to the user's own requirements. Moreover, other users profited from them in a longer term. Finally, the case studies clearly have shown the need for evolution. Users performed changes and adaptations.The development of ontologies in centralized settings is well studied and there are established methodologies. However, current experiences from projects suggest that ontology engineering should be subject to continuous improvement rather than a one-time effort and that ontologies promise the most benefits in decentralized rather than centralized systems. To this end we have conceived the DILIGENT methodology. DILIGENT supports domain experts, users, knowledge engineers and ontology engineers in collaboratively building a shared ontology in a distributed setting. Moreover, the methodology guides the participants in a fine grained way through the ontology evolution process, allowing for personalization. We have demonstrated the applicability of our process model in a cross-organizational case study in the realm of tourism industry and another in the judicial domain. Real users were using the ontology to satisfy their information needs for an extended period of time.Introduction
Formal concept analysis (FCA) FCA differs from other knowledge representation formalisms (like RDF (see chapter "Resource Description Framework"), description logics (see chapter "Description Logics"), OWL (see chapter "Web Ontology Language: OWL"), or conceptual graphs In the mid 1990s, a triadic version of FCA has been launched. Beside its extension and its intension, a tri-concept additionally contains a set of conditions under which extension and intension match. During the last dozen years, triadic FCA has been primarily of academic interest. With the rise of social bookmarking systems in the Web 2.0, however, they gained increased interest, as the data structures of social bookmarking systems -so-called folksonomies -match exactly the theory of triadic FCA.Organisation of the Survey
In Sects. 2-5, the theory of FCA is presented, including basic notions like "formal context" and "concept lattice", visualisations with (nested) line diagrams, conceptual scaling, and the relationship to association rule mining. Section 6 presents an FCA based approach for knowledge acquisition. Section 7 discusses the relationship between triadic FCA and folksonomies in detail. Section 8 concludes the paper with a selection of FCA based knowledge engineering applications.Formal Concept Analysis: A Theory About Concepts and Concept Hierarchies
This section presents the basic notions of FCA. Good starting points for a more in depth lecture are the textbooks Example 1. The left part of Fig. For a given context (G, M, I), we can define two derivation operators, both denoted by the symbol. They are used for defining formal concepts. Definition 2. For
Concept lattices can be visualised as line diagrams. Line diagrams follow the conventions for the visualisation of hierarchical concept systems as established in the German standard DIN 2331 The second part of Theorem 1 provides an efficient visualisation of concept lattices via line diagrams, as it states that a diagram is unambiguous even if each object name and each attribute name is displayed only once: in the line diagram, the name of an object g ∈ G is attached to the node representing the object concept γ(g) := ({g} , {g} ), and the name of an attribute m ∈ M is attached to the node representing the attribute concept μ(m) := ({m} , {m} ). This means that the name of an object g is always attached to the node representing the smallest concept with g in its extent; dually, the name of an attribute m is always attached to the node representing the largest concept with m in its intent. We can read the context relation from the diagram because an object g has an attribute m if and only if the concept labeled by g is a subconcept of the one labeled by m. The extent of a concept consists of all objects whose labels are attached to subconcepts, and, dually, the intent consists of all attributes attached to superconcepts. Example 3. For example, the concept in the very middle of Fig. The top concept of the diagram always has all objects in its extent. In this case, its intent is non-empty, as it contains the attribute "needs water to live". This indicates that all living beings addressed in the movie depend on water. We see also that the diagram -and thus the set of objects -can be decomposed into two parts: the animals are grouped under the attribute "can move around", while all plants "need chlorophyll to produce food".Dependencies between attributes can be described by implications. Definition 3. For X, Y ⊆ M , the implication X → Y holds in the context, if each object having all attributes in X also has all attributes in Y .Example 4. The implication {can move around, lives on land} → {has limbs} holds in this context. It can be read directly in the line diagram: the largest concept having both "can move around" and "lives on land" in its intent (i.e. the infimum of μ(can move around) and μ(lives on land), which is the concept that is second-most to the left) also has "has limbs" in its intent.Note that these implications hold only for those objects that are listed in the context. If one is interested in implications that "hold globally", the context needs thus to contain sufficiently many "typical" objects. Section 6 discusses a knowledge acquisition process for interactively determining these typical objects.Nested Line Diagrams
Nested line diagrams are used for visualising larger concept lattices. For their construction, the formal context is vertically split, and for each part a separate line diagram of its concept lattice is drawn. The line diagrams are then combined as shown in Fig. Example 5. Figure The combination of the two diagrams represents the direct product of both concept lattices. Its order relation can be read by replacing each of the four lines of the outer diagram by eight parallel lines linking corresponding nodes in the inner diagrams. The concept lattice given in Fig. It can be shown The non-realised concepts are not only displayed to indicate the structure of the inner scale, but also because they indicate implications: Each non-realised concept indicates that the attributes in its intent imply the attributes contained in the largest realised concept below. The implication discussed above ({can move around, lives on land} → {has limbs}) is indicated by the non-realised concept having as intent "lives on land" and "can move around", which is represented by the only empty node in the left ellipse in Fig. Nested line diagrams often result from conceptual scaling, which is discussed next.Conceptual Scaling
FCA is also able to deal with many-valued contexts, i.e. they may contain attribute-value pairs. From a many-valued context, a concept lattice cannot be computed directly. One has to transform it first into a a one-valued context. This transformation is called conceptual scaling Let S be the set of conceptual scales for the many-valued context K := (G, M, (W m ) m∈M , I). For any subset S ⊆ S of scales, we can now translate the many-valued context into a one-valued one: Definition 6. The derived context K S is defined by K S := G, SB ∈S M B , I S with (g, n) ∈ I S if there exists a scale S B ∈ S with m ∈ M B and w ∈ W m where (g, m, w) ∈ I and (g, n) ∈ I B . Example 6. The two attributes "one seed leaf" and "two seed leaves" in our running example have been derived from a many-valued attribute "Number of seed leaves", which has as set of possible values W Number of seed leaves := {0, 1, 2}. Applying the conceptual scale S {Number of seed leaves} that is displayed at the right of Fig. The concept lattice of the derived context can canonically be visualised in a nested line diagram. For each scale, its line diagram is pre-computed. The nested line diagram for any combination of scales can then be combined online. This combination of conceptual scaling and nested line diagrams is implemented in the open source software ToscanaJ. Iceberg Concept Lattices and Bases of Association Rules
The concept lattice of a formal context can be considered as a conceptual, hierarchical clustering of the set of objects, with the concept extents being the clusters and the intents being their descriptions To alleviate this complexity problem, the notion of iceberg concept lattices has been introduced in ) is larger than minsupp. Iceberg concept lattices show thus only the top-most part of a concept lattice.Example 8. In Fig. Iceberg concept lattices were developed as an answer of FCA to the association rule mining problem The support of any itemset B ⊆ M equals the support of the smallest concept intent containing it (which is just B ). Hence, for computing all association rules, it is sufficient to consider only frequent concept intents instead of all frequent itemsets.One can go even one step further and compute not all association rules, but only a basis, i.e. a non-redundant subset from which all other rules can be derived Example 9. The Luxenburger basis for the Mushrooms database with a minimum support of 70% and a minimum confidence of 95% is shown in Fig. Knowledge Acquisition with Formal Concept Analysis
Knowledge Acquisition aims at supporting the acquisition of knowledge from humans and its transformation into a formal model. The most prominent FCA technique is B. Ganter's Attribute Exploration Concept Exploration extends this approach to situations where both the object set and the attribute set of the context are not completely known a priori or too large Folksonomies and Triadic Concept Analysis
Social resource sharing systems are Web 2.0 systems that allow users to upload their resources, and to label them with arbitrary words, so-called tags. Each system has a specific type of resources it supports. Flickr, for instance, allows the sharing of photos, del.icio.us the sharing of bookmarks, CiteULikeIn their core, these systems are all very similar. Users can add resources to the system, and assign arbitrary tags to them. The collection of a users assignments is his personomy, the collection of all personomies constitutes the folksonomy. The user can explore his personomy, as well as the personomies of other users, in all dimensions: for a given user one can see all resources he has uploaded, together with the tags he has assigned to them (see Fig. With the emergence of social bookmarking systems, the interest in a triadic extension of FCA raised again. This extension was introduced by R. Wille and F. Lehmann in Structurally, triadic contexts are equal to folksonomies. Hence the whole theory of triadic FCA can be applied directly to folksonomies.In the first part of this section, we present a formal definition of folksonomies, before giving an overview over triadic FCA, and connecting its notions to those of folksonomies.Folksonomies
The word "folksonomy" is a blend of the words "taxonomy" and "folk", and stands for conceptual structures created by the people. Folksonomies are thus a bottom-up complement to more formalised Semantic Web technologies, as they rely on emergent semantics Definition 7 ([34]
). A folksonomy is a tuple F := (U, T, R, Y, ≺) where: where π i denotes the projection on the ith dimension.Users are typically described by their user ID, and tags may be arbitrary strings. What is considered as a resource depends on the type of system. For instance, in del.icio.us, the resources are URLs, in Flickr pictures, and in BibSonomy they are either URLs or bibliographic references.Triadic Formal Concept Analysis
Inspired by the pragmatic philosophy of Charles S. Peirce with its three universal categories Definition 8 ([38]).
Triadic concepts are defined in a manner analogue to the dyadic case.Definition 9 ([38])
Y such that none of its three components can be enlarged without violating this condition. A 1 is called the extent, A 2 the intent, and A 3 the mode of the tri-concept (A 1 , A 2 , A 3 ). We define three quasi-orders 1 , 2 , and 3 on the set of all tri-concepts, one for each dimension G, M , and B:Lemma 1. For two tri-concepts a and b, a i b and a j b imply b k a, for {i, j, k} = {1, 2, 3}.Lehmann and Wille present in Analysing Folksonomies with Triadic FCA
The notion of iceberg concept lattices/frequent closed itemsets has been lifted to the triadic case by Jäschke et al. Example 10. Figure A closer look on the tag hierarchy reveals the content of the most central publications in the system. The tag social co-occurs with most of the tags. On the level of generality defined by the support thresholds, this tag is (together with the tags ai [meaning Artificial Intelligence], . . . , tags) assigned by the users lkl kss and yish to the publications 19 and 30, (together with the tag bookmarking) by the users hotho, jaeschke, stumme to the publications 4 and 28, and (again together with the tag bookmarking) by the users brotkasting, jaeschke, stumme to the publications 28 and 29. The tags as well as the corresponding publication titles indicate that the two sets of users {lkl kss, yish} and {brotkasting, hotho, jaeschke, stumme} form two sub-communities which both work on social phenomena in the Web 2.0, but from different perspectives. A second topical group is spanned by the tag semantic, which occurs in three different contexts: semantic wikis, semantic web mining, and together with the tag "folksonomy". A detailed discussion of this diagram is given in One way to simplify the analysis of triadic data is to project it down to a two-dimensional dataset. There are different possible projections. Lehmann and Wille present, for instance, in We discussed in The focus of this section was on folksonomy research that is related to Formal Concepts Analysis. A general survey over theory and applications of folksonomies is out of the scope of this article. A collection of folksonomy related publications can be found at http://www.bibsonomy.org/tag/folksonomies.Ontology Engineering with Formal Concept Analysis
We conclude this survey on FCA with a short discussion of some FCA based ontology engineering solutions. A more detailed survey can be found in Browsing Ontologies
Ferré and Ridoux use FCA for building a virtual file system in which the files are considered as the objects of a formal context, and descriptors (in first order logic) are its attributes Non-atomic Attributes
The attributes of a formal context can be considered as unary predicates. In the standard context, they are atomic, but no one prevents us from defining them in a more complex knowledge representation. In this sense, first order logic and SQL were used for conceptual scaling Ontology Learning and Merging
The Attribute Exploration procedure described in Sect. 6 was used by Baader et al. and Rudolph for computing the hierarchy of all conjunctions of all concepts of a given description logic knowledge base The method FCA-Merge Class Design in Software Engineering
The task of designing class hierarchies in object-oriented (OO) programming has many similarities with the construction of an ontology. There are several FCA applications in this area. Reference Remark. An extended list of references can be found at BibSonomy. Introduction
The OntoClean methodology was first introduced in a series of conferencelength papers in 2000 In this chapter we present an informal overview of the four basic notions essence, identity, unity, and dependence, and their role in OntoClean, review the basic ontology pitfalls, and walk through the example that has appeared in pieces in previous papers and has been the basis of numerous tutorials and talks beginning with AAAI-2000.Background
The basic notions in OntoClean were not new, but existed in philosophy for some time. Indeed, the practice of modeling the world for information systems has many parallels in philosophy, whose scholars have been trying to describe the universe in a formal, logical way since the time of Aristotle. Philosophers have struggled with deep problems of existence, such as God, life and death, or whether a statue and the marble from which it is made are the same entity (see Properties, Classes, and Subsumption
Many terms have been borrowed by computer science from mathematics and logic, but unfortunately this borrowing has resulted often in a skewed meaning. In particular, the terms property and class are used in computer science with often drastically different meanings from the original. The use of the term property in RDF is an example of such unfortunate deviation from the usual logical sense.In this chapter, we shall consider properties as the meanings (or intensions) of expressions like being an apple or being a table, which correspond to unary predicates in first-order logic. Given a particular state of affairs (or possible world, if you prefer), we can associate to each property a class (its extension), which is the set of entities that exhibit that property in that particular situation. The members of this class will be called instances of the property. Classes are therefore sets of entities that share a property in common; they are the extensional counterpart of properties. In the following, we shall refer most of the time to properties rather than classes or predicates, to stress the fact that their ontological nature (characterized by means of meta-properties) does not depend on syntactic choices (as it would be for predicates), nor on specific states of affairs (as it would be for classes).The independence of properties from states of affairs gives us the opportunity to make clear the meaning of the term subsumption we shall adopt in this paper. A property p subsumes q if and only if, for every possible state of affairs, all instances of q are also instances of p. On the syntactic side, this corresponds to what is usually held for description logics, P subsumes Q if and only if there is no model of Q ∧ ¬P.The Basic Notions
Essence and Rigidity
A property of an entity is essential to that entity if it must be true of it in every possible situation, i.e., if it necessarily holds for that entity. For example, the property of having a brain is essential to human beings. Every human must have a brain in every possible situation.A special form of essentiality is rigidity; a property is rigid if it is essential to all its possible instances; an instance of a rigid property cannot stop being an instance of that property in a different situation. For example, while having a brain may be essential to humans, it is not essential to, say, scarecrows in the Wizard of Oz. If we were modeling the world of the Wizard of Oz, the property of having a brain would not be rigid, though still essential to humans. On the other hand, the property being a human is typically rigid, every human is necessarily so.The fact that we said "typically" in the previous statement requires an immediate clarification. The point of OntoClean is not to help people deciding about the ontological nature of a certain property; this choice depends on the way the domain at hand is conceptualized When a property is non-rigid, it can acquire or lose (some of) their instances depending on the situation at hand. Within non-rigid properties, we distinguish between properties that are essential to some entities and not essential to others (semi-rigid ), and properties that are not essential to all their instances (anti-rigid ). For example, the property being a student is typically anti-rigid -every instance of student can cease to be such in a suitable situation, whereas the property having a brain in our Wizard of Oz world is semi-rigid, since there are instances that must have a brain as well as others that consider a brain just as a (useful) optional.Rigidity and its variants are important meta-properties, every property in an ontology should be labeled as rigid, non-rigid, or anti-rigid. In addition to providing more information about what a property is intended to mean, these meta-properties impose constraints on the subsumption relation, which can be used to check the ontological consistency of taxonomic links. One of these constraints is that anti-rigid properties cannot subsume rigid properties. For example, the property being a student cannot subsume being a human if the former is anti-rigid and the latter is rigid. To see this, consider that, if p is an anti-rigid property, all its instances can cease to be such. This is certainly the case for student, since any student may cease being a student. However, no instance of human can cease to be a human, and if all humans would be necessarily students (the meaning of subsumption), then no person could cease to be a student, creating therefore an inconsistency.Identity and Unity
Although very subtle and difficult to explain without experience, identity and unity are perhaps the most important notions we use in our methodology. These two things are often confused with each other; in general, identity refers to the problem of being able to recognize individual entities in the world as being the same (or different), and unity refers to being able to recognize all the parts that form an individual entity.Identity criteria are the criteria we use to answer questions like, "is that my dog?" In point of fact, identity criteria are conditions used to determine equality (sufficient conditions) and that are entailed by equality (necessary conditions).It is perhaps simplest to think of identity criteria over time (diachronic identity criteria), e.g., how do we recognize people we know as the same person even though they may have changed? It is also very informative, however, to think of identity criteria at a single point in time (synchronic identity criteria). This may, at first glance, seem bizarre. How can you ask, "are these two entities the same entity?" If they are the same then there is one entity, it does not even make sense to ask the question.The answer is not that difficult. One of the most common decisions that must be made in ontological analysis concerns identifying circumstances in which one entity is actually two (or more). Consider the following example, drawn from actual experience: somebody proposed to introduce a property called time duration whose instances are things like one hour and two hours, and a property time interval referring to specific intervals of time, such as "1:00-2:00 next Tuesday" or "2:00-3:00 next Wednesday." The proposal was to make time duration subsume time interval, since all time intervals are time durations. Seems to make intuitive sense, but how can we evaluate this decision?In this case, an analysis based on the notion of identity can be informative. According to the identity criteria for time durations, two durations of the same length are the same duration. In other words, all one-hour time durations are identical -they are the same duration and therefore there is only one "one hour" time duration. On the other hand, according to the identity criteria for time intervals, two intervals of the same duration occurring at the same time are the same, but two intervals occurring at different times, even if they are the same duration, are different. Therefore the two example intervals above would be different intervals. This creates a contradiction: if all instances of time interval are also instances of time duration (as implied by the subsumption relationship), how can they be two instances of one property and a single instance of another? This is one of the most common confusions of natural language when used for describing the world. When we say "all time intervals are time durations" we really mean "all time intervals have a time duration" -the duration is a component of an interval, but it is not the interval itself. In this case we cannot model the relationship as subsumption, time intervals have durations (essentially) as qualities. More examples of such confusions are provided at the end of this article.One of the distinctions proposed by OntoClean is between properties that carry an identity criterion and properties that do not. The former are labeled with an ad hoc meta-property, +I. Since criteria of identity are inherited along property subsumption hierarchies, a further distinction is made to mark those properties that supply (rather just carrying) some "own" identity criteria, which are not inherited from the subsuming properties. These properties are marked with the label +O (where O stands for "own").Unfortunately, despite their relevance, recognizing identity criteria may be extremely hard. However, in many cases identity analysis can be limited to detecting the properties that are just necessary for keeping the identity of a given entity, i.e., what we have called the essential properties. Obviously, if two things do not have the same essential properties they are not identical. Take for instance the classical example of the statue and the clay: is the statue identical to the clay it is made of? Let us consider the essential properties: having (more or less) a certain shape is essential for the statue, but not essential for the clay. Therefore, they are different: we can say they have different identity criteria, even without knowing exactly what these criteria are. In practice, we can say that "sharing the essential property P," where P is essential for all the instances of a property Q different from P, is the weakest form of an identity criterion carried by Q. Such criterion can be used to make conclusions about non-identity, if not about identity.A second notion that is extremely useful in ontological analysis is Unity. Unity refers to the problem of describing the parts and boundaries of objects, such that we know in general what is part of the object, what is not, and under what conditions the object is whole.Unity can tell us a lot about the intended meaning of properties in an ontology. Certain properties pertain to wholes, that is, all their instances are wholes, others do not. For example, being (an amount of ) water does not have wholes as instances, since each amount can be arbitrarily scattered or confused with other amounts. In other words, knowing it is an amount of water does not tell us anything about its parts, and recognizing it as a single entity. On the other hand, being an ocean is a property that picks up whole objects, as its instances, such as "the Atlantic Ocean" is recognizable as a single entity. Of course, one might observe that oceans have vague boundaries, but this is not an issue here: the important difference with respect to the previous example is that in this case we have a criterion to tell, at least, what is not part of the Atlantic Ocean, and still part of some other ocean. This is impossible for amounts of water.In general, in addition to specifying whether or not properties have wholes as instances, it is also useful to analyze the specific conditions that must hold among the parts of a certain entity in order to consider it a whole. We call these conditions unity criteria (UC). They are usually expressed in terms of a suitable unifying relation, whose ontological nature determines different kinds of wholes. For example, we may distinguish topological wholes (a piece of coal), morphological wholes (a constellation), functional wholes (a hammer, a bikini).As these examples show, nothing prevents a whole from having parts that are themselves wholes (under different unifying relations). Indeed, a plural whole can be defined as a whole that is a mereological sum of wholes.In OntoClean, we distinguish with suitable meta-properties the properties all whose instances must carry a common UC (such as ocean) from those that do not. Among the latter, we further distinguish properties all of whose instances must be wholes, although with different UCs, from properties all of whose instances are not necessarily wholes. An example of the former kind may be legal agent, if we include both people and companies (with different UCs) among its instances. Amount of water is usually an example of the latter kind, since none of its instances must be wholes (this is compatible with the view that a particular amount of water may become a whole for a short while, e.g., while forming an iceberg. We say that ocean carries unity (+U), legal agent carries no unity (-U), and amount of water carries anti-unity (∼U).The difference between unity and anti-unity leads us again to interesting problems with subsumption. It may make sense to say that "Ocean" is a subclass of "Water," since all oceans are water. However, if we claim that instances of the latter must not be wholes, and instances of the former always are, then we have a contradiction. Problems like this again stem from the ambiguity of natural language, oceans are not "kinds of" water, they are composed of water.Constraints and Assumptions
A first observation descending immediately from our definitions regards some subsumption constraints. Given two properties, p and q, when q subsumes p the following constraints hold:1. If q is anti-rigid, then p must be anti-rigid 2. If q carries an identity criterion, then p must carry the same criterion 3. If q carries a unity criterion, then p must carry the same criterion 4. If q has anti-unity, then p must also have anti-unity 5. If q is dependent on property c, then p is dependent on property c Finally, we make the following assumptions regarding identity (adapted from Lowe • Sortal Individuation. Every domain element must instantiate some property carrying an IC (+I). In this way we satisfy Quine's dicto "No entity without identity" • Sortal Expandability. If something is an instance of different properties (for instance related to different times), then it must be also instance of a more general property carrying a criterion for its identity.Together, the two assumptions imply that every entity must instantiate a unique most general property carrying a criterion for its identity.An Extended Example
In this section we provide a walk-through of the way the OntoClean analysis can be used. This example is based on those presented at various tutorials and invited talks. We begin with a set of classes arranged in a taxonomy, as shown in Fig. Assigning Meta-Properties
The first step is to assign the meta-properties discussed above to each property in the taxonomy. When designing a new ontology, this step may occur first, before arranging the properties in a taxonomy. Note that the assignments discussed here are not meant to be definitive at all: rather, these represent prima facie decisions reflecting our intuitions about the meaning ascribed to the terms used. The point of this exercise is not so much to discuss the ontological nature of these properties, but rather to explore and demonstrate the logical consequences of making these choices. As we shall see, in some cases they will result contradictory with respect to the formal semantics of our meta-properties, although intuitive at a first sight. In our opinion, this proves the utility of a formal approach to ontology analysis and evaluation.Entity
Everything is necessarily an entity. Our meta-properties assignment is -I-U +R. This is the most abstract property, indeed it is not necessary having an explicit predicate for it.Location
A location is considered here as a generalized region of space. Our assignment is +O∼U+R. We assume the property as rigid since instances of locations cannot change being locations. The identity criterion is that two locations are the same if and only if they have the same parts. This kind of criterion is fairly common, and is known as mereological extensionality. It applies to all entities that are trivially defined to be the sum of their parts. It is important to realize that this criterion implies that a location or region cannot "expand"if so then the identity criteria would have to be different. So, extending a location makes it a different one. So we see that identity criteria are critical in specifying precisely what a property is intended to mean.Amount of Matter
We conceptualize an amount of matter as a clump of unstructured or scattered "stuff" such as a liter of water or a kilogram of clay. Amounts of matter should not be confused with substances, such as water or clay; an amount of matter is a particular amount of the substance. Therefore, amounts of matter are mereologically extensional, so we assign +O to this property. As discussed above, they are not necessarily wholes, so our assignment is ∼U. Finally, every amount of matter is necessarily so, therefore the property is +R.Red
What we have in mind here is the property of being a red thing, not the property of being a particular shade color. We see in this case that it is useful to ask ourselves what the instances of a certain property are. Do we have oranges and peppers in the extension of this property, or just their colors? Red entities share no common identity criteria, so our assignment is -I. A common confusion here regarding identity criteria concerns the fact that all instances of red are colored red, therefore we have a clear membership criterion. Membership criteria are not identity criteria, as the latter gives us information about how to distinguish entities from each other. Having a color red is common to all instances of this property, and thus is not informative at all for identity.A red amount of matter would be an instance of this property, which is not a whole, as would a red ball, which is a whole. Therefore we must choose -U, indicating that there is no common unity criterion for all instances.Finally, we choose -R since some instances of Red may be necessarily so, and most will not. This weak and unspecific combination of meta-properties indicates that this property is of minimal utility in an ontology, we call them attributions Agent
We intend here an entity that plays a causal part in some event. Just about anything can be an agent, a person, the wind, a bomb, etc. Thus there is no common identity nor unity criterion for all instances, and we choose -I-U. No instance of agent is necessarily an agent, thus the property is ∼R. Clearly this assignment of meta-properties selects a particular meaning of agent among the many possible ones. See for example Group
We see here a group as an unstructured finite collection of wholes. Instances of group are mereologically extensional as they are defined by their members, thus +O. Since, given a group, we have no way to isolate it from other groups, no group is per se a whole, thus ∼U. In any case, like many general terms, Group is fairly ambiguous, and once again this choice of identity criteria and anti-unity exposes the choice we have made. Finally, it seems plausible to assume that every instance of group is necessarily so, thus +R.Physical Object
We think here of physical objects as isolated material entities, i.e., something that can be "picked up and thrown" (at a suitable scale, since a planet would be considered an instance of a physical object as well. . . ). Under this vision, what characterizes physical objects is that they are topological wholes -so we assign +U to the corresponding property.For the sake of simplicity, we assume here that no two instances of this property can exist in the same spatial location at the same time. This is an identity criterion, so we assign +O to this property. Note that this is a synchronic identity criterion (see identity and unity, above) -we do not assume a common diachronic identity criterion for all physical objects.Physical object is a rigid property, so we have +R. To see this, consider the alternative: there must be some instance of the property that can, possibly, stop being a physical object, yet still exist and retain its identity. By assigning rigidity to this property, we assert that there is no such instance, and that every instance of Physical Object ceases to exist if it ceases to be a physical object.Living Being
Instances of living being must be wholes according to some common biological unity criterion. We do not need to specify it to assign +U to this property.For identity, it is difficult to assume a single criterion that holds for all instances of living being. The way we, e.g., distinguish people may be different from the way we distinguish dogs. However, a plausible diachronic criterion could be having the same DNA (although only-necessary, since it does not help in the case of clones). Moreover, we can easily think of essential properties that characterize living beings (e.g., the need of taking nutrients from the environment), and this is enough for assigning them +O.We assume living being to be a rigid property (+R), so if an entity ceases to be living then it ceases to exist. Notice that this is a precise choice that is totally dependent on our conceptualization: nothing would exclude considering life as a contingent (non-rigid) property; by considering it as rigid, we are indeed constructing a new kind of entity, justified by the fact that this property is very relevant for us.Food
Nothing is necessarily food, and just about anything is possibly food. In a linguistic sense, "food" is a role an entity may play in an eating event. Considering that anything that is food can also possibly not be food, we assign ∼R to this property. We also assume that any quantity of food is an amount of matter and inherits its extensional identity criterion, thus +I and ∼U.Animal
Like for living being, the identity criteria for animal may be difficult to characterize precisely, but we can devise numerous essential properties that apply only to them, or only-sufficient conditions that act as heuristics especially for diachronic identity criteria. Humans, in particular, are quite good at recognizing most individual animals, typically based on clues present in their material bodies. The undeniable fact is that we do recognize "the same" animal over time, so there must be some way that is accomplished. Therefore, we assign +O.The property is clearly rigid (+R); moreover, being subsumed by living being, it clearly carries unity (+U).Legal Agent
This is an agent that is recognized by law. It exists only because of a legal recognition. Legal agents are entities belonging to the so-called social reality, insofar their existence is the result of social interaction. All legal systems assign well-defined identity criteria to legal agents, based on, for example, an id number. Therefore, it seems plausible to assign +O. Concerning unity, if we include companies (as well as persons) among legal agents, then probably there is no unity criteria shared by all of them, so we assign -U. Finally, since nothing is necessarily a legal agent, we assign ∼R. For instance, we may assume that a typical legal entity, such as a person, becomes such only after a certain age.Group of People
A special kind of Group all of whose members are instances of Person. Identity and unity criteria are the same as Group, and thus we have +I∼U. Finally, we consider Group of People to be rigid, since any entity which is a group of people must necessarily be such.Social Entity
A group of people together for social reasons. Such as the "Bridge Club" (i.e., people who play cards together). We cannot imagine a common identity criteria for this property, however we assume it is rigid and carries unity.-I+U+R.Organization
Instances of this property are intended to be things like companies, departments, governments, etc. They are made up of people with play specific roles according to some structure. Like people, organizations seem to carry their own identity criterion, and are wholes with a functional notion of unity, so we assign +O+U+R.Fruit
We are thinking here of individual fruits, such as oranges or bananas. We assume they have their own essential properties, and can clearly be isolated from each other. Therefore, +O+U+R seems to be an obvious assignment.Apple
This likely adds its on essential properties to those of fruits, so we assign it +O+U+R.Red-Apple
Red apples do not have essential meta-properties in addition to apples. Moreover, no red apple is necessarily red, therefore we assign +I+U∼R.Vertebrate
This property is actually intended to be vertebrate-animal. This is a biological classification that adds new membership criteria to Animal (has-backbone), but apparently no new identity criteria: +I+U+R.Person
Like Living Entity and Animal, the Person property is +I+U. It seems clear that specializing from Vertebrate to Person we add some further essential properties, thus we assume that Person has its own identity criteria, and we assign +O.Butterfly and Caterpillar
Like Animal, Butterfly and Caterpillar have +I+U. However, every instance of Caterpillar can possibly become a non-caterpillar (namely a butterfly), and every instance of Butterfly can possibly be (indeed, must have been) a non-butterfly (namely a caterpillar), thus we assign ∼R to each.Country
Intuitively, a country is a place recognized by convention as having a certain political status. Identity may be difficult to characterize precisely, but some essential properties seem to be clearly there, so +O. Countries are certainly wholes, so +U. Interestingly, it seems clear that some countries, like Prussia, still exist but are no longer countries, so we must assign ∼R.Analyzing Rigid Properties The Backbone Taxonomy
We now focus our analysis on what we have called the backbone taxonomy, that is, the rigid properties in the ontology, organized according to their subsumption relationships. These properties are the most important to analyze first, since they represent the invariant aspects of the domain. Our sortal expandability and individuation principles guarantee that no element of the domain is "lost" due to this restriction, since every element must instantiate at least one of the backbone properties, that supplies an identity criterion for it.The backbone taxonomy based on the initial ontology is shown in Fig. After making the initial decisions regarding meta-properties and arranging the properties in a taxonomy, we are then in a position to verify whether any constraints imposed by the meta-properties are violated in the backbone. These violations have proven to be excellent indicators of misunderstandings and improperly constructed taxonomies. When a violation is encountered, we must reconsider the assigned meta-properties and/or the taxonomic link. and take some corrective action.Living beings are not amounts of matter. The first problem we encounter is between Amount of Matter and Living Being. The problem is that a ∼U property cannot subsume one with +U. While it certainly seems to make sense to say that all living beings are amounts of matter, based on the meaningFig. 2. The initial backbone taxonomy with meta-properties Backbone Constraint Violations
we have assigned there is an inconsistency: every amount of matter can be arbitrarily scattered, but this is certainly not the case for living beings. A further reason against this subsumption link is in the identity criteria: amounts of matter have an extensional identity, that is, they are different if any of their parts is substituted or annihilated -if you remove some clay from a lump of clay, it is a different amount. Living beings, on the other hand, can change parts and still remain the same -when you cut your fingernails off you do not become a different person. This is one of the most common modeling problems we have. Living beings are constituted of amounts of matter, they are not themselves the matter. Natural language convention fails to capture this subtle distinction, but it is a violation of the intended meaning to claim that all living beings are mereologically extensional.The solution here is to remove the subsumption link between these two properties, and represent the relationship as one of constitution.Physical objects are not amounts of matter. Again, we see a violation since a ∼U property cannot subsume one with +U. This is yet another example of constitution being confused with subsumption. Physical objects are not themselves amounts of matter, they are constituted of matter. The solution is to make Physical Object subsumed directly by Entity.Social entities are not groups of people. Another ∼U/+U violation, as well as a violation of identity criteria. Social entities are constituted of people, but, as with other examples here, they are not merely groups of people, they are more than that. A group of people does not require a unifying relation, as we assume these people can be however scattered in space, time, or motivations. On the contrary, a social entity must be somehow unified. Moreover, although both properties supply their own identity criteria, these criteria are mutally inconsistent. Take for instance two typical examples of social entities, such as a bridge club and a poker club. These are clearly two separate entities, even though precisely the same people may participate in both. Thus we would have a situation where, if the social entity was the group of people, the two clubs would be the same under the identity criteria of the group, and different under the identity criteria of the social entity. The solution of the puzzle is that this is, once again, a constitution relationship: a club is constituted by a group of people.Animals are not physical objects. Although no constraints involving metaproperties are violated in this subsumption link, a closer look at the identity criteria of the two properties involved reveals that the link is inconsistent. Animals, by our account, cease to exist at death, since being alive is an essential property for them. However their physical bodies remain for a time after: being alive is not essential to them. Indeed, under our assumption no physical object has being alive as an essential property. Now, if an animal is a physical object, as implied by subsumption, how could it be that it is at the same time necessarily alive and not necessarily alive? The answer is that there must be two entities, related by a form of constitution, and the subsumption link should be removed.In this example, it is not the meta-properties, but the methodology requiring to make identity criteria explicit in terms of essential properties that reveals the error.Analyzing Non-rigid Properties
Let us now turn our attention to the non-rigid properties, which -so to speak -"flesh out" the backbone taxonomy. In Among other things, the differences among these property kinds are based on a meta-property not discussed here, based on the notion of dependence. A proper grasping of this notion (which is rather difficult to formalize) is not essential for an introductory understanding of the OntoClean methodology, so we shall rely on intuitive examples only.Phased Sortals
The notion of a phased sortal was originally introduced by Wiggins In the typical case, phased sortals come into clusters of at least two properties -an instance of a phased sortal (e.g., Caterpillar ) should be able to "phase" into another one (e.g., Butterfly), and these clusters should have a common subsuming property providing an identity criterion for across phases, according to the sortal individuation principle.Caterpillars and butterflies. Consider now our example. Caterpillar and Butterfly appear in out initial taxonomy, but there is no single property that subsumes only the phases of the same entity. Our formal analysis shows that there must be such property. After some thinking, we find what we need: it is the property Lepidopteran, which is +O+U+R. This is what supplies the identity criteria needed to recognize the same entity across phases.Countries. The property Country does not, prima facie, appear to be a phased sortal, yet it meets our definition (+O∼R). This is an example where reasoning on the meta-properties assignments and their consequences helps us pushing our ontological analysis further: what are we talking of, here? Is it a region that occasionally becomes a country, and in this case acquires some extra (yet temporary) identity criteria? What happens when something is not a country any more? Does it cease to exist, or does it just undergo the change of a property, like changing from being sunny and being shady? While answering to these questions, we realize we are facing a common problem in building ontologies, that of lumping together multiple meanings of a term into a single property. It seems there are two different interpretations of "country," one as a geographical region, and another as a geopolitical entity. It is the latter that ceases to exist when the property does not hold any more.So there are two entities: the Country Prussia and the Geographical Region Prussia. These two entities are related to each other (e.g., countries occupy regions), but are not the same, and therefore we must break the current property into two.We assign +O+U+R to Country, and +I-U+R to Geographical Region. The intuition is that countries have their own identity criteria, while geographical regions inherit the identity of locations. Countries have clearly a unity, while this is not the case for arbitrary geographical regions. Both properties are now rigid. Interestingly enough, we replaced an anti-rigid property with two rigid properties.Roles
After analyzing phased sortals, we end up with the taxonomy shown in Fig. This is a different kind of problem in which subsumption is being used to represent a type restriction. The modeler intends to mean, not that all animals are agents, but that animals can be agents. This is a very common misuse of subsumption, often employed by object-oriented programmers. The correct way to represent this kind of relationship is with a covering, i.e., all agents are either animals or social entities. Clearly this is a different notion than subsumption. The solution is to remove the subsumption links and represent this information elsewhere.Legal Agent. The next problem we encounter is when the role Legal Agent is added below Agent, with its subsuming links to Person, Organization, and Country. Again, as with the previous example, we have a contradiction, an anti-rigid property cannot subsume a rigid one, so these subsumption links (shown as dotted lines at right) must be removed. As with the Agent role, being forced to remove these links forces us to reconsider the meaning of the Legal Agent property. A legal agent is simply an entity recognized by law as an agent in some transaction or contract. Again, as with the Agent example, this is not a true subsumption link, but rather another type restriction. The links should be removed and replaced with a covering axiom.Food. We chose to model the notion of food as a role, that is a property of things that may or can be food in some situation. So nothing is essentially food -even a stuffed turkey during a holiday feast or an enormous bowl of pasta with pesto sauce may avoid being eaten and end up not being food (it is possible, however unlikely). While our notion of what an apple means may seem to be violated by removing the subsumption link to food, the point is that we have chosen to represent the property in a particular way, as a role, and this link is inconsistent with that meaning and should be removed. In this case, the links are probably being used to represent purpose (see, e.g., Attributions
The final category of properties we consider are attributions. We have one such property in our example, Red, whose instances are intended to be red things. We think that in general it is not useful representing attributions explicitly in a taxonomy, and that the proper way to model attributions is with a simple attribute, like color, and a value, such as red. This quickly brings us to the notion of qualities, discussed in the related chapter of this handbook on Dolce, and we avoid that discussion here.Attributions do, however, come in handy on occasions. Their practical utility is often found in cases where there are a large number of entities that need to be partitioned according to the value of some attribute. We may have apples and pears, for example, and decide we need to partition them into red and green ones. Ontologically, however, the notion of red-thing does not have much significance, since there is nothing we can necessarily say of red-things, besides their color. This seems to us a very good reason for not consider attributions as part of the backbone. In other words, the backbone taxonomy helps in focusing on the more important classes for understanding the invariant aspects of domain structure, whereas attributions help in organizing the instances on an ad-hoc, temporary basis.Conclusion
The final, cleaned, taxonomy is shown in Fig. Introduction
Computational ontologies in the context of information systems are artifacts that encode a description of some world (actual, possible, counterfactual, impossible, desired, etc.), for some purpose. They have a (primarily logical) structure, and must match both domain and task: they allow the description of entities whose attributes and relations are of concern because of their relevance in a domain for some purpose, e.g. query, search, integration, matching, explanation, etc.Like any artifact, ontologies have a lifecycle: they are designed, implemented, evaluated, fixed, exploited, reused, etc. (cf. chapter "Ontology Engineering Methodology" for an in-depth examination of ontology engineering methodologies).In this chapter, we focus on patterns for ontology design Nowadays, an average user that is trying to build or reuse an ontology, or an existing knowledge resource, is typically left with just some limited assistance in using unfriendly logical structures, some large, hardly comprehensible ontologies, and a bunch of good practices that must be discovered from the literature. A typical usage scenario includes, e.g. a large set of web ontologies that are evaluated (usually in an implicit way) against the intended domain and tasks. The selected ontology (if any) is reused, and then an adaptation process is started in order to cope with the implicit requirements from an ontology project. This scenario is costly in many cases, and automatic selection mechanisms do not help with the adaptation process. Another typical scenario includes so-called "reference" or "core" ontologies that are supposed to be directly reused and specialized. Unfortunately, even if well designed, they are usually large and cover more knowledge than what a designer might need. In this case, it is hard to reuse only the "useful pieces" of the ontology, and consequently the cost of reuse is higher than developing a new ontology from scratch.On the other hand, the success of very simple and small ontologies like FOAF Under the assumption that there exist classes of problems that can be solved by applying common solutions (as it has been experienced in software engineering), we propose to support reusability on the design side specifically. We envision small (or cleverly modularized) ontologies with explicit documentation of design rationales, and best reengineering practices. These components need specific functionalities in order to be implemented in repositories, registries, catalogues, open discussion and evaluation forums, and ultimately in new-generation ontology design tools. In this chapter, we describe small, motivated ontologies that can be used as building blocks in ontology design. A formal framework for (collaborative) ontology design that justifies the use of building blocks with explicit rationales is presented in We call the basic building blocks to be used in ontology design Content Ontology Design Patterns (CP) Throughout experiences in ontology engineering projectsMoreover, since CPs are strictly related to small use cases, they are transparent with respect to the rationales applied to the design of a certain ontology. CPs are therefore an additional tool to achieve tasks such as ontology evaluation, matching, modularization, etc. For example, an ontology can be evaluated against the presence of certain patterns (which act as unit tests for ontologies, cf. CPs are a very beneficial kind of patterns for ontology design, because they provide solutions to domain-oriented problems, and are directly reusable. On one hand, CPs are comparable to software engineering (SE) design patterns for what concerns the way they are documented and communicated. On the other hand, the intuition behind their usage is analogous to that of software engineering (object oriented) reusable libraries, e.g. Java libraries. A similar intuition is at the base of approaches to modularization of ontologies, e.g. There are other types of ontology design patterns (OPs) that are beneficial for different purposes and targeted at different types of users. A typology of OPs will be also introduced in this chapter.In principle, OPs do not depend on any specific representation language. 2 In this context, we focus mainly on CPs; in order to provide the readers with concrete examples and a closer view on their exploitation on the Semantic Web, we have decided to refer to OWL CPs (cf. chapter "Web Ontology Language: OWL" for details on OWL). In fact, CPs fit well with Semantic Web requirements for reuse and interoperability of ontologies and data, and as part of our work we have set up the ontologydesignpatterns.org web portal, which collects and makes them available on the Web Chapter's content is organized as follows: Sect. 1.1 gives some background notions; Sect. 2 introduces the types of OPs, defines them, and provides the reader with some examples; Sect. 3 presents a sample catalogue of CPs; Sect. 4 describes ways to create and work with CPs, and Sect. 5 presents an example of their application. Finally, Sect. 6 provides some conclusions and remarks.Background
In the seventies, the architect and mathematician Christopher Alexander introduced the term "design pattern" for shared guidelines that help solve design problems. In Taking seriously the architectural metaphor, the notion has been eagerly endorsed by software engineering Ontology engineering literature has tackled the notion of design pattern at least since Types of Ontology Design Patterns
An ontology design pattern (OP) is a modelling solution to solve a recurrent ontology design problem. We have identified several types of OPs, and have grouped them into six families (cf. Fig. Although this chapter mainly focuses on CPs, in this section we give an overview of the OP families, with some examples. For more details, the reader can refer to Structural OPs
Structural OPs include Logical OPs and Architectural OPs. Logical OPs are compositions of logical constructs that solve a problem of expressivity, while Architectural OPs affect the overall shape of the ontology either internally or externally.Logical OPs are only expressed in terms of a logical vocabulary, because their signature (the set of predicate names, e.g. the set of classes and properties in an OWL ontology) is empty (with minor exceptions, e.g. the default inclusion of owl:Thing in OWL). On one hand, Logical OPs are independent from a specific domain of interest (i.e. they are content-independent), on the other hand, they depend on the expressivity of the logical formalism that is used for representation. In other words, Logical OPs help to solve design problems where the primitives of the representation language do not directly support certain logical constructs. For example, if the representation language is OWL, and a designer needs to represent a relation between more than two elements, a Logical OP is needed in order to express an n-ary relation semantics by only using class and binary relation primitives. The root of Logical OPs can be found in Logical macros provide a shortcut to model a recurrent intuitive logical expression, e.g. the combination of owl:allValuesFrom restriction with owl:someValuesFrom restriction.Transformation patterns translate a logical expression from a logical language into another, which approximates the semantics of the first, in order to find a trade-off between requirements and expressivity. For example, the so called n-ary relation pattern, documented in The application of Logical OPs has consequences on the results and efficiency of reasoning procedures. They can be used in order to document design choices and are particularly suitable for teaching good practices of ontology design as they provide designers with solutions to represent complex logical expressions.Architectural OPs affect the overall shape of the ontology: their aim is to constrain "how the ontology should look like". They can be of two types: (i) internal, defined in terms of collections of Logical OPs that have to be exclusively employed when designing an ontology, e.g. an OWL species (cf. chapter "Web Ontology Language: OWL"), or the varieties of description logics (cf. chapter "Description Logics"); (ii) external, defined in terms of meta-level constructs, e.g. the modular architecture consists of an ontology network, where the involved ontologies play the role of modules (according to definitions given in Architectural OPs emerged as design choices motivated by specific needs, e.g. computational complexity constraints. Such OPs are also useful as reference documentation for those initially approaching the design of an ontology.Reasoning OPs
Reasoning OPs are applications of Logical OPs oriented to obtain certain reasoning results, based on the behaviour implemented in a reasoning engine. Examples of Reasoning OPs include: classification, subsumption, inheritance, materialization, de-anonymizing, etc.Reasoning OPs, when declared on top of an ontology, inform about the state of that ontology, and let a system decide what reasoning has to be performed on the ontology in order to carry out queries, evaluation, etc. Examples of Reasoning OPs are so called normalizations. In Correspondence OPs
Correspondence OPs include Reengineering OPs and Mapping OPs.Reengineering OPs provide designers with solutions to the problem of transforming a conceptual model, which can even be a non-ontological resource, into a new ontology. Mapping OPs are patterns for creating semantic associations between two existing ontologies.Reengineering OPs are transformation rules applied in order to create a new ontology (target model) starting from elements of a source model. The target model is an ontology, while the source model can be either an ontology, or a non-ontological resource, e.g. a thesaurus concept, a data model pattern, a UML model, a linguistic structure, etc. Reengineering OPs are described in terms of metamodel transformation rules. We distinguish two types of Reengineering OPs.Schema reengineering patterns are rules for transforming, e.g. a non-OWL DL metamodel into an OWL DL ontology. For example, consider the use of SKOS Refactoring patterns provide designers with rules for transforming, i.e. refactoring, e.g. an existing OWL DL source ontology into a new OWL DL target ontology. In this case, the transformation rule has the effect of changing the type of the ontology elements that are involved in the refactoring. For example, let us consider the case in which an ontology defines an object property for representing the relation of preparing a coffee, which holds between agents and coffees. Now, let us consider a change of requirements, so that a designer has to represent that the coffee is prepared by an agent at a certain time by using a certain tool. In order to address such a change in OWL DL, a designer has to apply an n-ary relation Logical OP, because preparing a coffee has now four arguments: agent, coffee, time interval, and tool. The n-ary relation Logical OP plus the description of how to apply it in order to replace an object property from an existing ontology is a Refactoring OP.Mapping ontology design patterns Mapping OPs refer to the possible semantic relations between mappable elements, as defined in Presentation OPs
Presentation OPs deal with usability and readability of ontologies from a user perspective. They are meant as good practices that support the reuse of ontologies by facilitating their evaluation and selection. Examples are Naming OPs and Annotation OPs. The former are conventions about how to create names for namespace, files, and ontology elements in general (classes, properties, etc.). They are good practices that boost ontology readability and understanding by humans, by supporting homogeneity in naming procedures. Annotation OPs provide annotation properties or annotation property schemas that can be used in order to improve the understandability of ontologies and their elements.An example of Naming OP relates to namespace declared for ontologies. It is recommended to use the base URI of the organization that publishes the ontology (e.g. http://www.w3.org for the W3C, http://www.fao.org for the FAO, http://www.loa-cnr.it for the Laboratory for Applied Ontologies (LOA) etc.) followed by a reference directory for the ontologies (e.g. http://www.loa-cnr.it/ontologies/). Additionally, it is also important to choose an approach for encoding versioning, either on the name, or on the reference directory.Lexico-Syntactic OPs
Lexico-Syntactic OPs are linguistic structures or schemas that consist of certain types of words following a specific order, and that permit to generalize and extract some conclusions about the meaning they express. They are useful for associating simple Logical and Content OPs with natural language sentences, e.g. for didactic purposes.Content Ontology Design Patterns (CPs)
CPs encode conceptual, rather than logical design patterns. In other words, while Logical OPs solve design problems independently of a particular conceptualization, CPs propose patterns for solving design problems for the domain classes and properties that populate an ontology, therefore addressing content problems Towards a Catalogue and Repository of CPs
In this section we focus on CPs. We define them, and explain the dependencies between CPs and use cases (Sect. 3.1). Section 3.2 lists the characteristics that differentiate CPs as special ontologies (such characteristics cross the boundaries between ontology engineering, cognitive science, and linguistics). Finally, we describe two CPs (Sect. The way to document OPs can be compared to the typical way followed for SE patterns. The mainstream approach for describing SE patterns is to use a template, although there is no standard format. A description of the most wellknown SE pattern templates can be found at Martin Fowler's web site. In order to describe CPs, we follow a similar approach: each CP is associated with a catalogue entry including the following set of information fields.Name provides a name for the pattern; Intent describes the Generic Use Case addressed by the pattern; Competency questions contains examples of competency questions that the knowledge base associated with the CP needs to address; Also Known as provides other names (if any) with which the pattern is known; Scenarios provides examples of requirements, expressed in natural language, which can be modeled by using the pattern; Diagram depicts a UML class diagram representing the pattern; Elements describes the elements (classes and relations) included in the pattern, and their role within the pattern; Consequences provides a description of the benefits and/or possible trade-offs when using the pattern; Known uses gives examples of realistic ontologies where the pattern is used, Extracted from/Reengineered from provides the reference ontology/conceptual schema (if any), from which the pattern has been extracted/reused; Related patterns indicates other patterns (if any) that are either a specialization, generalization, composition, or component of the pattern being described. Furthermore, this field may indicate other patterns that are typically used in conjunction with the described one. Important similarities and differences with other patterns can be also described here; Building block provides references to implementations of the pattern, a URI. In the case of CPs for Semantic Web ontologies, this field provides the URI of an OWL file (containing an implementation of the pattern).Section 3.3 contains two examples of CPs that are described by means of a simplified version of the catalogue template. Such a catalogue can be found at the ontologydesignpatterns.org web portal CPs and Competency Questions
CPs are reusable solutions to recurrent modelling problems. As known from a long time in conceptual modelling (cf. the difference between class and use case diagrams in UML) and knowledge engineering (cf. the distinction between domain and task ontologies in UPML Ontologies are usually considered models for a domain, but their use case is usually unknown. As reusable solutions, CPs must explicitly encode both a domain and a use case. Since use cases are extremely diversified, a catalogue of CPs requires the notion of a "Generic Use Case" (GUC), i.e. a generalization of use cases that can be provided as examples for an issue of domain modelling. A GUC is the expression of a recurrent scenario in different domain ontology projects.Being generic at the use case level allows us to divide, or to refactor the design problems of a use case, by composing different GUCs. We can hierarchically organize GUCs from the most generic to the most specific ones, and from the "purest" (e.g. "which objects take part in a certain event?") to the most articulated and applied ones (e.g. "what protein is involved in the Jack/Stat biochemical pathway?").The intuition underlying GUC hierarchies is based on a methodological observation: ontologies must be built out of domain tasks that can be captured by means of competency questions General Characteristics of CPs
CPs are components that represent, and possibly help solving a modelling problem arising across different use cases. E.g. the agent-role pattern provides a solution to represent agents that play some role. We have sketched their theoretical basis in Sect. 2, and explained their dependance on use cases (Sect. 3.1). Before providing a sample list of CPs against an example use case (Sect. 3.3), we now describe a more inclusive set of general, pragmatic features of CPs. These features, besides positioning CPs in a wider scientific context, give hints on how to discover or to extract CPs from existing knowledge resources.Computational components. CPs are language-independent, and should be encoded in a higher-order representation language. Nevertheless, their (sample) representation in OWL is needed in order to (re)use them as building blocks over the Semantic Web.Small, autonomous components. Regardless of the particular way a CP has been created, it is a small, autonomous ontology. Smallness (typically two to ten classes with relations defined between them) and autonomy of CPs facilitate ontology designers: composing CPs enable them to govern the complexity of the whole ontology, because of the explicit rationales and the amount of know-how provided by the users of a same CP library. Smallness also allows diagrammatical visualizations that are aesthetically acceptable and easily memorizable.Hierarchical components. A CP can be an element in a partial order, where the ordering relation requires that at least one of the classes or properties in the pattern is specialized. A hierarchy of CPs can be built by specializing or generalizing some of the elements (either classes or relations).Inference-enabling components. There are combinations of ontology elements that do not allow any useful inference, e.g. a taxonomy with two sibling classes, an object property alone, etc. A CP allows some form of inference, e.g. a taxonomy with two sibling disjoint classes, a property with explicit domain and range set, a property and a class with a universal restriction on that property, etc.Cognitively relevant components. CP visualization must be intuitive and compact, and should catch relevant, "core" notions of a domain Linguistically relevant components. Many CPs nicely match linguistic patterns called frames. A frame can be described as a lexically founded OP. The richest repository of frames is FrameNet Best practice components. A CP should be used to describe a "best practice" of modelling. Best practices are intended here as local, thus derived from experts, emerging from real applications. The quality of CPs is currently based on the personal experience and taste of the proposers, or on the provenance of the knowledge resource where the pattern comes from. However, evidence from reusability across different projects, large-scale applications, and open rating systems will provide a good base for CP evaluation.Samples of CP Catalogue Entries
In this section we show two CPs taken from In the rest of this section we use the OWL terminology in order to describe the proposed design solutions, e.g. object property, datatype property, etc.The information realization CP
The information realization CP is extracted from the Dolce+DnS Ultra Lite ontology, The information realization CP is associated with information according to the catalogue entry fields reported below:Intent: to represent relations between information objects and their physical realizations.Competency questions: which physical object realizes a certain information object? Which information object is realized by a certain physical object?Diagram: Fig. Elements:• InformationObject: a piece of information, such as a musical composition, a text, a word, a picture, independently from how it is concretely realized. • InformationRealization: a concrete realization of an InformationObject, e.g. the written document containing the text of a law. General remarks: this CPFig. 2. The information realization CP UML graphical representation
The Time Indexed Person Role CPThe time indexed person role is a CP that represents time indexing for the relation between persons and roles they play, e.g. George W. Bush was the president of the United States in 2007. This CP is also extracted from the Dolce+DnS Ultra Lite ontology.According to its associated catalogue entry, the main information associated with this CP are the following:Intent: to represent time indexing for the relation between persons and roles they play.Competency questions: who was playing a certain roles during a given time interval? When did a certain person play a specific role?Diagram: see Fig. Elements:• Entity: anything: real, possible, or imaginary, which some modeller wants to talk about for some purpose.  • isSettingFor: a relation between time indexed role situations and related entities, e.g. "I was the director between 2000 and 2005", i.e. the situation in which I was a director is the setting for a the role of director, me, and the time interval. • hasSetting: the inverse relation of isSettingFor.General remarks: this CPCreating and Working with CPs
This section discusses how CPs can be created, and provides guidelines on how they can be practically (re)used. Section 4.1 describes four approaches to create CPs, while Sect. 4.2 shows the main operations that are performed for reusing a CP, and describes the possible situations of CP selection and usage that can occur in practice.Where do Content Ontology Design Patterns Come from?
CP creation and usage rely on a common set of operations.Import: consists of including a CP in the ontology under development. This is the basic mechanism for reusing CPs (and ontologies in general). By importing a CP, the importing ontology ensures the set of inferences allowed by the CP in its corresponding knowledge base. Elements of an imported CP cannot be modified. Specialization: can be referred to ontology elements or to CPs. Specialization between ontology elements of a CP consists of creating sub-classes of some CP's class and/or sub-properties of some CP's properties. A CP c 1 is a specialization of a CP c if c 1 imports c, and at least one ontology element from c 1 specializes an ontology element from c. Generalization: A CP c 1 is a generalization of a CP c if c 1 imports c, and at least one ontology element from c 1 generalizes an element from c. Composition: consists of associating classes (properties) of one CP with classes (properties) of other CPs, by means of some OWL axiom. Expansion: consists of adding new classes, properties and axioms to the ontology to the aim of covering the requirements that are not addressed by the reused CPs.CPs come from the experience of ontology engineers in modelling foundational (cf. chapter "Foundational Choices in DOLCE"), upper-level, core Reengineering from other data models. A CP can be the result of a reengineering process applied to different conceptual modelling languages, primitives, and styles. Knowledge resources that can be reengineered to produce candidate CPs are database schemas, knowledge organization systems such as thesauri, and lexica. For more references, the reader can refer to Specialization/Composition of other CPs. A CP can be created by composing other CPs, or by specializing another CP, (both composition and specialization can be combined with expansion, see below).Extraction from reference ontologies. A CP can be extracted from an existing ontology, which acts as the "source" ontology. In this case, the CP corresponds to a fragment of the source ontology, which constitutes its axiomatic background context. A CP is axiomatized according to the fragment it extracts. E.g. the co-participation CP depends on a set of axioms from the DOLCE ontology Creation by combining extraction, specialization, generalization, and expansion The definition of a CP can be the result of an extraction (see above), followed by specialization and/or generalization of some ontology elements, and expansion. 15   How to Use Content Ontology Design Patterns
Supporting reuse and alleviating difficulties in ontology design activities are the main goals of setting up a catalogue of CPs. In order to be able to reuse CPs, two main functionalities must be ensured: selection and application.Selection of CPs corresponds to finding the most appropriate CP for the actual domain modelling problem. Hence, selection includes search and evaluation of available CPs. This task can be performed by applying procedures for ontology selection Several situations of matching between GUCs and actual use cases can occur, each associated with a different approach to using CPs. The following summary assumes a manual (re)use of CPs. However, an initial library of CPs is already available Broader matching. The CP matches a GUC that is more general than the local use case: the CP's catalogue entry may contain reference to less general CPs that specialize it. If none of them is appropriate, the CP has firstly to be imported, then it has to be specialized in order to cover the domain part to be represented.Narrower matching. The CP matches a GUC that is more specific than the local use case: the CP's catalogue entry may contain references to more general CPs. If none of them is appropriate, a the CP has firstly to be imported, then it has to be generalized according to the local requirements.Partial matching. The CP partly matches a GUC that does not cover all aspects of the local use case (it is simpler): the CP's catalogue entry may contain references to CPs it is a component of. If none of such compound CPs is appropriate, the local use case has to be partitioned into smaller pieces. One of these pieces will be covered by the selected CP. For the other pieces, other CPs have to be selected. All selected CPs have to be imported and composed.In all the above situation, expansion is performed when needed.Use Case Example in the Music Industry Domain
As an example of usage we design a small fragment of an ontology for the music industry domain. The ontology fragment has to address the following competency questions:Which recordings of a certain song do exist in our archive? Who did play a certain musician role in a given band during a certain period?
The first competency question requires to distinguish between a song and its recording, while the second competency question highlights the issue of assigning a given musician role, e.g. singer, guitar player, etc., to a person who is member of a certain band, at a given period of time. The intent of the information realization is related to the first competency question with a broader matching. The intent of the time indexed person role partially and broadly matches the second competency question. Hence, we select these two CPs as building blocks for our ontology. 16  We proceed by importing and composing the two selected CPs in our ontology (the information realization CP is associated with the prefix ir:; the time indexed person role CP is associated with the prefix tipr:). Additionally, we might want to import the time interval CP 17 that allows us to assign a date to the time interval. In order to complete our ontology fragment we create: the class Song that specializes ir:InformationObject, the class Recording that specializes ir:InformationRealization, the class MusicianRole that specializes tipr:Role, the class Band, and the object property memberOf (and its inverse) with explicit domain, i.e. tipr:Person, and range, i.e. Band. A screenshot of the resulting ontology fragment is shown in Fig. Fig. 4. The music industry example
Conclusion and Remarks
Ontology design is a crucial research area for semantic technologies. Many bottlenecks in the wide adoption of semantic technologies depend on the difficulty of understanding ontologies and on the scarcity of tools supporting their lifecycle, from creation to adaptation, reuse, and management. The lessons learnt until now, either from the early adoption of Semantic Web solutions or from local, organizational applications, put a lot of emphasis on the need for simple, modular ontologies that are accessible and understandable by typical computer scientist and field experts, and on the dependability of these ontologies on existing knowledge resources. The quality of these components is expected to be evaluated with respect to known good practices, as well as in the large testbed of organizational or web-scale open rating systems. In order to allow the maximum transparency and flexibility, OPs are supplied with a rich set of metadata for their explanation, rationale declaration, use case history, evaluation criteria, etc. In this chapter, we have sketched a typology of OPs, then focused on Content Ontology Design Patterns (which are most beneficial to ontology design) in terms of their background, definition, communication means, related work beyond ontology engineering, exemplification, creation, and usage principles.There is still a lot of work to be carried out for populating repositories of patterns, discoverying or extracting them from existing ontologies, assisting users in their application, defining a robust semantics and algebra for them, etc. (cf. Introduction
Ontology engineering is slowly changing its status from an art to a science and in fact, during the last decade, several ontology engineering methodologies (see chapters "Ontology Engineering Methodology" and "Ontology Engineering and Evolution in a Distributed World Using DILIGENT") have been examined. But still, as pointed out in chapter "Exploring the Economical Aspects of Ontology Engineering", the task of engineering an ontology remains a resource-intensive and costly task. Therefore, techniques which support the task of ontology engineering are necessary to reduce the costs associated with the engineering and maintenance of ontologies. As data in various forms (textual, structured, visual, etc.) is massively available, many researchers have developed methods aiming at supporting the engineering of ontologies by data mining techniques, thus deriving meaningful relations which can support an ontology engineer in the task of modeling a domain. Such data-driven techniques supporting the task of engineering ontologies have become to be known as ontology learning. Ontology learning has indeed the potential to reduce the cost of creating and, most importantly, maintaining an ontology. This is the reason why a plethora of ontology learning frameworks have been developed in the last years and integrated with standard ontology engineering tools. Text-ToOnto Ontology Learning builds upon well-established techniques from a variety of disciplines, including natural language processing, machine learning, knowledge acquisition and ontology engineering. Because the fully automatic acquisition of knowledge by machines remains in the distant future, the overall process is considered to be semi-automatic with human intervention.Organization
This chapter is organized as follows: Sect. 2 introduces a generic architecture for ontology learning and its relevant components. In Sect. 3 we introduce various complementary basic ontology learning algorithms that may serve as a basis for ontology learning. Section 4 describes ontology learning frameworks and tools which have been implemented in the past. In particular, we also discuss our own system, Text2Onto, the successor of the TextToOnto framework An Architecture and Process Model for Ontology Learning
The purpose of this section is to introduce a generic ontology learning architecture and its major components. The architecture is graphically depicted in Fig. Ontology Management Component
The ontology engineer uses the ontology management component to manipulate ontologies. Ontology management tools typically facilitate the import, Techniques for ontology evolution as presented in Coordination Component
The ontology engineer uses this component to interact with the ontology learning components for resource processing as well as with the algorithm library.Comprehensive user interfaces should support the user in selecting relevant input data that are exploited in the further discovery process. Using the coordination component, the ontology engineer also chooses among a set of available resource processing methods and among a set of algorithms available in the algorithm library. A central task of the coordination component is further to sequentially arrange and apply the algorithms selected by the user, passing the results to each other.Resource Processing Component
This component contains a wide range of techniques for discovering, importing, analyzing and transforming relevant input data. An important subcomponent is the natural language processing system. The general task of the resource processing component is to generate a pre-processed data set as input for the algorithm library component.Resource processing strategies differ depending on the type of input data made available. Semi-structured documents, like dictionaries, may be transformed into a predefined relational structure. HTML documents can be indexed and reduced to free text. For processing free text, the system must have access to language-specific natural language processing systems. Nowadays, off-the-shelf frameworks such as GATE • A tokenizer and a sentence splitter to detect sentence and word boundaries.• A morphological analyser. For some languages a lemmatizer reducing words to their base form might suffice, whereas for languages with a richer morphology (e.g., German) a component for structuring a word into its components (lemma, prefix, affix, etc.) will be necessary. For most machine learning-based algorithms a simple stemming of the word might be sufficient (compare Chunkers are also called partial parsers. An example of a publicly available chunker is Steven Abney's CASS Algorithm Library Component
This component acts as the algorithmic backbone of the framework. A number of algorithms are provided for the extraction and maintenance of the ontology modeling primitives contained in the ontology model. Thus, the algorithm library contains the actual algorithms applied to learning. In particular, the algorithm library consists mainly of machine learning algorithms and versions of these customized for the purpose of ontology learning. In particular, machine learning algorithms typically contained in the library are depicted in Table Most of these machine learning algorithms can be obtained off-the-shelf in various versions from standard machine learning frameworks such as WEKA Concept discovery (extension and intension)
Learning concepts and concept hierarchies for example, there is a blackboard-style result structure -the POM (Possible Ontologies Model) -where all algorithms can update their results.Ontology Learning Algorithms
The various tasks relevant in ontology learning have been previously organized in a layer diagram showing the conceptual dependencies between different tasks. This ontology learning layer cake was introduced in an intensional description i(c), an extension [[c]
] and a reference function Ref c representing how the concept is symbolically realized in a text corpus, an image, etc. (see Term Extraction
The task at the lexical layers is to extract terms and arrange these into groups of synonymous words. A simple technique for extracting relevant terms that may indicate concepts is counting frequencies of terms in a given set of (linguistically preprocessed) documents, the corpus D. In general this approach is based on the assumption that a frequent term in a set of domain-specific texts indicates the occurrence of a relevant concept. Research in information retrieval has shown that there are more effective methods of term weighting than simple counting of frequencies. Weighting measures well-known from information retrieval such as tf.idf (see Further, the computational linguistics community has proposed a wide range of more sophisticated techniques for term extraction. An interesting measure is the C-value/NC-value measure presented in Synonym Extraction
In order to extract synonyms, most approaches rely on the distributional hypothesis claiming that words are semantically similar to the extent to which they share syntactic contexts Example 2. Assuming that we parse a text corpus and identify, for each noun, the verbs for which it appears at the object position, we can construct a matrix as follows:Each row represents the context of a word, while each column corresponds to one dimension of the context representation, in our case the different verbs that the nouns appear at the object position. Assuming the representation as binary vectors shown in the matrix above, we can for example calculate the similarity between the different terms using the Jaccard coefficient which compares the sets A and B of the non-negative dimensions of the vector representations two words a and b: Jaccard := Important approaches along these lines include the work of Grefenstette where P (x, y) is the probability for a joint occurrence of x and y and P (x) is the probability for the event x. The PMI is thus in essence the (logarithmic) ratio of the joint probability and the probability under the assumption of independence. In fact, if P (x, y) ≤ P (x)P (y), we will have a negative (or zero) value for the PMI, while in case P (x, y) > P(x)P (y), we will have a positive PMI value. The PMI can be calculated using Google and counting hits as follows:Hits(x AND y) MaxPages Hits(y) Hits(y)
where MaxPages is an approximation for the maximum number of English web pages. This measure can thus be used to calculate the statistical dependence of two words on the Web. If they are highly dependent, we can assume they are synonyms or at least highly semantically related. This approach to discover synonyms has been successfully applied to the TOEFL test (see Concept Learning
In this section we focus on approaches inducing concepts by clearly defining the intension of the concept. We will distinguish the following three paradigms:Conceptual Clustering
Conceptual clustering approaches such as Formal Concept Analysis ( Linguistic Analysis
Linguistic analysis techniques can be applied to derive an intensional description of a concept in the form of a natural language description. The approach of Velardi et al. Finally, given a populated knowledge base, approaches based on inductive learning such as Inductive Logic Programming can be applied to derive rules describing a group of instances intentionally. Such an approach can for example be used to reorganize a taxonomy or to discover gaps in conceptual definitions (compare Concept Hierarchy
Different methods have been applied to learn taxonomic relations from texts.
In what follows we briefly discuss approaches based on matching lexicosyntactic patterns, clustering, phrase analysis as well as classification.Lexico-Syntactic Patterns
In the 1980s, people working on extracting knowledge from machine readable dictionaries already realized that regularities in dictionary entries could be exploited to define patterns to automatically extract hyponym/hypernym and other lexical relations from dictionaries (compare In her seminal work, Hearst The patterns used by Hearst are the following: Overall, lexico-syntactic patterns have been shown to yield a reasonable precision for extracting is-a as well as part-of relations (e.g., Clustering
Clustering can be defined as the process of organizing objects into groups whose members are similar in some way based on a certain representation, typically in the form of vectors (see 1. Agglomerative: In the initialization phase, each term is defined to constitute a cluster of its own. In the growing phase, larger clusters are iteratively generated by merging the most similar/least dissimilar ones until some stopping criterion is reached. Examples of uses of agglomerative clustering techniques in the literature are Conceptual: Conceptual clustering builds a lattice of terms by investigat-
ing the exact overlap of descriptive attributes between two represented terms. In the worst case, the complexity of the resulting concept lattice is exponential in n. Thus, people either just compute a sublattice Either way one may construct a hierarchy of term clusters for detailed inspection by the ontology engineer.Example 3. Using hierarchical agglomerative clustering, we can build a cluster tree for the objects in Example 2. Let us assume we are using single linkage as measure of the similarity between clusters. First, we cluster excursion and trip as they have a similarity of 1.0. We then cluster bike and car as this is the next pair with the highest degree of similarity. We then build a cluster consisting of bike, car and apartment. Next, we either join the latter cluster with hotel or build a cluster between hotel and the already created cluster consisting of excursion and trip. Assuming that we traverse the similarity matrix from the upper left corner to the lower right one, we can add hotel to the cluster consisting of bike, car and apartment. At the top level we then join the clusters {hotel, apartment, bike, car} and {excursion, trip} producing a universal cluster containing all elements. The corresponding cluster tree would then look as follows:Phrase Analysis Some approaches rely on the fact that the internal structure of noun phrases can be used to discover taxonomic relations (compare Classification
When a substantial hierarchy is already given, e.g., by basic level categories from a general resource like WordNet Relations
In order to discover arbitrary relations between words, different techniques from the machine learning and statistical natural language processing community have found application in ontology learning. In order to discover "anonymous" associations between words, one can look for a strong co-occurrence between words within a certain boundary, i.e., a window of words, a sentence or a paragraph. Mädche et al. In the computational linguistics community, the task of discovering strong associations between words is typically called collocation discovery. In essence, the idea is to discover words which co-occur beyond chance in a statistically significant manner. Statistical significance is typically checked using some test such as the Student's t-test or the χ 2 -test (compare Axioms and Rules
Ontology learning approaches so far have focused on the acquisition of rather simple taxonomic hierarchies, properties as well as lexical and assertional knowledge. However, the success of OWL which allows for modeling far more expressive axiomatizations has led to some advances in the direction of learning complex ontologies and rules.Völker et al. One of the first methods for learning disjointness axioms relies on a statistical analysis of enumerations which has been implemented as part of the Text2Onto framework Pruning/Domain Adaptation
One relatively straightforward approach towards generating an appropriate domain ontology given a corpus is to prune an existing general ontology. Along these lines, Buitelaar et al. Ontology Learning Systems
In the last years, many different tools and frameworks for ontology learning have emerged. Needless to say that it is out of the scope of this chapter to discuss them all. Instead, we will provide a rather subjective snapshot of the current tool landscape. Some well-known and frequently cited tools are for example: OntoLearn TextToOnto Advanced Issues
In this section, we briefly discuss some advanced and open issues in ontology learning that are still under research. This section should help newcomers to get a feeling for the open questions and allow for a quicker entry into the field.Methodology
Certainly, besides providing tool support for ontology learning methods, it is crucial to define how ontology learning methods can be integrated into the process of engineering an ontology. Blueprints in this direction can be found in the work of Simperl et al. Evaluation
A crucial part of ontology learning is to evaluate how good the learned ontologies actually are. Such an evaluation can in turn guide and control the ontology learning process in the search towards an "optimal" ontology. However, the evaluation of ontology learning tools is a quite delicate issue as it is not clear what one could compare to. The critical issue in many cases is to define a gold standard which we can regard as ground truth an one can compare with (see Other approaches aim at approximating the appropriateness of some ontology by other means. Brewster et al. Expressivity
Many people argue that the main benefits of using ontologies for knowledge modeling become most evident in reasoning-based applications. Inferring new knowledge and drawing conclusions beyond explicit assertions is an important aspect of "intelligent" applications. However, the power of reasoning largely depends on the expressivity of the underlying knowledge representation formalism and its instantiation by means of a concrete ontology.The vast majority of today's lexical ontology learning focuses on the extraction of simple class descriptions and axioms, i.e., atomic concepts, subsumption and object properties, as well as ABox statements expressing concept or property instantiation. The expressivity of ontologies generated by lexical approaches, e.g., based on natural language processing techniques is mostly restricted to ALC (Attributive Language with Complements) or similar DL fragments such as AL-log. These rather simple, often informal ontologies have proven to be useful for many applications, or as Jim Hendler has put it "A little semantics goes a long way" (see Learning more expressive ontologies greatly facilitates the acquisition and evaluation of complex domain knowledge. But it also brings new challenges, e.g., with respect to logical inconsistencies that may arise as soon as any kind of negation or cardinality constraints are introduced into learned ontologies Finally, a tighter integration of lexical and logical ontology learning approaches will be required in order to prevent problems resulting from different semantic interpretations, e.g., of lexical and ontological relations (see discussion in Combination of Evidence
As it is very unlikely that we will be able to derive high-quality ontologies from one single source of evidence and using one single approach, a few researchers have addressed the challenge of learning ontologies by considering multiple sources of evidence. Cimiano et al. Dynamics and Evolution
Most ontology learning approaches assume that there is one static corpus from which to learn. However, collecting such a corpus can sometimes be a nontrivial task. Currently, some researchers are attempting to frame ontology learning as the task of keeping an equilibrium between a (growing) corpus, a (growing) set of extracted ontological primitives and a (changing) set of extraction patterns. While it seems very hard to define such an equilibrium in such a way that certain actions are triggered towards restoring it, first attempts in this direction can be found in the work of Iria et al. Conclusion
Ontology learning is a challenging and exciting research field at the intersection of machine learning, data and text mining, natural language processing and knowledge representation. While fully automatic knowledge acquisition techniques are not yet feasible (and possibly will nor should ever be), ontology learning techniques have a high potential to support ontology engineering activities. In fact, according to our view, ontology engineering can not be considered without the automatic or semi-automatic support of ontology learning methods. Future work should and will surely aim at developing a new generation of intuitive ontology learning tools which are able to learn expressive ontologies, but at the same time hide their internal complexity from the user. These new generation of tools should feature intuitive user interfaces as well as smoothly integrate into existing methodologies for ontology engineering.Ontology and the Lexicon
Graeme Hirst
Department of Computer Science, University of Toronto, Toronto, ON, Canada M5S 3G4, gh@cs.toronto.edu Summary. A lexicon is a linguistic object and hence is not the same thing as an ontology, which is non-linguistic. Nonetheless, word senses are in many ways similar to ontological concepts and the relationships found between word senses resemble the relationships found between concepts. Although the arbitrary and semi-arbitrary distinctions made by natural languages limit the degree to which these similarities can be exploited, a lexicon can nonetheless serve in the development of an ontology, especially in a technical domain.Lexicons and Lexical Knowledge
Lexicons
A lexicon is a list of words in a language -a vocabulary -along with some knowledge of how each word is used. A lexicon may be general or domainspecific; we might have, for example, a lexicon of several thousand common words of English or German, or a lexicon of the technical terms of dentistry in some language. The words that are of interest are usually open-class or content words, such as nouns, verbs, and adjectives, rather than closed-class or grammatical function words, such as articles, pronouns, and prepositions, whose behaviour is more tightly bound to the grammar of the language. A lexicon may also include multi-word expressions such as fixed phrases (by and large), phrasal verbs (tear apart), and other common expressions (merry Christmas!; teach someone 's grandmother to suck eggs; Elvis has left the building).Each word or phrase in a lexicon is described in a lexical entry; exactly what is included in each entry depends on the purpose of the particular lexicon. The details that are given (to be discussed further in Sects. 2.1 and 3.2 below) may include any of its properties of spelling or sound, grammatical behaviour, meaning, or use, and the nature of its relationships with other words. A lexical entry is therefore a potentially large record specifying many aspects of the linguistic behaviour and meaning of a word.Hence a lexicon can be viewed as an index that maps from the written form of a word to information about that word. This is not a one-to-one correspondence, however. Words that occur in more than one syntactic category will usually have a separate entry for each category; for example, flap would have one entry as a noun and another as a verb. Separate entries are usually also appropriate for each of the senses of a homonym -a word that has more than one unrelated sense even within a single syntactic category; for example, the noun pen would have distinct entries for the senses writing instrument, animal enclosure, and swan. Polysemy -related or overlapping senses -is a more-complex situation; sometimes the senses may be discrete enough that we can treat them as distinct: for example, window as both opening in wall and glass pane in opening in wall (fall through the window; break the window). But this is not always so; the word open, for example, has many overlapping senses concerning unfolding, expanding, revealing, moving to an open position, making openings in, and so on, and separating them into discrete senses, as the writers of dictionary definitions try to do, is not possible (see also Sects. 2.3 and 3.1 below).On the other hand, morphological variants of a word, such as plurals of nouns and inflected forms of verbs, will not normally warrant their own complete lexical entry. Rather, the entry for such forms need be little more than a pointer to that for the base form of the word. For example, the entries for takes, taking, took, and taken might just note that they are inflected forms of the base-form verb take, and point to that entry for other details; and conversely, the entry for take will point to the inflected forms. Similarly, flaps will be connected both to the noun flap as its plural and to the verb flap as its third-person singular. The sharing of information between entries is discussed further in Sect. 2.2 below.A lexicon may be just a simple list of entries, or a more-complex structure may be imposed upon it. For example, a lexicon may be organized hierarchically, with default inheritance of linguistic properties (see Sect. 2.2 below). However, the structures that will be of primary interest in this chapter are semantic, rather than morphological or syntactic; they will be discussed in Sect. 3.2 below.Computational Lexicons
An ordinary dictionary is an example of a lexicon. However, a dictionary is intended for use by humans, and its style and format are unsuitable for computational use in a text or natural language processing system without substantial revision. A particular problem is the dictionary's explications of the senses of each word in the form of definitions that are themselves written in natural language; computational applications that use word meanings usually require a more-formal representation of the knowledge. Nonetheless, a dictionary in a machine-readable format can serve as the basis for a computational lexicon, as in the acquilex project Perhaps the best-known and most widely used computational lexicon of English is WordNet Some other important general-purpose lexicons include celex Two important sources for obtaining lexicons are these:ELDA: The Evaluations and Language resources Distribution Agency (www.elda.org) distributes many European-language general-purpose and domain-specific lexicons, both monolingual and multilingual, including parole and EuroWordNet. LDC: The Linguistic Data Consortium (ldc.upenn.edu), although primarily a distributor of corpora, offers celex and several other lexicons.In addition, English WordNet is available free of charge from the project's Web page (wordnet.princeton.edu).Lexical Entries
What is in a Lexical Entry?
Any detail of the linguistic behaviour or use of a word may be included in its lexical entry: its phonetics (including pronunciations, syllabification, and stress pattern), written forms (including hyphenation points), morphology (including inflections and other affixation), syntactic and combinatory behaviour, constraints on its use, its relative frequency, and, of course, all aspects of its meaning. For our purposes in this chapter, the word's semantic properties, including relationships between the meanings of the word and those of other words, are the most important, and we will look at them in detail in Sect. 3.2 below.Thus, as mentioned earlier, a lexical entry is potentially quite a large record. For example, the celex lexicons of English, Dutch, and German Many linguistic applications will require only a subset of the information that may be found in the lexical entries of large, broad-coverage lexicons. Because of their emphasis on detailed knowledge about the linguistic behaviour of words, these large, complex lexicons are sometimes referred to as lexical knowledge bases, or LKBs. Some researchers distinguish LKBs from lexicons by regarding LKBs as the larger and more-abstract source from which instances of lexicons for particular applications may be generated. In the present chapter, we will not need to make this distinction, and will just use the term lexicon.Inheritance of Linguistic Properties
Generally speaking, the behaviour of words with respect to many non-semantic lexical properties in any given language tends to be regular: words that are phonetically, morphologically, or syntactically similar to one another usually exhibit similar phonetic, morphological, or syntactic behaviour. For example, in English most verbs form their past tense with either -ed or -d, and even most of those that do not do so fall into a few small categories of behaviour; and quite separately, verbs also cluster into a number of categories by their alternation behaviour (see Sect. .3 below).
It is therefore possible to categorize and subcategorize words by their behaviour -that is, build an ontology of lexical behaviour -and use these categories to construct a lexicon in which each word, by default, inherits the properties of the categories and subcategories of which it is a member. Of course, idiosyncratic properties (such as many of the combinatory properties listed in an ECD) will still have to be specified in each word's entry. Inheritance of properties facilitates both economy and consistency in a large lexicon. A hierarchical representation of lexical knowledge with property inheritance is really just a special case of this style or method of knowledge representation. Accordingly, the inheritance of properties in the lexicon and the design of formal languages for the representation of lexical knowledge have been areas of considerable study (e.g. It should be clear that a hierarchical representation of similarities in lexical behaviour is distinct from any such representation of the meaning of words; knowing that boy and girl both take -s to make their plural form whereas child does not tells us nothing about the relationship between the meanings of those words. Relationships between meanings, and the hierarchies or other structures that they might form, are a separate matter entirely; they will be discussed in Sect. 3.2.Generating Elements of the Lexicon
Even with inheritance of properties, compiling a lexicon is a large task. But it can be eased by recognizing that because of the many regularities in the ways that natural languages generate derived words and senses, many of the entries in a lexicon can be automatically predicted.For example, at the level of inflection and affixation, from the existence of the English word read, we can hypothesize that (among others) reading, reader, unreadable, and antireadability are also words in the lexicon, and in three out of these four cases we would be right. Viegas et al. At the level of word sense, there are also regularities in the polysemy of words. For example, the senses of the word book include both its sense as a physical object and its sense as information-content: The book fell on the floor; The book was exciting. (A problem for natural language processing, which need not concern us here, is that both senses may be used at once: The exciting book fell on the floor.) In fact, the same polysemy can be seen with any word denoting an information-containing object, and if a new one comes along, the polysemy applies automatically: The DVD fell on the floor; The DVD was exciting. There are many such regularities of polysemy; they have been codified in Pustejovsky's Word Senses and the Relationships Between Them
Most of the issues in the relationship between lexicons and ontologies pertain to the nature of the word senses in the lexicon and to relationships between those senses -that is, to the semantic structure of the lexicon.Word Senses
By definition, a word sense, or the "meaning" of a word, is a semantic object -a concept or conceptual structure of some kind, though exactly what kind is a matter of considerable debate, with a large literature on the topic. Among other possibilities, a word sense may be regarded as a purely mental object; or as a structure of some kind of primitive units of meaning; or as the set of all the things in the world that the sense may denote; or as a prototype that other objects resemble to a greater or lesser degree; or as an intension or description or identification procedure -possibly in terms of necessary and sufficient conditions -of all the things that the sense may denote.Word senses tend to be fuzzy objects with indistinct boundaries, as we have seen already with the example of open in Sect. 1.1 above. Whether or not a person may be called slim, for example, is, to some degree, a subjective judgement of the user of the word. To a first approximation, a word sense seems to be something like a category of objects in the world; so the word slim might be taken to denote exactly the category of slim objects, with its fuzziness and its subjectivity coming from the fuzziness and subjectivity of the category in the world, given all the problems that are inherent in categorization (see also Nonetheless, one position that could be taken is that a word sense is a category. This is particularly appealing in simple practical applications, where the deeper philosophical problems of meaning may be finessed or ignored. The problems are pushed to another level, that of the ontology; given some ontology, each word sense is represented simply as a pointer to some concept or category within the ontology. In some technical domains this may be entirely appropriate (see Sect. 5.1 below). But sometimes this move may in fact make matters worse: all the problems of categorization remain, and the additional requirement is placed on the ontology of mirroring some natural language or languages, which is by no means straightforward (see Sect. 4 below); nonetheless, an ontology may act as an interpretation of the word senses in a lexicon (see Sect. 5.4 below).In addition to the denotative elements of meaning that refer to the world, word senses also have connotation, which may be used to express the user's attitude: a speaker who chooses the word sozzled instead of drunk is exhibiting informality, whereas one who chooses inebriated is being formal; a speaker who describes a person as slim or slender is implying that the person's relative narrowness is attractive to the speaker, whereas the choice of skinny for the same person would imply unattractiveness.Lexical Relationships
Regardless of exactly how one conceives of word senses, because they pertain in some manner to categories in the world itself, lexical relationships between word senses mirror, perhaps imperfectly, certain relationships that hold between the categories themselves. The nature of lexical relationships and the degree to which they may be taken as ontological relationships are the topics of most of the rest of this chapter. In the space available, we can do no more than introduce the main ideas of lexical relationships; for detailed treatments, see The "classical" lexical relationships pertain to identity of meaning, inclusion of meaning, part-whole relationships, and opposite meanings. Identity of meaning is synonymy: Two or more words are synonyms (with respect to one sense of each) if one may substitute for another in a text without changing the meaning of the text. This test may be construed more or less strictly; words may be synonyms in one context but not another; often, putative synonyms will vary in connotation or linguistic style (as in the drunk and slim examples in Sect. 3.1 above), and this might or might not be considered significant. More usually, "synonyms" are actually merely near-synonyms (see Sect. 4.1 below).The primary inclusion relations are hyponymy and its inverse hypernymy (also known as hyperonymy) The part-whole relationships meronymy and holonymy may be glossed roughly as has-part and part-of, but we again avoid these ontologically biased terms. The notion of part-whole is overloaded; for example, the relationship between wheel and bicycle is not the same as that of professor and faculty or tree and forest; the first relationship is that of functional component, the second is group membership, and the third is element of a collection. For analysis of part-whole relationships, see Words that are opposites, generally speaking, share most elements of their meaning, except for being positioned at the two extremes of one particular dimension. Thus hot and cold are opposites -antonyms, in fact -but telephone and Abelian group are not, even though they have no properties in common (that is, they are "opposite" in every feature or dimension). Cruse These "classical" lexical relationships are the ones that are included in the WordNet lexicon. Synonymy is represented, as mentioned earlier, by means of synsets: if two words have identical senses, they are members of the same synset. Synsets are then connected to one another by pointers representing inclusion, part-whole, and opposite relations, thereby creating hierarchies.There are many other kinds of lexical relationships in addition to the "classical" ones. They include temporal relationships such as happens-before (marry-divorce) Synonymy, inclusion, and associative relations are often the basis of the structure of a thesaurus. While general-purpose thesauri, such as Roget's Lexicons are not (Really) Ontologies
The obvious parallel between the hypernymy relation in a lexicon and the subsumption relation in an ontology suggests that lexicons are very similar to ontologies. It even suggests that perhaps a lexicon, together with the lexical relations defined on it, is an ontology (or is a kind of ontology in the ontology of ontologies). In this view, we identify word senses with ontological categories and lexical relations with ontological relations. The motivation for this identification is clear from the preceding discussion (Sect. 3.2).Nonetheless, a lexicon, especially one that is not specific to a technical domain (see Sect. 5.1 below), is not a very good ontology. An ontology, after all, is a set of categories of objects or ideas in the world, along with certain relationships among them; it is not a linguistic object. A lexicon, on the other hand, depends, by definition, on a natural language and the word senses in it. These give, at best, an ersatz ontology, as the following sections will show.Overlapping Word Senses and Near-Synonymy
It is usually assumed in an ontology that subcategories of a category are disjoint (cf. Consider, for example, the English words error and mistake, and some words that denote kinds of mistakes or errors: blunder, slip, lapse, faux pas, bull, howler, and boner. How can we arrange these in a hierarchy? First we need to know the precise meaning of each and what distinguishes one from another. Fortunately, lexicographers take on such tasks, and the data for this group of words is given in Webster's New Dictionary of Synonyms Error implies a straying from a proper course and suggests guilt as may lie in failure to take proper advantage of a guide . . . Mistake implies misconception, misunderstanding, a wrong but not always blameworthy judgment, or inadvertence; it expresses less severe criticism than error. Blunder is harsher than mistake or error; it commonly implies ignorance or stupidity, sometimes blameworthiness. Slip carries a stronger implication of inadvertence or accident than mistake, and often, in addition, connotes triviality. Lapse, though sometimes used interchangeably with slip, stresses forgetfulness, weakness, or inattention more than accident; thus, one says a lapse of memory or a slip of the pen, but not vice versa. Faux pas is most frequently applied to a mistake in etiquette. Bull, howler, and boner are rather informal terms applicable to blunders that typically have an amusing aspect.Fig. 1. An entry (abridged) from
Webster's New Dictionary of Synonyms These observations have led to the proposal Gaps in the Lexicon
A lexicon, by definition, will omit any reference to ontological categories that are not lexicalized in the language -categories that would require a (possibly long) multi-word description in order to be referred to in the language. That is, the words in a lexicon, even if they may be taken to represent categories, are merely a subset of the categories that would be present in an ontology covering the same domain. In fact, every language exhibits lexical gaps relative to other languages; that is, it simply lacks any word corresponding to a category that is lexicalized in some other language or languages. For example, Dutch has no words corresponding to the English words container or coy; Spanish has no word corresponding to the English verb to stab "to injure by puncturing with a sharp weapon"; English has no single word for the German Gemütlichkeit "combination of cosiness, cheerfulness, and social pleasantness" or for the French bavure "embarrassing bureaucratic error". On the face of it, this seems to argue for deriving a language-independent ontology from the union of the lexicons of many languages (as attempted by Emele et al. Quite apart from lexical gaps in one language relative to another, there are many categories that are not lexicalized in any language. After all, it is clear that the number of categories in the world far exceeds the number of word senses in a language, and while different languages present different inventories of senses, as we have just argued, it nonetheless remains true that, by and large, all will cover more or less the same "conceptual territory", namely the concepts most salient or important to daily life, and these will be much the same across different languages, especially different languages of similar cultures. As the world changes, new concepts will arise and may be lexicalized, either as a new sense for an existing word (such as browser "software tool for viewing the World Wide Web"), as a compositional fixed phrase (road rage), or as a completely new word or phrase (demutualization "conversion of a mutual life insurance company to a company with shareholders", proteomics, DVD). That large areas remain unlexicalized is clear from the popularity of games and pastimes such as Sniglets ("words that do not appear in the dictionary but should") But even where natural languages "cover the same territory", each different language will often present a different and mutually incompatible set of word senses, as each language lexicalizes somewhat different categorizations or perspectives of the world. It is rare for words that are translation equivalents to be completely identical in sense; more usually, they are merely cross-lingual near-synonyms (see Sect. 4.1 above).An area of special ontological interest in which the vocabularies of natural languages tend to be particularly sparse is the upper ontology (see chapter "Foundational Choices in DOLCE"). Obviously, all natural languages need to be able to talk about the upper levels of the ontology. Hence, one might have thought that at this level we would find natural languages to be in essential agreement about how the world is categorized, simply because the distinctions seem to be so fundamental and so basic to our biologically based, and therefore presumably universal, cognitive processes and perception of the world. But natural languages instead prefer to concentrate the richest and most commonly used parts of their vocabulary in roughly the middle of the hierarchy, an area that has come to be known as the basic-level categories; categories in this area maximize both informativeness and distinctiveness Linguistic Categorizations That are not Ontological
And yet, even though natural languages omit many distinctions that we would plausibly want in an ontology, they also make semantic distinctions -that is, distinctions that are seemingly based on the real-world properties of objects -that we probably would not want to include in an ontology. An example of this is semantic categorizations that are required for "correct" word choice within the language and yet are seemingly arbitrary or unmotivated from a strictly ontological point of view. For example, Chinese requires that a noun be preceded by an appropriate classifier in contexts involving numbers and certain quantifiers:In the Chinese expression liang tiao yu ('two fish'), the classifier tiao, which has a semantic indication for "long and rope-like" objects, must be present between the number (two) and the head noun (fish). Since tiao also occurs with other nouns in a quantifying structure, we can assume that these nouns belong to one class by sharing similar semantic features denoted by the classifier tiao: she 'snake', tui 'leg', kuzi 'pair of pants', he 'river', bandeng 'bench'. There are about 900 such classifiers in Chinese; they are based on characteristics such as shape, aggregation, and value Often, such linguistic categorizations are not even a reliable reflection of the world. For example, many languages distinguish in their syntax between objects that are discrete and those that are not: countable and mass nouns. This is also an important distinction for many ontologies; but one should not look in the lexicon to find the ontological data, for in practice, the actual linguistic categorization is rather arbitrary and not a very accurate or consistent reflection of discreteness and non-discreteness in the world. For example, in English, spaghetti is a mass noun, but noodle is countable; the English word furniture is a mass noun, but the French meuble and (in some uses) the German Möbel are countable. Similarly, in Chinese, the classifier tiao mentioned above is not a reliable indicator of a long and ropelike shape: because it applies to pants it also applies, by extension, to any piece of clothing one puts one's legs through, such as youyongku "swimming trunks" A particularly important area in which languages make semantic distinctions that are nonetheless ontologically arbitrary is in the behaviour of verbs in their diathesis alternations -that is, alternations in the optionality and syntactic realization of the verb's arguments, sometimes with accompanying changes in meaning (1) Nadia sprayed water on the plants.(2) Nadia sprayed the plants with water.(3) Water sprayed on the plants. (4) * The plants sprayed with water.(The " * " on (4) denotes syntactic ill-formedness.) These examples (from In view of the many different possible syntactic arrangements of the arguments of a verb, and the many different possible combinations of requirement, prohibition, and optionality for each argument in each position, a large number of different kinds of alternations are possible. However, if we classify verbs by the syntactic alternations that they may and may not undergo, as Levin 239).
Even what is perhaps the most basic and seemingly ontological distinction made by languages, the distinction between nouns, verbs, and other syntactic categories, is not as ontologically well-founded as it might seem. From the viewpoint of object-dominant languages For example, in a situation in which English might say There's a rope lying on the ground, Atsugewi [a language of Northern California] might use the single polysynthetic verb form ẃoswalak•a . . . [This can] be glossed as 'a-flexible-linear-object-is-located on-the-ground because-of-gravity-actingon-it'. But to suggest its nounless flavor, the Atsugewi form can perhaps be fancifully rendered in English as: "it gravitically-linearizes-aground". In this example, then, Atsugewi refers to two physical entities, a ropelike object and the ground underfoot, without any nouns. (Talmy Language, Cognition, and the World
All the discussion above on the distinction between lexicon and ontology is really nothing more than a few examples of issues and problems that arise in discussions of the relationship between language, cognition, and our view of the world. This is, of course, a Big Question on which there is an enormous literature, and we cannot possibly do more than just allude to it here in order to put the preceding discussion into perspective. Issues include the degree of mutual causal influence between one's view of the world, one's culture, one's thought, one's language, and the structure of cognitive processes. The Sapir-Whorf hypothesis or principle of linguistic relativity, in its strongest form, states that language determines thought:We dissect nature along lines laid down by our native languages. The categories and types that we isolate from the world of phenomena we do not find there because they stare every observer in the face; on the contrary, the world is presented in a kaleidoscopic flux of impressions which has to be organized by our minds -and this means largely by the linguistic systems in our minds. We cut nature up, organize it into concepts, and ascribe significances as we do, largely because we are parties to an agreement to organize it in this way -an agreement that holds throughout our speech community and is codified in the patterns of our language. The agreement is, of course, an implicit and unstated one, but its terms are absolutely obligatory; we cannot talk at all except by subscribing to the organization and classification of data which the agreement decrees. (Whorf No two languages are ever sufficiently similar to be considered as representing the same social reality. The worlds in which different societies live are distinct worlds, not merely the same world with different labels attached. (Sapir These quotations imply a pessimistic outlook for the enterprise of practical, language-independent ontology (or even of translation between two languages, which as a distinct position is often associated with Quine From a practical standpoint in ontology creation, however, while an overly language-dependent or lexicon-dependent ontology might be avoided for all the reasons discussed above, there is still much in the nature of natural languages that can help the creation of ontologies: it might be a good strategy to adopt or adapt the worldview of a language into one's ontology, or to merge the views of two different languages. For example, languages offer a rich analysis in their views of the structure of events and of space that can serve as the basis for ontologies; see, for example, the work of Talmy In fact, an ontology without natural language labels attached to classes or properties is almost useless, because without this kind of grounding it is very difficult, if not impossible, for humans to map an ontology to their own conceptualization, i.e. the ontology lacks human-interpretability. (Völker et al. Lexically Based Ontologies and Ontologically Based Lexicons
Despite all the discussion in the previous section, it is possible that a lexicon with a semantic hierarchy might serve as the basis for a useful ontology, and an ontology may serve as a grounding for a lexicon. This may be so in particular in technical domains, in which vocabulary and ontology are more closely tied than in more-general domains. But it may also be the case for more-general vocabularies when language dependence and relative ontological simplicity are not problematic or are even desirable -for example if the ontology is to be used primarily in general-purpose, domain-independent text-processing applications in the language in question and hence inferences from the semantic properties of words have special prominence over domain-dependent or application-dependent inferences. In particular, Dahlgren Technical Domains
In highly technical domains, it is usual for the correspondence between the vocabulary and the ontology of the domain to be closer than in the case of everyday words and concepts. This is because it is in the nature of technical or scientific work to try to identify and organize the concepts of the domain clearly and precisely and to name them unambiguously (and preferably with minimal synonymy). In some fields of study, there is a recognized authority that maintains and publishes a categorization and its associated nomenclature. For example, in psychiatry, the Diagnostic and Statistical Manual of the American Psychiatric Association Obviously, the construction of explicit, definitive ontologies, or even explicit, definitive vocabularies, does not occur in all technical domains. Nor is there always general consensus in technical domains on the nature of the concepts of the domain or uniformity in the use of its nomenclature. On the contrary, technical terms may exhibit the same vagueness, polysemy, and near-synonymy that we see exhibited in the general vocabulary. For example, in the domain of ontologies in information systems, the terms ontology, concept, and category are all quite imprecise, as may be seen throughout this volume; nonetheless, they are technical terms: the latter two are used in a more-precise way than the same words are in everyday speech.However, in technical domains where explicit vocabularies exist (including glossaries, lexicons, and dictionaries of technical terms, and so on, whether backed by an authority or not), an ontology exists at least implicitly, as we will see in Sect. 5.2 below. And where an explicit ontology exists, an explicit vocabulary certainly does; indeed, it is often said that the construction of any domain-specific ontology implies the parallel construction of a vocabulary for it; e.g. An example of a technical ontology with a parallel vocabulary is the Unified Medical Language System (UMLS) (e.g. Developing a Lexically Based Ontology
It has long been observed that a dictionary implicitly contains an ontology, or at least a semantic hierarchy, in the genus terms in its definitions. For example, if automobile is defined as a self-propelled passenger vehicle that usually has four wheels and an internal-combustion engine, then it is implied that automobile is a hyponym of vehicle and even that automobile IS-A vehicle; semantic or ontological part-whole relations are also implied.Experiments on automatically extracting an ontology or semantic hierarchy from a machine-readable dictionary were first carried out in the late 1970s. Amsler Often, the literature on these projects equivocates on whether the resulting hierarchies or networks should be thought of as purely linguistic objects -after all, they are built from words and word senses -or whether they have an ontological status outside language. If the source dictionary is that of a technical domain, the claim for ontological status is stronger. The claim is also strengthened if new, non-lexically derived nodes are added to the structure. For example, in The Wordtree, a complex, strictly binary ontology of transitive actions by Burger Finding Covert Categories
One way that a hierarchy derived from a machine-readable dictionary might become more ontological is by the addition of categories that are unlexicalized in the language upon which it is based. Sometimes, these categories are implicitly reified by the presence of other words in the vocabulary, and, following Cruse Barrière and Popowich which could be glossed as "things that carry people". This pattern occurs in the definitions of many words, including boat, train, camel, and donkey. It thus represents a covert category that can be named and added to a semantic hierarchy as a new hypernym (or subsumer, now) of the nodes that were derived from these words, in addition to any other hypernym that they already had. The name for the covert category may be derived from the subgraph, such as carry-object-person-agent for the example above. The hierarchy thus becomes more than just lexical relations, although less than a complete ontology; nonetheless, the new nodes could be helpful in text processing. The accuracy of the method is limited by the degree to which polysemy can be resolved; for example, in the category of things that people play, it finds, among others, music, baseball, and outside, representing different senses of play. Thus the output of the method must be regarded only as suggestions that require validation by a human.Although Ontologies for Lexicons
As mentioned in Sect. 3.1, most theories of what a word sense is relate it in some way to the world. Thus, an ontology, as a non-linguistic object that moredirectly represents the world, may provide an interpretation or grounding of word senses. A simple, albeit limited, way to do this is to map between word senses and elements of or structures in the ontology. Of course, this will work only to the extent that the ontology can capture the full essence of the meanings. We noted in Sect. 5.1 above that the UMLS grounds its Metathesaurus this way.In machine translation and other multilingual applications, a mapping like this could act as an interlingua, enabling the words in one language to be interpreted in another. However, greater independence from any particular language is required; at the very least, the ontology should not favour, say, Japanese over English if it is to be used in translation between those two languages. In the 12-language simple lexicon Hovy and Nirenburg Conclusion
In this chapter, we have discussed the relationship between lexicons, which are linguistic objects, and ontologies, which are not. The relationship is muddied by the difficult and vexed relationship between language, thought, and the world: insofar as word-meanings are objects in the world, they may participate in ontologies for non-linguistic purposes, but they are inherently limited by their linguistic heritage; but non-linguistic ontologies may be equally limited when adapted to applications such as text and language processing. Introduction
Today's software systems are growing in size and complexity. They often consist of a big number of software components, developed by heterogeneous groups at different times, with varying skills and goals. These components are still supposed to exchange and share data, and to cooperate for the advantage of the users and their communities. Such software systems are increasingly connected to each other, communicating on behalf of their users between different platforms and for changing purposes. The biggest such system is the Semantic Web Ontologies are used in order to specify in a standard way the knowledge that is exchanged and shared between the different systems, and within the systems by the various components. Ontologies are engineering artifacts that define the formal semantics of the terms used, and the relations between these terms. They provide an "explicit specification of a conceptualization" This chapter discusses the evaluation of web ontologies, i.e. ontologies specified in one of the standard web ontology languages (for now RDF(S) Web ontologies as defined by the RDF or OWL standards do not include only terminological knowledge -the terms used to describe data, and the formal relations between these terms -but may also include the knowledge bases themselves, i.e. terms describing individuals and ground facts asserting the state of affairs between these individuals. In many cases such knowledge bases are not regarded as being proper ontologies The next section will discuss a number of ontology quality criteria as provided by literature. This will offer a frame of reference for the evaluation methods described in the rest of the chapter. As we will see though, the quality criteria will not be trivially mappable to evaluation methods. Section 3 defines several aspects of an ontology, so that the following sections can work on these aspects: vocabulary (Sect. 4), syntax (Sect. 5), structure (Sect. 6), semantics (Sect. 7), representation (Sect. 8), and finally the context in which the ontology is used (Sect. 9). The chapter closes with a discussion of further approaches and an outlook at future work in this area.Criteria
Ontology evaluation can target a number of several different criteria. In this section we will list criteria from literature, namely five papers describing principles for good ontologies Gómez-Pérez A complementing overview article dealing with ontology validation is provided by Obrst et al. Besides the two basic terms of verification and validation, the following quality criteria are discussed in the literature. In order to arrive at a concise description we take the liberty to collapse similar criteria.• Accuracy • Completeness These quality criteria define a good ontology. But just like the answer to the question how good is the ontology? is usually not a simple one, the same holds true for these quality criteria: they are all desiderata, goals to guide the creation and evaluation of the ontology. None of them can be directly measured.Concrete evaluation methods are required in order to assess specific features of an ontology. The relationship between criteria and methods is complex: criteria provide justifications for the methods, whereas the result of a method will provide an indicator for how well one or more criteria are met. Most methods provide indicators for more than one criteria, therefore criteria are a bad choice to structure evaluation methods. In the following sections we describes evaluation methods. In order to give some structure for the description of the methods, we first introduce different aspects of an ontology. These aspects provide guidance for the rest of the chapter.Aspects
An ontology is a complex, multi-layered information resource. In this section we will identify different aspects that are amenable to the automatic, domain-and task-independent evaluation of an ontology. Based on the evaluations of the different ontology aspects, evaluators can then integrate the different evaluation results in order to achieve an aggregated, qualitative ontology evaluation. For each aspect we will show evaluation approaches within the following sections.Each aspect of an ontology that can be evaluated must represent a degree of freedom (if there is no degree of freedom, there can be no evaluation since it is the only choice). So each aspect describes some choices that have been made during the design of the ontology.• Vocabulary. The vocabulary of an ontology is the set of all names in that ontology, be it URI references or literals, i.e. a value with a datatype or a language identifier. This aspect deals with the different choices with regards to the used URIs or literals. • Syntax. Web ontologies can be described in a number of different surface syntaxes like RDF/XML Note that in this chapter we assume that logical consistency or coherence of the ontology is given, i.e. that any inconsistencies or incoherences have been previously resolved using other methods. There is a wide field of work discussing these logical properties, and also well-developed and active research in debugging inconsistency and incoherence. Ontologies are inconsistent if they do not allow any model to fulfill the axioms of the ontology. Incoherent ontologies have classes with an empty intension Vocabulary
The vocabulary of a web ontology is the set of all names used in it. Names can be either URIs or literals. The set of all URIs of an ontology is called the signature of the ontology (and is thus the subset of the vocabulary without the literals). Literals are either typed (i.e. they consist of a tuple with a literal value and a URI identifying the datatype of the value) or untyped. Untyped literals may have a language tag (based on Most of the nodes in the ontology graph are named with URIs A further issue concerns the naming of the URIs: even though the URI standard URIs should also, if possible, reuse a common URI for a specific resource instead of introducing a new one. In order to enable easier sharing, exchange, and aggregation of information on the Semantic Web, the reuse of commonly used URIs instead of inventing new ones will be helpful. At the time of writing most domains do not have yet a lexicon of URIs, but it is expected that projects like Swoogle In web ontologies, the type of a name may often be inferred, e.g. a triple like ex:i rdf:type ex:A will let us infer that ex:i is an owl:Individual and ex:A an owl:Class. This automatism leads to the problem that it is impossible for a reasoner to discern if, e.g. ex:Adress is a new entity or merely a typo of ex:Address. This can be checked by declaring names, so that a tool can check if all used names are properly declared. This further brings the additional benefit of a more efficient parsing of ontologies Labelling and commenting the URIs should follow a style guide: all URIs should have labels, the languages that need to be supported are defined Besides URIs, an ontology often contains data values like numbers, strings or dates. They are given in the form of typed literals, i.e. a value and a URI identifying the datatype of the value. Usually, one of a number of datatypes given in the XML Schema Definition Syntax
Web ontologies are serialized in a big (and growing) number of different surface syntaxes. They may be separated into different groups: the ones that describe a graph (which in return describes the ontology), and the ones that describe the ontology directly (a graph can still be calculated from the ontology based on the transformation described in All these different syntaxes should be transformable automatically from and into each other. Note that many of the syntaxes allow for comments which are not part of the semantics of the ontology. For example, an XML comment like <!--Created with Protege --> will usually be lost when transforming syntaxes. Often such comments can be expressed as statements with an explicit semantics, like an ontology property describing the used tool. This way the content of the comment would be available to tools using the ontology.Common features that can be evaluated over most of the syntaxes in a fairly uniform way are the proper indentation in the file and the order of the triples (for graph-based syntaxes) or axioms (for ontology-based syntaxes). Triples forming complex axioms should be grouped together, as well as groups of axioms forming an ontology pattern (see chapter "Ontology Design Patterns"). Often axioms should precede facts, and facts about the same individual should be grouped together.A speciality of the RDF/XML syntax is that it can be used to let ontologies, particularly simple knowledge bases, resemble traditional XML documents and even be accompanied by a schema in DTD Most serialization formats include mechanisms to abbreviate URI references. They are often based on known XML based approaches, like entities Structure
The most widely explored measures used on ontologies are structural measures. Graph measures are applied to the complete or partial RDF graph describing the ontology. An example of an extensively investigated subgraph would be the one consisting only of edges with the name rdfs:subClassOf and the nodes connected by these edges (i.e. the explicit class hierarchy). This subgraph can be checked to see if the explicit class hierarchy is a tree, a set of trees, or if it has circularities, etc. If it is indeed a tree, the depth and breadth of the tree can be measured. Current literature defines more than forty different metrics ( • They can be calculated effectively from the ontology graph. Graph metrics libraries are readily available and reusable for this task. • They yield simple numbers. This makes tracking the evolution of the ontology easy, because even in case the meaning of the result itself is not well understood, its change often is. • Their results can be checked automatically against constraints, e.g. constraining the maximal number of outgoing edges of the type rdfs:subClassOf from a single node to 5 can be checked on each commit of the ontology to a version control system. Upon violation, an appropriate message can be created. • They can be simply visualized and reported.Due to these advantages, quite some ontology toolkits provide ready access to a number of these metrics (see chapter "Exploring the Economical Aspects of Ontology Engineering"). Also a number of ontology repositories provide annotations of ontologies with such metrics (see chapter "Ontology Repositories").Structural metrics are often not well-defined. That is, based on their definitions in literature, it is hard to implement them unambiguously. Also there is often confusion with regards to their meaning: for example, measure (M29) in Besides measures counting structural features of the ontology, the structure can also be investigated with regards to certain patterns. The best known example is to regard cycles within the taxonomic structure of the ontology as an error Even though the complexity of the OWL DL language is known, this does not give much information on particular ontologies. The expressivity of the used language fragment merely defines an upper bound on the complexity that applies to the reasoning tasks. A simple list of the constructs used within the ontology allows to further refine the used DL fragment, and thus a possibly lower complexity bound. For example, OWL DL is known to correspond to the description logic SHOIN (D) Semantics
Most current metrics do not take the semantics of the ontology being described by the RDF graph into account, but consider only the structural properties described in the previous section. But the structure of an ontology is often of less interest than its actual semantics, especially when merging ontologies.In order to measure the properties of the models described by the structure, normalization can be used Five normalization steps are defined:1. Name all classes, so no anonymous complex class descriptions are left 2. Name anonymous individuals 3. Materialize the subsumption hierarchy and normalize names (so that classes, properties and individuals that have more than one name use only one of them, see the example in Fig. Normalization offers the advantage that metrics are much easier defined on the normalized ontology since some properties are guaranteed: the ontology graph will have no cycles, the number of normal names and classes will correspond, and problems of mapping (see chapter "Ontology Mapping") and redundancy are dealt with. For example, most metrics defining the depth of an ontology (like metric (M3) in Another aspect of semantic metrics is their stability with regards to the open world assumption of OWL DL ontologies When the original ontology was built, the engineer knew the minimal depth of the ontology. But the stable metric measuring the minimal depth remained 1 because no axioms prevented the collapse of the taxonomy. By adding further axioms to strengthen the taxonomy (for example complete partitions) the minimal depth will raise, indicating a more robust ontology with regards to future changes. Stable metrics are indicators for stable ontologies.Stability with regards to the knowledge base can also be used by closing certain classes. In some cases we know that a knowledge base offers complete coverage: for example, we may publish a complete list of all members of a certain work group, or a complete list of all countries. In this case we can use enumerations to close off the class and declare its completeness. But note that such a closure often has undesirable computational side effects in many reasoners.Completeness is one of the ontology quality criteria. There are different types of completeness. Here we will consider language completeness. Two more types -data completeness and domain completeness will be discussed in Sects. 9 and 10.Language completeness is defined on a certain ontology with regards to a specific ontology language (or subset of the language). Given a specific signature (i.e. set of names), language completeness measures the ratio between the knowledge that can be expressed and the knowledge that is actually given. For example, if we have an ontology with the signature Adam, Eve, Apple, knows, eats, Person, we can ask which of the individuals are persons, and which of the individuals know each other. Thus assuming a simple assertional language like RDF, language completeness with regards to that language (or short: assertional completeness) is achieved by knowing about all possible ground facts that can be described by the ontology (i.e. for each fact∀j ∈ O} we can say if it is true or not). An expressive ontology language allows numerous more questions to be asked besides this ground facts: is the domain of knows a Person? Is it its range? Is eats a subproperty of knows? In order to have a language complete ontology with regards to the more expressive language, the ontology must offer defined answers for all questions that can be asked with the given language. Relational exploration is a method to explore language fragments of higher expressivity, and to calculate the smallest set of questions that have to be answered in order to achieve a complete ontology Representation
Representational aspects of the ontology deal with the relation between the semantics and the structure, i.e. how are the semantics structurally represented? This will often uncover mistakes and omissions within the relation between the formal specification and the shared conceptualization -or at least the models which are supposedly isomorphic to the conceptualizations.In order to evaluate features of the representation, we compare the results of the structural measures to the results of the semantic measures. Using the normalization described in Sect. 7, we can even often use the same (or a very similar) metric, applied before and after normalization, and compare the respective results. Any deviations between the results of the two measurements indicate elements of the ontology that require further investigation. For example, consider the ontology given in Fig. By contrasting the two ontology structures in Fig. Context
There are a number of approaches in literature describing the creation and definition of artifacts accompanying the ontology. An evaluating tool can load both the additional artifact and the ontology and then perform further evaluations. The additional artifact thus provides a formalized context.One of the earliest approaches toward ontology evaluation was the introduction of competency questions, i.e. questions that the ontology should be able to answer Unit tests for ontologies e. every axiom in T + can be inferred from the tested ontology O, and on the other hand no axiom in T -can be inferred from O. The main purpose of unit tests for ontologies is similar to their purpose in software engineering: whenever an error is encountered with an axiom which is falsely inferred or respectively incorrectly not inferred), the ontology maintainer may add this piece of knowledge to the appropriate test ontology. Whenever the ontology is changed, the changed ontology can be automatically checked against the test ontology, containing the formalized knowledge of previously encountered errors. There exists an extension for Protégé that allows for some of the unit tests described here Certain language constructs, or their combination, may increase reasoning time considerably. In order to avoid this, ontologies are often kept simple. But instead of abstaining from the use of more complex constructs, an ontology could also be modularized with regards to its complexity: a highly axiomatized ontology may come in different versions, one just defining the used vocabulary, and maybe their explicitly stated taxonomic relations, and a second ontology adding much more knowledge, like disjointness or domain and range declarations. If the simple ontology is properly build, it can lead to an ontology which often yields the same results to queries as the complete ontology (depending, naturally, on the actual queries). The additional axioms of the highly axiomatized ontology can be used in order to check the consistency of the ontology and the knowledge base with regards to the higher axiomatized version, but for querying them the simple ontology may suffice.This idea can be extended to include ontology extensions to the original ontology that are formalized in more expressive languages than the current web standards. For example, Another approach is to describe what kind of information an ontology is supposed to have, using autoepistemic constructs like the K-and Aoperators A further example for the use of more expressive ontology languages complementing an existing ontology is the application of higher order consistency rules. The best known example of such an approach is the OntoClean methodology (see chapter "An Overview of OntoClean"). OntoClean defines constraints with regards to the subsumptions of classes tagged with certain metaproperties. One approach to using second order semantics on a given ontology is to reify the ontology in OWL DL Other Aspects
As stated in the introduction to this chapter, we focused on the domainand task-independent evaluation of the ontology. This ignored other relevant evaluation methods that will be briefly discussed and referenced in this section.An ontology is usually defined as a social artifact It specifies the shared agreement of a group of stakeholders. Thus one quality criteria of the ontology will be its sharedness, the agreement of the stakeholders with the conceptualization specified in the ontology. This is relevant with regards to vocabulary (is it complete with regards to the domain? are terms missing?) and semantics (do they represent the relations between the terms correctly?). Aspects like syntax, structure, and representation are usually of a lesser concern to the stakeholders. Modern ontology engineering methodologies like DILIGENT A related question is how to actually ground the terms in an ontology The bigger a group that commits to an ontology (and the shared conceptualization it purports), the harder it is to reach a consensus -but also the bigger the potential benefit. Thus the status of the ontology with regards to relevant standardization bodies in the given domain is a major criteria when evaluating an ontology. Ontologies may be standardized or certified by a number of bodies like W3C, Oasis, IETF, and other organizations that may standardize ontologies in their area of expertise -or just a group of peers that declare an ontology a standard (cf. the history of RSS There are several approaches for task-dependent evaluations Domain completeness is given when an ontology covers the complete domain of interest. This can be only measured automatically if the complete domain is accessible automatically and can be compared to the ontology. A way to assess the completeness of an ontology with respect to a certain text corpus is to use ontology learning techniques to construct an ontology from the corpus Often ontologies are tightly interwoven with an application, so that the ontology cannot be simply exchanged. It may drive parts of the user interface, the internal data management, and parts of it may be hard-coded into the application. On the other hand, the user never accesses an ontology directly, but always through some application. Often the application needs to be evaluated with the ontology, regarding the ontology as merely another component of the used tool. Such a situation has the advantage that well-known software evaluation methods can be applied, since the system can be regarded as an integrated system where the fact that an ontology is used is of less importance.Conclusions
In this chapter we discussed a number of evaluation methods for the different aspects of an ontology. It should be clear that none of these methods, neither alone nor in combination, can guarantee a good ontology. But they can at least recognize problematic parts of an ontology, i.e. they can tell you when an ontology is not good.As we have seen, on the one hand, a number of quality criteria have been suggested in literature. On the other hand, a number of evaluation methods have been designed, implemented, and experimentally verified. Their actual relation is only badly understood and superficially investigated, if at all. Whereas Gangemi et al. It is obvious that domain-and task-independent evaluation techniques, as discussed here, provide some common and minimum quality level, but can only go a certain way. In order to properly evaluate an ontology, the evaluator always needs to come up with methods appropriate for the domain and task at hand, and decide on the relative importance of the evaluation criteria. But the minimum quality level discussed here will at least provide the ontology engineer with the confidence that they eliminated many errors and can publish the ontology.It seems that a proper ontology evaluation -being able to tell that an ontology will be good for a certain task and in a certain environment -will remain a task for a human level intelligence. But the more problems ontology evaluation techniques can eliminate before-hand, the more will humans be able to concentrate on the tasks they are best at: comparing the formal specification in the computer with their human conceptualization, and thus enabling computers and humans to speak a common language. Introduction
In order to discuss ontology engineering environments, we first need to clarify what we mean by ontology engineering. Ontology engineering is a successor of knowledge engineering which has been considered as a key technology for building knowledge-intensive systems. Although knowledge engineering has contributed to eliciting expertise, organizing it into a computational structure, and building knowledge bases, AI researchers have noticed the necessity of a more robust and theoretically sound engineering which enables knowledge sharing/reuse and formulation of the problem solving process itself. Knowledge engineering technology has thus developed into "ontology engineering" where "ontology" is the key concept to investigate.There is another story concerning the importance of ontology engineering. It is the Semantic Web. The Semantic Web strongly requires semantic interoperability among metadata which are made using semantic tags defined in different ontologies. The issue here is to build good ontologies to come up with meaningful sets of tags which are made interoperable by ontology alignment.Although the importance of ontology is well-understood, it is also known that building a good ontology is a hard task. This is why there have been developed some methodologies for ontology development [Chapter 6, 9] and have been built a number of ontology representation and editing tools.This chapter discusses factors of an ontology engineering environment thorough comparison of some tools. The purpose is not to rank them but to discuss characteristics of them intended to give a guideline for users to S. Staab and R. Studer (eds.), Handbook on Ontologies, International Handbooks choose an appropriate tool for their purpose. While over a hundred tools are developed to date, because of the space limitation, this chapter takes up On-toEdit Trends of Ontology Engineering Environment
In the 1990s, several ontology engineering environments, such as Ontolingua Server, WebOnto, Ontosaurus, have been developed as the advancement of ontology engineering. Reference In the 2000s, OIL, DAML and DAML+OIL, which are the predecessors of OWL [Chapter 4], were published, and ontology engineering tools for those languages were developed. The representatives of them are OilEd, OntoEdit, Protégé and so on. After RDF(S) [Chapter 3], and OWL were published, these tools supported them as well as many other tools did. In Ontology Tools SurveyFactors of an Ontology Engineering Environment
A comprehensive evaluation of ontology engineering tools is found in Ontology development phase
The first key task of ontology engineering is ontology construction. It includes constructions of class hierarchies, describing definitions of classes, defining relations between classes, and so on. Ontology engineering environments should support the process with the following characteristics. Development methodology Though an ontology development requires a sophisticated development methodology, a methodology itself is not sufficient. Developers need an integrated environment which helps them build an ontology in every phase of the building process. In other words, a computer system should navigate developers in the ontology building process according to a methodology. Collaborative development Building an ontology is often done with collaboration of multiple developers who need help in orchestration of the collaborative activities.Compliance with an ontological theory (Theory-awareness) An ontology is not just a set of concepts but at least a "well-organized" set of concepts. An environment is expected to guide users to a well-organized ontology which largely depends on the environment's discipline of what an ontology should be rather than an ad hoc classification of concepts or a frame representation. This is why an environment needs to be compliant with a sophisticated theory of ontology. OntoEdit
OntoEdit Figure Ontology Development Phase
Requirement Specification Phase
Two tools, OntoKick and Mind2Onto, are prepared for supporting this phase of ontology capture. OntoKick is designed for computer engineers who are familiar with software development process and tries to build relevant structures for building informal ontology description by obtaining competency questions proposed in Ontology Evaluation and Refinement Phase
Refinement Phase Evaluation Phase
The key process in this phase is use of competency questions obtained in the first phase to see if the designed ontology satisfies the requirements. To do this, OntoEdit provides users with a function to form a set of instances and axioms used as a test set for evaluating the ontology against the competency questions. It also provides users with debugging tools for ease of identify and correct incorrect part of the ontology. It maintains the dependency between competency questions and concepts derived from them to facilitate the debugging process. This allows users to trace back to the origins of each concept. Another unique feature of this phase is that collaborative evaluation is also supported by introducing the name space so that the inference engine can process each of test sets given by multiple users. Further, it enables local evaluation corresponding to respective test sets followed by global evaluation using the combined test. Like WebODE, OntoEdit supports OntoClean [Chapter 9] methodology to build a better is-a hierarchy.Inference
OntoEdit employs Ontobroker Hozo
HozoWhen an ontology and its instance model seriously reflects the real world, users have to be careful not to confuse the Role such as teacher, mother, fuel, etc. with other basic concepts (natural type) such as human, water, oil, etc. Let us take an example: <teacher is-a human>. Assume John is a teacher of a school. Given the usual semantics of is-a, since John is an instance of teacher then he is also an instance of human at the same time. When he quits being a teacher, he cannot be an instance of teacher so that you need to delete the instance-of link between John and teacher. However, you have to restore an instance-of link between John and human, otherwise John dies. This problem would be difficult for a model with no idea of roles to represent changes in the roles played by John (e.g., teacher, husband, patient) according to contexts or aspects.In Hozo, three different classes are introduced to deal with the concept of role appropriately.Role-concept A concept representing a role dependent on a context (e.g., teacher role) Basic concept A concept which does not need other concepts for being defined (e.g., human) Role holder An entity of a basic concept which is holding the role (e.g., teacher)A basic concept is used as the class constraint which indicates potential players who can play the role (role concepts). Then an instance that satisfies the class constraint plays the role and becomes a role holder. Hozo supports to define such a role concept as well as a basic concept.Ontology Development Phase
Like other editors, Ontology Editor in Hozo provides users with a graphical interface through which they can browse and modify ontologies by simple mouse operations. How to deal with "role concept" and "relation" on the basis of fundamental consideration is discussed in 1. Navigation pane provides several functionalities for browsing the ontology such as displaying the ontology in a hierarchical structure according to Collaborative Development
Collaborative development of an ontology is supported in Hozo Ontology Use Phase
Functionality and GUI of Hozo's instance editor is the same as the one for ontology. The consistency of all the instances with the ontology is automatically guaranteed, since a user is given valid classes and their slot value restrictions by the editor when he/she creates an instance. Inference mechanism of Hozo is not very sophisticated. Axioms are defined for each class but it works as a semantic constraint checker like WebODE. Hozo has an experience in modeling of a real-scale Oil-refinery plant with about 2000 instances including even pipes and their topological configuration which is consistent with the Oil-refinery plant ontology developed with domain experts WebODE
WebODE 6 [1] is a scalable and integrated workbench for ontology engineering and is considered as a Web evolution of ODE(Ontology Development Environment Ontology Development Phase
WebODE has ontology editing service, WAB: WebODE Axiom Builder service, inference engine service, interoperability service and ontology documentation service in this phase. The ontology editor provides users with form based and graphical user interfaces, WAB provides an easy graphical interface for defining axioms. It enables users to define an axiom by using templates given by the tool with simple mouse operations. Axioms are translated into Prolog. The inference engine is based on Prolog and OKBC protocol to make it implementation independent. Interoperability services provided by WebODE are of variety. It includes ontology access API, ontology export/import in XML-family languages, translation of classes into Java beans to enable Jess system to read them and OKBC compliance.ODEClean [5]
Like OntoEdit, WebODE supports OntoClean methodology to build a more convincing is-a hierarchy. Ontology for OntoClean is composed of top level universal ontology developed by Guarino [Chapter 9], a set of meta-properties and OntoClean axioms which are translated into Prolog to be interpreted by WebODE inference engine. It is given to the ODEClean which works on the basis of it.Collaborative Development
The collaborative editing of an ontology is supported by a mechanism that allows users to establish the type of access to the ontologies developed through the notion of groups of users. Synchronization mechanism is also introduced to enable several users to safely edit the same ontology. Ontologies are automatically documented in different formats such as HTML tables with Methontology's intermediate representations, concept taxonomies and XML.Ontology Use Phase
To support the use process of ontology, WebODE has several functionalities. Like Hozo, it allows users to have multiple sets of instances for an ontology by introducing instance sets depending on different scenarios, and conceptual views from the same conceptual model, which allows creating and storing different parts of the ontology, highlighting and/or customizing the visualization of the ontology for each user. WebPicker is a set of wrappers to enable users to bring classification of products in the e-Commerce world into WebODE ontology. ODEMerge is a module for merging ontologies with the help of correspondence information given by the user. Methontology and ODE have been used for building many ontologies including chemical ontology WebODE is also used for developing some semantic web frameworks such as ODE SWS SWOOP
SWOOPOntology Development Phase
A key feature of its design rationale is to realize user interface like the standard web browser. It consists of an address bar, history buttons (back, next), a navigation sidebar, bookmarks and so on (Fig. Collaborative Development
For collaborative ontology development, SWOOP supports collaborative annotation and version control. The collaborative annotation is based on the standard W3C Annotea protocols. Users can share annotations about change of ontologies and discussions through a public Annotea server. The version control supports undo/redo with logging of changes and save of checkpoints. While the change logs can be used to track back the changes, the checkpoints are used as a snapshot of ontology at particular time.Ontology Evaluation and Refinement Phase
SWOOP contains two reasoners: RDFS-like and Pellet. The former is a lightweight reasoner for RDFS, and the latter is a powerful reasoner for OWL-DL. Pellet is based on the tableaux algorithms and can be used to check inconsistencies of definition in ontologies. SWOOP provides functions for ontology debugging and repair using the description logic reasoner Protégé
Protégé1. Extensible knowledge model to enable users to redefine the representational primitives 2. A customizable output file format to adapt any formal language 3. A customizable user interface 4. Powerful plug-in architecture to enable integration with other applications Fig. Ontology Development Phase
The system provides two main ways for ontology development such as Protégé-Frames and Protégé-OWL. The former supports frame-based knowledge model which is compatible to OKBC. And the latter is extensions of them using the Protégé OWL plug-in Protégé has a semi-automatic tool for ontology merging and alignment named PROMPT Comparison and Discussion
The five environments are compared according to the factors presented above. Table Ontology Development Phase
Development Methodology
Philosophy of supporting ontology development is partly based on viewing an ontology as a software product. Common features of OntoEdit and WebODE On the other hand, Hozo mainly takes Theory-Awareness
Ontology building is not easy. This is partly because a good guideline is not available which people badly need when articulating the target world and organizing a taxonomic hierarchy of concepts. An ontology engineering environment has to be helpful also in this respect. WebODE and OntoEdit support Guarino's Ontoclean method. Guarino and his group have been investigating basic theories for ontology for several years and have come up with a sophisticated methodology which identifies inappropriate organization of is-a hierarchy of concepts. Developers who develop an ontology based on their intuition tend to misuse of is-a relation and to use it in more situations than are valid, which Guarino called "is-a overloading." Ontoclean is based on the idea of meta-property which contributes to proper categorization of concepts at the meta-level and hence to appropriate organization of is-a hierarchy.OntoEdit and WebODE way of ontology cleaning can be said that postprocessing way. On the contrary, Hozo tries to incorporate the fruits of ontological theories during the development process. One of the major causes of producing an inappropriate is-a hierarchy from Guarino's theory is lack of the concept of Role such as teacher, mother, food, etc. which has different characteristics from so-called basic concepts like human, tree, fish, etc. Ontology editor in Hozo incorporates a way of representing the concept of Role.Ontology Use Phase
Standards Compliance
All the five have support standards ontology languages such as RDF(S) and OWL. Hozo only can export its ontology and model in RDF(S) and OWL. Protégé also supports Semantic Web Rule Language (SWRL).Ontology/Model(Instance) Server
Hozo and WebODE has an ontology/model server which allows agents to access the ontologies and instance models through internet. OntoEdit and Protégé have an ontology server. SWOOP does not have a specific ontology server but can download ontologies in general web servers.Ontology Evaluation and Refinement Phase Evaluation Methodology
OntoEdit and WebODE support OntoClean methodology to build a better is-a hierarchy. SWOOP provides functions for ontology debugging and repair using the tableaux based reasoning. It explains the result of reasoning with a guideline to repair the inconsistencies of ontologies. Hozo and Protégé have no evaluation methodology.Inference Service
All the five have inference mechanisms. Hozo supports only inference for constraint checking of own language.Refinement Support
OntoEdit and WebODE have a debugging tool based on OntoClean. Hozo has a function to Check changes and suggest countermeasures for modification by comparison of ontologies. SWOOP provides a debugging tool based on reasoning and change management and a version control mechanism with logging of changes. Protégé also supports change monument and a semi-automatic ontology alignment tool named PROMPT.Software Level Issue
Friendly GUI All the five have sophisticated GUI such as visualization of class hierarchies and editing tool for constraints (axioms) of classes. It makes users free from coding using a complicated language. In Hozo, visualization of an ontology is default, and users can browse and edit it graphically. SWOOP supports in-line/GUI based editing functions and visualization of ontologies as just like a web page. Others provides mainly GUI based editing functions with some graphical visualization tool of ontologies. For instance, Protégé supports several ontology visualization pulig-ins such as OWL-Viz and Jambalaya.Architecture and Extensibility
While WebODE and Hozo employ standardized API to the main ontology base, OntoEdit and SWOOP supports a plug-in architecture. Protégé provides both of API and plug-in. Both enable a module can easily added or deleted to make the environment extensible. WebODE, OntoEdit and Hozo are webbased, while Protégé is basically not. But Protégé has another mode to support server-client architecture.Other Environments
In this section, other environments are summarized. We discuss four tools which have characteristic functionalities to aid user's ontology development.OntoGen
OntoGenKeywords extraction: In the system, two keyword extraction methods, the concept's centroid model and SVM linear model, are implemented. The extracted keywords by both methods are shown with related information about the number of related documents, average inner-cluster similarity measure, and so on. Concept suggestion: The system suggests sub-concepts of the selected concept to the user based on two different approaches: an unsupervised approach and a supervised one. In the unsupervised approach, OntoGen supports four clustering methods: k-means, LSI, PH k-means, and a categorization according to the labels in the input data. The user can select one of the methods, and supervises the parameters for the method. Then the system suggests sub-concepts using the selected method with the parameters. The supervised approach is based on SVM active learning method. In the approach, the user enters a query the active learning system then the system asks if a particular document (instance) belongs to the selected concept and the user answers yes or no. After repetition of this learning process the system outputs most important keywords with information about positively classified into the concept. Document management: The system manages documents related to concepts.When a new concept is added to ontology, it automatically assigns documents to it according to the similarity between documents. The system also has a functionality to detect if documents related to a concept belongs to its super concept. The user can know inconsistency between ontologies through the result.CampTools Ontology Editor
CampTools ontology editor (COE) The system also provides a cluster-based vicinity concepts view. In the view, the system shows concepts which relevant to selected concept based on multiviewpoint clustering analysis (MVP-CA) software developed by Pragati, Inc..OntoBilder
OntoBuilder Concluding Remarks
A lot of ontology engineering environments have been developed. Although some are powerful as a software tool, but many are passive in the sense that few guidance or suggestion is made by the environment. Theory-awareness should be enriched further to make the environment more sophisticated. Especially, more effective guidelines for appropriate class and relationship identification are needed. Collaboration support becomes more and more important as ontology building requirements increases. Ontology alignment is also crucial for reusing the existing ontologies and for facilitating their interoperability. Combination of the strong functions of each environment of the five would realize a novel and better environment, which suggests that we are heading right directions to go.Introduction
The dissemination of ontologies and ontology-based applications within enterprizes requires methods and tools which are able to deal with both the technical and the economic challenges of ontology engineering . In order for ontologies to be efficiently built and deployed at large scale one needs mature technologies supporting the entire ontology, as well as proved and tested means to plan and control the overall engineering process as part of more general IT-related processes within an enterprize. A wide range of ontology engineering methodologies have emerged in the Semantic Web community. Apart from minor differences in the level of detail adopted for the description of the S. Staab and R. Studer (eds.), Handbook on Ontologies, International Handbooks process model they define ontology engineering as an iterative process, which shows major similarities to models emerged in the neighbored research field of software engineering. However existing methodologies do not consider the economic factors commonly related to every real-world engineering project, in particular the estimation of development and maintenance costs using predefined cost models, and the impact of such cost information on the operation of an engineering process.With ONTOCOM we present the first existing approach in this newly emerging field of ontology engineering Cost estimation methods have a long-standing tradition in more mature engineering disciplines such as software engineering or industrial production The outline of this chapter is as follows. We start by reviewing the economical aspects of ontology engineering in the context of corporate IT, motivating the need for cost estimation models in this field in Sect. 2. Provided reliable means to estimate costs during particular stages of the life cycle of an ontology, Sect. 3 introduces a series of use cases showing how cost information could be utilized to optimize the operation of an ontology-related project. The remaining sections are dedicated to the design and evaluation of a cost model for ontologies. We first analyze potential methods for cost prediction in Sect. 4. Section 5 introduces the ONTOCOM model based on the previously identified most promising methods. Details about its application in concrete ontology engineering projects are provided in Sect. 6. Section 7 summarizes the lessons learned from our research and the planned future work.Economical Aspects of Information Technology
In this section we situate ontology engineering in the IT landscape of an enterprize. We discuss how the development and deployment of ontologies influences the enterprize information architecture and analyze the most important economical aspects related to this setting. As a result we argue for the necessity of reliable instruments for cost prediction for ontology engineering, which is an essential part of the data architecture of an enterprize.Enterprize Information Architecture
An enterprize information architecture includes the products, procedures, organizational structures and IT systems of an enterprize. The design of enterprize architectures and their continuously adaptation to new environmental requirements are realized with the help of so-called architecture development frameworks such as the Zachmann framework According to the latter an enterprize information architecture is typically modeled iteratively at four levels: Business, Application, Data, and Technology. The design process starts at the business level with the definition of a business strategy, followed by the specification of requirements which lead to an overall architecture vision and to a business, product and process architecture. The following steps are more technically oriented. IT specialists design the application architecture, the actual technology architecture implementing the application, and the information system/data architecture, which refers to the data models, or in our case ontologies, used in the application (cf. Fig. So far the ontology engineering research community has focused mainly on methods and tools for building and managing ontologies without reflecting on the implications arising from enterprize architecture development processes.  These implications are twofold: on the one hand the development of an enterprize information architecture results, among other things, in a series of domain and functional requirements regarding the scope and the utilization of the ontology; on the other hand this imposes non-functional boundaries such as the maximal development effort to be invested in the realization of the ontology. Consequently the requirements in the first category need to be matched to the estimated costs related to the development and the maintenance of the ontology.Governance
A second essential aspect in the context of an enterprize IT infrastructure is the operation and maintenance of the underlying IT systems, tasks typically accomplished according to a governance framework evaluated and approved by dedicated institutes such as COBIT. 3 Governance frameworks cover the organization, control, steering and diffusion of corporate IT system development:modeling method (IDEF1), data modeling method (IDEF1X), process description capture method (IDEF3), object oriented design method (IDEF4), and finally the ontology description capture method (IDEF5), which can be mapped to the aforementioned architectural models foreseen by enterprize architecture development frameworks. More information about IDEF is available at http://www.idef.com/ 3 http://www.itgi.org/ Organization Organizational aspects describe the different roles relevant for the development of an IT system in an organization, their responsibilities and decision procedures. Steering Steering includes the definition of processes and activities in which the participants act in order to achieve the overall goals. The handling of intellectual property rights is specified as well in this step. This is an important aspect for ontology engineering as a systematic use of ontologies is an important pre-requisite for achieving application interoperability. Control Control covers the definition of indicators in order to monitor the processes defined in the previous step and be able to detect unintended consequences of a system development or operation. From an ontology engineering perspective this step includes metrics for the characterization of ontologies and the associated development and maintenance processes. Diffusion Finally, one of the most important aspects in large organizations is the definition of appropriate roll-out mechanisms in order to guarantee that the whole organization is able to follow the proposed processes.In our work we focus on cost effectiveness, as one of the most important indicators at controlling level in an governance framework. Obviously the costs associated to the development of an IT system, of which ontology engineering is an important part, should be lower than the benefit expected to be obtained through its deployment. The estimation of the efforts related to engineering an ontology is crucial for the planning of data architecture change projects. Taking into account the operation of IT systems, it is worthwhile to pay attention to the reusability of an ontology. A reusable ontology is likely to be more user-friendly, thus reducing the training effort required in the roll-out phase of the associated IT system, while typically involving additional effort in the development. For monitoring purposes it is furthermore interesting to compare the estimated and the actual effort values in order to judge the ability of the organization to develop ontologies.Provided these various usage scenarios, we argue for the necessity of extending existing IT-specific cost estimation approaches towards ontologies. In the next section we provide a second motivating scenario for this requirement, explaining how knowledge about the efforts implied by the development, maintenance and deployment of an ontology influences its life cycle.Usage of Cost Information During the Life Cycle of an Ontology
The previous section was concerned with the relevance of cost information in the IT organization from a management perspective. This section focuses on the implementation side of ontologies, and thus on the relevance of cost information from a development and maintenance perspective. Figure The costs may be estimated with methods proposed in this chapter. The quantification of the benefits are covered by recent proposals to value IT development projects The effort related to ontology engineering is relevant in the initial project planning phase. In this phase the project manager assigns available resources to the planned tasks, taking into account the estimates of the effort associated to them, which are crucial for an on-time delivery of the project outcomes. These estimates are updated during the remaining project phases, as at the beginning of the project information about available skills and other influencing factors might not be available or reliable. Re-estimations during the project allow for adjustments in the project plan and provide a basis for the calculation of the total resources necessary to complete the project. In terms of ontology engineering the availability of cost information helps to make decisions related to the expected quality, size and granularity of the ontology to be developed. This is particularly of interest for ontology engineering methodologies following an iterative or rapid prototyping approach.Effort estimates can be used in equal measure for controlling and benchmarking purposes. As such estimates are derived from previous project experiences, the comparison between planned and actual effort values is a benchmark against previous projects, either external or internal. If the variation exceeds certain thresholds the project manager can take early countermeasures or, at least, can thoroughly examine the project and detect the underlying reasons for the variation.Towards the end of the ontology life cycle effort estimations for ontology maintenance can support repair versus replacement decisions. This of course requires knowledge about the total cost of ownership of an ontology and about the cost statements with respect to particular ontology management activities.To summarize cost estimation models for ontologies are necessary in order to align the discipline of ontology engineering with common IT practice within enterprizes, as well as for shaping the life cycle of ontologies in an economically based fashion. The remaining sections describe how such cost models can be designed, evaluated and used in an organization.Design of an Ontology Cost Estimation Model
In order to design a cost estimation model for ontologies we can resort to established approaches in the field of software measurement, which describe the steps required for this purpose and the way they can be effectively performed within a company (cf. Fig. Extract In this step the engineering team identifies the cost factors and decides upon the method(s) used to generate the estimates.Fig. We now turn to a description of the first step. Information about the validation of a particular model (in our case based on a parametric approach) are available in Generic Methods
Estimating costs for engineering processes can be performed according to several methods, often used in conjunction in order to avoid individual limitations Expert judgment/Delphi method The Delphi method is based on a structured process for collecting and distilling knowledge from a group of human experts by means of a series of questionnaires interspersed with controlled opinion feedback. The involvement of human experts using their past project experiences is a major advantage of the approach. Its most extensive critique point is related to the difficulties to explicitly state the decision criteria used by the contributing experts and to its inherent dependency of the availability of experts to carry on the process. Analogy method The main idea of this method is the extrapolation of available data from similar projects to estimate the costs of the proposed project.The method is suitable in situations where empirical data from previous projects is available and trustworthy, and depends on the accuracy in establishing real differences between completed and current projects. Decomposition method This method involves generating a work breakdown structure, i.e., breaking a product into smaller components or a project into activities and tasks in order to produce a lower-level, more detailed description of the product/project at hand, which in turn allows more accurate cost estimates. The total costs are calculated as average values, possibly adjusted on the basis of the complexity of the components/tasks considered. The successful application of the method depends of the availability of necessary information related to the work breakdown structure.Parametric/algorithmic method This method involves the usage of mathematical equations based on research and historical data from previous projects. The method analyzes main cost drivers of a specific class of projects and their dependencies and uses statistical techniques to refine and customize the corresponding formulas. As in the case of the analogy method the generation of a proved and tested cost model using the parametric method is directly related to the availability of reliable and relevant data to be used in calibrating the initial core model.Orthogonally to the aforementioned methods we mention two high-level approaches to cost estimation (cf. Table Bottom-up estimation
This approach involves identifying and estimating costs of individual project components separately and subsequently summing up the outcomes to produce an estimation for the overall project. As such the bottom-up approach is at the core of the decomposition method introduced above. Top-down estimation By contrast the top-down method relies on overall project parameters. For this purpose, the project is partitioned into lowerlevel components and life cycle phases beginning at the highest level. The approach produces are total project estimates, in which individual process tasks or product components are responsible for a proportion of the total costs.The decomposition method is based on a bottom-up approach. Estimation by expert judgment, analogy or parametric equations can be carried out in a top-down or a bottom-up fashion, also depending of the stage of the project in which the estimates need to calculated. Top-down estimation is more applicable to early cost estimates when only global properties are known, but it can be less accurate due to the less focus on lower-level parameters and technical challenges usually predictable later in the process life cycle, at most. The bottom-up approach produces results of higher-quality, provided a realistic work breakdown structure and means to estimate the costs of the lower-level units the product/project has been decomposed into.Methods Feasible for Ontology Engineering
In the following we examine the advantages and disadvantages of each of the aforementioned approaches given the product-and process-related characteristics of ontology engineering and the current state of the art in the field:Expert judgment/Delphi method The expert judgement method seems to be appropriate for our goals since large amount of expert knowledge with respect to ontologies is already available in the Semantic Web community, while the costs of the related engineering efforts are not. Experts' opinion on this topic can be used to compliment the results of other estimation methods. Analogy method The analogy method requires knowledge about the features of an ontology, or of an ontology development process, which are relevant for cost estimation purposes. Further on it assumes that an accurate comparison function for ontologies is defined, and that we are aware of cost information from previous projects. While several similarity measures for ontologies have already been proposed in the Semantic Web community, no case studies on ontology costs are currently available. There is a need to perform an in-depth analysis of the cost factors relevant for ontology engineering projects, as a basis for the definition of such an analogy function and its customization in accordance to previous experiences. Decomposition method This method implies the availability of cost information with respect to single low-level engineering tasks, such as costs involved in the conceptualization of single concepts or in the instantiation of the ontology. Due to the lack of available information the decomposition method can not be applied yet to ontology engineering. Parametric/algorithmic method Apart from the lack of costs-related information which should be used to calibrate cost estimation formula for ontologies, the analysis of the main cost drivers affecting the ontology engineering process can be performed on the basis of existing case studies on ontology building, representing an important step toward the elaboration of a predictable cost estimation strategy for ontology engineering processes. The resulting parametric cost model has to be constantly refined and customized when cost information becomes available. Nevertheless the definition of a fixed spectrum of cost factors is important for a controlled collection of existing real-world project data, a task which is fundamental for the subsequent model calibration. This would also be useful for the design and customization of alternative prediction strategies, such as the aforementioned analogy approach.Given the fact that cost estimation has been marginally explored in the Semantic Web community so far, and that little is known about the underlying cost factors , a bottom-up approach to the previously introduced methods is currently not practicable, though it would produce more accurate results. In turn, expert judgment, analogy and parametric cost estimates could be obtained in a top-down fashion, if the corresponding methods are clearly defined and customized in the context of ontology engineering. A summary of the results of this feasibility analysis is depicted in Table Section 5 introduces the ontology cost model ONTOCOM and discuss ways to improve its prediction quality. ONTOCOM follows a top-down approach to cost estimation, by identifying the cost drivers associated to the most important phases of the ontology life cycle and calculating a global effort estimate on the basis of different prediction methods. The current version of ONTO-COM investigates the usage of the parametric, the analogy and the Delphi methods to ontology engineering.ONTOCOM: A Cost Model for Ontology Engineering
In this section we introduce the generic ONTOCOM cost estimation model. The model is generic in that it assumes a sequential ontology life cycle, according to which an ontology is conceptualized, implemented and evaluated, after an initial analysis of the requirements it should fulfill (see below). By contrast ONTOCOM does not consider alternative engineering strategies such as rapid prototyping or agile methods, which are based on different life cycles. For the parametric method the result of these steps is a statistical prediction model (i.e., a parameterized mathematical formula). Its parameters are given start values in pre-defined intervals, and are subsequently calibrated on the basis of previous project data. This empirical information complemented by expert opinions is used to evaluate and revise the predictions of the initial a priori model, thus creating a validated a posteriori model. The analogy method works similarly. It is based on a similarity equation, which is a mathematical formula aggregating similarity functions on the basic cost dimensions in a weighed fashion. The weights need to be specified according to empirical calibration and/or expert judgement, just as in the case of the parametric method. The Delphi method can be applied independently of any prediction formula or analogy function (see below).The parametric equation has been carefully evaluated using statistical calibration and Bayes analysis as described in The Work Breakdown Structure
The top-level partitioning of a generic ontology engineering process can be realized by taking into account available process-driven methodologies in this field Depending on the ontology life cycle underlying the process-driven methodology, the aforementioned four steps are to be seen as a sequential workflow or as parallel activities. Methontology We now introduce the cost drivers associated to this work breakdown structure.The ONTOCOM Cost Drivers
The ONTOCOM cost drivers, which are proved to have a direct impact on the total development efforts, can be roughly divided into three categories Product-related cost drivers account for the impact of the characteristics of the product to be engineered (i.e., the ontology) on the overall costs. The following cost drivers were identified for the task of ontology building:• Domain Analysis Complexity (DCPLX) to account for those features of the application setting which influence the complexity of the engineering outcomes • Conceptualization Complexity (CCPLX) to account for the impact of a complex conceptual model on the overall costs • Implementation Complexity (ICPLX) to take into consideration the additional efforts arisen from the usage of a specific implementation language • Instantiation Complexity (DATA) to capture the effects that the instance data requirements have on the overall process • Required Reusability (REUSE) to capture the additional effort associated with the development of a reusable ontology item Evaluation Complexity (OE) to account for the additional efforts eventually invested in generating test cases and evaluating test results, and • Documentation Needs (DOCU) to state for the additional costs caused by high documentation requirements Personnel-related cost drivers emphasize the role of team experience, ability and continuity w.r.t. the effort invested in the engineering process:• Ontologist/Domain Expert Capability (OCAP/DECAP) to account for the perceived ability and efficiency of the single actors involved in the process (ontologist and domain expert) as well as their teamwork capabilities rience of the project team w.r.t. the representation language and the ontology management tools • Personnel Continuity (PCON) to mirror the frequency of the personnel changes in the team Project-related cost drivers relate to overall characteristics of an ontology engineering process and their impact on the total costs:• Support tools for Ontology Engineering (TOOL) to measure the effects of using ontology management tools in the engineering process, and • Multisite Development (SITE) to mirror the usage of the communication support tools in a location-distributed teamThe ONTOCOM cost drivers have been defined after extensively surveying recent ontology engineering literature and conducting expert interviews, and from empirical findings of numerous case studies in the field When using the model the project manager needs to specifies the current rating level for each cost driver according to the setting to which the estimation applies.The Parametric Method
The parametric method integrates the efforts associated with each component of this work breakdown structure to a mathematical formula as described below:According to the parametric method the total development efforts are associated with cost drivers specific for the ontology engineering process and its main activities. Experiences in related engineering areas In order to determine the effort multipliers associated with the rating levels and to select non-redundant cost drivers we followed a three-stage approach: first experts estimated the a priori effort multipliers based on their experience as regarding ontology engineering. Second we applied linear regression to real world project data to obtain a second estimation of the effort multipliers. The Analogy Method
The analogy method has several advantages when compared to the parametric one, the most important being probably that its usage in a new measurement environment does not require additional calibration efforts, which potentially lead to varying accuracy levels for particular cost drivers. These advantages come, however, at the cost of significant computational power required to calculate similarities, therefore both methods can be seen as candidate techniques to be applied in conjunction The analogy method defines similarities for each of the cost drivers associated to the work breakdown structure and cumulates the results linearly in a weighed equation:In Equation The Delphi Method
The Delphi or expert judgement method for cost estimation During the first brainstorming meeting the estimation team agrees upon the work breakdown structure, then the individual members provide estimates for the activities covered by this decomposition. In the second meeting the team aims at achieving a consensus on the final estimation by reviewing and revising the inputs of the members. This is an iterative process led by the moderator according to pre-defined rules. Once an agreement on the activitybased estimates has been achieved, the results are collected and compiled into a global figure, which can be used in the project.As aforementioned such consensus-driven estimations can be used in combination with other methods and for particular cost drivers or activities in order to adjust the effects of data entries which might be unavailable, unreliable or skewed. For example, we used expert estimations of the productivity range of the ONTOCOM cost drivers for the calibration of the parametric equation Using ONTOCOM
Starting from a typical ontology building scenario, in which a domain ontology is created from scratch by the engineering team, we simulate the cost estimation process according to the parametric method underlying ONTOCOM. Given the top-down nature of our approach this estimation can be realized in the early phases of a project. In accordance to the process model introduced above the prediction of the arising costs can be performed during the feasibility study or, more reliably, during the requirements analysis. Many of the input parameters required to exercise the cost estimation are expected to be accurately approximated during this phase: the expected size of the ontology, the engineering team, the tools to be used, the implementation language etc. The first step of the cost estimation is the specification of the size of the ontology to be built, expressed in thousands of ontological primitives (concepts, relations, axioms and instances): if we consider an ontology with 1,000 concepts, 200 relations (including is-a) and 100 axioms, the size parameter of the estimation formula will be calculated as follows:The next step is the specification of the cost driver ratings corresponding to the information available at this point (i.e., without reuse and maintenance factors, since the ontology is built manually from scratch). Depending on their impact on the overall development effort, if a particular activity increases the nominal efforts, then it should be rated with values such as High and Very high. Otherwise, if it causes a decrease of the nominal costs, then it would be rated with values such as Low and Very low. Cost drivers which are not relevant for a particular scenario, or are perceived to have a nominal impact on the overall estimate, should be rated with the nominal value 1, which does not influence the result of the prediction equation.Assuming that the ratings of the cost drivers are those depicted in Table 5 these ratings are replaced by numerical values. The value of the DCPLX cost driver was computed as an equally weighted, averaged sum of a highvalued rating for the domain complexity, a nominal rating for the requirements complexity and a high effort multiplier for the information sources complexity (for details of other rating values see The constant A has been set to 2.92 after the calibration of the model, while the economies of scale are so far not taken into consideration.In order to use ONTOCOM in a particular setting (enterprize, business domain, types of ontologies, to name only a few criteria) the generic model should be customized according to the following steps:• Refine and adapt the work breakdown structure in the light of the applied life cycle and process model followed when engineering the ontology • Define the statistical prediction model (i.e., a parameterized mathematical formula) • Calibrate the a priori model based on previous project data to create a valid (more accurate) a posteriori model • Use the calibrated model to predict development costs An example how the generic ONTOCOM model can be applied to a different ontology engineering methodology is described in Conclusions
Technologies related to the development, deployment and maintenance of ontologies have reached a maturity level that they become relevant for businesses. At this stage ontology engineering can no longer be accounted for in a stand alone manner, but should be integrated into the overall architecture and organization of an enterprize. We have shown how ontology engineering fits into existing architecture development frameworks: ontology engineering is an integral part of the information system architecture and influences the technology architecture of an enterprize. Companies complement their overall architecture with a governance framework setting the rules to organize, steer, control and diffuse its deployment. A major concern of IT governance is to timely identify changes in the architecture which are potentially of benefit and to control the realization of the expected benefits. In this context the availability of cost information related to the engineering of ontologies becomes important both at the beginning of an ontology engineering process and during its operation.In this chapter we have focused on the estimation of costs related to ontology engineering for planning purposes. We have discussed different methods to derive cost information from the environmental setting an existing knowledge and selected three for a more detailed presentation. Following a top-down approach all methods start with a definition of the work breakdown structure. The Delphi method is based on consensual expert estimates aligned to this work breakdown structure, which are aggregated by the project manager towards a final effort prediction. The parametric and analogy method define cost drivers and rating levels as a basis for the mathematical equations customized according to historical project data.The results from our case studies point in several directions. On the one hand incorporating cost-related aspects into ontology engineering practice is likely to facilitate the interaction between the ontology engineering community and business people. Cost information allow non-engineers to integrate ontology engineering into their management frameworks and makes the creation of ontologies more transparent from a business perspective. On the other hand the estimations are far from being precise yet. First results imply that the creation of glossary-like structures is well understood and the related effort predictable. By contrast the effort related to the creation of ontologies with a high axiomatization is hardly predictable and the exact correlations remain an open issue for future research.Hence, we see in number of new research directions for the economics of ontology engineering. From a management perspective open issues remain in the areas of controlling and the applicability of the cost models for nonexperts. Tool support and additional training materials are needed to ease non-experts the interaction with these models and to guarantee their correct usage. From a technical perspective, in the near future we intend to continue the data collection procedure in order to improve the quality of the generic model and its customizations. Much work needs to be done by many people, thus we see ONTOCOM as a seed for an urgently needed field of research, the cost estimation for ontologies. Any significant improvement in this field will substantially facilitate the uptake of semantic technologies for industrial projects. A second direction of research is related to the refinement of alternative methods for the estimation of critical input parameters such as the size of the prospected ontology. The analogy method seem to be a promising approach for this purpose.Part III
Ontologies Foundational Choices in DOLCE
Stefano Borgo and Claudio Masolo
Laboratory for Applied Ontology (ISTC-CNR), Trento, Italy, borgo@loa-cnr.it, masolo@loa-cnr.it Summary. Foundational ontologies are ontologies that have a large scope, can be highly reusable in different modeling scenarios, are philosophically and conceptually well founded, and are semantically transparent.After the analysis and comparison of alternative theories on general notions like 'having a property', 'being in time' and 'change through time', this paper shows how specific elements of these theories can be coherently integrated into a foundational ontology. The ontology is here proposed as an improvement of the core elements of the ontology dolce and is thus called dolce-core.Introduction
Chapter "What is an Ontology?" analyses what ontologies are and their peculiarities with respect to other methods and technologies that exist in conceptual modeling and knowledge representation. Foundational ontologies are ontologies that: (1) have a large scope, (2) can be highly reusable in different modeling scenarios, (3) are philosophically and conceptually well founded, and (4) are semantically transparent and (therefore) richly axiomatized.Foundational ontologies focus on very general and basic concepts (like the concepts of object, event, quality, role) and relations (like constitution, participation, dependence, parthood), that are not specific to particular domains but can be suitably refined to match application requirements. These notions have been largely investigated by philosophers and, even though foundational ontologies assume a modeling and engineering perspective (far from the absolutist view of most philosophical theories), one relies on philosophical considerations for the construction, comparison, organization, and assessment of the ontologies themselves.To achieve semantic transparency, a careful choice of the primitives and a precise characterization of their meaning are needed. This goal requires a formal language with clear semantics and adequate expressive power (in this chapter we will use first-order logic). Unfortunately, application concerns lead to work with languages that are suitable for run-time reasoning and one often has to give up on expressivity and semantic clearness. For these reasons, foundational ontologies are used in applications only in approximated forms via partial translations into different application-oriented languages. Thus, the relevance of foundational ontologies does not rely in their direct impact on applications but in their ability to providing conceptual handles with which to carry out a coherent and structured analysis of the domains of interest.The paper is organized as follows. Section 2 analyzes and compares alternative well founded theories on central notions like 'having a property', 'being in time' and 'change through time'. Then, in Sect. 3, we study how specific elements of these theories can be integrated into a foundational ontology that we call dolce-core and constitutes a first step in the revision of dolce 1 Foundational Distinctions
The literature on ontological choices is primarily of philosophical character. Several tenable positions for each issue have been individuated and some have been described to a rich level of detail. Unfortunately, there is no homogeneity in the depth of the analysis: some topics, like the theories of parthood and space, have been well studied others, e.g., the theories of dependence and unity still lack a stable landscape Theories of Properties
The nature of properties, the explanation of what it means that an individual has a property, and, more specifically, of how different individuals can have the same property, have been widely discussed and investigated ( Universalism
Figure The second diagram in Fig. Then, if John and Paul have the same weight, this does not mean that they have the same trope but that their distinct tropes (relative to weight) are somehow similar. Trope sameness is an a equivalent relation called indistinguishability or resemblance (≈): a and b share the property F if and only if a F ≈ b F . In short, trope theory reduces properties to equivalence classes of resembling tropes.Note that trope theory and universalism are not antithetical. One can rely on tropes and the inherence relation while substituting the classes of resembling tropes by universals and membership (between tropes and classes) by instantiation (between tropes and universals). That is, the universalist view can be adopted to classify the tropes instead of the entities as in the third diagram in Fig. Basic Properties, Quality Kinds, and Spaces
People compare entities along a variety of aspects such as color, weight, smell, etc. For each aspect, similarities are established depending on the tools people dispose of, or on the specific analysis they are interested in. This knowledge disparity is often dismissed by philosophers as an epistemological or empirical issue: the entities, they say, have a completely determined shade of color even though in practice it is not accessible to the observer. This attitude somehow prevents the assessment of a philosophical analysis of this issue, of course, but the available philosophical notions still provide a good starting point for building a philosophically based and yet application oriented framework.In In trope theory, sharing a basic property corresponds to having two exactly resembling tropes: two '1m long' entities have exactly similar (yet distinct) length-tropes. If they resemble each other inexactly, it is said that their lengthtropes resemble each other only up to a degree. One can add structure in the class of tropes by saying that 1m and 2m length-tropes have a higher degree of resemblance than the 1m and 30m length-tropes or analogously, that a scarlettrope and a crimson-trope resemble each other better than a scarlet-trope and a turquoise-trope. In this view, non basic properties are built as classes of inexactly resembling tropes. Exploiting the degrees of resemblance, all the tropes can be collected in few large classes. However, if we put together a 1mtrope and a 'red'-trope or a 1kg-trope, we contradict the initial intuition that the comparison between entities has to be done for 'homogeneous' properties, i.e., properties on the same aspect of entities: the comparison between the length of an object with the color of another object is not really plausible.General properties that identify specific aspects of entities (like "being colored", "being shaped", etc.) cannot be discharged: without these we cannot even conceive the functional laws of physics Properties in the same quality kind can be organized in taxonomies or in more sophisticated ways: from ordering (weight, length) to complex topological or geometrical relations (color splinter). Following Often spaces are motivated by applications or epistemological considerations, it is quite natural to associate each quality kind to several spaces, each organizing properties (and thus objects) according to different principles, instruments of investigation, applications concerns, etc. These spaces rely on relative notions of resemblance that are discussed, adopted, and abandoned by (communities of) intentional agents. This view of spaces as generated (and eventually destroyed) structures leads to model spaces as temporal entities. Alternative spaces can differ on several aspects: their structure, the level of detail the adopted measuring tool can reach, or the point of view that motivate them. This variety of spaces can be partially ordered according to the level of detail they are capable of distinguishing, a notion often called granularity.Concepts and Roles
The framework just introduced addresses two concerns: (1) representing intensional properties that are created (and eventually destroyed) by agents and (2) classifying qualities according to different points of view and granularities.The first point is important independently of the need to organize properties in spaces. Take properties like 'being a student', 'being money', 'being a catalyst', etc. that we will call concepts. These have a clear conceptual and intensional nature -they are defined in terms of relationships with external entities, e.g., 'a person enrolled in a university', and do not depend on their instances -but do not present any special internal structure. The rich framework given by quality kinds and spaces is largely pointless for these concepts. A mechanism more tailored to these properties is needed.Roles are a subclass of concepts. The nature and the representation of roles have been long discussed in a variety of fields: knowledge representation, conceptual modeling, multi-agent systems, linguistics, sociology, philosophy, and cognitive semantics (see Being in Time
The entities that are mostly studied in applied ontology are entities that exist in time. Temporal existence is often modeled via a predicate like PRE(x, t), whose informal reading is 'x is present at time t'.Since PRE is defined on times, these must be in the domain of quantification. However, this does not necessarily lead to strong ontological commitments on times: times could be constructed from events We give for granted that some entities are present at different times, i.e., they are persisting through time. The explanation of this apparently obvious fact may be quite intricate. Stage theory Two main philosophical positions accept the ontological existence of persisting entities: endurantism and perdurantism. Endurantists claim that one and the same entity is wholly present at different times (enduring) and read the formula PRE(a, t 1 ) ∧ PRE(a, t 2 ) as "a is wholly present at both the times t 1 and t 2 ". 'Being wholly present' is often contrasted with 'being partially present', i.e., the rationale of perdurantism. Perdurantists claim that the persistence through time is analogous to the extension in space: an entity has different parts at different times (perduring). The previous formula is then read by perdurantists as claiming "a has a part at t 1 and a (different) part at t 2 ". Therefore, in addition to a, perdurantists commit to the existence of the parts of a that exist only at t 1 and at t 2 , respectively.Despite the disagreement between perdurantism and stage theory on the nature of persisting entities, both the theories associate each persisting entity with a sequel of other entities. Indeed, the following property holds in these systems (it may fail for endurantists):Provided one does not give up on expressive power, it is formally an advantage to have a core theory compatible with different philosophical positions since one can use the very same framework and specialize it, when needed, with the additional constraints of one or the other theory. In this perspective, without (a3) the formula PRE(a, t 1 ) ∧ PRE(a, t 2 ) can be interpreted freely by endurantism, perdurantism, and stage theory.Property Change
Persisting objects change through time by changing their properties: a may be red at time t 1 and green at t 2 . A trope theorist explains change as trope substitution, (a8). If both tropes and universals are considered, a notion of "tropes changing over time" becomes available, we call these individual qualities. An individual quality, like a trope, inheres in a unique bearer but, differently from tropes, it can change over time. In this case we can explain change according to the following schemata that are similar to (a5) and (a7), respectivelyIn these approaches the color, weight, shape, etc. of an object are each modeled by a different individual quality, and the changing through time of these qualities explain changes in the bearers: intuitively, it is the individual color of an object a that changes from, say, fuchsia to green and the individual weight (a different individual quality of a) that changes from some weight to another. While (a10) is compatible with both an endurantist and a perdurantist reading about persistence of individual qualities, we see that (a11) is ontologically more demanding since it refers to temporal slices of individual qualities. On the other hand, (a11) has the advantage of being compatible with (a9) if we accept mereological sums of tropes. At the same time, (a9) is to be preferred to (a8) because in (a9) inst can be taken to be intensional.Of course, one should have some advantage for introducing yet another type of entities like individual qualities. After all, why aren't (a5) and (a7) enough? The usefulness of individual qualities relies on the fact that they are associated to one quality kind only and the latter usually has different spaces associated. A change in the same individual quality is described differently by the different points of views encoded by the spaces. For example, a change in color can be described according to both a RGB and a CYMK color-space. Having a unique individual color-quality related to all the relevant spaces allows for expressing that it is the same aspect of the object (the color) that changes. In Mereological Change: Endurantism vs. Perdurantism
The difference between the endurantist and perdurantist theories of persistence (Sect. 2.2) can be addressed in terms of the parthood relation. Classical endurantists think that "statements about what parts the object has must be made relative to some time or other" ( From these observations, perdurantists may indifferently adopt temporary parthood or parthood simpliciter as the primitive relation, while endurantists must rely on temporary parthood. In the perspective of foundational ontology, this is an important result, exploited in Sect. 3.2, since it shows that one can construct a fairly general ontology that is compatible with both endurantism and perdurantism.Parthood and Spatio-Temporal Inclusion
Perdurantists often see parthood as spatio-temporal inclusion and thus rely on extensional mereology (axioms (A1)-(A4) and definition (D2) of Sect. 3.2). This view pushes them to reject the existence of spatio-temporally coincident entities: if x and y have the same spatio-temporal extension then both P(x, y) and P(y, x) hold and consequently, due to antisymmetry of parthood, they are identical. This position is, however, more restrictive than the original proposal of Lesniewski In its general perspective, extensional mereology is a purely formal theory and it applies to all kinds of entities (the spatio-temporal entities are just one case). Events and Objects
We can all distinguish what changes from the changing event itself. A lively and long discussion on the ontological status of events and on what distinguishes them from objects has taken place especially in the philosophy of language Several authors collapse the object vs. event distinction to the endurant vs. perdurant one by identifying objects with endurants and events with perdurants. The unification is endorsed by the observations that the 'life of John' is only partially present at each time at which it exists (it has distinct temporal parts at each time at which it exists) and 'John' is wholly present whenever it exists (the existence of temporal parts is not required). However, if this match were correct, classical perdurantism would not be able to embrace the object vs. event distinction. The reason is easily stated: as shown in Sect. 2.3, all the entities in a perdurantist view have temporal parts when they exist but distinct entities cannot have exactly the same spatio-temporal location. Thus, since 'John' and 'the life of John' have exactly the same spatio-temporal location, perdurantists must identity them. Furthermore, it is not really an option to insist that 'John' is part of the 'life of John' or viceversa. These observations pushed some philosophers to reject as naive the previous identification and to look for a separate (and perhaps more general) foundation of the distinction between objects and events.Hacker • The properties (and qualities) that apply to material objects are different from those that apply to events. Typically, material objects have weight, size, shape, texture, etc. and are related by spatial relationships like congruence. Events, on the other hand, can be sudden, brief or prolonged, fast or slow, etc. and can occur before, after, simultaneously to other events. • Space plays a role in the identification of material objects and in their unity criteria, time in that of events. Material objects that are simultaneously located at different places are different and events that have different temporal locations are different Of course, even though events are primarily in time and objects primarily in other dimensions, there are strong interrelationships between them. Several authors By means of the relationship between objects and events (aka participation), it is possible to say that an object a exists at a certain time t "if and because" its life exists at t DOLCE-CORE: The New Basis for DOLCE
dolce The aim of dolce is to capture the intuitive and cognitive bias underlying common-sense while recognizing standard considerations and examples of linguistic nature. These claims are sustained by the accompanying documentation that carefully describes the foundational choices and motivates both the structure and the formalization of dolce. Generally speaking, dolce does not commit to a strong referentialist metaphysics (it does not make claims on the intrinsic nature of the world) nor to a scientific enterprise. Rather, it looks at reality from the mesoscopic and conceptual level aiming at a formal description of a specific conceptualization of the world. Technically, dolce is the result of a careful selection of constraints so to guarantee expressiveness, precision, and simplicity of use.In the following, we resume our discussion in the previous sections to present the ontological choices made by dolce. The discussion is limited to a fragment of the whole ontology (the core formed by the most general categories) and, in some cases, it departs from the published version Basic Categories
dolce-core is an ontology limited to entities that exist in time, called temporal particulars. While in dolce regions and spaces are abstract entities (i.e., entities that are outside time and space), dolce-core adopts a contextual perspective by introducing them as temporal entities that are created, adopted, abandoned, etc. Following dolce-core partitions temporal-particulars (pt) (hereafter particulars) into six basic categories: objects (o), events (e), individual qualities (q), regions (r), concepts (c), and arbitrary sums (as). All these categories are rigid: an entity cannot change from one category to another over time. Following the observations in Sect. 2.4, the dolce's categories ed (endurant) and pd (perdurant) are, respectively, renamed o (object) and e (event). Individual qualities are themselves partitioned into quality kinds (q i ). Each quality kind q i is associated to one or more spaces (s ij ): each individual quality in q i has location in (i.e., is associated to a region in each of) the associated spaces s ij . Since we impose that the spaces are disjoint, regions are themselves partitioned into the spaces s ij . For the sake of simplicity, we here consider a unique space t for (regions of) time.Parthood and Temporary Parthood
dolce-core carefully distinguishes spatio-temporal inclusion and parthood by adopting the axioms (A1)-(A4) of extensional mereology, see below. These axioms apply to all entities in the domain. The basic categories, with the exception of as, are homogeneous: the parts and the sums of entities belonging to one category are still in the same category (see (A5) and (A6)). as collects those mixed entities that are obtained as sum of elements in different basic categories. However, note that the ontology does not enforce any mereological sum of entities to exist. In particular, as may very well be an empty category. It is left to the user to enforce this constraint (perhaps limited to specific kinds of entities) when needed.In the following P(x, y) stands for 'x is part of y', O(x, y) for 'x overlaps with y', and SUM(z, x, y) for 'z is the mereological sum of x and y'.As anticipated in Sect. 2.2 we introduce the primitive predicate 'being present at' (PRE) to identify at which times entities exist. No commitment to a specific notion of time is taken in dolce-core. Nonetheless, in Sect. 3.4 we will analyze different readings of this predicate depending on the category of entities it applies to. PRE is defined on times (A7) and it is dissective and additive over time ((A8) and (A9)).As stated in Sect. 3.1, all the entities considered in dolce-core exist in time:To include entities not in time, one should add to dolce-core a more general category that includes both temporal and abstract particulars. In this general ontology, dolce-core provides the formalization of the subclass of temporal particulars.dolce-core adopts a temporary extensional mereology, also denoted by P, which is based on axioms (A12)-(A15), i.e., those of extensional mereology adapted to the extra temporal parameter. Further mereological aspects are enforced via the notion of time regular relation (see below). Expression P(x, y, t) stands for 'x is part of y at time t', analogously for O(x, y, t). (temporary extensionality) A15 If φ is o, e, q i , s jk or c: φ(y) ∧ P(x, y, t) → φ(x) (temporary dissectivity)Axiom (A3) implies that entities indistinguishable with respect to parthood are identical. Temporary coincidence (D4) provides a weaker form of identification: two entities x and y that are temporary coincident at time t, formally CC(x, y, t), are indistinguishable relatively to time t (they can still differ in general). Axiom (A16) states that in dolce-core parthood simpliciter can be defined on the basis of temporary parthood, i.e., temporary parthood is more informative. The opposite is true only committing to the existence of temporal parts that is not enforced here. This means that the axioms for temporary parthood are compatible with both the endurantist and perdurantist views of persistence through time. Note that axioms (A10) and (A16) make possible to define parthood simpliciter in terms of temporary parthood. Yet, we use two distinct primitives to avoid hidden commitments: in an extension of dolce-core that includes abstract entities, both the primitives are necessary (and the two axioms maintain their validity).D4 CC(x, y, t) P(x, y, t) ∧ P(y, x, t)Temporary parthood presents three main novelties with respect to the corresponding relationship of dolce: (1) it is defined on all the particulars that are in time; (2) the existence of sums is not guaranteed; (3) (A16) is new (in dolce it was given as a possible extension).dolce-core makes use of a few relations that satisfy the following structural axioms:We can rephrase these constraints as follows: if the relation holds at a time, it holds at any sub-time; if the relation holds at two times, then it holds also at the time spanning the two (provided it exists); if the relation holds for two entities at t, then it holds for entities temporally coincident with them at t.These constraints are important in setting the dolce-core framework and relations satisfying them are dubbed time regular. In particular, we enforce the temporal parthood of dolce-core to be time regular.Properties
dolce-core offers three different options to represent properties and temporary properties. The first option is standard and consists in the introduction of an extensional predicate. With this choice one cannot represent whether the property is related to contextual or social constructions nor its intensional aspects. In addition, to model change through time one needs to add a temporal parameter as in expression F (a, t), i.e., 'a has the property F at t'. This last solution allows to represent dynamics in the properties but, as anticipated, is not suited for roles The second option consists in reifying properties, that is, in associating them to entities in the category of concepts, c. In order to deal with concepts and to relate concepts to an entity according to the properties the latter has, a (possibly intensional) 'instance-of' relation, called classification (CF), is introduced in the ontology. CF(x, y, t) stands for 'x classifies y as it is at time t' and is characterized in dolce-core as a time regular relation that satisfies alsoThe idea is to use concepts to represent properties for which the intensional, contextual, or dynamic aspects are important (as in the case of roles The third option relies on the notions of individual quality, quality kind and (quality-)space introduced in Sect. 2.1. Each individual quality, say "the color of my car" or "the weight of John", and its host are in a special relationship called inherence (I). Formally, expression I(x, y), stands for "the individual quality x inheres in the entity y". 15 This relationship binds a specific bearer (A21) and each quality existentially depends on the entity that bears it (A22); in the previous examples the bearers are my car and John, respectively. Finally, axiom (A23) states that qualities exist during the whole life of their bearers. 16  We anticipated that individual qualities are grouped into quality kinds, say q i is the color-quality kind, q j the weight-quality kind, etc. These constraints are simple and we do not report them explicitly except for axiom (A24) according to which an entity can have at most one individual quality for each specific quality kind. Axioms (A25) and (A26) say that if two particulars coincide at t then they need to have qualities of the same kind and 14 Differently from Indeed, one can interpret trope substitution as a change of quality location. The position adopted in dolce-core is compatible with trope theory without committing to the view that change corresponds to trope substitution.these qualities also coincide at t. In other terms, entities coincident at t must have qualities that are indistinguishable at t. Axiom (A27) says that the sum of qualities of the same kind that inhere in two objects inheres in the sum of the objects (provided these sums exist).The location relation (L) provides the link between qualities and spaces. First, we require regions (and in particular spaces) not to change over the time they exist (A28). Expression L(x, y, t) is used to state "at time t, region x is the location of the individual quality y" as enforced (at least in part) by axioms (A30) and (A31). 17 Each individual quality in q i must be located at least in one of the associated spaces s ij (axioms (A34) and (A35)). The location in a single space is unique (A36) and a quality that has a location in a space needs to have some location in that space during its whole life (A37). (A38) says that two qualities coincident at t are also indistinguishable with respect to their locations. Together with (A25) and (A26), this axiom formalizes the substitutivity of temporary properties represented by qualities: two entities that coincide at t are indistinguishable at t with respect to their qualities.Axioms (A32) and (A33) characterize the fact that the location of an individual quality at t is the mereological sum of all the locations the quality has during t, i.e., at all the sub-times of t. Note that if a is the region corresponding to a property value of 1kg and b corresponds to a property value of 2kg, then the sum of a and b is the region including just the two mentioned and is distinguished from the region corresponding to the property value of 3kg. The sum of locations must not be confused with the 'sum' of property values since, in general, the latter strictly depends on the space structure while the first does not. 17 In dolce this relation is called quale and written ql. In dolce there is also a distinction between the immediate quale (a non temporary relation) and the temporary quale. dolce-core uses one temporary relation only since the temporal qualities of an event e at t correspond to the temporal qualities of the maximal part of e that spans t.(L-substitutivity)Objects and Events
dolce-core characterizes the distinction between objects and events following the discussion in Sect. 2.4. In this approach events are primarily in time while objects are primarily in space (in the case of physical objects) or in other dimensions. Since by (A10) qualities, concepts, and regions are in time as well, their participation to events (like their creation or destruction) is plausible. One can investigate this position further and note that q, c and r can be considered as specializations (subcategories) of o. However, to ensure generality, we made the assumption that qualities, concepts, and regions form categories disjoint from the category of objects.The dolce-core unified framework relies on the participation relation (PC) to relate the temporal qualities of events and the atemporal qualities of objects. Participation is taken to be a time regular relation defined between objects and events: PC(x, y, t) stands for "the object x participates in the event y at t". Axioms (A40) and (A41) capture the mutual existential dependence between events and objects. Axioms (A42) and (A43) make explicit the fact that participation relies on unity criteria neither for objects nor for events Regarding the property of 'being primarily in time', we introduce the quality kind 'being time-located'. (A45) makes explicit the temporal nature of the parameter t in the location relation. (A46) guarantees that the events have a time-quality. These axioms, together with (A10) and the axioms on inheritance and location guarantee that, for events, 'being in time' reduces to having a time-quality located in t. In addition, together with (A41) and (A44) they show that objects are in time because of their participation in events.Note that if we define the spatial location of events via the location of their participants, and the life of an object as the minimal event in which it (maximally) participates, we obtain that an object spatio-temporally coincides with its life. The distinction between participation and temporary parthood ensures that these two entities, although spatio-temporally coincident, are not identified.Conclusions
In writing this introductory paper, we had three major goals: (1) to distinguish foundational studies from the rest of the ontology research, (2) to introduce topics and methodology typical of foundational ontology and (3) to show a concrete example of how these theoretical arguments can be used to build a foundational ontology. Unfortunately, in literature there is no good reference that presents this research area at length and any attempt to introduce these topics in the limited space of a paper are deemed to be unsatisfactory on several aspects. At least, we hope that the paper gives the average reader the opportunity to appreciate the goals of this area of research as well as the subtle interactions between philosophy, logic and representational issues. Finally, we are glad of the opportunity to present the dolce-core system of Sect. 3 which is the first step, after the release of the dolce ontology in 2002, toward a new version of this ontological system.Introduction
The domain of software is a primary candidate for being formalized in an ontology. On the one hand, the domain is sufficiently complex with different paradigms (e.g., object orientation) and different aspects (e.g., security, legal information, interface descriptions, etc.). On the other hand, the domain is sufficiently stable, i.e., new paradigms and aspects occur rather seldom. Capturing this stable core in a reference ontology for software can be fruitful in order to prevent modeling from scratch. For example, the approaches described in the Chapter "Ontologies and Software Engineering" introduce individual formalizations of at least one paradigm or aspect although they share basic principles.In this chapter, we present such a reference ontology for software, called the Core Software Ontology, which formalizes common concepts in the software engineering realm, such as data, software with its different shades of meaning, classes, methods, etc. As we cannot possibly formalize a complete and comprehensive view of software, the Core Software Ontology is designed for extensibility in different directions. In order to demonstrate the extensibility, the chapter presents three examples of how to extend the core ontology with the notions of libraries, policies, and software components.The reference nature of such an ontology makes it important to clarify the intended meanings of its concepts and associations. Otherwise, users often have a hard time untangling the intended meanings. The prevailing type of ontologies, namely ones which are lightweight and quite often reduced to simple taxonomies, are not eligible for this purpose because they exhibit the following shortcomings (as identified in Conceptual Ambiguity: We will consider an ontology to be conceptually ambiguous if it is difficult for users to understand the intended meaning S. Staab and R. Studer (eds.), Handbook on Ontologies, International Handbooks of concepts, the associations between the concepts, and the relationships between concepts and modeled entities. Poor Axiomatization: Even when an ontology is easy to understand by many or most of its users, it may have only a poor axiomatization. Such a poor axiomatization will lead to an unsatisfying restriction of possible logical models (cf. Chapter "What is an Ontology?"). Loose Design: An ontology is afflicted with loose design, if it contains modeling artifacts. Modeling artifacts are concepts and associations which do not bear ontological meaning. Narrow Scope: An ontology exhibits narrow scope when it is unclear how a distinction could be made between the objects and events within an information system (regarding data and the manipulation of data) and the real-world objects and events external to such a system. As an example consider the distinction between a user account and its corresponding natural person(s).In order to remedy the shortcomings, we build the Core Software Ontology on a foundational ontology (cf. Chapter "Foundational Choices in DOLCE") and apply content ontology design patterns (cf. Chapter "Ontology Design Patterns"). We demonstrate that the formalization of the software domain can greatly benefit from the use of the DOLCE foundational ontology and the content ontology design patterns extracted from Descriptions & Situations, the Ontology of Plans, as well as the Ontology of Information Objects.The chapter is structured as follows: we start by presenting the origin and motivation of our ontology in Sect. 2 in order to understand which aspects have been taken into account and why. Subsequently, we sketch the formalization of the Core Software Ontology in Sect. 3 and some of its extensions in Sect. 4. For the complete formalization we refer the reader to Background
An initial ontology for software certainly cannot cover every single paradigm and aspect related to software. As an example, we limit ourselves to the concept of object orientation. In order to understand which aspects have been taken into account and why, we present here the origin and motivation of our ontology, viz., the work presented in The use cases relevant for developing and managing software components in application servers are: libraries and their dependencies, conflicting licenses of libraries, capability descriptions, component classification and discovery, semantics of parameters, support in error handling, reasoning with transactional settings and reasoning with security settings.The use cases relevant for developing and managing Web services are: analyzing message contexts, selecting service functionality, detecting loops in the interorganisational workflow, incompatible inputs and outputs, relating communication parameters, monitoring of changes, aggregating service information and quality of service.Altogether, the use cases let us derive a set of modeling requirements for deciding which aspects our ontology should formalize. The modeling requirements are: (i) libraries, licenses, component profiles, component taxonomies, API descriptions, semantic API descriptions, access rights and workflow information of software components and (ii) service profiles, service taxonomies, policies, workflow information, API descriptions, as well as semantic API descriptions of Web services.We do not claim that the modeling requirements are exhaustive. However, they allow us to constrain the initial modeling horizon. As demonstrated in the following, the ontology is designed in an extensible way such that further modeling requirements can be met easily.Formalization of the Software Domain
Our contribution starts in this section with the Core Software Ontology (CSO) which introduces fundamental concepts of the software domain such as software itself, data, classes, or methods. The purpose of the ontology is to provide a reference by specifying the intended meanings of software terms as precisely as possible, and to prevent the shortcomings mentioned in the introduction.The contribution continues in Sect. 4 where we extend the Core Software Ontology in different directions, e.g., in the direction of software components, resulting in a Core Ontology of Software Components. All of the ontologies have been presented in detail in Figure Software vs. Data
We start our discussion of the Core Software Ontology with a detailed discussion of software and data. In order to clarify both concepts, which are heavily inflicted by polysemy, it is necessary to identify and formalize the entities of the computational domain. The computational domain has a reality of its own, consisting of data manipulated by programs that implement algorithms. The programs that manipulate the data are usually referred to as software. Upon close inspection, it seems that the term software is overloaded and refers to at least three different concepts:1. The encoding of an algorithm specification in some kind of representation.Encoding can be either in mind, on paper, or any other form. The Quicksort algorithm can be represented as Java or pseudo code, for instance. This is SoftwareAsCode (which we abbreviate to Software) and is a kind of OIO:InformationObject. 1 2. The realization of the code in a concrete hardware. These realizations are the DOLCE:PhysicalEndurants that are stored on hard disc or residing in memory. Henceforth, we call them ComputationalObjects (a special kind of OIO:InformationRealization). This could be the appearance of the 1 Throughout the chapter, concepts and associations are written in sans serif and are labelled in a namespace-like manner. Namespace-prefixes indicate the ontology where concepts and associations are defined. If no namespace is given, concepts and associations are assumed to be defined in the ontology currently discussed. With respect to the formulae given in the following, the reader might refer to Chapters "Description Logics, Ontologies in F-Logic, Resource Description Framework (RDF), Web Ontology Language: OWL, Ontologies and Rules" for the logic background.Quicksort algorithm in main memory that can be interpreted and executed by the CPU. Hence, the difference between 1 and 2 is that 2 is physically present in some hardware. 3. The running system, which is the result of an execution of a Computational-Object. This is the form of software which manifests itself in a sequence of activities in the computational domain, e.g., the increment of a variable, the comparison of data, the storage of data on the hard disc, etc. This form of software is a DOLCE:Perdurant which we call ComputationalActivity.ComputationalObjects (item 2) are a specialization of OIO:Information-Realization (any entity that realizes an OIO:InformationObject) as introduced in the Ontology of Information Objects. ComputationalActivities (item 3) are a specialization of OoP:Activity as introduced in the Ontology of Plans. ComputationalObjects and ComputationalActivities are the entities that live in the computational domain.ComputationalObjects are characterized by the fact that they are necessarily dependent on Hardware which is a DOLCE:PhysicalObject. A suitable dependence association is axiomatized in DOLCE and is called specifically-ConstantlyDependsOn. A ComputationalObject is considered here as a spatiotemporally bounded entity, therefore it exists for the time a memory cell is realizing a certain Software, for instance. Copies of ComputationalObjects in the same or another Hardware are different, although related by some kind of "copy" association. For example, in the case of mobile agents, where people refer to a mobile agent as a piece of software that can move from machine to machine executing the "same" process, it is useful to make agents distinct because the "same" agent can perform differently from machine to machine. The similarity has to be caught via a specialized association, such as copy (which we do not define here) rather than via logical identity.The execution of a ComputationalObject leads to ComputationalActivities. ComputationalActivities require at least one ComputationalObject as a participant. The definitions below formalize the described properties. We consider the data that is manipulated by the programs as Comput-ationalObjects as well. This reflects the fact that the appearances in the main memory or on the hard disc can be interpreted as instructions for the CPU (i.e., as software) or can be treated as data from the viewpoint of another program. For example, the operating system manipulates application software (loading and unloading it into memory, etc.) much like application software manipulates application data.Hence, Data can also be considered as a special kind of OIO:Information-Object. The difference to Software is that Data does not OIO:express an OoP:Plan. (T1) Software(x) → Data(x)Interfaces, Classes, and Methods
Building on the fundamental notions of software and data introduced in the previous section, we now formalize the most important concepts of object orientation. We begin with a Class in Definition (D10) as a special kind of Software that encapsulates an arbitrary number of Data and an arbitrary number of Methods. Vice versa, a Method is defined as being a part of a Class, having input and output parameters and throwing exceptions. Invocations Between Software
We start with two associations, viz., executes and accesses, to formalize invocations between Software. Below, (D15) introduces executes as "shortcut" between Software, such as Class or Method, and a ComputationalTask. For example, the doGet() method of a servlet executes an invocation task.(D16) introduces accesses as "shortcut" between the ComputationalTask and the Software or Data that is being called or modified by the task. The sequence of executes and accesses can be further abbreviated by invokes which is declared as being transitive (cf. (D17) and (A7)). Axioms (A8) and (A9) are introduced for convenience. Regarding (A8), we say that also a Class executes a ComputationalTask when one of its Methods executes this task. Regarding (A9), we state that invokes also holds when we have succeeding tasks. Inputs and Outputs
Besides invocations, we also need to model the Inputs and Outputs of tasks. The Ontology of Plans does not provide such capabilities. Inputs and Outputs are required when we want to represent the information of a WS-BPEL workflow, for instance. Inputs and Outputs are DnS:Roles which are both DnS:playedBy Data and DnS:definedBy an OoP:Plan (cf. (D18), (D19) and (A12)). The relationships between Inputs (Outputs) and ComputationalTasks are modeled by inputFor / outputFor, as specified in (A10), and (A11). 4 Extensions to the Core Software OntologyIn this section, we continue our contribution of formalizing the software domain by extending the Core Software Ontology in three different directions. First, we start with the minor extension of libraries and licenses which is put in the Core Software Ontology itself. Second, our focus are access rights and policies which were originally put in the Core Software Ontology as well. However, Libraries and Licenses
We introduce the concepts of SoftwareLibrary and License in (D20) and (D21) below. Both occur in many programming languages and are a common means in software engineering. A SoftwareLibrary consists of a number of CSO:Classes and is classified as CSO:Data because it cannot be executed as a whole. The concept License is a special kind of LegalContract as introduced in the Core Legal Ontology In a second step, we specialize the descriptive entities of Descriptions & Situations, viz., DnS:Roles, DnS:Courses, DnS:Parameters, and DnS:Situation-Descriptions as follows. First, we introduce two DnS:Roles to represent the subject and the object of a policy in (D24) and (D25). PolicySubjects are DnS:AgentiveRoles and can be DnS:playedBy Users or UserGroups. PolicyObjects are DnS:NonAgentiveRoles and can be DnS:playedBy Data. Second, we need to represent the predicate of a policy by a special kind of DnS:Course. (D6) already introduces ComputationalTask which meets this requirement. We further aggregate such tasks to TaskCollections in (D26). The intuition behind TaskCollections are the security "roles" in operating systems or database systems. This means that a TaskCollection groups ComputationalTasks, such as read, write, or execute. Third, we introduce Constraints as special kinds of DnS:Parameter. The ComputationalTask or TaskCollections can be constrained in some way, e.g., a Web service policy might state that an invocation is only possible with Kerberos or X509 authentication (cf. (D27)). Finally, we construct a PolicyDescription, viz., a special kind of DnS:SituationDescription, from the aforementioned concepts. It is worthwhile to spend some words on the DnS:attitudeTowards association between DnS:Roles and DnS:Courses. The DnS:attitudeTowards association is a special kind of DnS:modalTarget and can be considered the descriptive counterpart of the DOLCE:participantIn association. It is used to state attitudes, attention, or even subjection that an object can have with respect to an action or process. In our case, DnS:attitudeTowards is used to state the relationship between PolicySubjects, as well as PolicyObjects (A24) and (A25) infer the closure of all resulting rights considering User-Groups and TaskCollections. A PolicySubject is granted rights on all tasks which are members of the TaskCollection. Similarly, a User is granted all access rights which are granted for his UserGroup.Core Ontology of Software Components
Software componentry is a loosely defined term for a software technology proposing that software should be developed by glueing prefabricated components together as in the field of electronics or mechanics. Software componentry also proposes encapsulating software functionality for multiple use in a context-independent way, composable with other components, and as a unit of independent deployment and versioning.Software components often take the form of object-oriented classes conforming to a framework specification. However, software components differ from classes. The basic idea in object-oriented programming is that software should be written according to a mental model of the actual or imagined objects it represents. Software componentry, by contrast, makes no such assumptions.The framework specifications prescribe (1) interfaces that must be implemented by components and (2) protocols that define how components interact with each other. Examples of framework specifications are Enterprise JavaBeans (EJB) and the Component Object Model (COM) from Microsoft.The definitions below formalize this intuition of software component as closely as possible. Assuming the object-oriented paradigm, (D31) below states that a SoftwareComponent is a special kind of CSO:Class that conforms to a FrameworkSpecification. According to the definition above, a Framework-Specification is (1) a DOLCE:Collection of CSO:Interfaces and (2) a special kind of OoP:Plan which specifies the interaction of components (cf. (D29)). Conformance means that at least one CSO:Interface prescribed by the Framework-Specification has to be implemented by the SoftwareComponent (cf. (D30)). The Core Ontology of Software Components also introduces component profiles that group relevant information of a software component such as its interfaces, policy descriptions, or plans. We expect that such an aggregation makes browsing and querying for developers more convenient. The component profile is envisioned to act as the central information source for a specific software component rather than having bits and pieces all over the place.(D32) and (A26) define a Profile as follows: First, it aggregates CSO:Policy-Descriptions, an OoP:Plan, the required SoftwareLibraries, the implemented Interfaces and additional Characteristics of a specific Software entity. Second, the link to the described Software is specified via the describes association. (D33) specializes this definition to ComponentProfile.Often, we need to express certain capabilities or features of components, such as the version, transactional or security settings. For this purpose, we introduce Characteristics on a Profile in (D34). It is expected that Component-Profiles are specialized and put into a taxonomy. For example, we might define a DatabaseConnectorProfile as a ComponentProfile that provides for specific Characteristics describing whether the underlying database supports transactions or SQL-99. A taxonomic structure further accommodates the developer in browsing and querying for ComponentProfiles in his system.Finally, (A27) specifies the profiles association as a "catch-all" for DnS:defines, DnS:unifies, OIO:about, as well as OIO:expressedBy. This is done for convenience in order to relieve the developer from modeling details, who will certainly have to deal with such information. Proof of Concept
In this section, we give some examples of how the Core Software Ontology and its extensions circumvent the four shortcomings, viz., conceptual ambiguity, poor axiomatization, loose design, and narrow scope, mentioned in the introduction. We argue that the use of the DOLCE foundational ontology as well as the use of content ontology design patterns help us here.Conceptual Disambiguation
As mentioned in the introduction, lightweight ontologies typically suffer from conceptual ambiguity. A prominent example is the notion of OWL-S:Service in In contrast to such commonly built ontologies we have captured the intended meanings of concepts and associations as precisely as possible. For this purpose, it proved to be rather helpful to capture the three different flavors of the term software via the information object content ontology design pattern, for instance.While our definitions of the terms "software" and "software component" may not be the only ones, the fact that they are highly axiomatized allows comparing them to alternative definitions and allows fostering discussions on alternative conceptualizations.Increased Axiomatization
Ontologies are often reduced to a simple taxonomy with domain and range restrictions on associations. This does not suffice to clarify the intended meaning of terms which is of central importance when building ontologies for reuse and reference purpose. As an example, consider control constructs, such as, fork or join, to specify workflow information. Many ontologies, such as OWL-S In our ontology we have made use of the Ontology of Plans which provides extensive axiomatization of OoP:Tasks and subconcepts thereof. OoP:Tasks are directly comparable to control constructs, but provide a heavyweight axiomatization. An example is SynchroTask (an instance of OoP:ControlTask) which matches the concept of a "join." A SynchroTask joins a set of tasks after a branching and waits for the execution of all tasks (except the optional ones) that are direct successors to a ConcurrencyTask or AnyOrderTask. Below we give the axiomatization of the SynchroTask as introduced in Another example is the link between the control constructs and the process steps. Very often, the intended meaning of such links remains unclear. Is it a parthood association? And if yes, is it temporary, transitive, etc.? Our ontology is very specific with respect to such notions because it builds on the Ontology of Plans. The latter exploits the DOLCE:temporaryComponent association which has a firm foundation as a special kind of the more basic DOLCE:component mereological association and DOLCE:partlyCompresent temporally indexing association. Both are characterized by formal restrictions on their application to other basic concepts.Improved Design
In our ontology, we propose to use contextualization as a design pattern. Contextualization allows us to move from software descriptions to the representation of different, possibly conflicting views with various granularity. The Descriptions & Situations ontology provides us with a corresponding content ontology design pattern with the basic primitives of context modeling such as the notion of roles, for instance. Roles allow us to talk about inputs and outputs on the abstract level, i.e., independent of the objects that play such roles.This pattern applies clearly defined semantics and scoping provided by Descriptions & Situations where we want to express that the output of a process is the input to another process. In our ontology, inputs and outputs can be modeled as DnS:Roles which serve as variables. Thus, CSO:Data can play multiple roles within the same or different descriptions. It is natural to express that the given CSO:Data is output with respect to one process, but input to another (cf. Fig. DnS:Situation Description
Wider Scope
Software resides on the boundary of the world inside an information system and the external world. Web services, in particular, may carry out operations to support a real-world service. Functionality, which is an essential property of a service, then arises from the entire process that comprises computational as well as real-world activities.The distinction between information objects, events, and physical objects is not explicitly made in most ontologies. In our ontology, this separation naturally follows from the use of DOLCE and the Ontology of Information Objects, where the distinction is an important part of the characterization of concepts. In particular, it becomes possible to be more precise about the kinds of relationships that can occur among objects or between objects and events.For example, we can distinguish a physical object (such as a natural person) from an information object (such as user account in an information system) and represent the link between the two. Figure OIO:InformationObject
CSO:ComputationalObject
Related Work
As already outlined in the introduction, the Chapter "Ontologies and Software Engineering" surveys a wealth of approaches that formalize at least one paradigm or aspect of software in an ontology. Although the respective ontologies share basic principles, they (1) rely on individual formalizations, and (2) are typically lightweight and not of a reference nature. Such approaches could benefit from our Core Software Ontology capturing the stable core and preventing modeling from scratch. The same proposition is valid for approaches like Furthermore, there has been some work that overlaps with the ideas presented here. For example, the COHSE Java ontologyAn example of a higher level software component ontology in use is provided by Finally, there are some ontologies which focus on specific aspects, whereas our ontology tries to relate the different aspects in a larger focus. Examples are the Core Plan Representation (CPR) Conclusion
The chapter has shown how to proceed in building a Core Software Ontology and extending it in different directions. The result is grounded in a foundational ontology and avoids the typical shortcomings of lightweight ontologies. Related, seminal approaches only weakly formalize the meaning of their terms and leave their disambiguation to the intuition of the reader, a situation that we here improve upon considerably.The reader may note that what is presented here are the reference ontologies in this domain. For actual work, these reference ontologies need to be reduced to knowledge representation schemes that are more amenable to operation.So far, the Core Ontology of Software Components has been applied in Introduction
Multimedia objects are ubiquitous, whether found via web search (e.g. Google 2 or Yahoo! 3 images), or via dedicated sites (e.g. Flickr 4 or YouTube 5 ) or in the repositories of private users or commercial organizations (film archives, broadcasters, photo agencies, etc.). The media objects are produced and consumed by professionals and amateurs alike. Unlike textual assets, whose content can be searched for using text strings, media search is dependent on processes that have either cumbersome requirements for feature comparison (e.g. color or texture) or rely on associated, more easily processable descriptions, selecting aspects of an image or video and expressing them as text, or as concepts from a predefined vocabulary. Individual annotation and tagging applications have not yet achieved a degree of interoperability that enables effective sharing of semantic metadata and that links the metadata to semantic data and ontologies found on the Semantic Web. MPEG-7 In order to avoid such problems, we advocate the use of Semantic Web languages and a core ontology for multimedia annotations, which is built based on rich ontological foundations provided by an ontology such as DOLCE (cf. Chapter "Foundational Choices in DOLCE") and sound ontology engineering principles. The result presented in this chapter is COMM, a core ontology for multimedia.In the next section, we illustrate the main problems when using MPEG-7 for describing multimedia resources on the web. In Sect. 3, we review existing multimedia ontologies and show why previous proposals are inadequate for semantic multimedia annotation. Subsequently, we define the requirements that a multimedia ontology should meet (Sect. 4) before we present COMM, an MPEG-7 based ontology, and discuss our design decisions based on our requirements (Sect. 5). In Sect. 6, we demonstrate the use of the ontology with the scenario from Sect. 2 and then conclude.Annotating Multimedia Assets on the Web
Let us imagine that Nathalie, a student in history, wants to create a multimedia presentation of the major international conferences and summits held in the last 60 years. Her starting point is the famous "Big Three" picture, taken at the Yalta (Crimea) Conference, showing the heads of government of the United States, the United Kingdom, and the Soviet Union during World War II. Nathalie uses an MPEG-7 compliant authoring tool for detecting and labeling relevant multimedia objects automatically. On the web, she finds three different face recognition web services which provide very good results for detecting Winston Churchill, Franklin D. Roosevelt and Josef Stalin, respectively. Having these tools, she would like to run the face recognition web services on images and import the extraction results into the authoring tool in order to automatically generate links from the detected face regions to detailed textual information about Churchill, Roosevelt and Stalin (image in Fig. Nathalie would then like to describe a recent video from a G8 summit, such as the retrospective A history of G8 violence made by Reuters. Fragment identification. Particular regions of the image need to be localized (anchor value in Semantic annotation. MPEG-7 is a natural candidate for representing the extraction results of multimedia analysis software such as a face recognition web service. The language, standardized in 2001, specifies a rich vocabulary of multimedia descriptors, which can be represented in either XML or a binary format. While it is possible to specify very detailed annotations using these descriptors, it is not possible to guarantee that MPEG-7 metadata generated by different agents will be mutually understood due to the lack of formal semantics of this language Web interoperability. Nathalie would like to link the multimedia presentation to historical information about the key figures of the Yalta Conference or the various G8 summits that is already available on the web. She has also found semantic metadata about the relationships between these figures that could improve the automatic generation of the multimedia presentation. However, she realizes that MPEG-7 cannot be combined with these concepts defined in domain-specific ontologies because of its closing to the web. As this example demonstrates, although MPEG-7 provides ways of associating semantics with (parts of) non-textual media assets, it is incompatible with (semantic) web technologies and has no formal description of the semantics encapsulated implicitly in the standard.Embedding into compound documents. Finally, Nathalie needs to compile the semantic annotations of the images, videos and textual stories into a semantically annotated compound document. However, the current state of the art does not provide a framework which allows the semantic annotation of compound documents. MPEG-7 solves only partially the problem as it is restricted to the description of audiovisual compound documents. Bearing the growing number of multimedia office documents in mind, this limitation is a serious drawback.Related Work
In the field of semantic image understanding, using a multimedia ontology infrastructure is regarded to be the first step for closing the, so-called, semantic gap between low-level signal processing results and explicit semantic descriptions of the concepts depicted in images. Furthermore, multimedia ontologies have the potential to increase the interoperability of applications producing and consuming multimedia annotations. The application of multimedia reasoning techniques on top of semantic multimedia annotations is also a research topic which is currently investigated Hunter These ontologies have been recently compared with COMM according to three criteria: (1) the way the multimedia ontology is linked with domain semantics, (2) the MPEG-7 coverage of the multimedia ontology, and (3) the scalability and modeling rationale of the conceptualization Finally, general models for annotations of non-multimedia content have been proposed by librarians. The Functional Requirements for Bibliographic Records (FRBR) 10 model specifies the conventions for bibliographic description of traditional books. The CIDOC Conceptual Reference Model (CRM) 11  defines the formal structure for describing the concepts and relationships used in cultural heritage documentation (cf. Chapter "Using the PSL Ontology"). Hunter has described how an MPEG-7 ontology could specialize CIDOC-CRM for describing multimedia objects in museums Requirements for Designing a Multimedia Ontology
Requirements for designing a multimedia ontology have been gathered and reported in the literature, e.g. in MPEG-7 compliance. MPEG-7 is an existing international standard, used both in the signal processing and the broadcasting communities. It contains a wealth of accumulated experience that needs to be included in a webbased ontology. In addition, existing annotations in MPEG-7 should be easily expressible in our ontology.Semantic interoperability. Annotations are only re-usable when the captured semantics can be shared among multiple systems and applications. Obtaining similar results from reasoning processes about terms in different environments can only be guaranteed if the semantics is sufficiently explicitly described. A multimedia ontology has to ensure that the intended meaning of the captured semantics can be shared among different systems.Syntactic interoperability. Systems are only able to share the semantics of annotations if there is a means of conveying this in some agreed-upon syntax. Given that the (semantic) web is an important repository of both media assets and annotations, a semantic description of the multimedia ontology should be expressible in a web language (e.g. OWL, RDF/XML or RDFa).Separation of concerns. Clear separation of subject matter (i.e. knowledge about depicted entities, such as the person Winston Churchill) from knowledge that is related to the administrative management or the structure and the features of multimedia documents (e.g. Churchill's face is to the left of Roosevelt's face) is required. Reusability of multimedia annotations can only be achieved if the connection between both ontologies is clearly specified by the multimedia ontology.Modularity. A complete multimedia ontology can be, as demonstrated by MPEG-7, very large. The design of a multimedia ontology should thus be 10 http://www.ifla.org/VII/s13/frbr/index.htm 11 http://cidoc.ics.forth.gr/ made modular, to minimize the execution overhead when used for multimedia annotation. Modularity is also a good engineering principle.Extensibility. While we intend to construct a comprehensive multimedia ontology, as ontology development methodologies demonstrate, this can never be complete. New concepts will always need to be added to the ontology. This requires a design that can always be extended, without changing the underlying model and assumptions and without affecting legacy annotations.Adding Formal Semantics to MPEG-7
MPEG-7 specifies the connection between semantic annotations and parts of media assets. We take it as a base of knowledge that needs to be expressible in our ontology. Therefore, we re-engineer MPEG-7 according to the intended semantics of the written standard. We satisfy our semantic interoperability not by aligning our ontology to the XML Schema definition of MPEG-7, but by providing a formal semantics for MPEG-7. We use a methodology based on a foundational, or top level, ontology as a basis for designing COMM (cf. Chapter "Ontology Engineering Methodology"). This provides a domain independent vocabulary that explicitly includes formal definitions of foundational categories, such as processes or physical objects, and eases the linkage of domain-specific ontologies because of the shared definitions of top level concepts. We briefly introduce our chosen foundational ontology in Sect. 5.1, and then present our multimedia ontology, COMM, in Sects. 5.2 and 5.3. Finally, we discuss why our ontology satisfies all our stated requirements in Sect. 5.4.COMM is available at http://multimedia.semanticweb.org/COMM/.DOLCE as Modeling Basis
Using the review in Multimedia Patterns
The patterns for D&S and OIO need to be extended for representing MPEG-7 concepts since they are not sufficiently specialized to the domain of multimedia annotation. This section introduces these extended multimedia design patterns, while Sect. 5.3 details two central concepts underlying these patterns: digital data and algorithms (cf. Chapter "Ontology Design Patterns"). In order to define design patterns, one has to identify repetitive structures and describe them at an abstract level. The two most important functionalities provided by MPEG-7 are: the decomposition of media assets and the (semantic) annotation of their parts, which we include in our multimedia ontology.Decomposition. MPEG-7 provides descriptors for spatial, temporal, spatiotemporal and media source decompositions of multimedia content into segments. A segment is the most general abstract concept in MPEG-7 and can refer to a region of an image, a piece of text, a temporal scene of a video or even to a moving object tracked during a period of time.Annotation. MPEG-7 defines a very large collection of descriptors that can be used to annotate a segment. These descriptors can be low-level visual features, audio features or more abstract concepts. They allow the annotation of the content of multimedia documents or the media asset itself.In the following, we first introduce the notion of multimedia data and then present the patterns that formalize the decomposition of multimedia content into segments, or allow the annotation of these segments. The decomposition pattern handles the structure of a multimedia document, while the media annotation pattern, the content annotation pattern and the semantic annotation pattern are useful for annotating the media, the features and the semantic content of the multimedia document respectively.Multimedia Data
This encapsulates the MPEG-7 notion of multimedia content and is a subconcept of digital-data Decomposition Pattern
Following the D&S pattern, we consider that a decomposition of a multimediadata entity is a situation 13 (a segment-decomposition) that satisfies a description, such as a segmentation-algorithm or a method (e.g. a user drawing a bounding box around a depicted face), which has been applied to perform the decomposition, see Fig. The decomposition pattern also reflects the need for localizing segments within the input segment of a decomposition as each output-segment-role requires a mask-role. Such a role has to be played by one or more digital-data entities which express one localization-descriptor. An example of such a descriptor is an ontological representation of the MPEG-7RegionLocatorType 14 for localizing regions in an image (see Fig. The specialization of the pattern for describing image decompositions is shown in Fig. Analogously, the pattern can be used to describe the decomposition of a video asset or of an ODF document (see Fig. Content Annotation Pattern.
This formalizes the attachment of metadata (i.e. annotations) to multimediadata (Fig. The actual metadata that is carried by a digital-data entity depends on the structured-data-description that is expressed by it. These descriptions are formalized using the digital data pattern (see Sect. 5.3). Applying the content annotation pattern for formalizing a specific annotation, e.g. a dominant-color-annotation which corresponds to the connection of a MPEG-7DominantColorType with a segment, requires only the specialization of the concept annotation, e.g. dominant-color-annotation. This concept is defined by being a setting for a digital-data entity that expresses one dominant-color-descriptor (a subconcept of structured-data-description which corresponds to the DominantColorType).Media Annotation Pattern
This forms the basis for describing the physical instances of multimedia content (Fig. One can thus represent that some visual content (e.g. the picture of a digital camera) is realized by a JPEG image with a size of 462848 bytes, using the MPEG-7MediaFormatType. Using the media annotation pattern, the metadata is attached by connecting a digital-data entity with the image. The digital-data plays an annotation-role while the image plays an annotated-media-role. An ontological representation of the MediaFormatType, namely an instance of the structured-data-description subconcept media-format-descriptor, is expressed by the digital-data entity. The tuple formed with the scalar "462848" and the string "JPEG" is the value of the two instances of the concepts file-size and file-format respectively. Both concepts are subconcepts of structured-data-parameter.Semantic Annotation Pattern
Even though MPEG-7 provides some general concepts (see An OWL Thing or a DOLCE particular (belonging to a domain-specific ontology) that is depicted by some multimedia content is not directly connected to it but rather through the way the annotation is obtained. Actually, a manual annotation method or its subconcept algorithm, such as a classification algorithm, has to be applied to determine this connection. It is embodied through a semantic-annotation that satisfies the applied method. This description specifies that the annotated multimedia-data has to play an annotated-data-role and the depicted Thing/particular has to play a semantic-label-role. The pattern also allows the integration of features which might be evaluated in the context of a classification algorithm. In that case, digital-data entities that represent these features would play an input-role.Basic Patterns
Specializing the D&S and OIO patterns for defining multimedia design patterns is enabled through the definition of basic design patterns, which formalize the notion of digital data and algorithm.Digital Data Pattern
Within the domain of multimedia annotation, the notion of digital data is central -both the multimedia content being annotated and the annotations themselves are expressed as digital data. We consider digital-data entities of arbitrary size to be information-objects, which are used for communication between machines. The OIO design pattern states that descriptions are expressed by information-objects, which have to be about facts (represented by particulars). These facts are settings for situations that have to satisfy the descriptions that are expressed by information-objects. This chain of constraints allows the modeling of complex data structures to store digital information. Our approach is as follows (see Fig. The digital data pattern can be used to formalize complex MPEG-7 low-level descriptors. Figure The MPEG-7 code example given in Fig. Algorithm Pattern
The production of multimedia annotation can involve the execution of algorithms or the application of computer assisted methods which are used to produce or manipulate digital-data. The recognition of a face in an image region is an example of the former, while manual annotation of the characters is an example of the latter.We consider algorithms to be methods that are applied to solve a computational problem (see Fig. Comparison with Requirements
We discuss now whether the requirements stated in Sect. 4 are satisfied with our proposed modeling of the multimedia ontology.The ontology is MPEG-7 compliant since the patterns have been designed with the aim of translating the standard into DOLCE. It covers the most important part of MPEG-7 that is commonly used for describing the structure and the content of multimedia documents. Our current investigation shows that parts of MPEG-7 that have not yet been considered (e.g. navigation & access) can be formalized analogously to the other descriptors through the definition of further patterns. The technical realization of the basic MPEG-7 data types (e.g. matrices and vectors) is not within the scope of the multimedia ontology. They are represented as ontological concepts, because the about relationship which connects digital-data with numerical entities is only defined between concepts. Thus, the definition of OWL data type properties is required to connect instances of data type concepts (subconcepts of the DOLCE abstract-region) with the actual numeric information (e.g. xsd:string). Currently, simple string representation formats are used for serializing data type concepts (e.g. rectangle) that are currently not covered by W3C standards. Future work includes the integration of the extended data types of OWL 1.1.Syntactic and semantic interoperability of our multimedia ontology is achieved by an OWL DL formalization. A clear separation of concerns is ensured through the use of the multimedia patterns: the decomposition pattern for handling the structure and the annotation pattern for dealing with the metadata.These patterns form the core of the modular architecture of the multimedia ontology. We follow the various MPEG-7 parts and organize the multimedia ontology into modules which cover (1) the descriptors related to a specific media type (e.g. visual, audio or text) and ( Through the use of multimedia design patterns, our ontology is also extensible, allowing the inclusion of further media types and descriptors (e.g. new low-level features) using the same patterns. As our patterns are grounded in the D&S pattern, it is straightforward to include further contextual knowledge (e.g. about provenance) by adding roles or parameters. Such extensions will not change the patterns, so that legacy annotations will remain valid.Expressing the Scenario in COMM
The interoperability problem with which Nathalie was faced in Sect. 2 can be solved by employing the COMM ontology for representing the metadata of all relevant multimedia objects and the presentation itself throughout the whole creation workflow. The student is shielded from details of the multimedia ontology by embedding it in authoring tools and feature analysis web services.The application of the Winston Churchill face recognizer results in an annotation RDF graph that is depicted in the upper part of Fig. Running the two remaining face recognizers for Roosevelt and Stalin will extend the decomposition further by two still regions, i.e. the image-data instances id2 and id3 as well as the corresponding still-region-roles, spatialmask-roles and digital-data instances expressing two more region-locatordescriptors (indicated at the right border of Fig. Decomposition of ODF documents is formalized analogously to image segmentation (see Fig. A plugin of this program could produce COMM metadata of the document in the background while it is produced by the user. The media independent design patterns of COMM allow the implementation of a generic mechanism for inserting metadata of arbitrary media assets into already existing metadata of an ODF document. In the case of Fig. Figure The two segments td and id1 are located within md by two digital-data instances (dd2 and dd3) which express two corresponding odf-locator-descriptor instances. The complete instantiations of the two odf-locator-descriptors are not shown in Fig. In order to ease the creation of multimedia annotations with our ontology, we have developed a Java API Conclusion and Future Work
We have presented COMM, an MPEG-7 based multimedia ontology, well-founded and composed of multimedia patterns. It satisfies the requirements, as they are described by the multimedia community itself, for a multimedia ontology framework. The ontology is completely formalized in OWL DL and a stable version is available with its API at: http://multimedia.semanticweb.org/COMM/. It has been used in projects such as K-Space and X-Media.The ontology already covers the main parts of the standard, and we are confident that the remaining parts can be covered by following our method for extracting more design patterns. Our modeling approach confirms that the ontology offers even more possibilities for multimedia annotation than MPEG-7 since it is interoperable with existing web ontologies. The explicit representation of algorithms in the multimedia patterns describes the multimedia analysis steps, something that is not possible in MPEG-7. The need for providing this kind of annotation is demonstrated in the algorithm use case of the W3C Multimedia Semantics Incubator Group. Introduction
Representing activities and the constraints on their occurrences is an integral aspect of commonsense reasoning, particularly in manufacturing, enterprise modelling, and autonomous agents or robots. In addition to the traditional concerns of knowledge representation and reasoning, the need to integrate software applications in these areas has become increasingly important. However, interoperability is hindered because the applications use different terminology and representations of the domain. These problems arise most acutely for systems that must manage the heterogeneity inherent in various domains and integrate models of different domains into coherent frameworks. For example, such integration occurs in business process reengineering, where enterprise models integrate processes, organizations, goals and customers. Even when applications use the same terminology, they often associate different semantics with the terms. This clash over the meaning of the terms prevents the seamless exchange of information among the applications. translators between every pair of applications that must cooperate. What is needed is some way of explicitly specifying the terminology of the applications in an unambiguous fashion.The Process Specification Language (PSL) ( How are Ontologies Used?
Applications of ontologies focus on their role as sharable and reusable representations of knowledge (Chapters "What is an Ontology?", "Ontology-Based Recommender Systems". Semantic heterogeneity is particularly acute problem for tasks that require correct and meaningful communication and integration among software systems, since different systems may ascribe disparate meanings to the same terms or use distinct terms to convey the same meaning. Ontologies support semantic integration through a shared understanding of the intended semantics of the terminology used by the software systems.The reusability of an ontology is determined relative to the genericity of its axiomatization. In one sense, the axioms of the ontology can be instantiated within different domains; this leads to the notion of domain theories that capture the knowledge for particular problems. In another sense, the axioms of the ontology capture those properties of the world that are valid across multiple domains; new ontologies can then be constructed as more domainspecific extensions of the generic ontologies.Specifying Domain Theories
Within the context of a process ontology , domain theories take the form of descriptions of processes as repeatable patterns of behaviour. The various forms of process representations are ubiquitous in industry: there is a plethora of business and engineering software applications -workflow, scheduling, discrete event simulation, process planning, business process modeling, and othersthat are designed explicitly for the construction of process models of various sorts A process ontology provides the underlying semantics for the process terminology that is common to the many disparate domains and software applications. This allows us to evaluate the consistency of process descriptions. In this way, ontologies can be used to support automated reasoning (such as theorem proving and constraint satisfaction) with the axioms of the ontology and domain theories alone.Ontologies also provide guidance in the specification of domain theories. For example, each class of activities in the PSL Ontology is associated with a specific class of sentences that are the correct process descriptions for that class. The primary focus of this chapter will be a survey of the various classes of activities in the ontology together with examples of the corresponding process descriptions.Semantic Integration
A semantics-preserving exchange of information between two software applications requires mappings between logically equivalent concepts in the ontology of each application. The challenge of semantic integration is therefore equivalent to the problem of generating such mappings (Chapter "Why Is Ontology Mapping Difficult?"), determining that they are correct, and providing a vehicle for executing the mappings, thus translating terms from one ontology into another.The Twenty Questions approach ([3]) describes a technique for the semiautomatic generation of semantic mappings from application ontologies to the PSL Ontology, which can then be used to automatically derive direct mappings between application ontologies.The work in Building New Ontologies
An ontology with a consistent and complete axiomatization of its intended semantics can be used as a semantic foundation for either building a new ontology or for augmenting an ontology that has an incomplete axiomatization (Chapter "Foundational Choices in DOLCE"). For example, the process model of the semantic web services ontology OWL-S (Chapter "Semantic Web Services", The Semantic Web Services Ontology (SWSO) ( Basic Ontological Distinctions
The PSL Ontology consists of a set of first-order logic theories within which there is a distinction between core theories and definitional extensions. 2 Core theories introduce new primitive concepts, while all terms introduced in a definitional extension that are conservatively defined using the terminology of the core theories.All core theories within the ontology are consistent extensions of PSL-Core (T psl core ), although not all extensions need be mutually consistent. Table Activity and Activity Occurrence
In general, business and engineering processes are described at the type levela process specification characterizes a certain general pattern that might admit of many instantiations which might differ considerably from one another. For example, the specification of the manufacturing process for making a car will describe different sequences of tasks for building the components of the car and may even describe alternative ways of producing subassemblies. A robust foundation for process modelling, therefore, should be able to characterize both the general process pattern described by a specification as well as the class of possible instantiations of that pattern. Moreover, such a foundation must be able to clearly represent the constraints that a process specification places on something's counting as an instantiations of the process, that is, the constraints on process execution.Within the PSL Ontology, an activity is a repeatable pattern of behaviour, while an activity occurrence corresponds to a concrete instantiation of this pattern. The relationship between activities and activity occurrences is represented by the occurrence of (o, a) relation. Activities may have multiple occurrences, or there may exist activities which never occur. Any activity occurrence corresponds to a unique activity.In contrast to many object-oriented approaches, activity occurrences are not considered to be instances of activities, since activities are not classes within the PSL Ontology. One can of course specify classes of activities in a process description. For example the term pickup(x, y) can denote the class of activities for picking up some object x with manipulator y, and the term move(x, y, z) can denote the class of activities for moving object x from location y to location z. Ground terms such as pickup(Block1, LeftHand) and 2 The complete set of axioms for the Time
Underlying the intuition that activity occurrences are the instantiations of activities is the notion that each activity occurrence is associated with unique timepoints that mark the begin and end of the occurrence. The PSL Ontology introduces two functions beginof and endof for this purpose.The set of timepoints is linearly ordered, forwards into the future, and backwards into the past. Within the PSL Ontology, the extension of the bef ore relation captures this linear ordering. There are also different ontological commitments about time that are not made within the PSL Ontology, such as the denseness of the timeline; any such commitments must be axiomatized within a theory that extends the PSL Ontology.There are some approaches (e.g. The core theory T duration for duration adds a metric to the timeline by mapping every pair of timepoints to a new sort called timeduration that satisfies the axioms of algebraic fields. Of course, the duration of an activity occurrence is of most interest, and is equal to the duration between the endof and beginof timepoints of the activity occurrence.Objects
Many debates have erupted within philosophy over the distinction between objects that are continuants (that is, they exist whole and entire at different times) and objects that are occurrents (that is, they have different parts existing at different times). Composition
A ubiquitous feature of process formalisms is the ability to compose simpler activities to form new complex activities (or conversely, to decompose any complex activity into a set of subactivities). The PSL Ontology incorporates this idea while making several distinctions between different kinds of composition that arise from the relationship between composition of activities and composition of activity occurrences.Subactivities
The PSL Ontology uses the subactivity relation to capture the basic intuitions for the composition of activities. The core theory T subactivity axiomatizes this relation as a discrete partial ordering (such as Fig. T subactivity alone does not specify any relationship between the occurrence of an activity and occurrences of its subactivities. For example, we can compose the primitive activities press and punch in Fig. Concurrency
Concurrency involves more than the fact that two activities occur at the same time, since concurrent activities may have different preconditions and effects than any of the activities if they occur alone. In particular, the activities may have interfering preconditions, so that even if two activities can possibly occur separately, they cannot occur concurrently (e.g. the oven cannot be used to bake a cake and a turkey at the same time) or the effects of two activities may clobber each other, so that the effects of the concurrent activity are different than the effects of the two activities if they occur separately ([8]); for example, the effect of lifting only the right side or only the left side of a table has the effect that the table is touching the floor. Lifting both the right and left sides concurrently has the effect of lifting the entire table. This observation leads to a notion of atomic activity which corresponds to some set of primitive activities. Concurrency is represented by the occurrence of concurrent activities rather than concurrent activity occurrences. The core theory T atomic axiomatizes the conc function that specifies the aggregation of sets of primitive activities into concurrent activities.Subactivity Occurrences
The core theory T actocc axiomatizes the subactivity occurrence relation, which is the composition relation over activity occurrences corresponding to the composition relation over activities. Occurrences of atomic activities are the minimal elements in this composition ordering -they do not have any nontrivial subactivity occurrences.Following the intuition that activity occurrences are occurrents rather than continuants, one can consider the subactivity occurrence to be a temporal part of the complex activity occurrence. The axioms of T actocc guarantee that any subactivity occurrence is "during" an occurrence of the complex activity.State and Change
Many applications of process ontologies are used to represent dynamic behaviour in the world so that software systems may make predictions about the future and explanations about the past. In particular, these predictions and explanations are often concerned with the state of the world and how that state changes. The PSL core theory T disc state is intended to capture the basic intuitions about state and its relationship to activities.Properties in the domain that can change are called fluents . Similar to the representation of activities, fluents can also be denoted by terms within the language. For example, in stock(Gadget1, Cambridge) denotes the fluent that represents the property that the object Gadget1 is available in stock at the Cambridge warehouse.Intuitively, a change in state is captured by the set of fluents that are either achieved or falsified by an activity occurrence. The prior(f, o) relation specifies that a fluent f is intuitively true prior to an activity occurrence o and the holds(f, o) relation specifies that a fluent f is intuitively true after an activity occurrence o. For example, before a delivery, Gadget1 is not in the Cambridge warehouse, but after delivery occurs, it is in stock: occurrence of (o, delivery(Gadget1, Cambridge)) ⊃ ¬prior(in stock(Gadget1, Cambridge), o) ∧holds(in stock(Gadget1, Cambridge), o)A fluent is changed by the occurrence of activities, and a fluent can only be changed by the occurrence of activities. Thus, if some fluent holds after an activity occurrence, but after an activity occurrence later along the branch it is false, then an activity must occur at some point between that changes the fluent. This also leads to the requirement that the fluent holding after an activity occurrence will be the same fluent holding prior to any immediately succeeding occurrence, since there cannot be an activity occurring between the two by definition.State does not change during the occurrence of an atomic activity. Consequently, the PSL Ontology cannot represent phenomena in which some feature of the world is changing as some continuous function of time (hence the name "Discrete State" for the extension). If state changes during an activity occurrence, then it must be an occurrence of a complex activity.Process Descriptions for Atomic Activities
Within the taxonomy of the PSL Ontology, activities are classified according to the kinds of constraints that their occurrences satisfy. A process description for an activity in some class imposes constraints on activity occurrences corresponding to the definition of the class. Classes of atomic activities are defined with respect to constraints that arise from the following two questions:• Under what conditions does an atomic activity occur? • How do occurrences of atomic activities change fluents?A detailed exposition of these constraints requires a closer look at the model theory of the core theory T occtree , in particular, the notion of occurrence trees.Occurrence Trees
An occurrence tree is a partially ordered set of atomic activity occurrences, such that for a given set of activities, all discrete sequences of their occurrences are branches of the tree. It is important to note that an occurrence tree contains all occurrences of all atomic activities; it is not simply the set of occurrences of a particular (possibly complex) activity. Because the tree is discrete, each activity occurrence in the tree has a unique successor occurrence of each activity.Although occurrence trees characterize all sequences of activity occurrences, not all of these sequences will intuitively be physically possible within the domain. This leads to the notion of the legal occurrence tree, which is the subtree of the occurrence tree that consists only of possible sequences of activity occurrences; The legal(o) relation specifies that the atomic activity occurrence o is an element of the legal occurrence tree.Constraints on Legal Occurrence
The process descriptions for atomic activities constrain the legal occurrence tree. The general form of such a process description is:where Φ(o) is a formula that specifies the constraint on the legal activity occurrence. Within the PSL Ontology, different classes of atomic activities correspond to different classes of formulae that are used to instantiate Φ(o) in the general process description. In particular, we consider cases in which the preconditions are based on state, time, or the occurrence of other activities.State-Based Preconditions
The most prevalent kind of precondition are markovian preconditions, in which the possibility of occurrence depends only on the state that holds prior to an activity occurrence, e.g.Mixing is not performed unless the moulding machine is clean.
In this case, the cleanliness of the machine is the state, and the occurrence of the mixing activity depends on whether or not this state holds:Note that for this particular class of activities, the consequent of the sentence is a formula that contains only prior literals.Time-Based Preconditions
In more general scenarios, there may be temporal preconditions that depend only on the time at which the activity is to occur, such as The pre-heating operation can only be performed on Tuesday or Thursday. which is axiomatized asThe consequent of this process description is a formula that contains only beginof literals.Occurrence Constraints
The possibility of an activity occurrence may depend on the occurrence of other activities. Consider the example: If we do not fold the metal after fabrication, we need to reheat it which is axiomatized asIn this case, an occurrence of the reheating activity will depend on the condition that there is no earlier legal occurrence of the folding activity.Time-Based Occurrence Constraints
Preconditions may also take the form of periodic occurrences, e.g. Drill bits are replaced every 10 min.In this example, occurrences of the replacement activity depend not only on the occurrence of an earlier replacement activity but also on the time at which that activity occurred.Effects
Effects characterize the ways in which activity occurrences change the state of the world. Such effects may be context-free, so that all occurrences of the activity change the same states, or they may be constrained by other conditions. The general form of such a process description is:where Φ(o) is a formula that specifies the constraint on the effects of the activity occurrence.State-Based Effects
The most common constraint is state-based effects that depend on some context: If the object is fragile, then it will break when dropped; if the object is elastic, then it will bounce when dropped.(∀o, x) occurrence of (o, drop(x)) ∧ prior(fragile(x), o)
⊃ holds(broken(x), o) (Time-Based Effects
Although process descriptions for the effects of atomic activities are most often specifying state-based effects, other kinds of constraints also arise in practice, such as time-based effects:If the rental car is returned after the due date, then the cost includes a late fee which is axiomatized byThe effects of the activity occurrence depend only on timepoints -the time at which the activity occurrence ends and the timepoint that is the due date of the rental.Occurrence-Based Effects
In some cases, the effects depend not only on when the activity occurs, but also on the timepoints at which other activity occurrences begin or end. For example, If we remove the coffee pot before the brewing activity completes, then the burner will be wet is axiomatized by and in this case, the formula in the process description contains multiple variables denoting different activity occurrences, as well as bef ore literals.Duration-Based Effects
For some classes of atomic activities, the effects are dependent on the duration of the activity occurrences. For example, The time on the clock display will change after holding the button for 3 s is axiomatized byThe effects do not depend on the time at which the activity occurs, so that the formula does not contain any bef ore literals.Process Descriptions for Complex Activities
Classes of complex activities are defined with respect to the following two questions:• What is the relationship between the occurrence of the complex activity and occurrences of its subactivities? • Under what conditions does a complex activity occur?An activity may have subactivities that do not occur; the only constraint is that any subactivity occurrence must correspond to a subtree of the activity tree that characterizes the occurrence of the activity.Activity Trees
The basic structure that characterizes occurrences of complex activities is the activity tree, which is a subtree of the legal occurrence tree that consists of all possible sequences of atomic subactivity occurrences beginning from a root subactivity occurrence. Each branch of an activity tree corresponds to a possible sequence of occurrences of subactivities of the complex activity.In a sense, an activity tree is a microcosm of the occurrence tree, in which we consider all of the ways in which the world unfolds in the context of an occurrence of the complex activity. For example, consider the occurrence tree in Fig. Three relations in particular are used in process descriptions for complex activities. The root(o, a) relation specifies that the atomic subactivity occurrence o is the root of the activity tree. The min precedes relation is the ordering relation over the atomic subactivity occurrences in the activity tree. In Fig. The axioms of T actocc guarantees that there is a one-to-one correspondence between branches of activity trees and complex activity occurrences. The axioms for subactivity occurrence relation guarantee that the branches of the activity trees for a subactivity are contained in the branches of the activity tree for the complex activity. In Fig. Branch Structure
Different subactivities may occur on different branches of the activity treedifferent occurrences of an activity may have different subactivity occurrences or different orderings on the same subactivity occurrences.In this sense, branches of the activity tree characterize the nondeterminism that arises from different ordering constraints or iteration. For example, the surf acing activity is intuitively nondeterministic; the activity trees for surf acing contain two branches, one branch consisting of an occurrence of polish and one branch consisting of an occurrence of paint.Complex activities can be classified with respect to symmetries of its activity trees. Concretely, these are axiomatized by relationships between the different branches of an activity tree. We will now take a closer look at the process descriptions for activities in these classes.Permuted Activities
For permuted activities, each branch of the activity tree is a different permutation of the same set of subactivity occurrences. For example, the informal process descriptionMaking the frame consists of cutting, punching, and pressing. can be formally written as (∀o, x) occurrence of (o, make frame(x))If we consider the activity trees that satisfy this sentence (Fig. The activity tree in Fig. Ordering Constraints
One of the most common intuitions about processes is the notion of process flow, or the specification of some ordering over the subactivities of an activity, such asMaking the car chassis involves making the body and making the frame in parallel, followed by final assembly.which is axiomatized by the process description (∀o, o 1 , o 2 , o 3 , x, y) occurrence of (o, make chassis(x, y)) ∧occurrence of (o 1 , make body(y)) ∧ occurrence of (o 2 , make frame(x))∧occurrence of (o 3 , final assembly(x, y))⊃ min precedes(o 1 , o 3 , make chassis(x, y)) ∧min precedes(o 2 , o 3 , make chassis(x, y)) Iteration
Iteration is captured by the class of repetitive activities, in which the activity tree can be decomposed into copies of some subtree (which intuitively corresponds to the activity tree of the subactivity that is being iterated). Nondeterministic iteration, such as Occurrences of painting consist of multiple occurrences of coating is axiomatized by a process description of the form (∀o 1 ) occurrence of (o 1 , painting) ⊃This process description says that for every occurrence of coating in an activity tree for painting, either there exists a next occurrence of coating or the leaf subactivity occurrence of the occurrence of coating is also the leaf occurrence of the occurrence of painting.Complex activities in which the number of iterations depends on achieving some state (analogous to while loops) is a property of a set of activity trees, as we shall see in the next section.Spectrum and Variation
A complex activity will in general have multiple activity trees within an occurrence tree, and not all activity trees for an activity need be isomorphic to each other. This property leads to the notion of the spectrum of an activity, which is the set of equivalence classes of isomorphic activity trees. While the former classes of activities compared branches within the same activity tree, we can also define classes with respect to the spectrum of the activity.The notion of variation within the PSL Ontology characterizes the conditions under which activity trees for a complex activity are isomorphic to each other. Different activity trees for the same activity can have different subactivity occurrences, or the activity trees may differ on the ordering of the subactivity occurrences.For conditional activities, the fluents that hold prior to the activity occurrence determine which subactivities occur, as in the constraint Within the painting activity, if the surface of the product is rough, then sand the product:which is written asAlternatively, the ordering over subactivity occurrences of an activity may depend on state, as in the constraint If the machine is not ready, then perform the painting before final assembly which can be written as (∀o, o 1 , o 2 , x, y) occurrence of (o, assembly(x, y)) ∧occurrence of (o 1 , paint(x)) ∧ occurrence of (o 2 , final(x))∧¬prior(ready(y), root occ(o)) ⊃ min precedes(root occ(o 1 ), root occ(o 2 ), assembly(x)) Distribution
The preceding two sections have presented some of the classes in the ontology that are defined with respect to the relationship between occurrences of complex activities and occurrences of their atomic subactivities. We now turn to the classes of complex activities that arise from constraints under which complex activities themselves occur.There may be branches of a subtree of the occurrence tree that are isomorphic to branches of an activity tree, yet they do not correspond to occurrences of the activity. For example, in Fig. The general form for process descriptions related to distribution is:(∀s) Φ(s) ⊃ (∃o) occurrence of (o, a) ∧ s = root occ(o) State determines when an activity must occur, so that the process description is written as (∀s, x) prior(order quantity(x, 3), s) ⊃ (∃o) occurrence of (o, deliver(x)) ∧ s = root occ(o) In either case, models of the process description specify the distribution of activity trees within the occurrence tree.Embedding Constraints
The PSL Ontology does not force the existence of complex activities; there may be subtrees of the occurrence tree that contain occurrences of subactivities, yet not be activity trees. We can exploit this property to represent the existence of activity attempts, intended effects, and temporal constraints; subtrees that do not satisfy the desired constraints will simply not correspond to activity trees for the activity.External Activity Occurrences
For a given complex activity, there may be external activities (that is, activities that are not subactivities) whose occurrence either interfere with the complex activity or which are necessary for the activity to occur. Examples of such necessary activities include either activities performed by external agents (such as a courier delivery or pickup) or it may be an activity such as setup. In the constraint To produce the chassis, first drill the series of 1 cm holes, followed by drilling the series of 2 cm holes, the activity that changes the drill bit fixture is not a subactivity of the process plan for producing the chassis, but is a setup activity that must occur between drilling the two sets of holes.Interruptability
Closely related to external activity occurrences is the notion of interruptability and activity attempts. With an interruptable activity, an external activity may occur without interfering with the original activity. For example, interruptable activities may be preempted or suspended:The assembly of computers for one customer can be halted to work on a rush order for another customerwhile noninterruptable activities may not: Pouring of metal from the furnace cannot be stopped once initiated.(∀s 1 , s 2 ) root(s 1 , pour metal) ∧ leaf (s 2 , pour metal) ∧min precedes(s 1 , s 2 , pour metal) ⊃ ¬(∃s 3 ) occurrence of (s 3 , stop) ∧ earlier(s 1 , s 3 ) ∧ earlier(s 3 , s 2 ) (21)In this latter example, if for some reason the metal pouring does stop, then we would intuitively consider this to be an activity attempt, rather than an occurrence of the activity.Intended Effects
There are many circumstances in which we want to make a distinction between the intended effects of an activity and the actual effects of the activity. For example, the manufacturing process plan for making some product in a steel company is defined with respect to the properties specified by customer and quality requirements (such as grade, surface properties, width, and thickness), but due to external nondeterministic factors, not every occurrence of the process will provide products that satisfy these requirements. Quality problems arise from this divergence of actual effects from intended effects.For example, informal process descriptions such as Bake the soup until it is opaque or Heat the solution until reaches 50 C can be formalized by sentences of the form (∀s) leaf (s, a) ⊃ holds(f, s) (In both of these examples, it is possible to terminate the activity occurrence before the intended state is achieved, but in the context of the intended effects, the activity occurrence will terminate only when the state is achieved.Temporal Constraints
With temporal constraints, subactivities are not allowed to occur at arbitrary times during occurrences of the activity. Examples of such constraints include schedules, which specify the possible times at which the subactivities may occur:The part will arrive 10 days after placing the order request (∀o, s 1 , s 2 ) min precedes(s 1 , s 2 , a) ∧ occurrence of (s 1 , a 1 ) ∧ occurrence of (s 2 , a 2 )⊃ duration(endof (s 2 ), endof (s 1 )) = 10 (23)In this example, the possible occurrences of the activity are restricted to those whose subactivities satisfy the temporal constraints.Summary
Within the increasingly complex environments of enterprise integration, electronic commerce, and the Semantic Web, where process models are maintained in different software applications, standards for the exchange of this information must address not only the syntax but also the semantics of process concepts. PSL draws upon well-known mathematical tools and techniques to provide a robust semantic foundation for the representation of process information. This foundation includes first-order theories for concepts together with complete characterizations of the soundness and completeness of these theories. In this chapter, we have seen how the PSL Ontology can be used to specify process descriptions for a broad range of problems and provide the semantic foundations for new ontologies.Introduction
This chapter provides an overview of how the use of ontologies may enhance biomedical research by providing a basis for a formalized, and shareable descriptions, of models of biological systems.A wide variety of artifacts are labeled as "ontologies" in the Biomedical domain, leading to much debate and confusion. The most widely used ontological artifact are controlled vocabularies (CVs). A CV provides a list of terms whose meanings are specifically defined. Terms from a CV are usually used for indexing records in a database. The Gene Ontology (GO) is the most widely used CV in databases serving biomedical researchers. The GO provides term for declaring the molecular function (MF), biological process (BP) and cellular component (CC) of gene products. The statements comprising these MF, BP and CC declaration are called annotations The second most prevalent kind of artifact is an information model (or data model). An information model provides an organizing structure to information pertaining to a domain of interest, such as microarray 1 data, and describes how different parts of the information at hand, such as the experimental condition and sample description, relate to each other. In biomedical research, Microarray Gene Expression Object Model (MAGE-OM) is an example of 1 An automated technique for simultaneously analyzing thousands of different DNA sequences or proteins affixed to a thumbnail-sized "chip" of glass or silicon. DNA microarrays can be used to monitor changes in the expression levels of genes in response to changes in environmental conditions or in healthy vs. diseased cells. Protein arrays can be used to study protein expression, protein-protein interactions, and interactions between proteins and other molecules. Fromwww.niaaa.nih.gov/publications/arh26-3/165-171.htm a widely known information model. MAGE-OM, along with the controlled terms that are used to populate the information model is referred to as the Microarray Gene Expression Data (MGED) Ontology. The MGED Ontology is used to describe the minimum information about a microarray experiment that is essential to make sense of the numbers comprising the microarray data. The third kind of artifact is an ontology in its true sense, which is increasingly being used for knowledge representation in Biomedicine. In this interpretation, an ontology is a specification of entities (or concepts) and relationships among them in a domain of discourse; along with declarations of the properties of each relationship, and, in some cases, a set of explicit axioms defined for those relations and entities. In biomedical research, several ontologies are striving towards this goal. The foremost is the Foundational Model of Anatomy (FMA), which is a computer-based knowledge source for anatomy and represents classes and relationships necessary for the symbolic modeling of the structure of the human body in a form that is understandable to humans and is also navigable, parseable, and interpretable by machine-based systems Uses of Ontologies in Biomedical Research
With the advent of high-throughput technologies,Currently, a significant amount of time and energy is spent in merely locating and retrieving information rather than thinking about what that information means. For example, a researcher trying to understand how the proteins participating in the cell cycle interact with each other, has to read several reviews to determine the list of proteins S/he should track, search databases such as Uniprot to retrieve annotations for the relevant proteins, follow the citations evidencing the annotations to determine the experiment/s that were performed on each protein and in some cases retrieve the actual data sets from special databases. All this information comes in different formats and from different sources. It is extremely difficult to manually search through the various sources and integrate this diverse information about biological systems to formulate hypotheses (or "Models") spanning a large number genes and proteins Until recently, the predominant use of ontologies in biomedicine has been to facilitate interoperability among databases by indexing them with standard terms to address the problem of locating and retrieving information. Even if the problem of locating information were solved, it is still difficult to formulate formal hypotheses and models comprising a large number genes and proteins Besides using ontologies for enhancing interoperability among databases and enabling data exchange, researchers have also used ontologies to create knowledge bases that store large amounts of knowledge in a structured manner The emerging trend in the use of ontologies in biomedical research is that, at the outset, ontology terms are used to name things, gradually proceeding toward naming connections between things -first to create information models and then progressing towards the creation of a formal representationIn this chapter we focus on the latter use. Chapter "Ontology-Based Recommender Systems" discusses the current applications of bio-ontologies that are focused around the theme of database interoperability and data integration. Bodenreider and Stevens Constructing Hypotheses and Models of Biological Systems
The discovery process in biomedical research is cyclical; Scientists examine existing data to formulate models that explain the data, design experiments to test the hypotheses and develop new hypotheses that incorporate the data generated during experimentation. Currently, in order to advance this cycle, the experimentalist must perform several tasks: (1) gather information of many different types about the biological entities that participate in a biological process (2) formulate hypotheses (or models) about the relationships among these entities, (3) examine the different data to evaluate the degree to which his/her hypothesis is supported and (4) refine the hypotheses to achieve the best possible match with the data. In todays data-rich environment, this is a very difficult, time-consuming and tedious task. For example, even to evaluate a simple hypothesis such as "protein A is a transcriptional activator of genes X, Y and Z", the experimentalist must examine the literature for evidence showing that protein A is a transcription factor or exhibits protein sequence homology with known transcriptional factors. S/he must look for evidence indicating DNA binding activity for protein A and if found, examine the promoters of X, Y and Z for presence of binding sequences for protein A. Moreover, each of the preceding steps incorporate a set of implicit assumptions such as sequence homology implying similarity of function.Finally, the refined hypotheses are subjected to experimental testing. Hypotheses that survive these tests -validated hypotheses -are published in scientific publications and represent the growing knowledge about biological entities, processes and relationships among them. Validated hypotheses are eventually synthesized into systems of relationships called "models" that account for the known behavior of the system and provide the grounds for further experimentation. Biologists' models are generally presented as diagrams showing the type, direction and strength of relationships among biological entities such as genes and proteins. Figure Creating a Formal Representation for Hypotheses and Models
If we accept the notion of an hypothesis (or a model) as the basis for an organizing framework for the data and information sources we wish to integrate and interpret, we immediately encounter several problems. As we have discussed, for a large number of participating entities such genes, proteins, clinical observations and laboratory data, it is extremely difficult to integrate current knowledge about the relationships within system under study to formulate hypotheses or models. The difficulty arises primarily because there is no shared formalism -akin to engineering drawings -in which to express such hypotheses or models and the interpretation they convey. Therefore, it is difficult to determine whether such hypotheses are internally consistent or are consistent with data, to refine inconsistent hypotheses and to understand the implications of complicated hypotheses It is widely recognized that one key challenge in managing this data overload is to represent the results of high-throughput experiments as well as clinical observations and patient records in a formal representation -a computer-interpretable standardized form that can be the basis for unambiguous descriptions of hypotheses and models This raises the following question: What are the desirable properties of a formal representation for hypotheses or models? Peleg et al. 1. A formal representation should be able to present structural, functional and dynamic views of a biological process. The structural and functional views show the entities that participate in a process and relationships among them. The dynamic view shows the process over time, shows branch points and conditional sub-processes. 2. A formal representation should include an associated ontology that unambiguously identifies the entities and relationships in a process. 3. It should be able to represent biological processes at various scales and should allow hierarchical representation of sub processes to manage complexity. 4. The representation should be able to incorporate new data as they become available and should be extensible to allow new categories of information as they come in to existence. 5. The representation should have a corresponding conceptual (mathematical) framework that allows verification of system properties using simulation and/or logical inference mechanisms. 6. The representation should have an intuitive visual layout.If we can devise a formal representation for hypotheses and the data at hand as well as the mechanisms to check the consistency of hypotheses with that data and prior knowledge, we can significantly streamline the task of interpreting diverse data. Moreover, if we develop such a formal representation, we can develop tools that can operate upon current data sets, information and existing knowledge to integrate them in an environment that supports the formulation and testing of hypotheses Developing such a formal representation is not a trivial task. Moreover it is unreasonable to expect one representation that will satisfy the needs of all users. However, the need for creating such representation for specific domain (or sub domain) of study has been proposed multiple times in Biomedicine Challenges for Developing a Formal Representation
Knowledge Representation
The first challenge is the systematic representation of the various kinds of biological entities that participate in any given disease process and the many qualitatively different kinds of relationships among them. This requires the development of an ontology for unambiguously representing biological entities and interactions among them. Specifically, an ontology allows us to represent domain-specific entities along with their definitions, a set of relationships among them, properties of each relationship, and, in some cases, a set of explicit axioms defined for those relations and entities. We require different ontologies to represent biological processes at different levels of granularity because biological processes and the relevant data can be considered at varying levels of detail, ranging from molecular mechanisms to general processes such as cell division and from raw data matrixes to qualitative relationships Ontologies have gained a lot of popularity in molecular biology over the last several years. The earliest ontologies describe properties of 'objects' such as genes, gene products and small molecules. The later ontologies describe the 'processes' that gene, proteins and small molecules participate in.Currently, there are several ontologies that allow representation of processes in a biological system by specifying relationships between biological entities for tasks ranging from modeling biological systems to extracting information from literature. At the simplest level, gene ontology BP annotations describe the processes a particular gene product might contribute to, which can be viewed as a minimal model of the biological process that does not contain any declaration of the specific relationships among its participants. At the other end is the Systems biology markup language, SBML, which can represent quantitative models of biochemical processes and pathways There are multiple ontologies to represent biological processes, models and hypotheses at varying degrees of granularity between the two extremes of a GO BP annotation and a SBML representation. For example, EcoCyc's ontology, which is used to represent information about metabolic pathways for E. coli In Sect. 3 we will discuss how ontologies enable the creation of a formal representation by enabling the knowledge representation task.Conceptual Representation
The second problem is to represent the biological system conceptually. The conceptual framework for a biological system enables a user to reason about a biological system and perform thought experiments. Thought experiments serve two functions: prediction and verification. Prediction allows scientists to make future claims based on models of a system. Verification provides guidance and feedback about the accuracy of the models through comparison, manipulation, and evaluation against available data and knowledge. Prediction and verification enable scientists to ask 'what if' questions about a system, form explanations, as well as make and test predictions. Currently, one of the major limiting factor is the conceptual representation of the "mathematics" of biological systems A conceptual framework for representing biological systems must accommodate the modularity and temporal behavior of biological systems, as well as handle their non-linearity and redundancy. The conceptual frameworks used to represent biological models vary from ordinary differential equations to Boolean The inability to represent disparate kinds of information, at different levels of detail, about biological systems in a common conceptual framework is a major limitation in the creation of formal representations of biological systems, and current efforts usually focus on a limited categories of information A promising approach is to represent the biological processes in a system as a sequence of 'events' that link particular 'states' of the system An event-based framework offers several other advantages as well: (1) It can explicitly represent states which allows for representing information such as commitment to a developmental pathway It is unlikely that any conceptual framework will be adequate to represent all biological systems and it is much more productive to represent the data, information and knowledge at hand in an explicit ontology and then map the various entity-types and relationships in the ontology to a particular conceptual frameworks needed under specific situations. For example, Rubin et al have represented anatomic knowledge about the heart and the circulatory system in an ontology and then mapped the ontology to differential equation models for blood pressure as well as to a reasoning service to predict the effect of penetrating injuries to the heart Knowledge and Data Acquisition
The final challenge is the gathering, storage, and encoding of existing information. Even if all of the above challenges are met, getting access to the information and converting (or encoding) it into the relevant ontology in an automated manner is a major challenge because the information resides in separate repositories, each with custom storage formats and diverse access methods. Moreover, most databases do not store information in an explicit ontology and groups that design ontologies capable of representing models of biological systems All the challenges described above are strongly inter-related as shown in the Fig. Fig. 2. Components of a Formal Representation:
A formal representation is a computer-interpretable standardized form that can be the basis for unambiguous descriptions of hypotheses and models in a domain of discourse. Knowledge representation comprises the methods and processes of systematically representing the various biological entities and the different kinds of relationships between them. The conceptual framework for a biological system enables a user to reason about a biological system and perform thought experiments. The adoption and success of a formal representation depends critically on the ability to gather and encode existing information and knowledgeOntologies Enable the Creation of a Formal Representation
As discussed till now, in order to create a unified formal representation, we need a conceptual framework that can represent models of biological systems. The conceptual framework should represent the temporal dynamics of the process and should not require a complete model rewrite on minor changes. The conceptual framework should provide systematic methods to evaluate, update, extend and revise models represented in that framework.It is obvious that no conceptual framework will be adequate under all circumstances. We are better off representing the data, information and knowledge at hand in an explicit ontology and then mapping the various entity-types and relationships in the ontology to a particular conceptual framework needed under specific situations The challenge then is to bridge the conceptual framework and the ontology to create the formal representation. For example, the relationship 'protein A activates protein B' is different from 'protein A activates gene X' though it may be described by the same words. When representing a biological process in a conceptual framework, it is essential to distinguish between the two meanings. An ontology can distinguish between the two relationships by providing different terms to represent the two meanings as well as by clearly specifying the two meanings. Having an associated ontology where each term in the ontology has a corresponding construct in the conceptual framework allows this distinction to be made in the conceptual model as well. Because an ontology unambiguously declares the entities and relationship among those entities, it can guide the design of the knowledgebases that store the various experimental and clinical data as well as prior domain knowledge in a manner that different conceptual frameworks can be overlayed on the primary data.Particularly, the use ontologies will help in maintaining a strict distinction between data and an interpretation based on the data. For example, the current diagnosis criteria for Multiple Sclerosis (MS) are based on observing at least two clinical episodes with certain symptoms at least 3 months apart and the presence of two plaques (on the spinal cord) on MRI. If we design our database of clinical records to store the diagnostic code for MS, then, if the diagnostic criteria for that diagnostic code change, we have to re-examine every record, re-diagnose MS and re-associate the correct codes with each record. However, if we define the interpretation about the existence of MS separately from the data structure used to store observational data, then we can change our criteria for MS and still reliably identify all cases of MS. Such separation of the definition of a biomedical concept from the decision (such as recommending a treatment) and computation (such as searching for correlations with environmental factors) has been demonstrated to increase both maintainability and efficiency of computer reasoning We believe that a particular conceptual framework along with the associated ontology is the optimal way to create a formal representation fit for a specific situation. For example, differential equations along with the Systems Biology Markup Language (SBML) create an appropriate formal representation for biochemical signaling pathways. Ontologies will play a central role in enabling such modularity and maintaining a separation between data, information and knowledge and the relevant conceptual (mathematical) framework.The formal representation resulting from such separation will be easily extensible to incorporate new data types as they become available as well as to incorporate novel conceptual frameworks as required. The hypotheses, models and underlying data can become compatible with each other in the context of the relevant conceptual framework, making it possible to bring together the implications of many kinds of data and information in a unified manner Unresolved Issues
Although, the use of ontologies in the creation of formal representations has a very strong case in its favor, there are several challenging issues that need to be addressed, which we discuss below:Abstraction Levels
Biomedical researchers study biological systems at various scales, ranging from electron microscopy images to patient populations. No ontology can span all these and multiple ontologies already exist for different abstraction levels. We need to create a mechanism by which ontologies at different abstraction levels can be effectively mapped to each other.Unambiguous Relationships
Within a particular abstraction level of representing a biological system, relationships need to be explicitly defined so that their interpretation is not subjective. The relationship ontology (RO) Consistency Across Abstraction Levels
Biomedical researchers cross multiple abstraction levels when describing biological systems. It is essential that relationships between entities at a particular abstraction levels can be consistently interpreted when we move to a different abstraction level (For example how does the mechanism of action of a drug at the protein level affect the efficacy of the drug in treating a patient).Bindings with Conceptual Frameworks
We have suggested a separation between the ontology used to structure knowledge about a biological system and the conceptual framework used to model the system mathematically. However, currently the process of establishing a correspondence between constructs in an ontology and constructs in a conceptual framework is quite ad hoc. Usually the ontology is designed with one conceptual framework in mind (e.g. SBML Role of the Semantic Web
The Semantic Web is an evolving extension of the World Wide Web in which web content can be expressed in a form that can be understood, interpreted and used by software agents (besides humans), thus permitting software agents to find, share and integrate information more easily. 7  Given the heterogeneity of biological data both in form and location, the Semantic Web is of considerable interest to the life sciences community; particularly because key issues such as the need for consistent data and knowledge representation can be addressed using the Resource Description Framework (RDF) and Web Ontology Language (OWL) The expectation from the Semantic Web in life sciences is that relationships that exist implicitly in the minds of scientists will be explicitly declared (using OWL ontologies) and then used to aggregate genomic, proteomic, cellular, physiological, and chemical data. Semantic definitions will specify which objects are related to others and how. Such linking will enable semantic tools However, not everyone is convinced that the Semantic Web will have such a revolutionizing effect on life sciences. There are implicit assumptions in the expected role of the Semantic Web, mainly that: (1) a simple syntax and the semantic of description logics will be sufficient (2) translation of existing information into the simple syntax as well as inferences on the simple semantics will work right Currently there is immense excitement about the Semantic Web and its possible contribution to advancing biomedical research; it remains to be seen wether it bears out in practice.Summary
In this chapter we have discussed how the use of ontologies for knowledge representation can aid in current biomedical research. We have argued that formally representing biological systems is necessary for advancing current biomedical research and that it is increasingly recognized that biologists need to use computational tools for performing thought experiments. We have described how biomedical ontologies can play a pivotal role in enabling that transition. We have outlined the hurdles facing the use of ontologies in creating formal representations that enable thought experiments. We have discussed the possible role of the Semantic Web in advancing this particular use of ontologies.Introduction
In the cultural heritage domain information systems are increasingly deployed, digital representations of physical objects are produced in immense numbers and there is a strong political pressure on memory institutions to make their holdings accessible to the public in digital form. The sector splits into a set of disciplines with highly specialized fields. Due to the resulting diversity, one can hardly speak about a "domain" in the sense of "domain ontologies" Interoperability between various highly specialized systems, integrated information access and information integration increasingly becomes a demand to support research, professional heritage administration, preservation, public curiosity and education. Therefore the sector is characterized by a complex schema integration problem of associating complementary information from various dedicated systems, which can be efficiently addressed by formal ontologies There is a proliferation of specialized terminology, but terminology is less used as a means of agreement between experts than as an intellectual tool for hypothesis building based on discriminating phenomena. Automated classification is a long established discipline of archaeology, but few terminological systems are widely accepted. The sector is, however, more focused on establishing knowledge about facts and context in the past than about classes of things and the laws of their behavior. Respectively, the concatenation of related facts by co-reference The Cultural Heritage Domain
Layman may think of cultural heritage primarily as fine arts collections and regard the description and indexing of these objects as relatively straightforward and reasoning more as a matter of scholarly reflection about their ideal values than a matter of logic. In reality, cultural heritage is more than as a domain. It comprises a broad spectrum of functions about the study and preservation of physical evidence of the past of all sorts of human activities What is Cultural Heritage?
In a narrower sense, we may regard the cultural heritage as the things preserved by the memory institutions, i.e. museums, sites and monuments records ("SMR"), archives and libraries. Their international umbrella organizations are: the International Council of Museums (ICOM,Following ICOM, "A museum is a non-profit making, permanent institution in the service of society and of its development, and open to the public, which acquires, conserves, researches, communicates and exhibits, for purposes of study, education and enjoyment, material evidence of people and their environment" To a certain degree, libraries may also preserve cultural heritage when they keep unique books, however their focus is on mediating access to nonunique information sources. In contrast, most cultural heritage objects are a rather mute evidence of past events that acquire relevance from understanding the context of their origin and history. The object may appear less as an information source in its own right than as an "illustration" of the past. This distinction is important to understand the difference between library and cultural heritage information, and the immense complexity of the latter.One can appreciate the diversity of cultural heritage from the following list of major kinds of collections:• History of arts and modern arts (graphics, painting, photography, sculpture, architecture, manuscripts, religious objects),• Historical heirloom (treaties, letters, manuscripts, drawings, photos, films, personal objects, weapons), • Archaeology (sherds, sculptures, tools, weapons, household items, human remains), • Design (furniture, tableware, cars, etc.), • Science and technology (machinery, tools, weapons, vehicles, famous experiments, discoveries), • Ethnology (costumes, tools, weapons, household items, religious objects, etc.) • Immobile sites (architecture, sculpture, rock art, caves) • To a certain degree, natural history collections, such as paleontology, biodiversity, mineralogy are also evidence of human activities (i.e. research) and hence culture.Handling information about all those kinds of things implies the use of very rich terminology, multilingual and often specific to particular communities or even to particular scholars. Agreement on common terminology is difficult and equivalent terms in other languages are often missing. It is an obvious challenge for employment of formal ontologies that poses not only technical problems, but also intellectual challenges in the approximation of intuitive or traditional concepts by logical definitions, such as the possible narrower and wider meanings of the same term, objective declaration of discriminating features or fuzzy transitions of instances from one class to another.Functions of Cultural Heritage Information
One can distinguish kinds of cultural heritage information systems by their major functions. Those are:• Collection management (acquisition, registration, "deaccession", inventory, loans, exhibitions, insurance, rights, protection zones) In each of these four areas quite distinct and highly specialized information systems exist, created and maintained by different players. On the other side, information in all those systems overlaps and should be mutually accessible in order to do the job. One of the major challenges of cultural heritage information management is the interoperability of those system and integration of information across function and discipline.Collection management systems are offered by several commercial vendors. They are mostly built on Relational or hierarchical database systems. Many customized systems are built on demand by IT experts. They support the technical management and administration of collections or sites and monuments. A comprehensive, internationally accepted definition of their functions can be found in Conservation information may be part of the collection management or separate from it. It deals with the scientific, material analysis of the objects, preventive measures and interventions. Loan management and historical research may need those data. Art and monument conservation is an underestimated sector of financial importance. Art conservators are scientists who need, similar to doctors, to accumulate and exchange immense knowledge about diagnosis methods, treatments and side effects Research information systems are highly specialized and mostly built on demand for specific projects. There are reference systems that list consolidated, uniform descriptions of all known items of a certain kind, such as Roman Inscriptions Presentation systems give access to cultural heritage information to the general public or a community of subscribers, in particular teachers and academics (see Chapter "Ontology-Based Recommender Systems"). We estimate that more than 95% of museum objects are not in any exhibition, and archives are mostly closed to the public. Therefore there is a strong political pressure to make at least object descriptions from the collection management systems publicly accessible. Museum portals (see Chapter "Ontology-Based Recommender Systems") may present parts of collections. The scale is immense: larger museums hold millions of objects. The Smithsonian Institutions hold over 100 million objects. Other presentation systems may take the form of an electronic exhibition, complementary information to a physical exhibition, or the form of an electronic publication that elaborates a particular subject matter. Ontologies play a major role to provide structured access points and to structure the subject matter itself in these systems.Recent efforts deal with the capturing and preservation of performing arts and oral tradition The Schema Integration Problem
Most of the professional systems referred to above are based on fairly complex database schemata. For instance, CIDOC proposed until 1995 a standard Relational Schema for museums with more than 400 tables. As described above, cultural heritage information is distributed in many different systems which complement each other. One source may relate Roman names to Roman inscriptions, another Roman inscriptions to stones, another stones to place of finding, and another places to coordinates Metadata and Application Profiles
Since libraries and Digital Libraries hold objects that contain data, they use to call the descriptions of their objects "metadata", i.e. data about data. This term has also been adopted by museums for their object descriptions, even though their objects are not data. There is a plethora of attempts to structure metadata as flat lists of properties, which may be aggregated in so-called "application profiles" Nevertheless, numerous digitization projects of cultural objects create digital libraries with Dublin Core metadata elements as minimal standard. Also wide-spread is the use of MPEG7 ( ISO21127
Information integration based on finding aids for the objects actually fails to integrate the information about the wider historical contexts these objects illustrate and from which they get their relevance. If a serious integration of the relevant contents of cultural heritage information is intended, richer models must be employed. For instance, the Research Libraries Group in California successfully integrated in their Cultural Materials Initiative data from about a thousand cultural institutions encoded in about a hundred different schemata into a far richer schema, virtually without loss of information. This schema was derived from the CIDOC CRM ontology, now ISO21127, which is currently the most elaborated ontology for the integration of cultural heritage information.The CIDOC CRM is a formal ontology The development team applied strict principles to admit only concepts that serve the functionality of global information integration, and other, more philosophical restrictions about the kind of discourse to be supported (for more details see Deliberately, the CIDOC CRM ontology is presented in a textual form to demonstrate independence from particular knowledge representation formats. There exists however a formal definition in TELOS The CRM is forced to use some workarounds we do not analyze here further. Terminology, i.e. classes that are not contributing as domain or range to the relationships expressed in data structures, are not part of the core ontology itself but regarded as instances of "Type" for practical reasons. 3. The normal human way to analyze the past is to split up the evolution of matters into discrete events in space and time. Thus the documented past can be formulated as series of events involving "Persistent Items" (also called endurants, see As a standard, the use of CRM concepts is not prescriptive, but provides a controlled language to describe common high-level semantics that allow for information integration at the schema level. It is intended to serve 1. As an intellectual guide to good practice of conceptual modeling in the sector. 2. As global model for information integration in a "Local as View" (LAV, Fig. 2. Historical events as meetings
As an intermediate model for data migration.
The coverage of the CRM for cultural heritage data has been validated by mappings from numerous data structures of the sector to the CRM. Even the common library format MARC ('Machine Readable Cataloguing') can be adequately mapped to it FRBRoo and Performing Arts
The FRBR model ('Functional Requirements for Bibliographic Records') was designed as an entity-relationship model by a study group appointed by the International Federation of Library Associations and Institutions (IFLA) during the period 1991-1997 Initial contacts in 2000 between the two communities eventually led to the formation in 2003 of the International Working Group on FRBR/CIDOC CRM Harmonisation. The common goals were to express the IFLA FRBR model with the concepts, ontological methodology and notation conventions provided by the CIDOC CRM, and to merge the two object-oriented models thus obtained. Although both communities have to deal with collections pertaining to cultural heritage, those collections are very different in nature: Most of library holdings are non-unique exemplars of publications, i.e. products of industrial processes. FRBR focuses therefore on the "abstract" characteristics that all copies of a single publication should typically display in order to be recognised as a copy of that publication. The history of individual copies and of the immaterial content is not regarded as particularly relevant in library catalogues and therefore widely ignored by FRBR. Of course, libraries do also hold unique items, such as manuscripts; but there are no internationally agreed standards how to deal with such materials, and FRBR mentions them but does not account for them in a very detailed way.Museums, on the other hand, are mainly concerned with unique items -the uniqueness of which is counterpoised by a focus on the cultural circumstances under which they were produced and through which they are interrelated. CIDOC CRM highlights therefore the physical description of singular items, the context in which they were produced, and the multiple ways in which they can be related to other singular items, categories of items, or even just ideological systems or cultural trends. Of course, museums may also have to deal with exemplars of industrially produced series of artefacts, but CIDOC CRM covers that notion just with the multi-purpose E55 Type class. Museum objects may be referred to in literature kept in libraries. Museum objects may illustrate subjects described in literature. Literature and objects may be created by the same persons, in common events.The Working Group has submitted the final draft of FRBRoo, i.e. the object-oriented version of FRBR, harmonized with CIDOC CRM, for public review by IFLA in February 2008. This formal ontology is intended to capture and represent the underlying semantics of bibliographic information and to facilitate the integration, mediation and interchange of bibliographic and museum information.The major innovation of FRBRoo is a realistic, explicit model of the intellectual creation process (see Fig. The idea is that products of our mind, as long as they stay in one person's mind only, are relatively volatile and not evident. In an event of first externalization, the "Expression Creation", concepts of a Work are made manifest by creating an Expression on a first physical carrier. This may be just another person's memory, as in the case of oral tradition, a paper manuscript or a computer disc. In its current draft version, FRBRoo includes a model of performing arts, connecting the interpretation of theatre plays with the recording and documentation of performances. It distinguishes and relates the three intellectual contributions (works) of the creation of the play, of the interpretation and the recording with the associated symbolic forms and physical carriers. This part of the model has been developed and tested in first examples in collaboration with the European funded project CASPAR on Digital Preservation. Even though there is a rising interest in documenting and preserving non-material culture, there are few other models about performing arts Other Core Ontologies
Independent from the CRM, the European funded project IndeCs, a consortium of multimedia experts, developed around 1997 a core model to trace the provenance of contributions and associated intellectual property rights in multimedia products and implemented a respective information system. This model was taken up by the ABC ontology. The latter is an outcome of the Harmony Project, which was funded cooperatively by the Distributed Systems Technology Cooperative Research Centre (DSTC) (Australia), the Joint Information Systems Committee (JISC) (UK), and the National Science Foundation Digital Libraries Initiative (NSF DLI2) (US). The original goal and continuing motivation of the ABC work arose from the need to integrate information from multiple genres of multimedia information within digital libraries. The researchers working on the Harmony Project have each been involved in a number of metadata initiatives including Dublin Core and MPEG-7.Complete details of the ABC ontology are described in Similarities between ABC and CRM aims and solutions were so striking, that both teams collaborated between 2001 and 2003 on a harmonization project, in which both ontologies adopted concepts from each other and rearranged properties and IsA hierarchies, until a merged representation was possible Characteristics of Ontologies for Cultural Heritage
Ontologies that deal with semantics equivalent to those of data structures, as the ones presented above, contain few classes and are rich in relationships Cultural heritage can be seen as the material evidence of human activities of social relevance in the past. Therefore • Information is mesoscopic, i.e. at a human scale, neither astronomic nor microscopic, except for microscopic analysis of traces and materials. Information is discrete. Processes are reported or become evident as discrete events involving discrete things, in contrast to geological or meteorological phenomena.• Information is event centric. Things, people and ideas connect and relate via events. • Its description is retrospective, in contrast to information to plan the future, such as for manufacturing.Information is naturally incomplete at some scale. It can be complemented but not be completed. Its description serves a kind of detective work to reconstruct possible pasts. The distinction between evidence and conclusion is vital. Therefore information cannot be normalized and integrated on the basis of the assumed past, such as on absolute dates, geographic coordinates, causeand-effect, states-and-state-transitions. The documentation of the process of observation is necessary to interpret correctly the observed evidence. Even the fact that some scholar classifies an object with a certain term is documented as a historical, intellectual process (this holds equally for biodiversity). Information is about material facts The above characteristics hold equally well for other descriptive sciences, such as geography, biodiversity, paleontology, clinical observation and epidemic studies, but also for the documentation of experiments and observations in natural sciences. Whereas the latter formulate their conclusions about their observations in categorical theories ("F=m*a", or "any non-supported material object in the atmosphere of Earth will fall"), scholars interpreting cultural heritage would generally hesitate to formulate their categorical conclusions or hypotheses in a formal representation (see also Also surprising is the fact that scholars hesitate to formulate in objective terms causes and causation Particularly in ethnology and archaeology (as in biodiversity), some information is documented in a partially categorical form, such as: "The boomerang is a hunting weapon of the Australian Aborigines". I.e. a particular community is associated with characteristic kinds of things and kinds of activities. The described object is seen as example of the category and an illustration of the activity. There is neither currently a formal ontology nor a suitable ontology language which would give a realistic account of the relationship between such partially categorical statements and the individual facts as perceived by the domain expert, and there is no dedicated "metaontology" which could be instantiated with such partially categorical statements.The CIDOC CRM makes a practical distinction between core classes and classes appearing as terminology motivated by the fact that they appear typically as data in data structures, in order to make fine distinctions between the kinds of the referred items. Even though knowledge representation does not distinguish between the two, it is an empirical fact that the sector uses to organize terminology differently, in vocabularies and thesauri, which may more and more be developed into formal ontologies in the proper sense. Consequently, the RDF schema SKOS Other terminologies of the sector characteristically pertain to:• Materials, conservation agents • Information objects • Processes, deterioration, activities • Social roles • Literary and iconographical subjectsIn the following section we describe the role of terminology and the most important ontologies in the sector.Terminology in Cultural Heritage
In many collaborations and discussions with museum curators and archaeologists we encountered a negative position towards the use of controlled vocabularies or even formal ontologies. Experts tend not to agree with the terminology used by colleagues The renowned archaeologist Franco Niccolucci posed the question, if archaeologists are "fuzzy" Information Access by Terminology
The diversity and number of small ontologies, in the order of a hundred to a thousand terms each, puts interesting challenges to ontology matching and alignment. Only automated tools have a chance to exploit this expert terminology for retrieval and reasoning across local systems.The task of librarians is not hypothesis building, but providing access to information. Quite naturally, they have a long tradition to agree on common terminology as access points. It is not easy for cultural heritage experts to appreciate the need for shared search terms (see for instance A problem with classification of material objects are the different aspects (facets), under which the classification may be done. Dominant aspects are the function of the object, its shape or appearance, elements or principles of construction The so-called facet analysis tries to resolve this problem (e.g. It is standard for museum portals and other cultural information systems that provide information about material objects to offer faceted access by type of object, person, place, date. MuseumFinland All terminological systems contain very general terms as root elements of their hierarchies. These may vary considerably and cause unnecessary inconsistency between the ontologies, because the purpose of these ontologies is not to solve the philosophical questions these general terms are associated with. For instance, the AAT subsumes under visual works material and immaterial things, such as paintings and electronic images. In the CIDOC CRM, material and immaterial things are disjoint concepts, because reasoning differs considerably for the two. In integrated information systems depending on rich data structures, this incompatibility can interfere with schema integration. The use of a shared core ontologies to enable interoperability between different domain ontologies has been proposed a decade ago by Major Terminological Systems
The AAT is the most widespread ontology in cultural heritage. It has the form of a thesaurus, compatible with ISO2788. Its topic is art and architecture, but covers a wide range of archaeological and ethnological materials as well as any kinds of object that may be subject of art in some way, such as weapons. It was originally developed by merging culture-relevant subject keywords from several large library systems. It is built for faceted classification. Its major facets are: Activities, Agents, Materials, Objects, Physical Attributes, Styles & Periods, Associated Concepts.The broader term and narrower term relationship are used in the sense of IsA. Its originally monohierarchical ("tree") generalization structure has been extended to polyhierarchical (directed acyclic graph). The AAT introduces so-called guide terms, (node labels in ISO2788) to group terms under minor facets, such as function or form, but there is no rigorous logic applied to this organization principle. The AAT has been translated into Spanish and Dutch.English Heritage (EH) maintains also a very large thesaurus of terms for mobile and immobile objects for the United Kingdom, as well as the French MERIMEE thesaurus.The multilingual thesaurus attached to the European HEREIN project Remarkable is the successful use of SHIC CAMEO is a searchable information center developed by the Museum of Fine Arts, Boston KOS of Particulars and Information Extraction
Understanding cultural heritage lives from contextual knowledge and concatenation of facts. Therefore it is most important to be precise about particular persons, places, historical periods and objects, which appear as the major constituents that connect multiple facts. Relevant resources about particulars are organized as Knowledge Organisation Systems, sometimes also called "ontologies", even though the term does not apply to lists of particulars.For instance, large reference lists of Persons are maintained by national libraries A large part of cultural heritage documentation, primary and secondary literature is in textual form. Even though databases have become standard tools for collection descriptions, curators prefer to express the relevant historical facts in free text. Therefore, automated information extraction (IE) becomes more and more important. Extracted information could be used to produce structured metadata and to instantiate ontology-based knowledge representation systems. Full text retrieval systems and text mining systems use to recognize concept names from ontologies. Ontologies should be tailored for this purpose, for instance be enriched with frequent synonyms. To our knowledge, there has been no such attempt for the more popular cultural heritage vocabularies.So-called Named Entity Recognition of names of persons, things, places As the core ontologies presented in this chapter show, event information is particularly important for cultural heritage. Automatic event recognition could bring a break-through in the access to relevant historical knowledge. Automatic event recognition is the next step after recognizing named entities and dates. An event can normally be described by the kind of action, the participating things and people, date and place. Event recognition should be combined with NER. So far, there has been not too much work in this direction (for instance, Conclusions
Current ontologies for cultural heritage exhibit a focus on the material and physical aspects of the past. This is quite natural, since "heritage" in the narrower sense implies material evidence of the past. Information about events in the physical world is central to the understanding of heritage information and explicit formal representation of events a key element to integrate heritage information. Interesting is the convergence of core ontologies to very similar forms, which can be integrated, and their independence from a particular "cultural" view. The work of historians is more a detective work than that of a judge. This determines widely the character and focus of cultural heritage ontologies. Information is incomplete. More important than the conclusions is the careful collection of all evidence that could support the one or the other view about the past. In contrast to that, natural sciences would get rid of experimental data after a theory has been sufficiently supported by experiments.Conclusions and judgment about the past are rather published in scholarly texts than encoded in data structures. This focus may be due to the characteristics of the reasoning in the sector, or just be enforced by the fact that IT methods have penetrated the sector from core documentation and management of physical collections. In the latter case, one may expect that cultural ontologies may in the future extend to other applications in the sector as well. May be formal ontologies dealing with the intellectual structuring of the sector, such as iconography, social interaction, and causation will find more attention in the future. Generally, we expect a greater diversity of conceptualization in the intellectual structure than in the description of material aspects, as represented by the CIDOC CRM.Since many scholars question the utility of standardized terminology, the formalization of the major terminological systems in the sector is still poor, but this may be overcome by a gradual transfer of know-how and better appreciation of the specifics of cultural conceptualization by ontology engineers. The sector shows enough interest in using ontologies to solve the interoperability of data structures and engages in real implementations. Ontology languages seem to be sufficiently expressive for terminological problems. In the area of data structures semantics, reification problems (i.e. simultaneous use of ontologies and documentation of the discourse about them and documenting facts together with their observation), as well as partially categorical statements cannot sufficiently be described with current ontology languages.In general, in the years of our collaboration with memory institutions and scholars we found that a major obstacle to introducing advanced computer science methods in the sector is a general underestimation of the complexity of cultural heritage conceptualization by the IT experts, which is equaled by the inability of domain experts to describe their conceptualizations in conscious, objective terms. Whoever wants to deal with the subject effectively must be prepared for a long knowledge engineering phase.Introduction
It is widely acknowledged that information access can benefit from the use of ontologies. For this purpose, available data has to be linked to concepts and relations in the corresponding ontology and access mechanisms have to be provided that support the integrated model consisting of ontology and data. The most common approach for linking data to ontologies is via an RDF representation of available data that describes the data as instances of the corresponding ontology that is represented in terms of an RDF Schema (compare chapter "Resource Description Framework"). Due to the practical relevance of data access based on RDF and RDF Schema, a lot of effort has been spent on the development of corresponding storage and retrieval infrastructures.In this chapter, we summarize the state of the art with respect to existing storage and retrieval technologies for RDF data. In particular, we first review the general architecture of RDF infrastructures that normally consist of a storage and a middleware layer. We discuss important aspects of these layers covering different storage formats for RDF data, common middleware functionalities such as RDF Schema reasoning and basic operations for data access and manipulation. Throughout the chapter, we discuss these aspects on a general level and only point to particular systems to provide examples of concrete implementations. We further discuss RDF query languages as the most common interface for interacting with ontology-based RDF data and present the SPARQL language in more detail. In Sect. 7 we also provide a very brief overview of existing approaches to extend RDF storage and retrieval systems to support more complex ontology languages than RDF Schema. We close with a discussion of current trends and speculate about future developments.Architecture of RDF Stores
An RDF store allows storage of RDF data and schema information, and provides methods to access that information. Thus, the two primary components of an RDF store are a repository and a middleware that builds on top of that repository. The middleware can be further divided into components as the access methods can be categorized into methods for adding, deleting, querying and exporting data. To describe the different components in detail, we assume a layered architecture as proposed in Different repositories are imaginable, e.g. main memory, files or databases, but the access methods should remain the same. Thus, it is reasonable to encapsulate the access to the repository in an own layer, which provides welldefined interfaces to the upper layers and can be exchanged if another repository is used. The inference support also resides in this layer as close to the repository as possible. Sesame The above mentioned access methods are located on a higher level and address the interfaces of the SAIL (or directly address the repository if there is no SAIL implementation). According to the different requirements of each access method they can be realized in different modules: The admin module provides the functionality for adding new data to and deleting data from the RDF store. Especially when loading data from files this requires parsing and validating RDF, so an RDF parser and an RDF validator are usually part of the admin module. The query module handles queries to the RDF store. As these queries can be formulated in any kind of RDF query language, several query modules may be necessary, each implementing a parser and handler for one query language. Finally, the export module allows a dump of the RDF store into files for data exchange with other systems.These modules can be accessed locally or remotely, e.g. using SOAP or RMI. This is why the highest layer in the middleware contains protocol handlers that can manage different access modes. Figure Storing RDF Data
RDF schemas and instances can be efficiently accessed and manipulated in main memory. For persistent storage the data can be serialized to files, but for large amounts of data the use of a database management system is more reasonable. Examining currently existing RDF stores we found that they are using relational and object-relational database management systems (RDBMS and ORDBMS).Storing RDF data in a relational database requires an appropriate table design. There are different approaches that can be classified in generic schemas, i.e. schemas that do not depend on the ontology, and ontology specific schemas. In the following we describe the most important table designs showing their advantages and shortcomings.Generic Schemas
The most simple generic schema is the triple store with only one table required in the database. The table contains three columns named Subject, Predicate and Object, thus reflecting the triple nature of RDF statements. This corresponds to the vertical representation for storing objects in a table in The greatest advantage of this schema is that no restructuring is required if the ontology changes. Adding new classes and properties to the ontology can be realized by a simple INSERT command in the table. On the other hand, performing a query means searching the whole database and queries involving joins become very expensive. Another aspect is that the class hierarchy cannot be modeled in this schema, what makes queries for all instances of a class rather complex.The triple store can be used in its pure form In a further refinement, the Triples table can be split horizontally into several tables, each modeling an RDF(S) property: These tables only need two columns for Subject and Object. The table names implicitly contain the predicates. This schema separates the ontology schema from its instances, explicitly models class and property hierarchies and distinguishes between class-valued and literal-valued properties Ontology Specific Schemas
Ontology specific schemas are changing when the ontology changes, i.e. when classes or properties are added or removed. The basic schema consists of one table with one column for the instance ID, one for the class name and one for each property in the ontology. Thus, one row in the table corresponds to one instance. This schema is corresponding to the horizontal representation in Another approach is vertically splitting the schema, what results in the one-table-per-property schema, also called the decomposition storage model. In this schema one table for each property is created with only two columns for subject and object. RDF(S) properties are also stored in such tables, e.g. the table for rdf:type contains the relationships between instances and their classes.This approach is reflecting the particular aspect of RDF that properties are not defined inside a class. However, complex queries considering many properties have to perform many joins, and queries for all instances of a class are similarly expensive as in the generic triple schema.In practice, a hybrid schema combining the table-per-class and table-perproperty schemas is used to benefit from the advantages of both of them. This schema contains one table for each class, only storing there a unique ID for the specific instance. This replaces the modeling of the rdf:type property. For all other properties tables are created as described in the table-per-propertyapproach (Fig. A possible modification of this schema is separating the ontology from the instances. In this case, only instances are stored in the tables described above. Information about the ontology schema is stored separately in four additional tables Class, Property, SubClass and SubProperty To reduce the number of tables, single-valued properties with a literal as range can be stored in the class tables. Adding new attributes would then require to change existing tables. Another variation is to store all class instances in one table called Instances. This is especially useful for ontologies where there is a large number of classes with only few or no instances Further Issues
There are further issues that may require an extension of the triple-based schemas and thus are affecting the design of the database tables:• Storing multiple ontologies in one database • Storing statements from multiple documents in one database Both points are concerning the aspect of provenance, which means keeping track of the source an RDF statement is coming from. When storing multiple ontologies in one database it should be considered that classes, and consequently the corresponding tables, can have the same name. Therefore, either the tables have to be named with a prefix referring to the source ontology The concept of named graphs Object-Oriented Features
Current ORDBMS provide the subtable facility which allows for a better modeling of the subclass and subproperty relationships. The table of a subclass is then created as a subtable of the superclass table. Consequently, querying for all instances of a class does not require searching for all triples with the rdfs:subClassOf property or looking up a SubClass table. However, this feature should be used carefully, as a new subtable can only be added at the bottom of the hierarchy. Otherwise, the complete table hierarchy needs to be rebuilt OracleAlthough the RDF model has several object-oriented characteristics and most RDF stores are internally working with an object model, approaches to store RDF data and schema information using object database management systems (ODBMS) are rarely known. (Object-)Relational databases are still predominant, when large amounts of data have to be persisted on a server, and object databases did not and will most probably not replace them. However, new developments of ODBMS may show some advantages over RDBMS in certain applications, e.g. for embeddable persistence solutions in mobile devices. This is why storing ontologies in an ODBMS is worth a closer look.RDF Middleware
What we call RDF middleware is the layer implementing the access to the physical RDF data store. Besides an inference mechanism, the access layer should provide functions for creating, querying and deleting data in the store. While adding data requires parsing and ideally a validation of the incoming RDF sentences, querying the RDF store needs the implementation of some kind of query language as well as an interpretation and a translation of this query language into calls to the physical RDF storage. Another important feature of this layer is the possibility to export data to a file for exchange with other systems.Inference for RDF
Inference for RDF is specified by the RDF(S) entailment rules described in • Inferring the transitive closure for the properties rdfs:subClassOf and rdfs:subPropertyOf • Inferring class memberships analysing the use of properties and their domains and/or ranges One approach is to compute the transitive closure using a recursive algorithm and to store it in database views. This algorithm constructs a view for each class, starting with the class table and adding the views of all of its subclasses examining the statements with the rdfs:subClassOf relationship in the database. Analogously, a view for each property is constructed from the rdfs:subPropertyOf relationships. A similar algorithm can be used to infer class membership from the properties of an instance An alternative is to use a production rule system that generates new facts from existing ones by forward chaining or applies backward chaining on a query presented to the system. This brings up an important aspect of the inference, namely the time, when the inference is executed. There are two possibilities:• Inference in advance (eager evaluation) • Inference at query runtime (lazy evaluation)The eager evaluation is computing the deductive closure in advance, so the time to evaluate a query is reduced Although we describe the inference mechanism as part of the middleware, the algorithms can also be defined as stored procedures in the database, leaving the inference task to the database management system. This depends on the capabilities of the DBMS used for storing the data.Querying Data
For formulating a query to the RDF store there are several approaches:• Implementing a proprietary query API • Implementing a query language Proprietary query APIs are defining their own query format. E.g. DLDB Most RDF stores are using one of the common RDF query languages like RQL, RDQL or SPARQL The syntax of the created SQL query usually depends on the underlying DBMS. This is why the implementation of an additional intermediate layer is reasonable that abstracts from the actual storage mechanism offering storage and retrieval functions. An example for that is the Storage And Inference Layer (SAIL) in Sesame. The layer can be exchanged according to the used DBMS and can even be placed on top of another SAIL to offer further functionality like caching recent query results An important aspect for accessing data is query optimization. It can be left to the database system, considering the sophisticated evaluation and optimization mechanisms of modern RDBMS. So, the query must be translated to SQL as completely as possible. This is the approach used by most RDF stores. Another approach is optimizing the query in the middleware itself, which is particularly interesting if the query engine should be independent of the underlying storage like in Sesame. Here, the query is translated into a set of SQL queries and joins or other operations are performed in the query engine. This not only requires an optimization strategy but also implies a transaction management, because one RQL query can result in multiple SQL queries and the state of the database must not change until all these queries are executed Adding, Deleting and Exporting Data
Adding data to the RDF store can be realized by creating new concepts, properties or instances in main memory using the API and then calling a function to store them into the knowledge base Delete operations in RDF stores have to be handled very carefully. While completely clearing the store is a quite simple function, deleting single statements can entail the deletion of other related statements. This not only requires recomputing the deductive closure for the RDF store, but also a mechanism for truth maintenance. Hence, deletions become quite costly To exchange data with other systems an export mechanism is required. Most RDF stores implement such an export function which allows to serialize the ontology and instance data from the RDF store into a file. The common formats for serializing RDF are N-Triples, N3 notation and RDF/XML notation.RDF Query Languages
As mentioned in the preceding section, the use of query languages is the most common way of interacting with an RDF store. Many query languages already exist that could, in principle, be used to interact with RDF data. The most obvious example is SQL, the standard query language for relational databases. In this section, we will explore what properties a query language for semistructured data, and in particular for RDF, should have, and what the difference is with existing approaches such as SQL. We will then discuss several proposals for query languages. In particular, we will describe the SPARQL query language in more detail.General Properties of Query Languages
We can identify several general properties with which one can characterize query languages. Here, we name six such properties:• Expressiveness: Expressiveness indicates how powerful queries can be formulated in a given language. Ideally, a query language should be expressive enough to allow the retrieval of any arbitrary combination of values from the queried model, that is, be complete with respect to its datamodel. Usually, expressiveness is restricted to maintain other properties such as safety and to allow an efficient (and optimizable) execution of queries. • Closure: The closure property requires that the results of an operation are again elements of the data model. This means that if a query language operates on a graph data model, the query results would again have to be graphs. • Adequacy: A query language is called adequate if it uses all concepts of the underlying data model. This property therefore complements the closure property: For the closure, a query result must not be outside the data model, for adequacy the entire data model needs to be exploited. • Orthogonality: The orthogonality of a query language requires that all operations may be used independently of the usage context. • Safety: A query language is considered safe, if every query that is syntactically correct returns a finite set of results (on a finite data set). Typical concepts that cause query languages to be unsafe are recursion and negation.Path Expressions
One of the main distinguishing features of query languages for semi-structured data is their ability to reach to arbitrary depths in the data graph. To do this, these languages all use the notion of path expressions. A path expression is a simple query, the result of which, for a given data graph, is a set of nodes. For example, consider the following bit of XML: The result of the path expression body.location.tel would be the set of nodes with the associated values "3,686", "555,722".Many useful regular expressions can be used in path expressions to facilitate more complex expressions than just specification of the complete path. For example, a regular expression location|name specifies either a location node or a name node. Another very useful pattern is the wildcard, which matches any node label. Using the symbol to express this, body.tel matches any path consisting of a body node followed by any node, followed by a tel node. Also, closure operations, like arbitrary repeats of a regular expression can be used. For example, body*.tel specifies the set of tel nodes that occur at arbitrary depth within the body node. At another level of abstraction, regular expressions can also be used to express matches on the actual string format of labels. For example the regular expression body." Path expressions, although they are an essential feature of query languages for semistructured data, can only return a subset of nodes in the database. They can not construct new nodes, perform joins, or test values stored in the database. In other words: path expressions are necessary but not sufficient for a good query language on semistructured data. A query language that lacks path expressions cannot be considered adequate, nor sufficiently expressive for querying semistructured data.Why not just SQL?
For strictly relational data (as opposed to semistructured data), SQL is by far the most widely supported query language, including support for large data storage, efficient indexing schemes, query optimizers, etc. It would therefore be attractive if we could use this robust and widely available technology for our purposes of querying semistructured data. Unfortunately, this can only be done at the cost of a very large gap between the data model in the repository (e.g. RDF) and the data-model on which the query language is based (the relational model).To exemplify this, let us look at how the scenario would look for an XML implementation in a relational database: as a first step, we would have to encode the XML data model in the relational model. This would be possible by assigning each node in an XML tree a unique identifier, with each entry in the relational database linking such a node with all its descendants and attributes. The problems start when we want to use this as the basis for querying the XML structure: each XML query should be compiled into an SQL query on the underlying relational tables. Typically, a single XML query (such as: "return all descendants of a given node") must be compiled into a complicated set of SQL queries. It is not even clear whether a finite set of SQL queries could be generated for every reasonable XML query.Although perhaps attractive as a short term solution, we feel that in the long run this is not an appropriate solution. Rather, techniques for large data storage, indexing schemes, query optimizers, etc., should be provided for the native data model (be it XML or RDF), instead of relying on these techniques for a completely different data model.Querying RDF
RDF documents and RDF schemata can be considered at three different levels of abstraction:1. At the syntactic level they are XML documents. 2. At the structure level they consist of a set of RDF triples. 3. At the semantic level they constitute one or more graphs with partially predefined semantics.We can query these documents at each of these three levels. We will briefly consider the pros and cons of doing so for each level in the next few sections.Querying at the Syntactic Level
As we have seen previously, RDF models can be written down in XML notation. It would therefore seem reasonable to assume that we can query RDF using an XML query language (e.g. XQueryQuerying at the Structure Level
When we abstract from the syntax any RDF document represents a set of triples, each triple representing a statement of the form subject-predicateobject. A number of query languages have been proposed and implemented that regard RDF documents as sets of such triples, and that allow to query such a triple set in various ways.However, querying at this level means that we now interpret any RDF model only as a set of triples, including those elements which have been given special semantics in RDF Schema. For example, the fact that rdfs:subClassOf is a transitive relation is ignored at this level.Querying at the Semantic Level
When we consider RDF models at the semantic level we query the full knowledge of everything that the RDF model entails, and not just those facts that happen to be represented explicitly.There are at least two options to achieve this goal:1. Compute and store the deductive closure of a graph as a basis for querying. 2. Let a query processor infer new statements as needed per query.While the choice of an RDF query language is, in principle, independent of the choice made in this respect, the fact remains that most RDF query languages have been designed to query a simple triple store and have no specific functionality or semantics to discriminate between data and schema information.SPARQL
The SPARQL Query Language Basic Queries
The SPARQL query language is based on matching graph patterns. The simplest graph pattern is the triple pattern, which is like an RDF triple, but with the possibility of a variable instead of an RDF term in the subject, predicate or object positions. Combining triple patterns gives a basic graph pattern, where an exact match to a graph is needed to fulfill a pattern.As a simple example, consider the following query: The above query retrieves all triple patterns where the property is rdf:type and the object is rdfs:Class. In other words, this query, when executed, will retrieve all classes.Note that like the namespace mechanism we have previously seen for writing down RDF in XML, SPARQL allows us to define prefixes for namespaces and use these in the query pattern, to make queries shorter and easier to read. In the rest of this chapter, we will omit the declaration of the "rdf" and "rdfs" prefixes, for brevity.To get all instances of a particular class, for example the FOAF vocabulary class "Person", we write: PREFIX foaf: <http://xmlns.com/foaf/0.1/> SELECT ?i WHERE { ?i rdf:type foaf:Person . } SPARQL makes no explicit commitment to support RDFS semantics. Therefore, the result of this query depends on whether or not the system answering the query supports RDFS semantics. If it does, then the result of this query will include all instances of the subclasses of Person as well. If it does not support RDFS semantics, then it will only retrieve those instances that are explicitly of type "Person".Using Select-From-Where
As in SQL, SPARQL queries have a SELECT-FROM-WHERE structure:SELECT specifies the projection: the number and order of retrieved data. FROM is used to specify the source being queried. This clause is optional; when not specified we can simply assume we are querying the knowledge base of a particular system. WHERE imposes constraints on possible solutions in the form of graph pattern templates and boolean constraints.For example, to retrieve all e-mail addresses of persons, we can write SELECT ?x ?y WHERE { ?x foaf:mbox ?y . } Here ?x and ?y are variables, and ?x foaf:mbox ?y represents a resourceproperty-value triple pattern.We can create more elaborate graph patterns to get more complex information from our queries. For example, to retrieve all persons with name "Bob" and their phone numbers, we can write SELECT ?x ?y WHERE { ?x foaf:name "Bob"; foaf:mbox ?y . } Here ?x foaf:name "Bob" collects all resources which have a name "Bob", as discussed, and binds the result to the variable ?x. The second pattern collects all triples with predicate mbox. There is an implicit join here, in that we restrict the second pattern only to those triples, the subject of which is in the variable ?x. Note that in this case we use a bit of syntax-shortcut as well: we use a semi-column to indicate that the following triple pattern shares its subject with the previous one, so the above query is equivalent to writing down: In SPARQL, we use a FILTER condition to indicate a boolean constraint. In this case, the constraint is the explicit join of the variables ?x and ?y by using an equality (=) operator.Optional Patterns
The graph patterns we have seen so far are mandatory patterns: either the knowledge base matches the complete pattern, in which case an answer is returned, or it does not, in which case the query does not produce a result. However, in many cases we may wish to be more flexible. Consider, for example, the following bit of RDF: <foaf:Person rdf:about="#bob"> <foaf:name>Bob</foaf:name> </foaf:Person> <foaf:Person rdf:about="#alice"> <foaf:name>Alice</foaf:name> <foaf:mbox>alice@example.org</foaf:mbox> </foaf:Person> As you can see, this fragment contains information on two people. For one person it only lists the name, for the other it also lists the e-mail address. Now, we want to query for all people and their e-mail addresses: The result of this query would be: ?name ?email Alice alice@example.org So, despite the fact that Bob is listed as a person, the query does not return him: the query pattern does not match because he has no e-mail address.As a solution we can adapt the query to use an optional pattern:SELECT ?name ?email WHERE { ?x rdf:type foaf:Person ; foaf:name ?name . OPTIONAL { ?x foaf:mbox ?email } }The meaning is roughly "give us all the names of persons, and if known also their e-mail address" and the result looks like this: ?name ?email Bob Alice alice@example.org This covers the basics of the SPARQL query language. For a full overview of the SPARQL language and an explanation of more advanced features, such as named graphs, we recommend reading the SPARQL specification at http://www.w3.org/TR/rdf-sparql-query/.Scalability of RDF Stores
In terms of data storage and retrieval, scalability and performance is a very important issue. The performance of an RDF store depends on various factors: the underlying database system, the database representation of the RDF schema and instances, the efficiency of the query engine, and the performance of the inference engine. A detailed overview of the scalability and performance of different RDF stores would be out of scope of this chapter, but we can mention some interesting points.Theoharis et al. An elaborate method and toolset to evaluate Semantic Web repositories as a whole is the Lehigh University Benchmark (LUBM) The W3C maintains a web site recording the size of the largest deployed installations of triple stores. Beyond RDF Schema
While the development of storage and retrieval systems for semantic data so far has been focussed on supporting RDF and RDF Schema there is also an interest in extending available infrastructures to more expressive languages. In particular, supporting more expressive ontology languages such as OWL-Lite and OWL-DL as well as expressive rule languages is a subject of active work. Other activities include the extension of representation and query languages with advanced features such as time The most straightforward extension of existing RDF infrastructures is a support for ontologies encoded in OWL. As OWL can be serialized in RDF, the corresponding models can be stored in any RDF repository without changing the systems. The structural complexity of the OWL encoding in RDF, especially the high number of blank nodes, however, makes the access to these models rather cumbersome. In order to overcome these problems, many RDF stores use dedicated APIs as part of the middleware layer to support the storage, retrieval and manipulation of OWL ontologies. While some systems such as Jena use their own ontology API, other systems like KAON adopted the proposal for a standardized OWL API described in Naturally, extensions to more expressive languages do not only aim at providing support at the syntactic level, but also with respect to the semantics of the corresponding languages. As mentioned above, most RDF stores support RDF Schema reasoning on the basis of a specialized set of deduction rules. A common way of extending this fixed schema is to provide support for user defined rule sets. These rule sets can be used for defining parts of the semantics of OWL An alternative way of supporting OWL semantics is to provide an interface to dedicated Description Logic reasoners (e.g. Racer, FaCT or Pellet) either via specialized data structures or on the basis of the standardized DIG API (http://dig.sourceforge.net/). Systems differ in the amount of derivable knowledge that is actually integrated into the RDF model for query answering. The BOR reasoner (http://www.ontotext.com/bor/) for example computes the subsumption hierarchy of an OWL ontology and stores the derived sub-Class relations in the RDF model for further processing. Furthermore, there are some RDF compatible systems that implement expressive rule languages such as KAON2 which implements disjunctive datalog Conclusion
After reviewing a number of existing RDF storage and retrieval systems, we can draw some conclusions about the state of the art and general trends in the fields. On the general level, we can say that there is strong convergence of technologies which is documented by the mergence of SPARQL as a standard query language but also in terms of features that are common to different systems. For instance, we can observe that most RDF stores are not really specialized database systems for RDF data but rather an intelligent middleware that wraps existing database technology. Besides providing special support for the graph data model that is characteristic for RDF data, the main functionality provided by this middleware is support for ontological reasoning. An observation that can be made in connection with these two main functions is the fact that almost all systems rely on relational databases that provide very limited support with respect to data model and reasoning. There are very little approaches that try to delegate some of these aspects to the storage model as well by using deductive or object oriented database technologies.With respect to further development of RDF technologies, we can identify two trends. The first one that was already mentioned in Sect. 7 is the extension of existing systems to more expressive representation languages. In this context, rule languages (compare chapter "Ontologies and Rules") are the most promising candidates because it has been shown that rule-based reasoning has the potential to scale to very large data sets whereas ontological reasoning based on description logics shows serious limitations when large numbers of instances are involved. The other major direction of development concerns the scalability of RDF infrastructures to internet scale. In this context, approaches for distributed RDF processing are becoming more and more important. Both aspects, expressive representation languages and distribution are essential with respect to realizing the vision of the semantic web and are therefore important steps towards real semantic web applications.Introduction
As part of the infrastructure for working with ontologies, reasoning systems are required. Reasoning is used at ontology development or maintenance time as well as at the time ontologies are used for solving application problems. In this section we will review so-called tableau-based decision procedures for inference problems arising in both contexts. We start with the satisfiability problem for a set of logical formulae. Speaking about ontologies, we focus on description logics, which provide the basis for standardized practical ontology languages. In this context, the set of formulae mentioned above is usually divided into a Tbox and an Abox for the intensional and extensional part of the ontology, respectively (see below for details). We are aware of the fact that ontology processing systems based on description logics also support some form of rules as well as means for specifying constraints among attributes of different individuals The main idea of tableau-based methods for satisfiability checking is to systematically construct a representation for a model of the input formulae. If all representations that are considered by the procedure turn out to contain an obvious contradiction (clash), a model representation cannot be found, and it is concluded that the set of formulae is unsatisfiable. In early publications on tableau-based proof procedures, in particular for first-order logics, the notation for the model representations was done using tables (tableaux in French). In recent approaches these tables are better described as graph structures. The name tableau is retained for historical reasons, however.Initially, tableau-based methods for description logics have been developed for decidability proofs, and due to this fact, they are highly nondeterministic for expressive description logics. It turned out, however, that they can indeed be efficiently implemented using appropriate search strategies and index structures such that for typical-case inputs, acceptable runtimes can be expected even though the worst-case complexity is high. In practical systems, tableau structures are efficiently maintained during branch and bound (or backtracking) with the result that tableau-based methods have been successfully employed in ontology reasoning systems such as FaCT++, Pellet, or RacerPro (cf. Although, in practical contexts, tableau-based methods are often applied in a refutation-based way (i.e., they are used to show unsatisfiability of a set of formulae), the graph structures computed for solving the ontology satisfiability problem can be reused for efficiently implementing higher-level reasoning services such as instance retrieval requests. In other words, in practical systems, tableau-based methods are not just used for satisfiability checking but are also used to compute index structures for subsequent calls to other reasoning services.In this chapter, tableau-based reasoning methods are formally introduced. We start with a nondeterministic basic version which subsequently will be extended with optimization techniques in order to demonstrate how practical systems can be built. We also demonstrate how computed tableau structures can be exploited in an ontology reasoning system. In order to make this chapter self-contained, we shortly introduce the syntax and semantics of the description logic ALC and introduce Tboxes and Aboxes.An overview on tableau algorithms for description logics can also be found in Syntax and Semantics of ALC
For a given application problem one chooses a set of elementary descriptions (or atomic descriptions) for concepts and roles representing unary and binary predicates, respectively. A set of individuals is fixed to denote specific objects of a certain domain. We use letters A and R for atomic concepts and roles, respectively. In addition, let {i, j, . . .} be the set of individuals. In ALC (Attributive Language with full Complement), descriptions for complex concepts C or D can be inductively built using the following grammar:We introduce the concept descriptions and ⊥ as abbreviations for A ¬A and A ¬A, respectively. Concept descriptions may be written in parentheses in order to avoid scoping ambiguities.For defining the semantics of concept and role descriptions we consider interpretations I that consist of a non-empty set Δ I , the domain, and an interpretation function • I , which assigns to every atomic concept A a set A I ⊆ Δ I and to every atomic role R a set R I ⊆ Δ I × Δ I . For complex concept descriptions the interpretation function is extended as follows:The semantics of description logics is based on the notion of satisfiability. An Abox is a set of assertions of the form C(i) or R(i, j) where C is a concept description, R is a role description, and i, j are individuals. A concept assertion C(i) is satisfied w.r.t. a Tbox T if for all models I of T it holds that i I ∈ C I . A role assertion R(i, j) is satisfied w.r.t. a Tbox T if (i I , j I ) ∈ R I for all models I of T . An interpretation satisfying all assertions in an Abox A is called a model for A. An Abox A is called consistent if such a model exists, it is called inconsistent otherwise.Decision Problems and Their Reductions
The definitions given in the previous section can be paraphrased as decision problems.The concept satisfiability problem is to check whether a model for a concept exists. The Tbox satisfiability problem is to check whether a model for the Tbox exists. The concept subsumption problem (w.r.t. a Tbox) is to check whether C I ⊆ D I holds (in all models of the Tbox).The Abox consistency problem for an Abox A (w.r.t. a Tbox) is the problem to determine whether there exists a model of A (that is also a model of the Tbox). Another problem is to test whether an individual i is an instance of a concept description C w.r.t. a Tbox and an Abox (instance test or instance problem). The instance retrieval problem w.r.t. a query concept description C is to find all individuals i mentioned in the assertions of an Abox such that i is an instance of C.The latter problem is a retrieval problem but, in theory, it can be reduced to several instance problems. Furthermore, the satisfiability problem for a concept C can be reduced to the consistency problem of the Abox {C(i)}. In order to solve the instance problem for an individual i and a concept description C one can check if the Abox {¬C(i)} is inconsistent. The concept subsumption problem can be reduced to an Abox consistency problem as well. If the Abox {C(i), ¬D(i)} is not consistent, C is subsumed by D Thus, in theory, all problems introduced above can be reduced to the Abox consistency problem. Note that in practical systems, specific algorithms might be used to decide a certain problem.Deciding the Consistency Problem for ALC Aboxes
A decision procedure for the ALC Abox consistency problem is described in this section using a so-called tableau-based algorithm. In order to simplify the presentation, in this section, we do not consider Abox consistency with respect to Tboxes. For Tboxes, among other extensions, additional machinery is required to ensure termination (see Sect. 3 for details).As indicated in the introduction, the main idea of the Abox consistency algorithm is to systematically generate a representation for a model. In this process which searches for a model, some representations are generated which contain an obvious contradiction (clash), i.e., for an individual i we have C(i) and ¬C(i) in an Abox. The assertions C(i) and ¬C(i) are called the culprits for the clash. In case of a clash, the generated representations turn out to not to describe a model.From a theoretical point of view, the algorithm described below is sound and complete but nondeterministic. In a practical implementation, indeterminism must be handled with systematic search techniques, and various heuristics have been described in the literature to guide the search process. If we see a tableau-based algorithm not as a theoretical vehicle for proving decidability of a logic, but as a practical way to solve the Abox consistency problem, then it becomes clear that it is important to be able to detect clashes as early as possible while the model representations are built. A representation with a clash no longer needs to be considered. Thus, we should be able to identify clashes not only for assertions with atomic concepts but also for assertions with complex concepts. Therefore, in contrast to other presentations of tableau algorithms we will not transform a concept into a form that makes the presentation (and analysis) of the tableau algorithm easier (negation normal form), but directly use a form that is efficient for detecting clashes in typicalcase inputs (encoded normal form). The ALC Abox consistency algorithm described below checks whether there exists a model for the input Abox.The algorithm operates on a set of Aboxes A. Each Abox represents an alternative to be investigated in the exhaustive model generation process. We also call such an internal Abox, i.e., an element of A, a tableau (but also use the term Abox in the following). Initially, the algorithm starts with a set A = {A} containing the input Abox A. A set of rules is applied to an Abox from this set until no more rule applications are possible or no more rule applications are needed to determine the result. The rules are introduced below. If a rule is applied to an Abox, the Abox is replaced by one or more Aboxes. If it is replaced by more than one Abox (the so-called successor Aboxes), we say that the rule introduces a so-called choice point. A rule introducing a choice point is called a nondeterministic rule. All other rules are called deterministic. In any case, the Abox to which a rule is applied is replaced with new Aboxes that are "copies" of the original one plus some additional assertions.If a tableau (Abox) is found to contain a clash, the tableau is called closed, otherwise it is called open. A tableau to which no rule can be applied is called complete. For a complete tableau the synonym completion is also used. If there exists a completion, i.e., an open tableau to which no more rules can be applied, the algorithm returns "yes" (indicating consistency). If all tableaux that could be generated by applying rules are closed, the algorithm returns "no" (inconsistency).Concept Normalization and Encoding
In order to speed up the clash test, concepts are normalized using several transformation steps. First, double negations are eliminated, i.e., ¬¬C is replaced with C. Then, maximal sequences of conjunctions (possibly with nested parentheses) are flattened and represented with an n-ary conjunction term {C 1 , C 2 , . . . , C n } (written as a prefix operator to the set of arguments). Corresponding representations {C1, C2, . . . , C n } are built for disjunctions. The interpretation function is extended in the obvious wayIf there are two concepts C and ¬C mentioned in a conjunction (disjunction) or the concept ⊥( ) appears, the whole termAfterwards, in an encoding process every concept description C and its negation ¬C is inductively associated with a unique identifier. For instance, one could use numbers as unique identifiers and store concepts as records in an array, or it is possible use pointers to records (or objects) as unique identifiers. The fact that conjunctions (or disjunctions) are represented as sets enables the assignment of the same unique identifier to syntactically different but semantically equivalent conjunctive and disjunctive concept descriptions. The assignment of unique identifier to a concept is known as encoding a concept. If we use a concept description in the following text, we assume that we refer to its unique identifier.The function neg(C) is used to find the negation of a concept. The implementation of this function should require constant time (i.e., be as efficient as possible). Tableau Rules
The tableau rules are applied to an Abox A as part of the set of Aboxes A on which the algorithm operates. A rule can be applied whenever the precondition is satisfied and A is not in the set of closed Aboxes (this is an implicit condition). Initially, the set of closed Aboxes is empty. Saying that A is replaced by an Abox or a sequence of Aboxes we mean that A is removed from A and the Aboxes generated by the rule are added to A. If a rule is applied to an assertion, we say the assertion is expanded :and for all i ∈ {1, . . . , n} it holds that C i (x) ∈ A, then replace A with a sequence of Aboxes A 1 , . . . , A n wherean individual not occurring in A). • Value restriction rule: If {(∀R.C)(x), R(x, y)} ⊆ A but C(y) ∈ A, then
replace A with A ∪ {C(y)}.• Negation rule: If ¬C(x) ∈ A but neg(C)(x) ∈ A, then replace A with (A ∪ { neg(C)(x)}. • Clash rule: If {C(x), neg(C)(x)} ⊆ A, then add A to the set of closed
Aboxes.The algorithm runs in a loop and applies a rule if its precondition is satisfied. A precondition of a rule is satisfied if there exists a substitution for the variables x or y with individuals such that the condition is satisfied. As indicated before, if the precondition is satisfied, a rule is applied to an Abox in A.Applying a rule means to execute the then-part applying the variable substitution computed from the if-part of the rule. The loop ends if a completion is found or if no rule is applicable. The algorithm returns "yes" if there exists a completion and "no" otherwise. In principle, the rules defined above can be applied in any order. Later we will see that a rule application strategy might impose restrictions on the order of rule applications. Restrictions are introduced to find completions "early", i.e., the strategy is used for optimization purposes. If the expressivity of the language is extended, a particular rule application strategy might also be necessary to ensure termination or soundness and completeness.A few additional definitions are appropriate for the analysis of the algorithm in the next subsection. If an Abox is replaced with one new Abox, the new Abox contains strictly more assertions. We call this process and-branching. Applying a nondeterministic rule (for the time being, the disjunction rule) might introduce several new Aboxes. We call this process or-branching.The individuals mentioned in the original Abox are called old individuals, all other individuals are called fresh. A sequence of role assertionsIn a path of length 1, x 2 is called the (direct) successor and x 1 is called predecessor of x 2 (for a role R). The individuals x i with i ∈ {2, n + 1} are called indirect successors of x 1 .Formal Properties
The formal properties of the algorithm are analyzed in three steps. We first show termination, and afterwards we prove soundness and completeness.The procedure terminates: First, no rule can be applied twice to the same Abox with the same bindings for the variables x, y due to the preconditions (no infinite and-branching). Second, although new individuals are introduced by applying the existential quantification rule, the quantification concept is of a smaller size than the original concept. Hence, there can be no infinite applications of the existential quantification rule. The length of the longest Abox is bounded by the size of the input Abox. Third, no rule deletes an assertion, and therefore, Aboxes can only grow (i.e., no so-called yo-yo effects can occur The algorithm is sound: If the algorithm returns "yes" there exists a model satisfying all assertions of the input Abox. This is shown as follows. If the algorithm returns "yes" there exists a completion. From the completion A a so-called canonical model I A = (Δ I A , • I A ) can be constructed (cf. For all atomic concept descriptions A let
By definition, all role assertions are satisfied by I A . Now, using induction on the structure of concepts, it is easy to show that I A also satisfies all concept assertions in A (see The algorithm is complete: If there exists a model for the input Abox, then the algorithms returns "yes". Or, by contraposition, it holds that if the algorithm returns "no", then there does not exist a model. If the algorithm returns "no", all tableaux are closed, i.e., there is a clash in each tableau. Under the assumption that there exists a model for an Abox to which a rule is applied, it is shown that a model for at least one of the generated Aboxes can be constructed by examining every rule (and hence, no alternative to be investigated is forgotten, for details see The ALC Abox consistency problem is PSPACE-complete, cf. Towards an Optimized Implementation
The tableau rules refer to assertions for specific individuals or check for a clash w.r.t. a specific individual. Thus, rather than using an arbitrary set data structure for representing a tableau, in a concrete implementation of the tableau algorithm, the set of assertions in an Abox is partitioned w.r.t. the individuals the assertions refer to (for C(i) and R(i, j) the assertion refers to i). We call such a partition an individual partition P i . The access to the partition of an individual i should require almost constant time. If there is an assertion R(i, j) ∈ P i , then there will also be a partition P j for j (possibly empty). We say P j depends on P i .Furthermore, looking at the preconditions of the rules, it is revealed that for each individual, the preconditions refer to specific concept constructors (conjunctions, disjunctions, existential quantifications, or value restrictions). Thus, for each individual partition, the set of conjunctions, disjunctions, existential quantifications, and value restrictions must be efficiently identifiable. For the latter two subsets, a further index over different roles might be considered.The selection of a particular rule to apply is nondeterministic in the algorithm above. Various kinds of heuristics have been investigated to reduce the number of rule applications for typical-case inputs. First, in a practical implementation, best results have been achieved if the clash rule is applied with highest priority. Since no rules are applied to closed tableaux by definition, the number of applicability tests for rules is reduced if clashes are found early. The overhead for the clash rule must be kept at a minimum, however. Usually, in concrete implementations, the clash rule is (implicitly) applied whenever a concept assertion C(x) is to be added to an Abox. Note that for ALC, role assertions are not directly involved in a clash test -a condition that is no longer true for more expressive logics. Checking whether the assertion neg(C)(x) is already an element of the tableau to which C(x) is to be added is a frequently executed operation in a practical implementation, and has to be implemented very efficiently. As part of this so-called clash test, in a practical implementation it might become apparent that C(x) is also already contained in the Abox. So, there is no maintenance effort for the Abox to which C(x) is added.The conjunction rule is applied with second-highest priority. Although the number of assertions to be handled in a partition is increased, the chance that a clash is detected early is also increased. Since in many contexts the Abox will indeed be consistent, conjunctions have to be "expanded" anyway. So, it is a good heuristic to prefer the conjunction rule over other rules.In order to reduce the number of Aboxes to be handled as parts of A, in practical systems, deterministic rules are preferred over nondeterministic ones. In order to reduce memory requirements (and to meet the complexity class of the Abox consistency problem) the so-called trace technique has been developed. Employing the trace technique, the disjunction rule is applied before the deterministic value restriction rule. Then, for each existential quantification assertion (∃R.C)(x), it is ensured that all potentially applicable value restrictions are indeed available in the Abox. Thus, the existential quantification rule can be combined with the value restriction rule. Rather than only adding a concept assertion C(y) based on the quantification concept as indicated in the existential quantification rule for a role R, additionally for every value restriction (∀R.D i )(x) the assertion D i (y) is added, with y being the fresh individual introduced by the exists quantification rule. Then all assertions {C(y), D 1 (y), . . . , D n (y)} can be treated in isolation. If they turn out not to lead to a clash, the assertions, and all those derived from them, can be removed (and (∃R.C)(x) must somehow be marked to avoid repetitive rule applications).In a practical implementation, the trace technique might not be adopted for various reasons. For instance, the removal of assertions might interfere with the idea to reuse of previous computation results, in particular if Tboxes are involved (see below). Or the strategy is to avoid the expansion of disjunctions but check the satisfiability of existential quantifications first.Dealing with Indeterminism in a Tableau Algorithm
In the description above, a nondeterministic rule (in ALC only the disjunction rule) generates a sequence of new Aboxes. If Aboxes are created in a naive way, this can hardly be efficient. Thus, a practical implementation must find a way to implement a structure-sharing strategy for copies of Aboxes in order to avoid structures to be copied repeatedly. Copying complete structures is memory-extensive as well as time-consuming. Even with a structure-sharing approach, the naive generation of successor Aboxes should be avoided due to the memory-management overhead involved.Obviously, the disjunction rule does not need to generate successors that immediately lead to a clash. If the disjunction rule would be applicable to some disjunct C i in an assertion ( {C 1 , . . . , C n })(x) and neg(C i )(x) ∈ A the corresponding successor Abox does not need to be generated (it will be closed according to the clash rule immediately). The detection of those situations requires some additional machinery in the implementation (boolean constraint propagation, BCP) Further optimizations that also have no impact on the correctness of the algorithm but provide for improved performance for typical-case inputs are possible. For instance, the disjunction rule requires as a precondition that the disjunct C i (x) is not already in A. Thus, looking for a completion, in a concrete implementation it is advantageous to first apply the disjunction rule to those concept assertions ( {C 1 , . . . , C n })(x) with disjuncts C i such that C i is also mentioned in many other disjunctive assertions for the individual x. The application of the disjunction rule for the other disjunctions for x involving C i is then "avoided" (due to the precondition of the disjunction rule). Efficiently finding those concepts C i such that the number of occurrences in all disjunctions applying to an individual x is maximized (or large) is non-trivial, however. There is a tradeoff between the time spent in search for occurrences of a concept assertion C i (x), management of index structures for speeding up this search process, and the gain of this in terms of or-branching reduction.Reusing previous results can help finding clashes early. If A is an Abox, then all Aboxes derived from A by applying a tableau rule are called sibling Aboxes. Information acquired for one successor Abox A of A can be propagated to sibling Aboxes of A . Let us consider the successor Aboxes of an application of the disjunction rule to an Abox A again. If it turns out that one of the successor Aboxes A with C i (x) being added contains a clash, then, neg(C i )(x) can be added to all (open) sibling Aboxes of A . Again, if neg(C i )(x) is explicitly present in a sibling Abox, an application of the disjunction rule to the sibling Abox might be prevented and a clash might be revealed earlier. On the negative side it has to be mentioned that applying rules to neg(C i )(x) in a tableau also causes some overhead. In a practical implementation one might avoid the applications of the rules to assertions added this way (without impact on soundness and completeness).Up to now, we have considered ways to find a completion earlier (i.e., we attempt to reduce or-branching). This heuristic is useful because the algorithm terminates if a completion is found. However, it is also possible to find ways to close tableaux early. Consider the following example Abox, which is obviously inconsistent (adapted from We assume that the tableau algorithm applies the disjunction rule to the first disjunction in A. We get two new Aboxes A 1 and A 2 . Both Aboxes are supersets of A. In Fig. In Fig. The dashed lines (curved) indicate the dependencies of the assertions that are added to the respective Aboxes (not all dependencies are shown for  This can be achieved as follows. The clash occurs in A 2n+3 . The culprit in A 2n+3 is A(j). The other culprit ¬A(j) is in A 2n+2 . Culprit assertions are indicated with an exclamation mark. Starting with the culprits and following the dashed lines the Aboxes are marked with exclamation markers. See Fig. In the example shown in Fig. A slightly modified example illustrates that the process does not necessarily close all Aboxes but only those which, due to the given culprits, do not lead to completions. The example is given as follows:In Fig. Fig. 2. Examining the dependencies reveals that A4 must not be automatically closed to retain completeness (see text)
and A 4 remain open, which is necessary not to miss a possible completion to be constructed. If all successor Aboxes of an Abox are marked with a clash, their assertions are also seen as clash culprits and exclamation markers are propagated via the curved dependency links as introduced before In the literature, the technique described here is known as backjumping, which is a restricted form of dependency-directed backtracking Usually, a partition for an individual is seen as a graph node with role assertions "pointing" to other graph nodes, and concept assertions defining the so-called "label" of a "node" (see below for a more formal introduction of a label). Hence, from an implementation point of view a tableau is seen as a graph with nodes and edges, both associated with a label. For nodes, the label is a set of concepts, and for edges it is a set of roles. To every path in the Abox trie from the root to a leaf (see Fig. Dealing with Tboxes
The tableau algorithm introduced above must be extended to work with nonempty Tboxes. In theory, it is possible to transform all GCIs of the Tbox into a single GCI of the form M . The transformation is very simple. Instead of writing a GCI as C i D i one could write ¬C i D i , and thus, M is the conjunction i {M i } of all M i = ¬C i D i stemming from the GCIs in the Tbox (M and M i are called global constraints). With the Tbox transformed into {M 1 , . . . , M n } we can see that the restriction M on the righthand side applies to all domain objects. The transformation is called internalization in the literature A problem with this rule is that the algorithm then does not terminate. M might contain existential quantifications which cause new individuals y to be created, for which M (y) is added and so on (infinite and-branching occurs). Some form of blocking must be enforced (see, e.g., The tableau algorithm can be slightly changed to exploit these insights. We give a definition for the label of a partition. The label of a partition for an individual x is defined as {C | C(x) ∈ P x }. Now, if there exists an individual partition P k and there is no rule applicable to P k nor to all partitions that depend on P k , then no rule need to be applied to an assertion in a partition P l if label(P l ) ⊆ label(P k ) and k, l are fresh variables (otherwise, partitions for old individuals might block each other). We say P l is blocked by the witness P k . The witness must be a fresh individual. Note that if there is a clash detected for k the whole tableau is closed and no rule is applied to l anyway. The condition label(P l ) ⊆ label(P k ) might become false if, due to other rule applications, new assertions are added for the individuals k or l in P k and P l , respectively. So, blocking conditions must be dynamically checked.In order to show soundness in case of blocked partitions, one constructs a canonical interpretation for an individual i for which there exists a blocked partition P i by defining tuples (i, x) ∈ R I for every assertion R(w, x) related to the witness w of i (for details see, e.g., One drawback from a practical point of view is that now a possibly large set of disjunctions is introduced for every individual mentioned in a tableau, since M i = ¬C i D i . Keeping in mind that, e.g., boolean constraint propagation is employed to deal with disjunctions in a practical system, it becomes clear that disjunctions always involve "heavy-weight" methods in a practical implementation of the tableau algorithm. For specific forms of GCIs, the disjunctions do not have to be explicitly generated, however. This is explained in the next subsection.Lazy Unfolding
Let us assume, there is a global constraint of the form L i = ¬A C in M such that A is an atomic concept description. Rather than adding this global constraint to every individual x, the idea is to only add C(x) if adding ¬A(x) would lead to a clash. For those global constraints, one can implicitly assume that individuals are instances of ¬A "if not stated otherwise" (see the construction of the canonical interpretation). The global constraint L i can be handled "in a lazy way" by a new rule which "unfolds" an assertion A(x) in a tableau (cf. We need some definitions for specifying the exact conditions under which soundness and completeness can be guaranteed. An atomic concept description A directly refers to an atomic description B if there exists a GCI A C such that B is mentioned in C but not in the scope of an existential quantification or value restriction. A refers to B if it directly refers to B or there exists an atomic concept description B such that A refers to B and B refers to B.A global constraint of the form L i = ¬A C need not be handled as a disjunction if A is an atomic concept description and C does not refer to A. Let us assume that global constraints that satisfy the conditions introduced above for L i are collected not in M but into a set L. There must be no other global constraint in L with a disjunct ¬A or disjunct A. Then, the following rule is used to deal with global constraints in L In other words, only if there is an assertion A(x) in a tableau, then C(x) must be added because assuming ¬A(x) would lead to a clash. In case we also collect assertions of the form A C into the set of concepts L (and not into M ) another rule must be added. Corresponding restrictions as for ¬A C apply:Lazy unfolding exploits the fact that one can safely assume that a domain object which is not explicitly enforced to be in A I (or (¬A) I in the second case) is an element of (¬A) I (or A I in the second case). See GCI Absorption
Global constraints in L are handled more effectively. If, initially, the global constraints are not of the form that they can be put into L but must be stored in M , the goal is to transform them in a way that a maximum number of global constraints can be put into L, and possibly none must be kept in M , without changing the semantics of the Tbox. This transformation process is known as GCI absorption (see In some cases, still some global constraints remain in M even if GCIs are transformed as describe above, unfortunately. For instance, this happens if there are two GCIs of the form A (∃R.A) C and ∃R.A A in a Tbox. The latter kind of GCI is only relevant for an individual x if there exists an assertion R(x, y) in tableau.For ∃R . C an effective treatment is easily possible. The same holds for range restrictions ∀R.C. Let domain(R) and range(R) denote sets of concepts (initially empty). We assume that all (∀R.⊥) C are removed from M , and for each (∀R.⊥) C removed, C is added to domain(R). In addition, all ⊥ ∀R.C are removed from M , and for each ⊥ ∪ ∀R.C removed, there is C added to range(R):• Domain restriction rule: If R(x, y) ∈ A and C ∈ domain(R), then replace
A with A ∪ {C(x)}. • Range restriction rule: If R(x, y) ∈ A and C ∈ range(R), then replace A with A ∪ {C(y)}.See For all x i , i ∈ {0, . . . , n-1}, a disjunction (∀R.¬A) A would be asserted, and choice points are set up. We assume that rules are first applied to the tableaux created for ∀R.¬A. After several rule applications, a clash w.r.t. ¬A(x n ) and A(x n ) would be detected. The situation could be even worse, if there was a GCI B ∃R.A A with B being an atomic concept for which there exists a GCI B D. Thus, there is no way to absorb B ∃R.A A into L using the above-mentioned techniques.In Binary Absorption
A GCI B ∃R.A
A should not be transformed into a disjunction to be placed in M . It can be transformed intowhere R -1 denotes the inverse of role R. The idea is to introduce a marker A 1 (y) for every y for which there is an assertion R(x, y). The first GCI of the list above can be handled effectively by absorbing it into range(R) as described before. A 2 is a marker indicating an instance of ∃R.A (see the second GCI). The third GCI now enforces y to be an instance of A in the tableau. In order to deal with a conjunction of two atomic concepts on the left-hand side of a GCI the lazy unfolding rules can be extended as follows • Lazy unfolding rule 3:No disjunctions of this particular type have to be handled after the transformation is applied. Soundness and completeness of this approach have been shown in Tableau Structures for Subsumption Problems
Tableau-based reasoning can be exploited for solving other reasoning problems as well. First, we consider the subsumption problem, then we turn to the instance problem and the retrieval problem.Subsumption problems occur very frequently if the so-called taxonomy of a Tbox is computed. This is usually done at ontology development time in order to check for modeling errors (unsatisfiable atomic concept descriptions, unwanted subsumption relationships, etc.). The taxonomy of a Tbox is a graph where the nodes are the atomic concept descriptions mentioned in the Tbox (including and ⊥), and the edges indicate whether a node is a most-specific subsumer of another node.For expressive languages such as ALC the subsumption problem A ? B can be reduced to the Abox consistency problem {(A ¬B)(i)} for some individual i. If the Abox is inconsistent the subsumption relation holds, otherwise it does not hold. For practical Tboxes, GCIs usually involve conjunctions on the right-hand side. Thus, if B is negated, disjunctions have to be handled by the tableau algorithm, which might lead to "unfocused" applications of the rules. For computing the taxonomy, many similar subsumption problems of the form A i ? B have to be solved, and hence, many Abox consistency problems {(A i ¬B)(i)} are the consequence. In almost all cases the subsumption relation between A and B does not hold, and hence, the Abox is likely to be shown to be consistent. Quite some number of applications of tableau rules might be required, however (with large or-branching, and for realistic ontology considerable and-branching as well).Therefore, in In • For every A ∈ L 1 there does not exist an ¬A ∈ L i .• For every ¬A ∈ L 1 there does not exist an A ∈ L i .• For every ∃R.C ∈ L 1 there does not exist an ∀R.D ∈ L i .• For every ∀R.C ∈ L 1 there does not exist an ∃R.D ∈ L i .If non-subsumption cannot be concluded, the "full" test whether {(C ¬D)(i)} is consistent is performed. However, practical experiments have shown that this is not often required Conclusion
The tableau algorithm introduced in this section can be extended to deal with additional concept and role constructors. With the addition of new constructors, the rule application strategy becomes important for termination and correctness, not only for optimization. Furthermore, the blocking condition might become more complex. For instance, the following constructs have been investigated in the literature and tableau algorithms have been specified:• Concrete domains (with feature composition) Introduction
Tableau algorithms, introduced in chapter "Tableau-Based Reasoning", are nowadays the state-of-the-art for reasoning with description logic (DL) ontologies. This is mainly due to optimizations of the original algorithm that heuristically guide the search for a model. DLs such as the ones underlying the Web Ontology Language (OWL) (see chapter "Web Ontology Language OWL") are, however, complex logics, so no one reasoning method can be identified as the best. Rather, comparing different methods and identifying which ones are suitable for which types of problems can give us crucial insights into building practical reasoning systems. Therefore, alternatives to tableau calculi have been explored in the past.Resolution and its refinements Since resolution has been quite successful as a general theorem proving technique, it is natural to apply it to ontology reasoning. Decision procedures for various DLs have been developed in the past. It turns out that, even for relatively complex DLs, resolution-based algorithms can be derived easily and are quite elegant. While tableau algorithms need sophisticated blocking techniques to ensure termination In this chapter, we outline the principles underlying most known resolution-based procedures for DLs. After introducing the basic notions in Sect. 2, we present a decision procedure for the DL ALCHI in Sect. 3. This DL provides many features characteristic of the DL languages, such as full Boolean connectives, (restricted) existential and universal quantification, inverse roles, and role hierarchies. Furthermore, the resolution decision procedure for this DL conveys the basic principles without overloading the presentation with technical detail. We also overview the problems involved in extending the algorithm to more expressive DLs.Deductive databases have been successfully applied to answering queries over large data sets, so it is natural to apply them to DL reasoning with large ABoxes. To enable this, in Sect. 4 we present a technique that reduces an ALCHI knowledge base to a disjunctive datalog program without affecting the set of entailed ground facts. Thus, one can answer DL queries using the resulting disjunctive program, and, in doing so, one can apply known optimization techniques such as magic sets The techniques presented in this chapter have been implemented in the DL reasoner KAON2. Preliminaries
The Description Logic ALCHI
Description logics have been introduced in detail in chapter "Description Logics", but, to make this chapter self-contained, we present the definition of the DL ALCHI . For a set of role names N R , a role is either some  individuals. An ALCHI knowledge base K is a triple (R, T , A). With |K| we denote the number of symbols needed to encode K. We say that K is extensionally reduced if, in all ABox assertions C(a), the concept C is a concept name or the negation of a concept name. Any K can be made extensionally reduced by replacing each assertion C(a) where C is not of the appropriate form with an assertion A C (a) and an axiom A C C, for A C a new concept name. In chapter "Description Logics", DLs are given a direct model-theoretic semantics. In this chapter, however, we use an equivalent semantics based on translation into first-order logic. In particular, we translate an ALCHI knowledge base K into a first-order formula π(K), where π is the operator defined in Table The basic inference problem for ALCHI is checking satisfiability of Kthat is, checking whether π(K) is a satisfiable first-order formula. As discussed in chapter "Description Logics", other inference problems can be reduced to knowledge base satisfiability.The negation-normal form nnf(C) of a concept C is the concept equivalent to C in which negation occurs only in front of concept names. The concept nnf(C) can be computed in time polynomial in the size of C by well-known transformations The Ordered Resolution Calculus
We use the well-known definitions of constants, variables, function symbols, terms, predicates, and formulae of first-order logic The empty clause is written as . Terms and formulae that do not contain variables are called ground . We say that formulae ϕ and ψ are equisatisfiable if ϕ is satisfiable if and only if ψ is satisfiable.A substitution is mapping of variables to terms that is not identity on a finite number of variables; we often write it as {x 1 → t 1 , . . . , x n → t n }. An application of a substitution σ to a term t (formula ϕ) is written tσ (ϕσ) and it is the term (formula) obtained by replacing each free occurrence of a variable x with xσ. A substitution σ is a unifier of terms s and t if sσ = tσ. A unifier σ of s and t is called a most general unifier if, for each unifier η of s and t, a substitution ξ exists such that xη = (xσ)ξ for every variable x. If a most general unifier σ of s and t exists, it is unique up to variable renaming The skolemization of a formula ϕ, written sk(ϕ), is obtained from ϕ by successively replacing each subformula ∃x : ψ occurring positively or a subformula ∀x : ψ occurring negatively in ϕ with a formula ψ{x → f (x 1 , . . . , x n )}, where f is a new function symbol and x 1 , . . . , x n are the free variables of ψ different from x. It is well-known that ϕ and sk(ϕ) are equisatisfiable Ordered resolution An inference rule is a template that specifies how a conclusion is derived given a set of premises; an inference is an application of an inference rule to concrete premises. With R we denote the ordered resolution calculus, consisting of the following inference rules, where the clauses C ∨ A ∨ B and D ∨ ¬B are called the main premises, C ∨ A is called the side premise, and Cσ ∨ Aσ and Cσ ∨ Dσ are called conclusions (as usual in resolution theorem proving, we make a technical assumption that the premises do not have variables in common):Positive factoring:
where (i ) σ = MGU(A, B), (ii ) Aσ is maximal with respect to Cσ ∨ Bσ and no literal is selected in Cσ ∨ Aσ ∨ Bσ.Ordered resolution:
where (i ) σ = MGU(A, B), (ii ) Aσ is strictly maximal with respect to Cσ and no literal is selected in Cσ ∨ Aσ, (iii ) ¬Bσ is either selected in Dσ ∨ ¬Bσ, or it is maximal with respect to Dσ and no literal is selected in Dσ ∨ ¬Bσ.Ordered resolution is compatible with powerful redundancy elimination techniques, which allow deleting certain clauses during the theorem proving process without loss of completeness If a clause C is a tautology, then it is redundant in any set of clauses N . A sound and complete tautology check would itself require theorem proving, and would therefore be difficult to realize. Therefore, one usually only checks for syntactic tautologies -that is, clauses containing the literals A and ¬A. A clause C subsumes a clause D if there is a substitution σ such that Cσ ⊆ D and |C| < |D|. If a clause C is subsumed by a clause from a set of clauses N , then C is redundant in N .A derivation by R from a set of clauses N is a sequence of sets of clauses N 0 , N 1 , . . . such that N 0 = N and, for i ≥ 0, either (1) N i+1 = N i ∪ {C} where C is the conclusion of an inference by R from premises in N i , or (2) N i+1 = N i \ {C} where C is redundant in N i . Each derivation must be fair Disjunctive Datalog
We recapitulate the basic notions of disjunctive datalog . , B m
where A i and B j are datalog atoms. The literals A i are called head literals, whereas the literals B i are called body literals. Each rule is required to be safe -that is, each variable occurring in the rule must occur in at least one body atom. A fact is a rule with m = 0. For the semantics, we take a rule to be equivalent to a clauseWe consider only Herbrand models, and say that a model M of P is minimal if there is no model M of P such that M M . A ground literal A is a cautious answer of P (written P |= c A) if A is true in all minimal models of P . First-order entailment coincides with cautious entailment for positive ground atoms.Deciding Satisfiability of ALCHI by Resolution
The fundamental principles for deciding a first-order fragment L by resolution have been established by Joyner Translating the Knowledge Base into Clauses
The first step in deciding satisfiability of K is to transform K into an equisatisfiable set of clauses Ξ(K). A straightforward way of doing so is to compute Cls(π(K)). Such an approach, however, has two important drawbacks. First, the size of the resulting clause set could be exponential in the size of π(K), due to nesting of and . Second, we should exploit the structure of the formula π(K) in our algorithm, but Cls(π(K)) does not reflect this structure.To avoid these problems, we preprocess K using the structural transformation Intuitively, this transformation replaces complex concepts with simpler ones. The knowledge base Θ(K) does not contain , so it can be translated into clauses without an exponential blowup. Note: A and B are concept names or ; C, C1, and C2 are arbitrary concepts; R and S are roles; and QX is a new concept name not occurring in K that is unique for X.Lemma 1. An ALCHI knowledge base K and Θ(K) are equisatisfiable.Proof. Consider a single application of Θ. It is obvious that the axioms obtained after the transformation imply the axiom before the transformation, which proves the (⇐) direction. For the (⇒) direction, simply observe that each interpretation I of K can be extended to an interpretation I of Θ(K) by interpreting each newly introduced concept Q X as X.To obtain a set of clauses corresponding to K, we translate Θ(K) into firstorder logic using the operator π from Table We now show that clausification does not affect the satisfiability of a knowledge base, and that it produces clauses of a certain syntactic structure: Lemma 2. The following claims hold for each ALCHI knowledge base K:The number of recursive invocations of Θ and the number of new concepts(3) It is easy to see that Θ(K) contains only axioms from the left-hand side of Table The function symbol f is different for each axiom.Saturation by Ordered Resolution
Since ordered resolution (R) is a sound and complete calculus, we can use it to check satisfiability of Ξ(K). To obtain a decision procedure, we just need to ensure that each saturation of Ξ(K) by R terminates; that is, we must ensure that we can derive only finitely many clauses from Ξ(K) by applying the rules of R. There are two main reasons why we might derive an infinite number of clauses.First, we might derive clauses with ever deeper terms. This is shown by the following example, in which the selected literals are underlined:Second, we might derive clauses with an unbounded number of variables. For example, the following inference increases the number of variables by one, and repeating it for the conclusion produces clauses with an arbitrary number of variables:The inferences that ordered resolution performs on a given set of premises are determined by the parameters of the calculus -the literal ordering and the selection function. By choosing these parameters appropriately, we can restrict the resolution inferences in a way that allows us to establish a bound on the term depth and on the number of variables. In the first example, if we ensure that C(f (x)) ¬C(x), then the second premise can participate in an inference only on literal C(f (x)); since C(f (x)) and C(a) do not unify, no inference of R is applicable to C(a) and ¬C(x) ∨ C(f (x)). In the second example, the undesirable inference can be prevented if we select ¬R(x, y).The following definition fixes the parameters for R that, as we shall see shortly, restrict the inferences on Ξ(K) in a way which ensures termination.Definition 3. Let R DL denote the calculus R parameterized as follows:
• The literal ordering is any admissible ordering such that, for all function symbols f and predicates R, C, and D, we have R(x, f (x)) ¬C(x) and D(f (x)) ¬C(x). • The selection function selects every negative binary literal in each clause.An ordering compatible with Definition 3 can be obtained by instantiating a lexicographic path ordering for t a term of the form x, f (x), or a; P(a) is a possibly empty disjunction of the form P1(a1) ∨ • • • ∨ Pm(am); and the empty clause P is of type 5.It is easy to see that an application of R DL to clauses from Table Lemma 3. Each R DL inference, when applied to ALCHI-clauses, produces an ALCHI-clause.
Proof. We summarize all possible R DL inferences on all types of ALCHIclauses in Table The following lemma shows that the number of ALCHI-clauses is finite for a finite knowledge base K. In fact, the bound on the number of derivable clauses can be used to estimate the complexity of the algorithm.Lemma 4. For an ALCHI knowledge base K, the longest ALCHI-clause over the signature of Ξ(K) is polynomial in |K|, and the number of such clauses different up to variable renaming is exponential in |K|.
Proof. The number c of unary predicates in the signature of Ξ(K) is linear in |K|, since each concept introduced by Θ corresponds to one nonliteral subconcept of C. Similarly, the number f of unary function symbols in the signature of Ξ(K) is linear in |K|, since each function symbol is introduced by skolemizing one concept of the form ∃R.C. Consider now the longest ALCHI-clause Cl 6 of type 6. Such a clause contains a possibly negated literal A(x) for each unary predicate A, and a possibly negated literal A(f (x)) for each pair of unary predicate and function symbols, yielding at most = 2c + 2cf literals, which is polynomial in |K|. Each ALCHI-clause of type 2 is a subset of Cl 6 , so there are 2 such clauses; that is, the number of clauses is exponential in |K|. For other ALCHI-clause types, the bounds on the length and on the number of clauses can be derived in an analogous way.We now state the main result of this section: Theorem 1. For an ALCHI knowledge base K, saturating Ξ(K) by R DL decides satisfiability of K and runs in time that is at most exponential in |K|.Proof. By Lemma 4, the number of clauses derivable by R DL from Ξ(K) is exponential in |K|. Each inference can be performed in time polynomial in the size of clauses. Hence, the saturation terminates after performing at most an exponential number of steps. Since R DL is sound and complete, it decides satisfiability of Ξ(K), and by Lemma 2 of K as well, in time that is exponential in |K|.An Example
We now present a simple example. Let K be the following knowledge base:Let us assume that we want to check whether K |= D(a); as shown in chapter "Description Logics", this so if and only if K ∪ {¬D(a)} is unsatisfiable. Hence, let K be the knowledge base K extended with the assertion ¬D(a).To check satisfiability of K using resolution, we first apply structural transformation. For (1), we obtain the following:By Definition (1), we should introduce a new name for the concepts ¬A and B; however, both Q 1 ∀S.¬A and Q 2 ∃R.B can be translated into ALCHIclauses in a straightforward way. Hence, we do not further apply Θ, and neither we do so for (2) and (3). We obtain the set Ξ(K ) as follows (the meaning of underlining will be explained shortly):To saturate Ξ(K ) by R DL , we use a literal ordering compatible with Definition 3, where we break ties by comparing predicates alphabetically. The literals that are either selected or maximal are underlined. We now saturate Ξ(K ); R(xx+yy) means that a clause was obtained by resolving (xx) and (yy).We derived the empty clause, so the set of clauses Ξ(K ) is unsatisfiable, and so is K , which implies K |= D(a).Extending the Algorithm to the More Expressive DLs
We now overview the problems encountered in extending this basic algorithm to more expressive DLs and point to the relevant literature for the solutions.Boolean Role Expressions
The DL ALB If n = 0, such clauses can cause termination problems. For example, resolving the clauses ( The clause Resolving such clauses with other clauses of that form can easily produce clauses with an arbitrary number of variables. For example, resolving ( This problem, however, can be solved in a simple way: since A(x) and B(y) are variable-disjoint, similarly as in the DPLL procedure Transitivity Axioms
Many DLs allow roles to be declared as transitive Such clauses are difficult for resolution. For example, if we also have the clause Clause ( To prevent the increase in the number of variables, one might select the negative literal in There are several ways to address this problem. In Another solution is to replace transitivity axioms with new concept inclusion axioms that capture the effects of the transitivity axioms. Roughly speaking, a transitivity axiom Trans(S) is replaced with axioms ∀R.C ∀S.(∀S.C), for each R with S * R and C a "relevant" concept from K; for more details, please see Number Restrictions
As explained in chapter "Description Logics", many DLs provide for number restrictions n R.C and n R.C. The algorithm from this section can be extended to such concepts by using the well-known translation of number restrictions into first-order logic:These translations employs the equality predicate ≈. Ordered resolution alone is not an efficient calculus for theorem proving with equality. Therefore, deciding DLs with number restrictions typically requires the application of a calculus optimized for theorem proving with equality. Basic superposition In Nominals
Another common construct considered in DLs are nominals. Although such a result has not been published, it would be straightforward to extend the algorithms from Reasoning by Reduction to Logic Programming
We now present an algorithm for reducing an ALCHI knowledge base to a disjunctive datalog program that entails the same set of ground atoms. As discussed in The Main Difficulty
For an ALCHI knowledge base K, our goal is to derive a disjunctive datalog program DD(K) such that K |= α if and only if DD(K) |= α for α of the form A(a) or R(a, b). Thus, we can use DD(K) instead of K for query answering, and in doing so, we can apply all optimization techniques known from deductive databases, such as magic sets As shown in Table A naïve attempt to reduce K into disjunctive datalog is to translate K into a first-order formula π(K), skolemize it, translate it into conjunctive normal form, and rewrite the obtained set of clauses into rules. For K, such an approach produces the following logic program LP(K):Clearly, K and LP(K) entail the same set of ground facts. The program LP(K), however, contains a function symbol in a recursive rule This problem could be solved by employing an appropriate cycle detection mechanism. In To avoid potential problems with termination, our goal is to derive a true disjunctive datalog program DD(K) without function symbols. For such a program, queries can be evaluated using any standard technique; furthermore, all existing optimization techniques known from deductive databases can be applied directly. Hence, the main problem that we deal with is the elimination of function symbols from LP(K).The Translation Algorithm
From Table occurs in some rule only in the head, then the literal HU (x) is added to the rule body.• The fact HU (a) is added to the program for each constant a occurring in K.If K is not extensionally reduced, then DD(K) = DD(K ), where K is an extensionally reduced knowledge base obtained from K as explained in Sect. 2.1.We now state the properties of DD(K): Theorem 2. The following claims hold for each ALCHI knowledge base K:for A a concept name and R a role.The number of literals in each rule in DD(K) is at most polynomial, the number of rules in DD(K) is at most exponential, and DD(K) can be computed in time exponential in |K|.Proof. (1) Table (2) Simply observe that K |= α if and only if K ∪ {¬α} is unsatisfiable. The latter is the case if and only if DD(K ∪ {← α}) = DD(K) ∪ {← α} is unsatisfiable, which is the case if and only if DD(K) |= c α.(3) Follows in the same manner as (2). (4) Follows immediately from Lemma 4.An Example
We now continue the example from Sect. 3.3 and compute a disjunctive datalog program DD(K). The first step in the algorithm is to compute Ξ(T ∪ R); clearly, it consists of the clauses ( Finally, we add to DD(K) the ABox and the facts involving HU :It is straightforward to verify that DD(K) |= D(a), in accordance with Theorem 2.It is instructive to compare the algorithm from this section with tableaux algorithms from chapter "Tableau-Based Reasoning". Tableau algorithms introduce new individuals in order to satisfy the existential quantifiers. In contrast, the programs obtained by the reduction do not represent such individuals at all. In our example, DD(K) is function-free, so the universe of the program is restricted to the constants explicitly mentioned in it. Thus, the models of K and DD(K) coincide only on positive ground facts, and are unrelated for the facts involving unnamed objects.To understand why the saturation of the TBox and RBox by R DL is necessary, consider the role of each rule in DD(K). While the axiom (2) in K is applicable to all individuals in a model, the rule ( Discussion
By Theorem 2, the program DD(K) is independent of the query, as long as the query is a concept name or a role. Hence, DD(K) can be computed once, and can be used to answer any query involving only concept names. If the query involves a complex concept C (even if C is a negated concept name), then query answering can be reduced to entailment of positive ground facts, by introducing a new name Q and by adding the axiom C Q to the TBox. Obviously, DD(K ∪ {C Q}) may depend on C. Namely, by saturating Γ (T ∪ R), the reduction algorithm derives all nonground consequences of K, and a complex query concept can introduce new nonground consequences, which should be taken into account in the reduction.Theorem 2 allows |DD(K)| to be exponential in |K|, which may seem discouraging. Note, however, that the number of rules depends on |T ∪ R| and not on |A|. This is important for data complexity Adding Number Restrictions
The reduction algorithm presented in By resolving This clause differs from clauses of type 7 from Table After saturation of TBox and RBox, the nonground clauses from the saturated set are transformed in a certain way that reflects such an encoding of the ground clauses. It is important to understand that the constants such as a f have no deeper semantic meaning; they are just a proof-theoretic aid that allows the simulation of inferences of basic superposition in disjunctive datalog.Conclusion
This chapter overviews the algorithms for reasoning in description logics by resolution. These algorithms are interesting because they are worst-case optimal, but are also suitable for practical implementation A challenge for future research is to obtain a more elegant and perhaps worst-case optimal algorithm for reasoning with nominals. Namely, reasoning with nominals requires reasoning about the cardinality of sets, which is known to be difficult for resolution. Another challenge is to provide methods for dealing with transitivity and general role inclusion axioms, such as the ones available in the DL SROIQ Ontology Repositories
Jens Hartmann 1 , Raúl Palma 2 , and Asunción Gómez-Pérez 21 Center for Computing Technologies (TZI), University of Bremen, Germany, jh@tzi.de 2 Ontology Engineering Group, Laboratorio de Inteligencia Artificial, Facultad de Informática, Universidad Politécnica de Madrid, Spain, palma@fi.upm.es, asun@fi.upm.es Summary. The growing use and application of ontologies in the last years has led to an increased interest of researchers and practitioners in the development of ontologies, either from scratch or by reusing existing ones. Reusing existing ontologies instead of creating new ones from scratch has many benefits: It lowers the time and cost of development, avoids duplicate efforts, ensures interoperability, etc. In fact, ontology reuse is one of the key enablers for the realization of the Semantic Web. However, currently, ontologies are mostly developed from scratch, due to several reasons. First, ontologies are usually tailored to work for specific applications, restricting its potential reusability. Second, developers usually follow a monolithic approach when developing ontologies, usually covering different domains, hampering the reusability of relevant parts for other applications. Third, ontologies are rather difficult to find due to the lack of standards for documenting them and appropriate tools supporting intelligent ontology discovery and selection by end users. In this chapter, we define a generic ontology repository framework that enables the implementation of fully-fledged ontology repositories providing the technological support to the aforementioned issues. We distinguish between the ontology repository itself and the software to manage the repository, and describe their main aspects and services. Finally, we present two exemplary systems based on this framework.Introduction
Knowledge reuse and access is one of the leading motivations for the Semantic Web. Driven by those intensions an increasing amount of ontologies can be found nowadays on the Web distributed among personal or institutional web pages. One of the key problems the ontology engineering community has to face at the moment is that most ontologies are built from scratch -rather than reusing existing ones -leading to high engineering efforts and costs. One of the main reasons is that most existing ontologies are build having a specific application scenario in mind, making them similar to custom software. This leads to ontologies that are tailored to work with specific applications, but are S. Staab and R. Studer (eds.), Handbook on Ontologies, International Handbooks not knowledge representation artifacts in the traditional sense. When designing these ontologies, engineers focus on expected behavior in the application rather than on reuse and interoperability with other ontologies. Another problem is that ontologies trying to cover domains in the knowledge representation sense are often too big to be reused efficiently. These ontologies try to capture the complete domain knowledge whilst ontology engineers normally only need to reuse certain parts for their ontology. Nevertheless, having modular ontologies is not enough to facilitate the reusability of ontologies if developers are not able to find them efficiently. We need an appropriate infrastructure that enables an intelligent ontology discovery and selection by end users. Currently, initial collections of ontologies have been created during the last years, e.g., the DAML Library. These circumstances point out the strong requirement for novel methods facilitating an efficient access and reuse of ontologies within a large scalable and reliable infrastructure, so called ontology repositories. The storage of knowledge encoded by ontologies can only be part of the solution. Crucial for ontology repositories is that additional knowledge about ontologies, so called meta knowledge, is managed together in such a repository.We propose a Generic Ontology Repository Framework (GORF) including specific module support, tailored to exactly those requirements. We base our framework on experiences gained by realizing our ontology repository OnthologyIn the following we discuss essential aspects of ontology repositories. To begin with, in Sect. 2 we describe the historical development from data to ontology repositories. In Sect. 3 we will describe the generic architecture of an ontology repository and corresponding management systems. Core elements and services of an Ontology Repository are discussed in Sect. 4. Further we illustrate management systems for ontology repositories in Sect. 5 and exemplify a centralized vs. a decentralized solution in Sect. 6. We conclude in Sect. 7 and comment on further steps.From Data Repositories to Ontology Repositories
In this section we present an overview of how repositories have evolved throughout time from general purpose data repositories to specialized ontology repositories.In literature exist many different meanings and definitions to what a data repository is, and in general to what a repository is. Hence we will first discuss what we understand by a data repository, instead of giving another definition. We consider a data repository as a collection of digital data that is available to one or more entities (e.g., users, systems) for a variety of purposes (e.g., learning, administrative processes, research) and that has the characteristics proposed by Heery and Anderson • Content is deposited in a repository, whether by the content creator, owner or third party. • The repository architecture manages content as well as metadata.• The repository offers a minimum set of basic services, e.g., put, get, search, access control. • The repository must be sustainable and trusted, well-supported and wellmanaged.The term data library is usually used in the literature to refer to subject specific datasets (e.g., climate data library, time series data library, geospatial data library). Moreover, a data library tends to house local data collections and provides access to them through various means. Thus, in general a data library usually provides access to the complete dataset instead of providing the basic services (e.g., search, put, get) a data repository offers.Around the middle 1990s the term digital library (previously also known as electronic library or virtual library) was first made popular by the NSF/ DARPA/NASA Digital Libraries Initiative. According to Data warehouses Similarly to data repository, it is also possible to find many different meaning and definitions to what is a knowledge base. Yet, in general, a knowledge base is a central repository of knowledge artifacts. Usually a knowledge base may use an ontology to formally represent its content and its classification scheme, but it may also include unstructured or unformalized information expressed in natural language or procedural code. Also, in contrast to a data repository, usually the purpose of the knowledge base is to allow automated deductive reasoning over the stored knowledge (i.e., decide how to act by running formal reasoning procedures over the base of knowledge).It is not surprising that some years ago, the ontology and semantic web community became interested in using repositories to hold semantic content (e.g., ontologies). Within the last years, ontologies have seen an enormous development and application in many domains, especially in the context of the semantic web. Academia and industry are developing and using ontologies to provide new technologies and support daily operations. Therefore, currently there exists a large amount of ontologies developed by many different parties which makes necessary the means to share and reuse them.Initial efforts to collect the base of existing ontologies proposed the creation of library systems (i.e., known as Ontology library systems) that offered various functions for managing, adapting and standardizing groups of ontologies Currently, efforts are put in the creation of ontology repositories. An ontology repository is similar to what Ding et al defined as an ontology library system Generic Ontology Repository Framework
Ontology reuse is still rarely encountered today. This is partly due to the problem of finding suitable ontologies to reuse, and the way most ontologies are created, namely without reusability in mind. Also, most of the established ontologies containing domain knowledge are simply too big to be easily reused, and no quality information is available on web ontologies.We argue that ontology engineers can adopt from software engineers a way how ontologies could be designed, namely modular. This way, small, reusable components (ontology modules) are produced during creation. To manage and provide access to ontologies we propose an Generic Ontology Repository Framework GORF with specific module and rating support. So not only ontologies or modules can be found in a single place, one can also see reviews about their quality or their usefulness in different scenarios. This way ontology engineers have a one-stop-shop for reusable knowledge artifacts.The term ontology repository can be seen as evolved term coming from the classical understanding of data repositories Definition 1 (Ontology Repository and Management System). An Ontology Repository (OR) is a structured collection of ontologies (schema and instances), modules and additional meta knowledge by using an Ontology Metadata Vocabulary. References and relations between ontologies and their modules build the semantic model of an ontology repository. Access to resources is realized through semantically-enabled interfaces applicable for humans and machines. Therefore a repository provides a formal query language.
Software to manage an ontology repository is known as Ontology Repository Management System (ORMS). An ORMS is a system to store, organize, modify and extract knowledge from an Ontology Repository.The main driving motivation creating ontology repositories is to support knowledge access and reuse for humans and machines. Hence ontology repositories on the one hand act as a storage facility and on the other provide access to knowledge through defined interfaces and policies. To achieve these goals, comprehensive facets must be considered by an ontology repository when handling ontologies. In general, these facets can be separated into access-related and storage-related aspects. A general requirement is that ontology repositories can support the entire ontology lifetime, i.e., ranging from the ontology engineering process to the desired application within specialized tools or tasks. Additionally, long term knowledge conservation is one of the crucial ontology repository tasks.On a technical level, practical realizations of ontology repositories might differ in their concrete implementation. In contrast to that, relevant components or services on a conceptual level are reusable among different technical solutions. Consequentially, we now present a conceptual framework for ontology repositories. Based on different ontology repository implementations and realizations in the past To be more specific the Generic Ontology Repository Framework (GORF) extends conceptually the SEAL (SEmantic portAL) Ontology Repositories
The framework for ontology repositories includes five conceptual layers. These layers can be seen as knowledge workflows, from the bottom to the top layer. In the following, we discuss each layer briefly.Knowledge Access
Maintaining the vision of the Semantic Web To sum up, the following aspects show up as elementary access functions:• Presentation and visualization: The access layer generates flexible graphical user interfaces for users in different formats, e.g., HTML output. Underlying templates define where, how, and when the framework presents knowledge to the user. Furthermore, the framework is able to dynamically generate ontology browsing interfaces and navigation bars. So, the presented framework for ontology repositories must provide several views of the stored knowledge. The presentation layer generates graphical user interfaces for users, e.g., by producing HTML output. Underlying templates define where, how, and when the framework presents knowledge to the user. Portals based on SEAL dynamically generate the ontology's browsing interface and navigation bar. To support users interacting with the portal, we developed a context-sensitive help system that provides useful tips and explanations based on the current context. • Searching and querying a repository: Our framework offers several search and query functionalities to the user. These include both standard fulltext search forms and complex query forms, such as allowing a query for specific concepts or using simple query logic for a set of queries.• Personalization: Although an ontology represents a commonly shared conceptualization of a domain, users typically have their own personal views and may request different visualizations. So, a semantic portal, especially a community one, should provide personalization services, which can become key enablers for successful repositories.Besides functionalities for accessing knowledge and rendering its presentation, the access component defines the interface used by the ORMS to manage the OR.Knowledge Processes and Services
Crucial to ontology repositories are processes and services for handling the stored knowledge within the repository:• Rating: GORF will support an Topic-Specific Trust Open Rating System (TS-ORS) that can provide means to ensure the quality of ontologies and modules in the repository. Open Rating Systems (ORS) • Mapping: Mapping -frequently also called alignment -of ontologies is a core task to achieve interoperability and therefore a foundational service of an ontology repository. Because most ontologies reflect a subject-oriented view of the world, different people will model knowledge differently. Being able to link these different representations is important for the success of the Semantic Web. Thus, an ontology repository needs to support mapping mechanisms. • Evaluation: In contrast to ontology rating that is a subjective assessing of an ontology, ontology evaluation can be seen as an assessment of the quality and the adequacy of an ontology or parts of it regarding a specific aim, goal or context. So far, several methods for evaluating ontologies have been proposed. An overview can be found in The ORS partly addresses this issue in GORF. • Security: Due to intellectual property rights, commercial licenses, patents or copyrights, not all knowledge artifacts may be accessible by the public. Therefore clear access control and right management functionalities are required. While knowledge access might be restricted, meta knowledge like OMV Additional processes or services might be added or attached through an extension component. For example, reasoning or validation services might be included here.Knowledge Organization
The developed framework GORF can handle massive amounts of knowledge stored in a repository. Additionally, the technology allows for having multiple portals as access points on top of one ontology repository. Using the knowledge representation mechanism, we developed several continuative knowledgeorganization methods to provide fast and effective access to knowledge:• Modularization: The introduced framework aims to use ontology modules as key elements to be stored in a repository -alongside existing ontologies -for the reasons motivated beforehand. The core idea behind ontology modularization is the identification of reusable knowledge artifacts which are adaptable to different tasks and remain domain-independent. In contrast to an ontology, which aims at providing a domain-specific conceptualization in one construct for a set of tasks, a module represents a shared, domain-independent conceptualization which is adaptive to and intended for re-occurring tasks and applications. In general, ontology modules are comparable to software libraries in the software engineering domain.A key pre-condition for modularization is an expressive modeling language for ontologies, e.g., The main principle of modularization is described in an abstract way whereby we distinguish the process of modularization on existing ontologies and modularization whilst an ontology is being created. The first task, modularization on existing ontologies, can be considered as an ontology re-engineering task and the latter one represents an ontology engineering task. We assume that in most cases the effort required to identify useful modules in large and possibly unknown ontologies is too expensive for manual modularization. So automatic or at least semi-automatic mechanisms are required. First steps in this direction can be found in When new ontologies are created, they can be designed with a modulebased approach in mind. Software engineers are used to program following the object-oriented modeling paradigm • Lifecycle: The support for evolution of knowledge, in particular for ontologies, is a major requirement for ontology repositories. In contrast to static content, ontologies are changing and demand mechanisms for updating and evolving knowledge over time. Analogically to the Dublin Core Metadata Initiative's (DCMI) Metadata Registry,Summarizing, the knowledge organization layer provides efficient methods for handling and organizing knowledge in repositories.Knowledge Storage
To support the envisioned large, scalable application scenario of GORF, we use a highly scalable storage mechanism. Distributed repositories are set up in a cluster for handling several requests. Therefore the storage layer includes components for querying, transactions and replication of knowledge within such repositories.• Query processor: The query processing component handles queries for single knowledge artifacts in a repository. As query language we prefer standardized languages like the well-known SPARQL 11 query language: • Transaction and consistency: Performing and handling access to knowledge simultaneously requires sophisticated mechanisms preventing inconsistencies. Thus, transaction and consistency components analyze and check queries against the underlying knowledge storages. • Replication: Being designed for scenarios with a high number of users and queries, GORF provides adaptable knowledge replication mechanisms. Those mechanisms replicate knowledge storages and additionally distribute them among pre-defined spaces.The knowledge storage layer provides mechanisms for handling and accessing distributed knowledge sources, which are described below.Knowledge Sources
The presented approach provides a sophisticated framework for integrating knowledge from different sources like files, data bases, ontologies or other semantic portals. The framework is capable of using existing sources along with their attached infrastructure. Therefore, our framework can rest atop existing technologies and act as a kind of semantic layer for these technologies to use the developed integration mechanisms.Knowledge sources are typically distributed and heterogeneous, and tend to change during semantic interrelation, aggravating the task of integrating information into one common knowledge repository. The layer comprises two modules. The generic knowledge integration module shares and integrates knowledge from previously unknown sources. The interconnected-integration module handles sources that are closely interconnected technically and semantically. This module mainly integrates content such as other portals and semantic metadata.Ontology Repository Management Systems
An Ontology Repository Management System (ORMS) is a semanticallyenabled software to store, organize, modify and extract knowledge from an Ontology Repository. Two systems, namely OysterIn general, the main tasks of an ORMS are providing access to knowledge resources, supporting retrieval and allocating sufficient management mechanisms.Retrieval
Ontology retrieval for humans and machines is a key functionality of an ontology repository management system. The retrieval component provides mechanisms to manage search and discovery functions of an ontology repository. For example consider the allocation of indices or the provision of metadata.Browsing and Navigation
Semantically-driven navigation through knowledge stocks enables users to identify new and potentially useful knowledge artifacts within a repository. The navigation through repositories can be guided by specialized ontologies for semantic navigation, as introduced in Management
An ontology repository is administrated through a management component which contains all administrative functionalities required to store, organize and maintain the knowledge within a repository. In general, manageable components in a repository provide interfaces to the management component. The main task hereby is to collect all manageable functionalities and to enable a standardized and centralized access to all relevant administrative functionalities. The entire business logic is implemented within each repository component itself. Thus, the management component itself does not contain real business logic. As a result, components within GORF are easily interchangeable and the whole framework remains flexible and scalable.To sum up, an Ontology Repository Management System (ORMS) is a powerful tool to manage ontology repositories, even several distributed ones together. This way, established workflows and processes can be easily interchanged among other repositories reducing maintenance efforts and increasing the usability of such repositories.Centralized Vs. Decentralized Systems
We now present exemplary running systems based on GORF. In detail, we present two complementary applications, namely the decentralized P2P system Oyster and the centralized ontology portal Onthology. In general, the two tools differ in their usage perspective and are appropriate for different tasks. However, as we will see, only the combined application of both tools will offer users the full potential of ontology management.Centralized Systems
Ensuring a scalable and reliable access to ontologies, optimization techniques are required. One well-known approach is a hybrid storage mechanism from the data warehouse area which materializes content to provide faster access. We present the conceptual design of a centralized ontology portal and its implementation, so-called Onthology standing for "anthology of ontologies."Scope
Centralized systems allow to reflect long-term community processes in which some ontologies become well accepted for a domain or community and others become less important. Such well accepted ontologies and in particular their metadata need to be stored in a central metadata portal which can be accessed easily by a large number of users whereby the management procedures are well defined. Hence, a main goal of a centralized metadata portal is to act as large evidence storage of metadata resp. their related ontologies to facilitate access, reuse and sharing as required for the Semantic Web.Actors
We identified several different user roles for Onthology: The visitor is an anonymous user, he is allowed to browse the public content of the portal. A visitor can become a user by completing an application form on the website. In order to avoid unnecessary administrative work, a user is added automatically to the membership database. Users can customize their portal, e.g., the content of their start-page or their bookmarks. If a user wants to submit metadata to the portal, this submission has to be reviewed before it is published. Onthology establishes a review process in order to ensure a certain level of quality. Reviewers check the new submissions before they are published. The technical administrator is responsible for any other task mainly the maintenance of the portal.Functionalities
Functionalities of Onthology can be separated into two groups based on the usage. Indeed, basic functionalities which are provided to every user who accesses the portal and sophisticated functionalities for reviewers and administrators. The main operations a user can perform on the repository are (1) Search, (2) Submit and (3) Export.The search and export can be performed by any visitor without being registered to the repository. Since providing new metadata is based on a certain community confidence, a visitor has to register at the portal to be able to submit data.Architecture
Onthology consists of an ontology repository and an ORMS. Exemplary, Sesame Decentralized Systems
In this section we describe the distributed ontology registry (Oyster).Oyster Oyster Design
The Oyster system In Oyster, ontologies are used extensively in order to provide its main functions described in the following:Creating and importing metadata: Oyster enables users to create metadata about ontologies manually, as well as to import ontology files and to automatically extract the ontology metadata available, letting the user fill in missing values. The ontology metadata entries are aligned and formally represented according to two ontologies: (1) the OMV ontology, (2) a topic hierarchy (i.e., the DMOZ topic hierarchy), which describes specific categories of subjects to define the domain of the ontology.Formulating queries: A user can search for ontologies using simple keyword searches, or using more advanced, semantic searches. Here, queries are formulated in terms of these two ontologies. This means queries can refer to fields like name, acronym, ontology language, etc., or queries may refer to specific topic terms.Routing queries: Users may query a single specific peer (e.g., their own computer, because they can have many ontologies stored locally and finding the right one for a specific task can be time consuming, or users may want to query another peer in particular because this peer is a known big provider of information), or a specific set of peers (e.g., all the members of a specific organization), or the entire network of peers (e.g., when users have no idea where to search), in which case queries are routed automatically in the network.Processing results: Finally, results matching a query are presented in a result list. The answer of a query might be very large, and contain many duplicates due to the distributed nature and potentially large size of the P2P network. Such duplicates might not be exact copies because of the semi structured nature of the metadata, so the ontologies are used again to measure the semantic similarity between different answers and to remove apparent duplicates. As proposed by the ontology metadata standard, all the different realizations of an ontology (ontology documents) can be grouped by the same ontology base to give a more organized view of the results.Oyster Architecture
The high-level design of the architecture of a single Oyster node in the Peerto-Peer system is shown in Fig. The Local Repository of a node contains the metadata about ontologies that it provides to the network. It supports query formulation and processing and provides the information for peer selection. In Oyster, the Local Repository is based on KAON2 and it supports SPARQL as its query language.The Knowledge Integrator component is responsible for the extraction and integration of knowledge sources (i.e., ontologies) into the Local Repository.Oyster supports automatic extraction of metadata for OWL, DAML+OIL, and RDF-S ontology languages. This component is also in charge of how duplicate query results are detected and merged.The Query Manager is the component responsible for the coordination of the process of distributing queries. It receives queries from the user interface, API or from other peers. Either way it tries to answer the query or distribute it further according to the content of the query. The decision to which peers a query should be sent is based on the scope of the query (i.e., a specific set of peers or entire network) and optionally on the knowledge about the expertise of other peers. Fig. 2. Overview of Oyster architecture
The Informer component is in charge of proactively advertising the available knowledge of a Peer in the Peer-to-Peer network and to discover peers along with their expertise. This is realized by sending advertisements about the expertise of a peer. In Oyster, these expertise descriptions contain a set of topics (i.e., ontology domains) that the peer is an expert in. Peers may accept these advertisements, thus creating a semantic link to the other peer. These semantic links form a semantic topology, which is the basis for intelligent query routing.The Peer-to-Peer network sub-layer is the component responsible for the network communication between peers. It provides communication services for the data exchange with remote nodes, i.e., to propagate advertisement messages and to realize the access to remote repositories. In Oyster, we rely on an RMI-based implementation, however, other communication protocols would be possible as well.The API, WS and GUI components provide alternative ways for accessing Oyster functionalities, i.e., the API defines a set of methods that expose all of the functionalities, the web service encapsulates the API exposing a reduced set of functionalities and the GUIs provide ready-to-use clients for the Oyster network.Additional registry functionalities can be provided by engineering components. Some of these components are described in Discussion
Both presented applications are covering a variety of different tasks. Indeed, for a user who wants to store metadata individually similar to managing his personal favorite song list, a repository is required to which a user has full access and can perform any operation (e.g., create, edit or delete metadata) without any consequences to other users. Exemplary, users from academia or industry might use a personal repository for a task-dependent investigation, or ontology engineers might use it during their ontology development process to capture information about different ontology versions. We argue, that a decentralized system is the technique of choice, since it allows the maximum of individuality while it still ensures exchange with other users.Centralized systems allow to reflect long-term community processes in which some ontologies become well accepted for a domain or community and others become less important. Such well accepted ontologies and in particular their metadata need to be stored in a central metadata portal which can be accessed easily by a large number of users whereby the management procedures are well defined. Obviously, personal repositories are quite limited from this perspective. Actually, the Oyster system and Onthology are not necessarily two completely separated repositories. Indeed, they are interconnected and they exchange metadata between each other. We are currently supporting the access of metadata stored in Onthology from any Oyster peer.The benefit of connecting both systems lies mainly in the simple use of existing ontology metadata information within Oyster. So, while users are applying or even developing their own ontologies they can manage their own metadata along with other existing metadata in one application (in Oyster). If some metadata entries from Oyster have reached a certain confidence, an import into Onthology can be performed easily. In combination, both systems ensure efficient and effective ontology metadata management for various use cases.Conclusions
Ontology repositories will be a crucial cornerstone facilitating efficient knowledge access and reuse especially in the context of the Semantic Web. We have presented our Generic Ontology Repository Framework GORF including rating and module support. We expect that there will be a shift in ontology engineering towards developing ontologies in a modular way. We are optimistic that then the critical mass of ontology modules in our repository can be reached, and ontology engineers will start reusing them and providing new ones.Already existing realizations like Onthology and Oyster illustrate the benefits of such systems. We assume that ontology repositories will play an important role in realizing the Semantic Web vision.Ontology Mapping
Natalya F. Noy
Stanford University, Stanford, CA, 94305, USA, noy@stanford.eduWhy Is Ontology Mapping Difficult?
A quick scan through ontologies mentioned in this book, would indicate that many ontologies in use today overlap in content. Even for such, seemingly uncontroversial, domains, as anatomy, there are several ontologies representing them. Consider, for instance, the ontology repository at the National Center for Biomedical Ontologies However, if we want to have the applications using different ontologies to "talk" to one another, or if we want to integrate data that is annotated with or structured according to different ontologies, we must first find the correspondences between concepts in these ontologies. The process of finding such correspondences is called ontology mapping. Ontology mapping (also referred to as ontology matching, or ontology alignment) is one of the most active areas of ontology research. Creating high-quality ontology mappings automatically is the holy grail of the Semantic Web research. Ontologies have gained popularity in the AI community as a means for establishing explicit formal vocabulary to share between applications. Therefore, one can say that one of the goals of using ontologies is not to have the problem of heterogeneity at all. It is of course unrealistic to hope that there will be an agreement on one or even a small set of ontologies. While having some common ground either within an application area or for some high-level general concepts could alleviate the problem of semantic heterogeneity, we will still need to map between ontologies, whether they extend the same foundational ontology or are developed independently.We define an ontology mapping as a set of correspondences between components of two ontologies. These correspondences can be equivalence relationships, they can be subclass or superclass relationships, transformation rules, and so on. The process of finding ontology mapping is often referred to as ontology matching.So, what are the types of differences between ontologies? In part summarizing earlier surveys, Klein However, even for ontologies expressed in the same language, possible ontology-level mismatches abound. A partial list of ontology-level mismatches includes using the same linguistic terms to describe different concepts; using different terms to describe the same concept; using different modeling paradigms (e.g., using interval logic or points for temporal representation); using different modeling conventions and levels of granularity; having ontologies with differing coverage of the domain, and so on.Let us start with an example to illustrate the problem. We will use this example throughout the chapter. Suppose we have two airlines and ontologies describing their two respective reservation systems. Figure In both ontologies, this class has a number of properties describing the reservation. In the ontology for the first airline (the top figure, white rectangles representing classes), there is a reservation number (string property reservationNumber), the date the reservation was made (reservationDate), the price of the ticket, the string representing the airports where the flight departs from (from) and where it lands (to). The records representing the time and date of the departure and arrival (instances of the TimeAndDate class) are values for the departure and arrival properties. There is a reference to the aircraft (property aircraft pointing to a class PlaneModel) and a property where all passengers are listed (passengers). Each passenger record is an instance of class Passenger, or, more specifically, one of its subclasses, Child or Adult.The second ontology (the bottom figure, gray rectangles representing classes) has a similar structure: each reservation also has a number  (recordLocator) and the date it was made (reservationDate). There is information about the price of the ticket (property price) and the aircraft (property aircraft with values that are instances of the class Aircraft, or, more specifically, one of its subclasses; two of the subclasses, Boeing and Airbus are presented in the figure; there could be many more, one for each aircraft maker). Departure and arrival are represented as instances of the class AirportAndTime that encapsulates both the airport and the departure date and time. Passenger list for the reservation is also a collection of instances of the class Customer that looks quite similar to the Passenger class in the first ontology. Now suppose the two airlines decided to merge. The merge means that the airlines must integrate their reservation systems. This integration requires reconciliation of the two different ontologies used to describe the reservations in the two airlines. The result of this reconciliation would be an ontology mapping that would enable transformation of reservation records into a single database.The two ontologies look rather similar, the information that they capture is roughly the same and the level of granularity for this information is very comparable; many terms are identical or similar as well. However, after careful examination, we can see that the mapping is not at all straightforward even for a human expert, let alone for tools that attempt to determine the mapping automatically:• We can say that the two classes Reservation are similar to each other: their names are the same and they represent the same information. • It is easy for a human expert to see that the properties reservationNumber and recordLocator are equivalent, but it is not clear how an automatic system can identify this fact; as humans, we have enough domain knowledge to know that these terms usually refer to the same concept in the context of airline reservations. • Both Reservation classes have a property reservationDate. However, in the first ontology the values of this property are instances of the class Date and in the second they are simply strings. In the merged ontology we will have to choose one representation or the other and to convert dates from one format to another when we reconcile the records. • Both ontologies have the property price, which has integer values. One would think that this property is very easy to reconcile. However, it is easy to imagine that in one system the price refers to the price for the whole reservation and in the other it is a price of a single ticket and needs to be multiplied by the number of passengers to get the price for the full reservation. Note that just by looking at the ontology, we simply cannot tell what the price refers to in either case. We need additional information, for example in the form of documentation to determine whether the price properties in the two ontologies are equivalent.• In the first ontology, the information about the departure and arrival location and the time and date of arrival is represented directly in the reservation record. In the second ontology, the departure and arrival information (the airport and the date and time) are encapsulated as instances of the class AirportAndTime. • Classes PlaneModel and Aircraft are equivalent -they both represent various aircraft makes and models -but have different names. • The classes Passenger and Customer have different names but are equivalent in this context. Note that in general, for any two ontologies, classes named Passenger and Customer may not be equivalent, but they are in this context and given the structure of these two ontologies. • In the first ontology, there are two subclasses of the class Passenger:Child and Adult. In the second ontology, the breakdown is different, with the class Infant representing children under 2. Thus the class Child in the first ontology is the union of the classes Child and Infant from the second; the two classes Child are not themselves equivalent. • Finally, while in the first ontology the various makes of an aircraft are represented as a string property make of the class PlaneModel, in the second ontology this distinction is made in different subclasses, such as Boeing and Airbus.As you can see, even such small example of two ontology snippets with very similar domain coverage and granularity, poses many difficulties in both determining the correspondences between classes and properties and finding them automatically.The goal of this chapter is to discuss the major thrusts of approaches to semantic integration produced by various projects in the ontology community and the user-centered tools that support the ontology mapping in practice. We do not attempt to provide a comprehensive review of the state of the art in ontology mapping. We refer the reader to an excellent and thorough review by Euzenat and Shvaiko We discuss four dimensions of ontology-mapping research in this chapter:Mapping discovery: Given two ontologies, how do we find similarities between them, determine which concepts and properties represent similar notions, and so on. Interactive specification of mappings: tools for enabling users and ontology developers to define the compare ontologies interactively, define the mappings, perhaps with the semiautomated help from the tool itself. Declarative formal representations of mappings: Given two ontologies, how do we represent the mappings between them to enable reasoning with mappings. Reasoning with mappings: Once the mappings are defined, what do we do with them, what types of reasoning are involved?In the rest of this chapter, we explore these dimensions.Discovering Mappings
Many researchers agree that one of the major bottlenecks in semantic integration is mapping discovery. There are simply too many ontologies and database schemas available and they are too large to have manual definition of correspondences as the primary source of mapping discovery. Furthermore, in the world where software agents will roam the (semantic) web, they will need to map structures they know about to new structures they come across on-thefly. Hence, the task of finding mappings (semi-) automatically has been an active area of research in both database and ontology communities Using a Shared Ontology
Recall that the goal of ontologies is to facilitate knowledge sharing. As a result, ontologies are often developed with the explicit goal of providing the basis for future semantic integration. Here, the vision is that a general upper ontology is agreed upon by developers of different applications, who then extend this general ontology with concepts and properties specific to their applications. A number of very general ontologies formalizing notions such as processes and events, time and space, physical objects, and so on, are being developed and some of them are becoming accepted standards (chapter "Foundational Choices in DOLCE"). The explicit goal of these ontologies is to have domain-specific ontologies extend them, thus providing the grounding in common vocabulary for these ontologies. Two of the ontologies that are built specifically with the purpose of being formal foundational ontologies are the Suggested Upper Merged Ontology (SUMO) While many researchers hope that domain-and application-specific ontologies will reuse the foundational ontologies, like SUMO and DOLCE, and that such reuse will indeed facilitate semantic interoperation between applications based on these ontologies, we do not yet have enough experience reports with such approaches to claim it a success. There are reports on both the successes There are also implemented semantic-integration tools that exploit the idea that if two ontologies extend the same reference ontology in a consistent way, then finding correspondences between their concepts is easier. For example, the Process Specification Language (PSL) Finally, the third approach is to use a reference ontology or terminology as background knowledge that helps in aligning the ontologies that need to be aligned. In this model, the source ontologies are first mapped to a reference ontology that serves one of the two purposes: (1) providing wider domain coverage to bridge the coverage gap of the source ontologies or (2) providing the additional structure and semantic richness that the source ontologies lack. For example, the developers of S-Match Structure-Based, Machine-Learning, and Other Approaches
It is certainly helpful to have ontologies that we need to match to refer to the same foundational ontology or to conform to the same reference ontology. However, we often do not have this "luxury" and need to create mappings between ontologies that perhaps use the same specification language but do not have any vocabulary beyond the specification language in common. Most researchers agree that automatic mapping between ontologies in this context is beyond our grasp at the moment, but many techniques have produced good results.Ontologies are often richly structured, with many links between definitions of classes and properties. Thus, many approaches exploit this richness by comparing various elements of the structure of the ontologies to be mapped. Consider again the example in Fig. Sabou and colleagues Many researchers have now shown that the real power of these various methods for discovering mappings lies in their combination. The tools that have been showing the most success in performance recently Researchers have also recognized that finding simple one-to-one mappings, essentially representing equivalent or similar concepts, is not sufficient. Two of the key aspects that researchers also look at are complex mappings and approximate mappings.Complex mappings express specialization or generalization relationship between concepts, or perhaps even contain an expression linking entities together. For instance, the class Child in the first ontology in Fig. In many cases, exact mappings either cannot be derived or simply do not exist: the entities from different ontologies may be related, but, have, for example, largely overlapping meaning. Consider, for instance, different frequent-flyer programs for airlines. The frequent-flyer program in an ontology describing one airline would be very similar but not exactly the same as a frequent-flyer program for another airline. Gligorov and colleagues Interactive Tools for Specifying Mappings
As the results of the OAEI tests show, even the best automatic tools for discovering mappings still leave a lot of work for the user to do: to verify the mappings and to add the ones that the algorithms missed.However, many of the ontology-mapping tools focus only on the algorithm and provide only rudimentary user interface. The most common interface is the use of command line to provide the file names or URIs for the source ontologies and to get a text listing of the mappings.Clio In Sect. 2, we focused on the tools to discover mappings and in Sect. 4 on various representations. We will now discuss an interactive tool with a graphical user interface that allows developers to leverage the best components of other tools easily by integrating them in a single plugin framework.In the Protégé group, we have developed a suite of tools to support users in the process of ontology mappings. While in the early days of the Prompt suite, we focused on algorithms We believe that in order for the ontology-mapping tools to "step out" of the research labs and to be adopted in real-world and industrial setting, both the performance of automatic ontology-mapping algorithms and the quality of cognitive support in ontology-mapping tools must improve. Recognizing that in many cases, researchers must focus on one or the other of these tasks, we have developed a plugin framework that covers the full spectrum of ontology mapping, from specifying algorithms for initial comparison to executing the mappings. We have developed a reference implementation for each of the steps, including a number of cognitive aids. Developers can plug in their own components and have the plugins developed by others (including our team) fill in the missing pieces to have a comprehensive end-user tool.The Prompt plugin framework Perform initial comparison of the ontologies: an algorithm compares two ontologies and produces a list of candidate mappings. Present candidate mappings to the user enabling him to analyze the results.This step includes components for cognitive support (various visualizations of the source and target ontologies, options to filter content presented in the display, etc.) and interactive comparison algorithms that are invoked either explicitly by the user or as a result of mappings being verified. Fine tune and save the mappings in a declarative mapping format. Execute mappings to transform instances from source to target or to perform other operations.In the current implementation, developers can replace components of any of the steps in this list, and our plan is to make all of the steps replaceable.For example, the integration of FOAM and Prompt is one of the Prompt plugins available as part of Prompt distribution. A developer of an algorithm plugin can specify not only how to invoke the algorithm, but also how the configuration screen presented to the user should look like. Representations of Mappings
While developing tools for automatic and semiautomatic ontology matching is a large thrust of semantic-integration research in the ontology community, it is definitely not the only one. The high expressive power of ontology languages provides the opportunity for representing mappings themselves in expressive terms. We will discuss several representations of mappings here: using the ontology language itself to express mappings; defining bridging axioms in firstorder logic to represent transformations; representing mappings as instances in an ontology of mappings; and using views to describe mappings from a global ontology to local ontologies.We can use the constructs provided by the OWL language itself to express mappings between concepts in different ontologies (chapter "Web Ontology Language: OWL"). To express equivalence between classes, properties and individuals, we can use the following three OWL constructs, respectively: owl:equivalentClass, owl:equivalentProperty, and owl:sameAs. For instance, we can say that a class Boeing in one ontology (Fig. Sometimes, however, we want to be more precise about the nature of the mapping and to separate the definition of the mappings from the definition of ontology concepts. The C-OWL language, for example, takes such approach In the OntoMerge system Several researchers use ontologies themselves to represent mappings declaratively, as instances in an ontology. The mapping ontology by Crubézy and colleagues Finally, researchers also used views to define mappings between ontologies, similar to defining mappings in information integration, both in global-asview (GAV) and local-as-view (LAV) setting. The OIS framework We Have the Mappings: Now What?
Naturally, defining the mappings between ontologies, either automatically, semiautomatically, or interactively, is not a goal in itself. The resulting mappings are used for various integration tasks: data transformation, query answering, or web-service composition, to name a few.Given that ontologies are often used for reasoning, it is only natural that many of these integration tasks involve reasoning over the source ontologies and the mappings. For example, the OntoMerge system mentioned earlier For the second task that OntoMerge deals with -generating ontology extensions -consider for example, two ontologies describing Web services: OWL-SIn the OIS framework The State of the Art
The best way to assess the state of the art in creating ontology mappings automatically is to look at the results of the Ontology Alignment Evaluation Initiative (OAEI). The OAEI organizers published the ontologies to be compared and the tool developers apply their tools to find correspondences between these ontologies. The organizers then compare the results to a set of reference alignments to determine precision and recall of the performance of individual tools. The ontologies range in the size and complexity, from relatively small ontologies that are designed primarily to understand which features of ontologies tools take into account, to large "real-world" ontologies representing such complex domains as anatomy.The performance of the tools varies depending on the setting. For the simplest test cases, the best tools have close to perfect recall and precision. For the large more complex test cases such as anatomy ontologies, for example, the F-measure (the harmonic means of precision and recall) for the best performing generic domain independent tools (such as Falcon-AO The data provided by the OAEI initiative has affected quite dramatically the field of ontology-mapping algorithms by providing a well-documented and well-studied set of reference alignments that tool-developers can use to assess their methods: It is impossible to publish a paper about an ontology-mapping algorithm today without providing the results of how the algorithm performs on the OAEI data set. It is important to note, however, that OAEI does not evaluate all types of ontology-mapping systems: For instance, this type of evaluation is not well suited for interactive ontology-mapping tools, such as Prompt. At the same time, the OAEI results show that user interaction will still be required in the foreseeable future if an application needs precise and complete mappings.Finally, in addition to improving precision and recall of ontology-mapping algorithms, there are a number of other fundamental questions that researchers are addressing: How do we explain the mappings produced by the tools to the users? What are the best ways to support interactive ontology mapping? Are imperfect or inconsistent alignment useful in some settings or must alignments always be 100% precise to be useful? What are the settings that con tolerate approximate mappings? What are the levels of precision and recall that make such imprecise mappings useful? How do we maintain mappings between ontologies as the ontologies evolve? When ontologies change, many of the mappings remain valid, but some are probably invalidated? How do we know which ones? Can we design a "tollbox" of mapping approaches that we can custom-tailor to fit a given problem? Can generic domain-independent ontology-mapping methods perform acceptably well, or do we need domain-specific approaches? Researchers are actively investigating these and many other intriguing challenges in ontology mapping and we can fully expect this research area to continue to provide new advances and to make automatic semantic integration -that holy grail of the Semantic Webmore attainable.Introduction
Fast growth of communication and mobile technologies, constant demands for new services, and increased number of computer users, are some of the key reasons of the constantly increasing need for more software. This naturally requires effective methods for engineering software that will be able to respond adequately to the needs for which the software was built, and yet to allow for higher levels of productivity of software engineers. However, today's state of the art and practice demonstrates that both perspectives are still suffering from serious problems. On one hand, the Standish Group published its wellknown "Chaos Report" in 1994 in which it was noted that only 16% of software projects were successful, 31% were failures, and some 53% were challenged. The 2006 report demonstrates a bit better situation where 35% of software projects were successful, 19% were failures, and 46% were challenged While software is a technical category designed to perform specific tasks by using computer hardware, it is also a social category which nowadays is used in almost every aspect of human's life. In fact, software is a knowledge repository where knowledge is largely related to the application domain, and not S. Staab and R. Studer (eds.), Handbook on Ontologies, International Handbooks to software as an entity Researchers have so far proposed many different synergies between software engineering and ontologies. For example, ontologies are proposed to be used in requirement engineering In this chapter, we start from defining software engineering as an application context for ontologies, and proceed to defining a framework that identifies places in software life cycle where ontologies can contribute to improve the current state of software engineering. We consequently have organized the structure of this chapter to use this framework for analyzing the use of ontologies in different phases of software life cycle. Note that the chapter does not discusses Semantic Web rules (see chapter "Ontologies and Rules") or upper layers of the Semantic Web cake, but fully focuses on ontologies in software engineering.Software Engineering
The goal of this section is to define software engineering, describe some typical software life cycle phases, artifacts used and produced in them, participants, their interactions, and relevant domain and application knowledge. Based on this discussion, we define a unified framework for the use of ontologies in software engineering to which we are going to refer in the rest of the chapter.The most commonly used definition of software engineering is the one given in the IEEE Standard Glossary for Software Engineering • Analysis phase determines what has to be done in a software system. After determining what kind of software is needed to be developed, the requirements phase is the first and the most important step. In this phase, the requirements for a software product are defined and documented. This is usually done in collaboration with end-users and domain experts, where the critical point is to establish common understanding of the domain under study. Once requirements are defined, they are formally specified in the form of a legal document. Typically, this document includes functional requirements, performance requirements, interface requirements, design requirements, and development standards; to eliminate all ambiguousness, incompleteness, and contradictions. Modeling approaches are recommended at this stage (e.g., RUP recommends using UML use cases and class diagrams), while some researchers recommend using even some more formal approaches (e.g., Petri nets Besides importance of a high-quality and up-to-date documentation, this stage also requires testing, such as acceptance testing (by end-users) and integration testing (i.e., checking the integration with other components). • Maintenance phase is the process of modifying a software system or component after delivery to correct faults, improve performance or other attributes, or adapt to a changed environment, i.e., any change after acceptance of the software by the client. This phase highly depends on the quality of documentation in order to trace parts of software to be changed. Of course, this phase also assumes documenting all changes as well as testing software for its compliance to the initial and newly-defined requirements. • Retirement phase is the period of time in the software life cycle during which support for a software product is stopped. This may happen in cases where a drastic change in design, implementation, or documentation has occurred. This phase also has to be well-documented to explain why a software product is retired.However, the current software practice suffers from a lack of traceability of all artifacts and elements produced/used in different stages of the life cycle (e.g., requirement documents and source code), that can substantially affect software development and especially software maintenance It is also very important to mention that every software product strongly relies on the application-specific domain knowledge, standards, and policies related to the software system under study. In addition, every software development process follows some methodologies,Analysis
According to the Standish Group report from 1994,Ontologies as Requirement Engineering Products
The above arguments motivated researchers to look at ontologies as a solution to improve the state of the art in this area. Breitman and Leite argue that ontologies should be sub-products of the requirement engineering phase The use of upper-level ontologies is also well-known in software engineering when developing domain models that are usually part of the requirement specification. Typically, an upper-level ontology (e.g., Bunge-Wand-Weber [BWW] model) is used as a definition of the background theory (or the perspective to the world) based on which the domain model is built. Current software development methodologies (e.g., RUP) suggest UML-based domain models as the results of the analysis phase. The current experience demonstrates that if one wants to make such domain models valid w.r.t. the upper level ontology, then a modeling language should be constrained in order to allow the use of only those models that are compliant to the upper ontology. For example, Evermann and Wand Requirement Engineering Approaches
Requirement engineering phase assumes the use of many different sources, which are not only end-users and domain experts, but also policies and standards. Requirement engineering also implies the use of different methodologies such as goal-driven, viewpoints-oriented, and scenario-based approaches, or their combinations Requirement Engineering Collaboration
Collaboration appears to be the crucial activity in successful requirements engineering, especially in the current global software development landscape. The main challenges to be addressed are Not only are Wikis means of collaboration in requirements engineering, but stakeholders also communicate by other communication channels and tools such as chats and discussion forums Requirements Verification
Testing of identified and specified requirements is a critical activity of the analysis phase, as it is very important to make sure that all involved stakeholders with different backgrounds and levels of knowledge agree upon the requirements specification. Probably, the most effective way is to use formal model-based animations (e.g., UML use-cases and classes) that present defined requirements. However, as UML does not have formally defined semantics, it is very hard to run simulations that formally analyze the models defined However, Petri nets are a formalism for modeling processes rather than for modeling a structure (e.g., domain model) of a system under study. The question is then how to combine domain ontologies developed in some of the above-mentioned ways and process formalisms such as Petri nets? Brockmans et al. proposed a mechanism for semantic annotation of Petri nets by using concepts from domain ontologies Design
As already mentioned, the design phase assumes a comprehensive definition of the software system under study. As a result, this phase heavily relies on the use of modeling principles and best software practices such as software patterns. Due to the importance of modeling in this phase, in this section, we first introduce model-driven engineering (MDE) as a software engineering discipline that promotes software development fully based on modeling principles. Then, we discuss how MDE helps to integrate ontologies into software design, and finally, we conclude this section by discussing how ontologies can be applied to improve the use of design patterns.Model-Driven Engineering
Model Driven Engineering (MDE) is a new software engineering discipline in which the process heavily relies on the use of models MDA consists of three layers, namely: M1 (model) for defining models of systems under study; M2 (metamodel) for defining modeling languages (e.g., UML and Common Warehouse Metamodel [CWM]); and M3 (metametamodel) where only one metamodeling language is defined (i.e., MOF) MDE and Ontologies
Cranefield was the first to explore the synergy of software modeling languages and ontologies The above-mentioned activities initiated a standardization process at the Object Management Group (OMG) to issue a request for proposals for the Ontology Definition Metamodel (ODM) in 2003. The aim was to define a MOF-based metamodel for the OWL (cf. chapter "Resource Description Framework") and RDF(S) (cf. chapter "Web Ontology Language: OWL") ontology languages (i.e., ODM), corresponding ontology UML profile (to use standard UML tools for modeling ontologies), and transformations between ODM and other relevant ontology and modeling languages. These activities resulted in the OMG's ODM specification Note also that application (and domain) ontologies can also be used in the design of software architectures. Grønmo et al. demonstrated how MDE principles can be used to model Semantic Web services (i.e., OWL-S) by extending UML activity diagrams Software Models and Business Vocabularies
Not always should domain ontologies be defined in the analysis phase, but the requirements specification can be only in the form of documents written in natural language. This implies that we should define our domain models (i.e., ontologies) in the design phase from scratch. So, for this task it will be useful to have an automatic or a semi-automatic approach to produce ontologies for requirement documents (see chapter "Ontology Engineering Environments"). Moreover, we should also be able to update textual requirement documents automatically with the changes of ontologies.Semantics of Business Vocabulary and Business Rules (SBVR) is a promising solution to the above problem Ontologies and Model Reasoning
Software modeling tools are usually very intuitive and allow software designers to use a visual notation of modeling languages (e.g., UML). However, today's software modeling tools lack the support for formal validation of software models, and discovering some potentially hidden implications of such models (e.g., inconsistencies and redundancies), which may impact the overall quality of software designs Ontologies and Model Transformations
Model transformation plays an important role and represents the central operation for handling models in MDE Both of these applications of ontologies to model transformations have been recognized as valuable contributions to the MDE area. However, there are many important research questions that should be solved such as: combinations of both approaches to make the process of model transformation more effective; applying ontologies and ontology matching at the model (M1) level of MDA, for example, to improve software refactoring; and applications of ontology matching to contribute round-trip engineering (i.e., code generation and reverse engineering) by complementing the efforts for model-to-text and text-to-model transformations Ontologies and Software Patterns
Using the experience form the urban architecture, software engineering adopted the concept of software patterns as an attempt to describe successful solutions to common software problems. The pattern, in short, is a thing, which happens in the world, and the rule which tells us how and when to create it. A pattern language is a network of multiple patterns, with links between related patterns. The most known type of software patterns are design patterns which nowadays are used in almost all applications. Patterns are, in fact, shared knowledge of software engineering, and represent a way for common understanding of software designs. Patterns are described in literary forms, such as sonnets. This works fine if patterns are intended to be understood by software engineers, but if they need to be interpreted by tools, there is a need for a formal representation of patterns Implementation and Integration
The design of software products should specify how the system should finally be implemented and integrated with other software systems, so that the software product eventually accomplishes the requirements initially set. This phase usually looks at lower computer-specific details and is done by using programming languages. Although the goal of MDE is to allow for automatically generating as much implementation code as possible along with many promising results, the current state of the art indicates that many implementation details should still be done manually. This section explores the potentials of using ontologies in the implementation and integration phases.Implementation
In this section, we distinguish between three different approaches to the use of ontologies in software implementation.First, as already indicated, some approaches claim that ontologies could be used in the same manner as models in MDE. Thus, we should be able to generate the implementation of a software system from an ontology, possibly the domain ontology that we created in the analysis phase and refined in the design phase. Following this approach, Cranefield created transformations of UML models to Java code (e.g., classes) besides the RDF(S) ontologies Second, given the AI origins of ontologies, ontologies can also be used in the implementation of software systems in a more declarative way, but yet to use conventional object-oriented programming languages (e.g., Java). HP's Jena Semantic Web framework offers a Java API for handling RDF(S) and OWL ontologies. Examples of alternatives for Jena are the Protégé OWL API and Protégé-Frames API Third, ontologies can be used as a part of the implementation logic in software systems that are implemented by using rule-based languages (e.g., Jess or JBoss Rules). This is the most flexible software implementation approach, as it not only allows for dynamically changing ontologies, but also rules. Then, an inference engine is responsible for execution of rules. Given that most of rule languages define rules over vocabularies and ontologies, this implementation technique can nicely be applied to ontologies Besides the above-mentioned approaches, the use of ontology-based semantic annotations can additionally improve software development life cycle. For example, Java annotation mechanism can be used to semantically interconnect parts of Java code and ontology conceptualization. Not does this can only be useful for JavaBeansIntegration
The most important contribution of ontologies to software integration is semantic Web services. Semantic Web services, as the augmentation of Web service descriptions through Semantic Web annotations, facilitate the higher automation of service discovery, composition, invocation, and monitoring on the Web. In this section, we focus on a relevant topic: semantic middleware.The concept of middleware is applied to managing heterogeneity of various software components and technologies used in a distribute software system. However, it is very important to have environments for developing such middleware-based distributed systems. Application servers are componentbased middleware platforms that provide functionalities for developing distributed systems which can use the components developed the developers or third parties. The current management of the functionalities of application severs is based on the use of administrative tools and XML configuration. While this brings a lot of flexibility, there are still many complexity management issues for developers and administrators. These issues are chiefly cased by the lack of an explicit representation of the data in configuration files, or having no commitment to any abstract model that can improve the interpretation of data when developing and analyzing distributed systems Studying the above issues, Oberle The organization of ontologies on which KAON SERVER is based, indicates why it is important to ground domain ontologies in upper-level ontologies (e.g., DOLCE) in the early software life cycle development phases (e.g., analysis and design). There are many potential benefits for this approach. For example, if our requirement domain models are based on upper-level ontologies, requirement engineers will be able to search for suitable components in the analysis phase. Moreover, the implementation of such systems can later be capable of more flexible integrations with software systems. Indeed, a similar approach for integration of business processes based on the use of Semantic Web services have already been proposed Maintenance
Any change ever since the client accepts the software, is related to the maintenance software development life cycle phase. When developing software, software engineers need a lot of knowledge about application domain, technologies used, algorithms applied, software testing, and past and new requirements. However, this knowledge is usually not recorded and for software maintainers (which are not necessarily the original software developers) it is very hard to fully understand the system being maintained. It is then not surprising why software maintainers spend 40-60% of their time just to understand the system being maintained To enable the support for managing knowledge of software maintenance, Anquetil et al. developed a comprehensive ontology for software maintenance consisting of five sub-ontologies Witte et al. address the above problems by developing two ontologies, namely, the source code ontology (i.e., an ontology of major concepts of object-oriented programming languages) and documentation ontology (i.e., an ontology of different concepts that may appear in documents related to programming, such as programming languages and data structures) Other authors demonstrate that it is possible to perform even more advanced software analysis by using ontologies For software maintainers it can also be important to know what designs are implemented in the maintained source code As we initially indicated, software is a social category, and so is software maintenance. Thus, it is also important to allow for capturing other relevant knowledge related to software maintenance (e.g., exchange of experiences on discussion forums). Capturing such type of knowledge facilitates communication between developers and helps with locating the developers with the most suitable skills, experiences, and reputations. The Dhruv system addresses this problem and facilitates connecting three different types of knowledge, i.e., content, interaction, and community. There are certainly a lot of potentials to experiment with the use of ontologies for social networking (e.g., FOAF) to build networks of software developers. Additionally, this can also be applied to analyze the trustworthiness of software based on the level of its developer's reputation. A good example in the line of this research direction is the Baetle ontology (http://code.google.com/p/baetle/) that combines a bug ontology with several other ones (e.g., atom, workflow, and description of a project's ontologies). In addition, software maintenance can benefit from the use of domain ontologies that were built during the software development (as described in the previous sections) or extracted from already developed artifacts Conclusions
To the best of our knowledge, there has been no approach addressing the issues of the retirement phase. Although retirement usually means the end of the use of a software product, it could be very important for software developers to be able to create repositories of retired software, as each retired software system contains a lot of knowledge encoded in its implementation To sum up the current state of the use of ontologies, we refer back to Sect. 2 and analyze the orthogonal dimensions to the software life cycle phases (i.e., documenting, testing, artifacts, interaction/collaboration, and participants). Documentation is probably the most commonly analyzed application of ontologies with ontologies used in all software life cycle phases. Domain, upper-level, and document structure ontologies are chiefly used to improve documentation. Still, the documentation activity could additionally benefit from ontologies by developing intelligent tools for software annotation that will for example have features for checking validity of documentation statements w.r.t. the software artifacts Using ontologies for software testing is probably the least explored aspect of software engineering. In fact, we have seen that (upper-level) ontologies are only used to validate requirements and detect design errors The use of ontologies for various software artifacts is probably one of the areas that has attracted a lot of attention so far. Domain and upper-level ontologies, ontologies for documentation, source code, bugs, ontology-based models, model transformations, requirements, and design patterns, are just some examples that are used for important software engineering tasks such as adding more semantics to the artifacts, improving traceability links, consistency checking of models, generating model transformations, and software metrics. While all these attempts are well-recognized by both the Semantic Web and software engineering communities, further exploration of semantic annotation mechanisms of software models and implementation code, integration of ontologies and metamodeling architectures, and a comprehensive traceability model of software artifacts, are some of the biggest challenges concerning the aspects of software knowledge artifacts.Interaction and collaboration are fundamental requirements for successful software engineering. The current efforts already demonstrate some interesting results for some of the software life cycle phases (e.g., facilitating mutual understanding of stakeholders and semantic Wikis for requirement acquisition). However, social aspects of design, implementation, integration, and maintenance phases are almost the dark side of ontologies Another important area is to describe software processes and methodologies. Not only do ontologies of methodologies have potentials to be related with modeling languages Introduction
The Semantic Web Semantic Web services In order for the vision of Semantic Web services to be realized, it is necessary to identify all the aspects related to the description of Web services in a single conceptual framework. The Web Service Modeling Ontology WSMO WSMO: An Ontology for Modeling Web Services
The Web Service Modeling Ontology WSMO Goals
Web Services Mediators Ontologies Fig. Figure • Ontologies provide formal and explicit specifications of the vocabulary used by the other modeling elements in WSMO. The use of shared ontologies specified in formal languages increases interoperability and allows for automated processing of the descriptions. See chapters "What Is an Ontology", "Description Logics", "Ontologies in F-Logic", "Resource Description Framework", and "Web Ontology Language: OWL" in this handbook for descriptions of the nature of ontologies and the formal and shared languages used for their specification. • A Web service is a piece of functionality accessible over the Web. A WSMO Web service is made up of three parts, namely: -The capability, which describes the functionality offered by the service.-The interface, Core Ontologies and ontology languages have been explained in detail throughout this book (chapters "What Is an Ontology" "Description Logis", "Ontologies in F-Logic", "Resource Description Framework", and "Web Ontology Language: OWL"). We focus here on the structure of Web service and goal descriptions and how they relate to each other. When clear from the context, we refer to WSMO Web service and goal descriptions simply as a services and goals, respectively. Recall that Web services define the information needed for a machine to interpret the usability of a Web service to fulfill a requester's requirements, which are encoded as a goal. Figure To perform Web service discovery, in other words to automatically find services that can fulfill the user's requirements, the capability of the goal is compared with the capabilities of known services. A capability is a description of the functionality provided by a service (or requested by a requester) and is Fig. Based on these considerations a capability description comprises four main elements. Preconditions describe conditions on the state of the information space prior to execution. Therefore, preconditions specify requirements on the inputs the service, e.g., typing. There may exist additional conditions that must hold in the real world in order for the service to successfully execute. These conditions, called Assumptions, are not necessarily checked by the service before execution but are crucial to the successful execution of the service (e.g., the balance on a credit card must be sufficient to conclude a purchase). Postconditions describe conditions on the state of the information space after execution has occurred, thus describing properties of the outputs of the service, as well as the relationship between the inputs and the outputs. Many services will have real world effects, for example when purchasing a book using a book selling service a physical book will be delivered to the requester. Effects are conditions that are guaranteed to hold in the real world after execution.The process of discovering services by comparing the capabilities of goal and Web service descriptions may yield a number of services that are capable of achieving the user's goals. However, compatibility of the capabilities of a given goal and Web service does not mean that a given Web service is desirable for the requester. The interface of a Web service specifies how to interact with the service in terms of a choreography, this choreography essentially provides information about the relationships between different operations on the Web service, for example the login operation of a book selling service must be invoked before the buyBook operation. A choreography can also be specified within the goal, essentially allowing the provider to specify the desired interaction pattern. The choreographies within the goal and discovered Web service descriptions can be compared in order to filter out those services whose interaction pattern is incompatible with that of the requester.The interface of a Web service description also contains an orchestration description. An orchestration specifies which services this service relies upon to provide its functionality, for example the description of a book selling service may specify that a specific delivery service is relied upon for final delivery of books. The goal may also contain such an orchestration description specifying the desired external services the discovered service should rely upon. Discovered Web services that do not meet these requirements may be eliminated, e.g., services that do not use the requested delivery service are not desired by the requester and thus can be ignored.After discovering those services whose functionally meets the requester's requirements and filtering out those that do not match in terms of their interaction pattern or the services upon which they rely there may still be multiple services that can achieve the user's goal. In this case the most desirable a Web service must be selected from the list. To perform this selection the non-functional properties of the discovered Web services are compared against the requested non-functional properties within the goal. Non-functional properties, as their name suggests, are used to capture non-functional aspects of a given Web service. These non-functional properties typically provide a description of the Quality of Service of the service, e.g., reliability, scalability, and security. By comparing the requested non-functional properties of the goal to those of the discovered services we can eliminate those services that do not meet the minimum requirements laid out by the goal and rank the remaining services to find the service that best fits the requester's non-functional requirements. Having selected the right service for the requester, based on functional, interface and non-functional parameters, automatic invocation of the selected service is possible using the choreography description of the service.In the remainder of this chapter we are concerned with the functional description of goals and Web services, i.e., the description of capabilities.Ontologies in Web Service Descriptions
Current Web service standards lack the necessary means to enable automation of the service usage process; they do not allow specifying the semantics of services in a machine-processable manner. In this section we describe how ontologies and formal Semantic Web languages may be used for describing user requirements and Web service functionality.Most Web service usage tasks require descriptions of there functionality and/or its interfaces. Web service discovery requires a description of the desired (goal), as well as the provided (service) functionality, and a means to compare them. Web service composition requires a description of the functionality of all services taking part in the composition, as well as their interaction, in order to verify whether the considered combination realizes the requested functionality. Invocation of services requires a description of the choreography of the service, in order to know how to invoke it, and to know which data, should be sent to the service, in which format, and which output may be expected. Non-functional descriptions of services (e.g., cost, security) play an important role in the selection of services according to user preferences. Ontologies play an important role in the description of the functional and non-functional aspects, as well as the interfaces, of services.In this section we review three typical uses of ontologies in goal and Web service descriptions:1. Modeling goals and Web services as concepts in a task ontology 2. Modeling inputs and outputs as ontology concepts 3. Using an ontology to provide the basic vocabulary for the rich functional description of goals and servicesThese paradigms are of increasing complexity, and allow increasingly detailed description of goals and services. The first and third paradigm are only concerned with the description of the functionality, the second paradigm is mostly concerned with (a simplified view of) the interface of the service.In this presentation we limit ourselves to the use of ontologies in the functional description of goals and services and do not address the use of ontologies in non-functional or behavioral descriptions. For more details about the use of ontologies for non-functional description of services we refer the reader to Goals and Web Services as Task Ontology Concepts
One could view the tasks that may be performed by a Web service as the domain of interest of a particular task ontology. The concepts in this ontology represent the tasks that may be performed by a service, and that may be requested in a goal. Furthermore, such tasks can be organized in a task hierarchy, and further relations between tasks (such as simple kinds of composition) may be expressed in the ontology, depending on the expressiveness of the ontology language. Figure If the functionality of a goal is expressed as a concept in the same, or a related, task ontology, then Web service discovery can be reduced to simply In particular, if a description logic-based language (e.g., OWL DL In description logics concepts represent sets, and sub-concept and concept equivalence relations correspond to set inclusion and set equality, respectively. One can thus understand a Web service description WS as a set of elements that can be delivered through execution of the Web service. In this setting, the goal description G represents all the elements desired by the user. As pointed out in Given an ontology O, a concept WS (representing the Web service), and a concept G (representing the goal), the following matching notions are distinguished:Exact match
The goal G is equivalent to the Web service WS, given the ontology O, denoted G ≡ O WS.PlugIn match
The goal G is a sub-concept of the Web service WS, given the ontology O, and thus the Web service can be "plugged in" place of the goal, denoted G O WS.Subsume match
The Consider now a goal G 2 corresponding to a request for a service that provides shipping: G 2 = Shipping. Clearly, G 2 and S are disjoint (by the disjointness of Shipping and Selling, depicted in Fig. Considering the structure of WSMO capability descriptions (precondition, postcondition, etc.), there does not appear to be a means for referencing concepts in a task ontology. However, a capability description is a description of the functionality of the service. Likewise, concepts in a task ontology represent all elements that are requested by or can be delivered through a goal or Web service, respectively. Therefore, such concepts are in fact descriptions of service functionality, and are thus capability descriptions that can be used as such in WSMO Web service and goal descriptions.Inputs and Outputs as Ontology Concepts
Most classical approaches to Web service description (e.g., WSDL) model Web services in terms of the structure of the input and output messages of the service. This is analogous to the way the interfaces of functions and methods are typically described in popular programming languages such as C++ and Java; the description of input and output messages of the service corresponds to the signature of a function or method. This description of the signature of a Web service tells the user the format of the messages to be sent to the service, and those returned by the service. However, the structure of a message describes only the format of the message, not its semantics (intention).The situation can be improved by using a shared ontology for the description of inputs and outputs in goal and the Web service descriptions. In the goal description, the input corresponds to the information the user is able or willing to provide, and the output corresponds to the desired output of the service. In the service description, the input corresponds to the information the provider requires before the service can be executed, and the output corresponds to the information that is produced by the service after successful execution.This scenario is similar to the one described in the previous section. However, in contrast to the previous scenario, the service description does not correspond to the functionality of the service, but to the type of information the service takes as input, and the type of information of its output. In case the service is an information-providing service, the functionality of the service can actually be described in terms of inputs and outputs. When describing inputs and outputs using description logic concepts, matching notions similar to the ones in the previous section can be defined. There are, however, two complicating factors: we need to distinguish between input and output and the service may require several inputs and have several outputs, as illustrated by the following example.Example 2. Consider the simple domain ontology for the book and music selling domain in Fig. Consider now the service S from Example 1. As input, the service expects instances of the concepts Product (the item to be purchased) and CCInfo (the credit card information of the customer); as output, the ser- Clearly, the requester can provide all inputs required by the service. However, the service can only provide some of the outputs required by the requester.We now extend the matching notions of the previous section to matching notions for signatures:• A Signature full match means that all inputs requested by the service are provided by the goal, and all outputs requested by the goal are provided by the service. • A Signature output match means that all outputs requested by the goal are provided by the service. • A Partial signature match means that some outputs requested by the goal are provided by the service. • A Signature non-match means that none of the outputs requested by the goal are provided by the service.We now define these notions formally. Recall the description logic notation introduced in chapter "Description Logics".We represent the individual inputs and outputs of a service S as description logic concepts S I1 , . . . , S Im and S O1 , . . . , S On , respectively, where S Ii and S Oj (1 ≤ i ≤ m, 1 ≤ j ≤ n) range over the individual inputs and outputs of the service. Likewise for the goal G.We now define the overall Web service and goal inputs and outputs (S I , S O , G I , G O ) as description logic concepts, based on the individual inputs and outputs:From the definition we can see that whenever there is a Signature full match, there is a Signature output match, and whenever there is a Signature output match, there is a Partial signature match. Conversely, if there is a Signature nonmatch, then there is no Partial signature match; if there is no Partial signature match, there is no Signature output match; and if there is no Signature output match, then there is no Signature full match. A similar model for describing inputs and outputs as ontology concepts, and its use in the context of Web service discovery, was introduced by Sycara et al. • Sycara et al. use one concept to represent all inputs, and one concept to represent all outputs, whereas we consider an arbitrary number of inputs and outputs, and create the input and output concepts S I and S O using the relations hasInput and hasOutput for the purpose of matching goals and services. • Where our notions of matching are mostly concerned with the outputs of the service, Sycara et al. distinguish between input and output matching. Our conjecture is that, for the task of Web service discovery, the service requester is mostly interested in Web services that provide the desired outputs, rather than services that accept its knowledge as input. Generally speaking, a service requester will have far more knowledge than is required as inputs for a single Web service invocation, and we do not expect that all knowledge of the requester will be explicitly described in a goal, nor do we expect that the requester is able to guess exactly which imports will be required by the Web services that are potentially of interest when creating the goal description.In WSMO, the precondition and postcondition of a goal or Web service capability define conditions on the input and output of a service. In fact, the concepts representing the inputs to the service are conditions on the input: the input is required to be a member of this particular concept; similarly, the concepts representing the outputs are conditions on the output of the service. Therefore, the input concepts are part of a Web service precondition, and the output concepts are part of a Web service postcondition.Ontologies as Terminologies for Web Service Description
The approach of describing the functionality of Web services in terms of inputs and outputs, introduced in the previous section, has a number of limitations, for example:• It is not possible to describe the relationship between the inputs and outputs; output concepts provide a very limited notion of postcondition. • This approach to description only deals with inputs and outputs, and thus a partial description of the pre-and postconditions in the capability of a goal or Web service. Consequently, the description only deals with the information space, and not assumptions on the state of the world, and effects in the real world of the execution of the service.Consider the description of the service S in Example 2. The service has an input of type Product and an output of type PurchaseConfirmation. However, it is not entirely clear from the description to which product this purchase confirmation refers. One could assume that it refers to the input product; one could also imagine that the purchase is of a similar product, in case the requested product is not available. Generally speaking, the output of the service is related to the input, and it is usually beneficial -and indeed necessaryto specify this relationship, especially in a setting, as with Semantic Web services, where service finding and usage are meant to be automated. Concerning assumptions and effects, consider the descriptions of the service S and the goal G 1 in Example 2. One would expect a number of assumptions to be part of the description of S, e.g., the requested product is in stock, and the balance on the credit card is sufficient, and one would expect certain effects to be part of the description of G 1 , e.g., the product is delivered to the address provided as input to the service. As pointed out above, the description of inputs and outputs as ontology concepts is not sufficient to describe the functionality of world-altering services. In fact, it also has limitations when considering information-providing services, because it is not possible to describe the relationship between the input and output when viewing them merely as concepts of an ontology. In the remainder of this section we consider more expressive ways of describing goal and Web service capabilities, in which it is possible to describe the relationships between inputs, outputs, assumptions, and effects.The execution of a Web service alters the state of the world. Therefore, we need a notion of state. We principally distinguish between two states: the pre-state is the state before and the post-state is the state after execution of a service. Additionally, WSMO distinguishes between the state of the information space and the state of the real world. Therefore, when considering a single service, we are concerned with four states, namely: (1) the pre-state of the information space, (2) the post-state of the information space, (3) the pre-state of the real world, and (4) the post-state of the real world. The precondition and assumption are conditions on the pre-states of the information space and real world, respectively. The postcondition and effect are conditions on the post-states of the information space and real-world, respectively, and describe the relationship between the respective pre-and post-states. Specifically, the information space consists of the inputs and outputs of the service, and thus the precondition consists of conditions on the inputs, and the postcondition consists of conditions on the outputs and relationships between inputs and outputs.Even though this type of description is the most expressive we consider in this chapter, relatively little research has been done into the use of such expressive Web service description, when compared with the more simple kind of description based on task ontologies and interface description that we described above. A notable exception is For both approaches it is the case that if the elements of the capability are specified using first-order logic formulas, which is by definition the case for the situation calculus, then typical reasoning tasks can be reduced to corresponding reasoning tasks in first-order logic. Realizability, which corresponds to checking whether a Web service description can in theory be realized, i.e., there may be a Web service that realizes the description, can be reduced to satisfiability checking in first-order logic, and functional refinement, which corresponds to checking whether the capability of one goal or Web service is a refinement of the capability of another, can be reduced to checking entailment in first-order logic.For the purposes of this presentation we do not give formal definitions of either of these tasks; we illustrate the notion of functional refinement using an example. For convenience, we only consider preconditions and postconditions in the examples; however, they may be straightforwardly extended to include also assumptions and effects, which are treated analogously. We use firstorder logic formulas for the description of the capabilities in the examples. Note that the free variables in the precondition φ pre and postcondition φ post are shared between the conditions. Intuitively, a capability corresponds to an implication (∀)φ pre ⇒ φ post , where (∀) denotes universal closure, and ⇒ denotes state change (as opposed to material implication in classical logic): for all inputs (variable assignments) holds that if the pre-state is such that φ pre is true, then the state will change to a post-state, in which φ post is true. Classes in an ontology correspond to unary predicates.Example 4. We use the bookselling ontology in Fig. Consider a book and CD selling and shipping Web service S with a precondition S pre and a postcondition S post . With the precondition S pre we want to specify that there should be an input that identifies a book or a CD and there should be one which is a member of the class Address. The individual inputs are denoted by the variables x and y, respectively:With the postcondition S post we want to specify that there is a confirmation of purchase, denoted by the variable z, and that the product x is shipped to y:Consider now a goal G representing requests for a bookselling and shipping service, with the precondition G pre that specifies the willingness to provide a book and address:The postcondition G post specifies the requirement that the book is shipped to the provided address: G post ≡ shippedTo(x, y).One can now verify that S is a functional refinement of G: any state that is compliant with (i.e., that satisfies) the precondition of the goal G pre is also compliant with the precondition of the Web service S pre ; and any poststate compliant with the postcondition of the Web service S post is also compliant with the postcondition of the goal G post . So, any execution of the Web service S with inputs satisfying the precondition of the goal (e.g., the inputs provided by the user) completely fulfills the user's requirements, and thus there is a match between the goal and the service.Other Frameworks for Semantic Web Service Description
In this chapter we have described how ontologies can be used in the description of Web services, in the context of WSMO, a framework for the description of semantic Web services. Now, there are several other frameworks for describing semantic Web services that also allow using ontologies in Web service descriptions. In this section we briefly review the most prominent other frameworks for semantic Web service description. The frameworks we consider are WSDL-S WSDL-S / SAWSDL
WSDL-S was proposed as a member submission to the W3C in November 2005 between the LSDIS Laboratory and IBM In contrast to WSMO, SAWSDL is a lightweight approach to Web service description. It extends WSDL and allows associating semantic annotations with Web services, building on pre-existing standards. Using the extensibility of SAWSDL, semantic annotations in the form of URI references to external models, such as WSMO or OWL-S (presented in the next subsection) can be added to the interface, operation and message constructs. SAWSDL is independent from the language used for defining the semantic models and explicitly regards the possibility of using WSMO, OWL-S and UML as potential candidates, as illustrated in the SAWSDL usage guide SAWSDL extends WSDL with a set of attributes and elements that may be used to associate semantic annotations with WSDL descriptions. For the annotation of individual Web services, a bottom-up approach is followed, meaning that WSDL message types, used for Web service inputs and outputs, are mapped to the concepts in domain-specific ontologies (see also Sect. 3.2). Additionally, WSDL operations, which are description of Web service functionality, may be mapped to ontological concepts in a task ontology (see also Sect. 3.1). The user goals may be represented using service templates based on concepts from domain ontologies.OWL-S
OWL-S (formerly known as DAML-S) The Service Profile contains a number of non-functional properties of the Web service, including: the service name, a textual description of the service, contact information of the service responsible, an external categorization of the service, and, finally, an expandable list of non-predefined properties.The functional characterization of Web services is expressed in terms of the information transformation and the state change produced by the execution of the service. The state change, modeled by preconditions and effects, refer to the change on the state of the world as a consequence of executing the service, and information transformation, modeled by inputs and outputs, which refer to what information is required and what information is produced (generally depending on the information provided as input) by the service. The schema to describe IOPEs (inputs, outputs, preconditions, and effects) instances is defined in the Service Model, not in the Service Profile. Therefore, these instances are described in the Service Model and referenced from the Service Profile. Such IOPEs may be used for expressive Web service descriptions, as illustrated in Sect. 3.3, although outputs cannot be used for expressing the relationship between the inputs and outputs. Then, when leaving out the preconditions and effects from the IOPE, one obtains a description of inputs and outputs as in Sect. 3.2. Additionally, service models may refer to a service category, which could be a concept in a task ontology (cf. Sect. 3.1).Both OWL-S and WSMO aim (and claim) to provide the necessary means for creating semantic descriptions for Web services, i.e., to enable the vision of the Semantic Web services. Although both approaches have identical aims, there are certain differences between the two. One striking difference is that, in contrast to OWL-S, WSMO defines the concept of mediator as a firstclass citizen of the framework. The other major difference is the structure of Web service descriptions. OWL-S defines an ontology comprising the main elements of a service, where the service element serves as an organizational point of reference for declaring Web services. The description of an individual Web service is an instance of the Service concept. The structure of WSMO descriptions is somewhat different as it offers four main top-level elements (ontology, Web service, goal and mediator ) that can refer to each other either by using mediators or by importing ontologies (i.e., ontologies that contain the vocabulary to be used in the semantic descriptions).SWSF
The Semantic Web Services Framework (SWSF) Conclusions
Existing E-Business solutions typically require the implementation of costly and custom infrastructures by each of the business partners involved. Web service technologies are a milestone on the path towards flexible interoperability among distributed and independent software systems. However, while Web services provide a uniform infrastructure for the provision of services over the Web, they deliver only syntactical descriptions that are hardly amenable to automation. The process of dynamically creating ad-hoc interactions between companies, as envision by Web services, remains unattainable. Semantic Web services, as presented in this chapter, are an application of Semantic Web technologies, and ontologies in particular, to Web service description. Such semantic descriptions enable machine processing of an automated reasoning about Web service functionality, as well as the mechanisms used to invoke them and the data used as inputs and outputs.We have seen how ontologies can be used for the formal description of both user requests and Web service functionality. Specifically, the three ways of using ontologies in Web service descriptions we addressed in this chapter were: describing goals and Web services as concepts in a task ontology, describing inputs and outputs using concepts in a domain ontology, and using ontologies as terminologies for expressive state-based Web service descriptions. We have also described how the first and second approach can be formalized using Description Logics (such as OWL DL) and how Description Logic reasoning can be used for the task of Web service discovery.Comparing the approaches to describing Web services, there is a difference in the detail and preciseness of the descriptions both between and within the approaches. At the one end of the spectrum there are the lightweight task ontologies (such as the one depicted in Fig. As discussed in detail in chapter "Exploring the Economical Aspects of Ontology Engineering", creating ontologies, and especially heavyweight ontologies, have an (often considerable) cost. The engineering of Web service descriptions based on such ontologies brings additional cost, which can be considered relatively low in case they are based on task ontologies, but will be high for the case of detailed state-based descriptions. The existence of ontologies on the (Semantic) Web, which may be reused in Web service descriptions, would reduce the cost of describing goals and Web services. Nonetheless, authors of such descriptions will need to make a trade-off between the detail of the descriptions -descriptions with higher detail will lead to more accurate results in Web service discovery and a higher degree of automation in selection and execution -and the effort required to create the descriptions.Ontologies for Machine Learning
Stephan BlöhdornIntroduction
Recent efforts of research and industry in the area of the Semantic Web (SW) and ontologies together with the standardization of the Resource Description Framework (RDF) and the Web Ontology Language (OWL) In the last years, research has actively addressed the problem of learning knowledge structures for the Semantic Web in the context of both Ontology Learning (the topic of chapter "Ontology Learning" in this volume) and Information Extraction (the topic of chapter "Information Extraction").Along another line, research has recently started to investigate how existing SW data sources can be mined and analysed by inductive learning techniques. Two communities contribute to this trend. On the one hand, the Semantic Web community has started to incorporate concepts of inductive learning into their research work, an area which is also referred to as Semantic Web Mining In this chapter, we review various attempts to combine ML techniques and ontologies, semantically annotated data or both. This is an exciting and rapidly expanding but also highly scattered research area. Our exposition includes both, references to explicit Semantic Web Mining research and references to activities in the ML community that show links to SW research efforts. In order to structure the different contributing research fields, the next section starts with an overview over the whole research area and the content of this chapter.The Machine Learning and Semantic Web Research Landscape
The use of ontologies and comparable declarative knowledge representation paradigms within ML tasks is an emerging field of research which draws from contributions from various communities and is shaped by a large number of diverse paradigms. In this section, we aim at structuring this research field along two major dimensions.On the one hand, approaches can be organised according to the type of the objects of interest to be analysed by the learning techniques: "Ordinary" data: In this setting, the objects of interest are arbitrary data items which have already been the subject of investigation in conventional ML settings. However, their content and conventional representation can be mapped to entities found in ontological structure. As an example, consider textual data which on the one hand has a classical representation for ML in terms of the Bag-of-Words (BOW) model but whose content can on the other hand be described further by means of lexical ontologies such as WordNet. Ontology Entities: In this setting, the objects of interest are parts of an ontological structure themselves. It covers all cases where ontological entities become the focus of the mining activities. This class could be further divided according to the entity type, i.e. whether entities reside on the schema or on the instance level of the ontology. Ontologies: Finally, this group of approaches covers all cases, where sets of ontological axioms, i.e. whole ontologies or parts of ontologies are the object of ML interest.On the other hand, the contributions in the field can be roughly organised according to the structural component of the ML technique which is modified primarily:Feature representation: Techniques of this class use knowledge from the ontology to modify classical feature representations, e.g. by adding features which can be deduced from the ontology. Similarities and distances: While techniques of the previous class explicitly transform the data representation, techniques in this class achieve a similar effect implicitly. This is done by distorting the pairwise instance similarities or distances which form the input to many ML algorithms by means of calculations which take the structure and knowledge of the ontology into account. Model class: For this class of approaches, the knowledge about the dependencies of entities in the ontology becomes part of the overall ML model, e.g. in the form of constraints on the solution space or in the form of probabilistic dependencies between instances and features. Algorithm: For this class of techniques, the knowledge encoded in the ontology enters the overall machine learning technique only on the algorithmic level.Table The topics of this chapter are arranged as follows: In Sect. 3, we sketch a number of approaches to exploit background knowledge encoded in ontologies within Text Mining applications. In this context, ontologies support the generation of informative features for the use with classical Text Mining (TM) algorithms. This section is more comprehensive and detailed than the other sections, which, as a tribute to the limited space, can only sketch some of the main ideas and point to further sources of information for the interested reader. In Sect. 4, we review a number of approaches from the area of Similarity Measures that make use of semantic knowledge representation mechanisms. In particular we also cover an exciting field of modern ML research, Kernel Methods, that make use of a specific class of such similarity measures. In Sect. 5, we survey a field of ML research called Link Mining which addresses various learning tasks on data that exhibits a link structure. Then we sketch how this work can be adapted to ontological data. In Sect. 6, we introduce the field of Statistical Relational Learning that naturally lends itself to application to SW data and sketch its relations to the somewhat more traditional field of Inductive Logic Programming. We conclude with a short summary in Sect. 7.Ontologies for Text Mining
The term Text Mining (TM) was first phrased by Feldman and Dagan From the data mining perspective, Text Mining mainly targets three different application areas. On the one hand, Text Clustering is of interest to allow for a better way to explore huge text collections and to add structure for navigation. On the other hand, Text Classification aims at learning models that enable the assignment of thematic categories to unseen texts, e.g. to support news providers by classifying their incoming news, but also for spam detection. The survey by Sebastiani Ontologies represent additional background knowledge which can be exploited to better solve typical Text Mining tasks Preprocessing
In this part, we shortly sketch both the conventional and the ontologyenhanced representation of text data for ML settings.Text Representation
In Text Mining, documents are typically represented as so called Bag-of-Words vectors as originally proposed by Salton Fig. 1. Bag of words example
Non-descriptive words, so called stopwords, are often removed from this representation based on stopword lists. Term weighting techniques, such as TFIDF weight tf , the frequency of a word in a document, with idf , a factor that discounts its importance when it appears in many documents in the corpus. It is defined as: tfidf(d, t) := log(tf (d, t) + 1) * log |D| df (t) ,where df (t) is the document frequency of term t that counts in how many documents term t appears. See Amati et al. Incorporating Background Knowledge from Ontologies
The background knowledge we will exploit further on is encoded in an ontology. The ontological background knowledge is incorporated into the vector space model by applying additional preprocessing steps. After deriving the typical bag of words representation, the vector dimensions are mapped to concepts of a given ontology or knowledge base.Enriching the term vectors with explicit concepts from the ontology has two benefits. First it resolves synonyms; and second it introduces more general concepts which help to identify related topics and provides some kind of connection between documents which addresses the same or a very similar topic with different words. For instance, a document about beef may not be related to a document about pork by the cluster algorithm if there are only "beef" and "pork" in the term vector. But if the more general concept "meat" is added to both documents, their semantic relationship is revealed. We have investigated the influence of three different strategies for adding or/and replacing terms by concepts on the clustering/classification performance For our purpose, a knowledge base needs a lexical component to allow for an appropriate mapping of the words of the text documents to the concepts of the ontology. Obviously, this mapping yields new challenges like the handling of the emerging concept vector and the detection of the meaning of a word to find the right concept. In this context, an important problem is to find the right concept for a word in a given context which have more than one meaning. This is the word sense disambiguation problem The mapping of words to concepts solves also the synonymy problem. Adding additional hypernyms/super concepts allows for relating very similar topics which are the content of different documents but which a user would expect in the same cluster. By changing the document representation in a way that different words of the vector are mapped to the same (super) concept, to represent the same or a very similar topic by a common representation, the clustering algorithm should be better able to group such documents together. By adding more super-concepts we start to add noise and in result the performance will drop because topics become related which have not so much in common. The rightmost vector in Fig. Approaches for Different Learning Tasks
We now focus on some of our own results that use ontologies to improve clustering and classification tasks Text Clustering
Text document clustering methods can be used to find groups of documents with similar content. The result of a clustering is typically a partition of the set of documents. Each cluster consists of a set of documents. Usually the quality of a clustering is considered better if the contents of the documents within one cluster are more similar and between the clusters more dissimilar. Most clustering methods group the documents only by considering their distribution in document space (for example, in the vector space model for text documents). A good survey can be found in We illustrate the integration of background knowledge into the text clustering process by results of Hotho et al. Not only the performance of Text Clustering can be improved by using background knowledge. The integration of super-concepts provides also a very good basis for clustering visualization. Hotho et al. To date, the work on integrating background knowledge into text clustering is quite heterogeneous. Green Text Classification
The automatic process of learning a model, based on a given set of training examples, which is then able to predict the class label of a new text document is known as Text Classification. Early methods that produced good results were Rocchio, k-Nearest Neigbhour (kNN) or neural networks in the middle of the 1990s. Meanwhile, more advanced Machine Learning approaches like Support Vector Machines (SVMs) or Boosting show very impressive Text Classification performance. A good survey is presented by Sebastiani In this section, we report the main idea of integrating formally represented knowledge into the learning step with the goal to improve the prediction performance. We follow the presentation of our work in Fig. 2. Excursus: Linear Classification
the lexical level (e.g. detection of multi-word expressions) and the generalization on the conceptual level (resolving synonyms and adding super-concepts).Other results from similar settings including background knowledge are reported in Related Approaches
A number of other approaches have varied the basic settings we have reported in the previous section. Beside the usual preprocessing of text, several other approaches like classification on n-grams or smoothing with Latent Semantic Indexing (LSI) were investigated to improve the performance in selected Text Mining settings. In the following, we report on two particularly interesting directions.Text Mining with Automatically Learned Ontologies
So far, the ontological structures employed for the classification and clustering task are created manually by knowledge engineers which requires a high initial modelling effort. Research on Ontology Learning (covered in chapter "Ontology Learning" of this volume) has started to address this problem by developing methods for the automatic construction of conceptual structures out of large text corpora mostly in an unsupervised process. To reduce the modelling effort, the next step is to first learn an ontology from text which perfectly matches with the topics of the corpus and then add this newly extracted knowledge to the mining process as described in the previous sections. This approach was undertaken by Bloehdorn et al. Using Background Knowledge from Ontologies in Information Retrieval
Several researchers have reported positive results concerning query expansion in the context of IR applications. In early work on the topic, Salton and Lesk Similarities and Kernel Functions for Knowledge Structures
Various Machine Learning algorithms can be designed in such a way that the only required input is a matrix of pairwise similarities or distances among the input items. The definition of appropriate similarity and dissimilarity measures is a topic that plays a key role in different areas of Artificial Intelligence. This chapter deals with a selection of approaches for defining appropriate similarity or dissimilarity measures in the context of Machine Learning with Ontologies. In the context of the classification provided in Table In the simplest case, algorithms can work with an arbitrary similarity function which is usually only required to be positive, reflexive and symmetric. Algorithms that work with distances rather than similarities often require that the distance (i.e. dissimilarity) measures comply with the requirements of a metric. Examples of algorithms that pose only minor requirements on the properties of the employed measures are the k-Nearest Neighbour (kNN) algorithm for classification or agglomerative clustering techniques. Classical similarity measures defined on feature vectors are the inner product or the cosine, and the corresponding canonical dissimilarity measure is the Euclidean distance as e.g. required for k-means clustering.Most naturally, there are strong relations between feature representations and (dis-)similarity measures: Changes to the feature representation may imply different measures and changes to the measures may implicitly correspond to a modified feature representation. A particularly interesting class of similarity measures are kernel functions. Kernel functions compute the similarities of data instances in such a way that the result is equivalent to an inner product in some (possibly unknown) vector space. Formally, any function κ : X × X → R that for all x, z ∈ X satisfies κ(x, z) = φ(x), φ(z) is a valid kernel, whereby X is some input domain under consideration and φ is a mapping from X to a feature space F . It can be shown that the class of such functions can be characterised as the class of functions which are positive semi-definite. The reason for the large interest in kernel methods is the fact that the correspondence to a vector space makes it possible to use kernel functions together with many Machine Learning algorithms whose models are tied to a geometric interpretation within the corresponding vector space without representing objects explicitly in this space. An example would be the notion of a separating hyperplane in the case of classification with linear classifiers as described in Fig. Ontology-Based Kernel Functions for Semantic Smoothing
Semantic Smoothing Kernels are a technique for incorporating ontological background knowledge into a kernel function for vector representations of textual data. This kernels implicitly mimic parts of the effects of the explicit feature transformations we have investigated in the previous section. Semantic smoothing kernels were initially proposed by Siolas and d'Alche Buc Advanced approaches have exploited the property that kernels can be combined and embedded in one another. As an example, Bloehdorn and Moschitti Similarities and Dissimilarities for Ontology Entities
A general framework for similarity measures in ontologies is proposed by Ehrig et al. Bernstein et al. D'Amato et al. Kernel Functions for Ontology Entities
Most of the work on kernels for structured data is rooted in the influential work on convolution kernels by Haussler Gärtner et al. A first endeavour to investigate the use of kernels for actual Semantic Web data is presented by Bloehdorn and Sure In a different spirit, Raedt and Passerini On the schema-level, Fanizzi and d'Amato Link Mining
In many ways, ontological data can be seen as a collection of linked resources. Typically, as in the case of RDF or OWL, they form heterogeneous networks with many resource and link types, whereby the type information itself is again arranged in a linked structure, e.g. a subsumption hierarchy. But also datasets that are not explicitly encoded according to SW standards may exhibit a certain level of semantics that is present within a link structure. As an example, consider bibliographic data linking publications, authors, and venues.Link mining refers to data mining techniques that explicitly consider such links when building predictive or descriptive models of linked data. A good survey over the field is given by Getoor and Diehl Semantic Network Analysis of Ontologies
Since the last decade, the social network analysis community has started to discover the Internet and the Web as fruitful application domains for their techniques (e.g. analysing the link structure of the Internet Ding et al. Link-Based Object Classification
This group of techniques refers to algorithms that directly exploit the link structure in a graph (e.g. in an ontology) to classify objects (e.g. ontological entities). A good overview of this kind of approaches is given in Subgraph Detection and Graph Matching
This topic builds upon subgraph mining techniques to find frequent or informative substructures in graph instances. An example of the utilization of such algorithms on Semantic Web data is given by Ramakrishnan et al. Graph matching refers to techniques that try to detect similar substructures in pairs of graphs. Seen in the context of the Semantic Web, this field immediately evokes thoughts about the field of Ontology Mapping. Work in this area includes Doan et al. Interesting recent work in this direction that explicitly takes into account the interaction between mining the structure of the ontologies to be aligned and checking the semantics of the resulting mappings is reported by Udrea et al. Graph Classification
Unlike link-based object classification, which attempts to mine the nodes in a graph, graph classification is a learning problem in which the goal is to classify an entire graph as a positive or negative example in a classification setting. A typical approach to address this problem is to discover features on the input graphs, thereby building on subgraph mining techniques to find frequent or informative substructures in the graph instances. The detected substructures are then used for transforming the overall graphs into vectorial data, and then traditional classifiers are used for classifying the instances. However, finding all frequent substructures is usually computationally prohibitive.Again, kernel methods have been designed to efficiently work on graph data, but these approaches have usually been restricted to specific kinds of graphs (e.g. trees). Kernels for arbitrary graphs have proved to be more difficult to design. Approaches in this direction are reported in Statistical Relational Learning
In this section we start with a short review of Inductive Logic Programming approaches and relate them to Semantic Web paradigms. Then we focus on the upcoming new research area Statistical Relational Learning which combines logic and probabilistic learning approaches. We will give some references to this area and discuss connections to Semantic Web. In the context of our overview of the research landscape, these approaches can deal with various types of input items but do so mainly on the level of modifying the learning model or modifying the actual learning algorithms.ILP and the Semantic Web
Inductive Logic Programming (ILP), a term phrased by Muggleton As Semantic Web standards are largely built upon the foundations of firstorder logic, the application of ILP techniques to learn on Semantic Web data is a natural step. Lisi ILP systems tuned towards learning in description logics (which form the basis of most Semantic Web endeavours, particularly OWL) are presented by Lisi and Esposito Combination of Probabilistic and Logic-Based Learning Approaches
Statistical Relational Learning (SRL) is a relatively young research area which focuses on the combination of probabilistic and logic models with the goal to be better able to describe real world phenomena. Traditional statistical machine learning is able to capture uncertainty, but only within one relation -whereas traditional ILP and relational learning approaches are able to work on multiple relations, but cannot handle noise. The combination of both approaches tries to overcome these limitations, which is a critical point when working with heterogeneous and richly interlinked Semantic Web data. Methods developed in this area are applied to richly structured data which is available for, e.g. hypertext classification, topic prediction of bibliographic entries, or in any kind of social networks. Other applications areas for SRL includes communication data, customer networks, collaborative filtering, trust networks, biological data, sensor networks, and natural language data. The book by Getoor There are four distinct areas which are the starting points of SRL research: (1) ILP, (2) statistical learning as well as (3) probabilistic and (4) logical inference. Researchers from these areas extended in the last years well known approaches to bridge the gap between logical and probabilistic approaches. One example is the extension of the popular propositional rule learning algorithm CN2 to ICL (Inductive Constraint Logic) in Statistical Relational Learning Challenges and Applications on the Semantic Web
There exist many application areas for SRL. We focus here on applications with relation to the Semantic Web. The emergent field of Statistical Relational Learning offers a variety of methods to overcome existing Semantic Web problems; E.g. the logic used in the Semantic Web was not designed to deal with uncertainty but SRL has made first steps to combine logic and uncertainty. On the other hand these problems cause new challenges for the Statistical Relational Learning community as new kind of data arises. In the rest of this section we will shortly review first solutions combining both areas.Statistical relational learning techniques are well suited to knowledgeintensive learning, because they allow input knowledge to be expressed in a rich relational language, while being able to handle noise in this input. In general, many different types of knowledge can potentially be integrated into SRL. First steps towards an automatic knowledge integration are reported by Domingos et al. Many good examples of applications of SRL techniques to the Semantic Web come from the area of ontology mapping. Given initial mappings between knowledge structures from different sources, one can learn generalizations of them using SRL techniques like the content learner and the name learner, both utilizing the well know naive Bayes classifier on a bag of tokens. Amongst others, Doan et al. Popescul et al. Conclusion and Outlook
In this paper, we have reviewed contributions from different communities to the emerging field of Semantic Web Mining by discussing Machine Learning techniques that directly or indirectly use ontologies or related declarative knowledge representation paradigms. At this point, the work in this area is highly scattered among various subfields of Machine Learning theory and practice and among current Semantic Web research efforts. All these areas continue to evolve and are likely to undergo various transformations in the years to come.We have introduced Inductive Logic Programming in Sect. 6.1 as the predecessor of today's Semantic Web mining efforts which continually evolves to accommodate more and more of the current Semantic Web development. From a theoretical perspective, the kernel methods paradigm and the techniques from the field of statistical relational learning we presented in Sects. 4 and 6, respectively, are likely to have the highest impact on modern Semantic Web mining efforts. From a practical perspective, the fields of text mining, presented in Sect. 3, and link mining, presented in Sect. 5 already show ontology-enhanced applications actually working or -in the case of link mining -point to practical approaches that show substantial potential to be applied and transferred to the field of the Semantic Web.Introduction
As the volume of textual information is exponentially increasing, it is more than ever a key issue for knowledge management to develop intelligent tools and methods to give access to document content and extract relevant information. Information Extraction (IE) is one of the main research fields that attempt to fulfill this need. It aims at automatically extracting well-defined and domain specific data from free or semi-structured textual documents. The extraction of instances of appointments from on-line news is a typical example. IE interprets "Yesterday, Mr. Smith as been appointed as Chief Executive Officer of AAACompany Inc." into the knowledge structure: Appointment (Smith, AAACompany, CEO, Yesterday) where the arguments respectively play the role of person, company, title and date of the appointment. Once S. Staab and R. Studer (eds.), Handbook on Ontologies, International Handbooks formalised in such a way, the content of the document may support formal calculus or logical inference as needed by knowledge management applications.The relation between IE and ontologies can be considered in two nonindependent manners. As IE can extract ontological information from documents, it is exploited by ontology learning and population methods for enriching ontologies. This issue is specifically discussed in chapter "Ontology Learning". Conversely, this chapter focuses on how ontologies can be exploited to interpret the textual document content for IE purposes. We will show here that IE is an ontology-based activity and we will argue that future effort in IE should focus on formalising and reinforcing the relation between the text extraction and the ontology model.Examples from the biology domain will illustrate the presentation of IE concepts. Biology is a relevant application domain because of the importance of text-mining for the biology community, the availability of structured resources such as document collections and nomenclatures, the clear expression of application requirements and finally, the amount of evaluation material (e.g. Genia What Is IE?
Definition
The IE field was initiated by the DARPA MUC program (Message Understanding Conference) in 1987 A typical IE task is illustrated by Fig. In the simplest case, extracted textual fragments fill the form slots and no more text pre-processing is required. However IE cannot be reduced to Sentence: ''GerE stimulates the expression of cotA.'' Genic interaction form Agent: Protein(GerE) Target: Gene(cotA) Fig. IE Overall Process
Operationally, IE relies on document pre-processing and extraction rules (typically regular expressions or patterns) to identify and interpret the target text. The extraction rules specify the conditions the preprocessed text must match and how the relevant textual fragments can be interpreted to fill the forms. Figure The rule assumes that gene and protein names occurring on both sides of an interaction verb denote a genic interaction between the corresponding protein and gene.A typical IE system includes three processing steps 1. Text analysis: From text segmentation into sentences and words in the simplest case to full linguistic analysis. In the example from Figs. The rules are usually declarative but they may be expressed in different ways.The rule example of Fig. Fig. 2. Extraction rule example
more complex ones extract several related values at the same time. This is referred to as multi-slot extraction, which requires a relational formalism • Entity form filling requires to identify items in the text that represent domain referential entities (e.g. protein and gene names). • Domain event form filling requires to extract events that represent actual relations between entities (e.g. the agent role of a protein in a genic interaction). • Merging forms issued from different parts of the text that provide information about a same entity or event.• Scenario forms relate several event and entity forms that, considered together, describe a temporal or logical sequence of actions and events.Text Processing
From the very beginning, the main issue in IE appeared to be the design of efficient extraction rules able to extract all relevant information pieces and only the relevant ones. The difficulty comes from the intrinsic richness and complexity of natural language where a given word or phrase may have different meanings (polysemy) and several formulations may express the same information (paraphrases). If the rules rely on surface clues (i.e. the presence of a given specific lexical item, the word distance or order), a whole set of very specific rules must be designed for each new IE application.If the text is pre-analysed, information extraction rules can be expressed in a more abstract and powerful way. In that case, the rules apply on the result of the pre-analysis, which is a normalised representation of the text. For instance, the successor relations of the rule of Fig. Fig. 3. Abstract extraction rule example
biology can be considered as single words, while a single token can be viewed as the contraction of several words. The lemmatisation associates a normalised form (the lemma) to each word (infinitive form for verbs, singular form for nouns and pronouns) by removing marks that bear flexional features. The morphological tagging associates morphological features (tense, number, gender, presence of non-alphabetical characters and case) to words. The syntactic analysis performs two dependent tasks. The part-of-speech (POS) tagging assigns a syntactic category to words (e.g. noun, verb, adverb).The parsing identifies the sentence structure by grouping words into phrases. Depending on the parser, syntactic dependencies between words or phrases (e.g. subject-verb dependency) may also be computed.The semantic analysis builds a formal representation of the text meaning. In IE, the semantic analysis is traditionally restricted to (1) the identification of the semantic textual units (named entities and terms) that refer to the relevant domain objects, (2) the semantic typing that associates concepts to those semantic units, and (3) the tagging of domain specific relations between them.The text analysis process relies on linguistic and domain knowledge. The most traditional lexical resource is the named entity dictionary, a nomenclature of the names of domain entities, such as genes and proteins in biology, but other resources can also be exploited. We will show in the following that IE is an ontology-driven approach to text analysis that heavily relies on lexical and ontological resources.IE as a Text-Ontology Mapping
The overall process of IE aims at mapping text to ontology. IE selects and interprets relevant pieces of the input text in terms of form slot values. The form slot values are derived from the semantic analysis while the form itself represents an ontological knowledge structure.This mapping can be formalised into the annotation of the text by the ontology, as shown in Fig. The lexical knowledge plays the role of mediator. Various types of lexical resources can be exploited in IE, from the named entity dictionaries to the domain terminologies or ontological thesauri. The lexical mediation between the text and the ontology is complex to formalise. First, there is no one-to-one relation neither between text fragments and lexicon entries nor between those entries and ontological entities due to linguistic phenomena, like variation, polysemy and ellipsis. We will show in the following sections that the lexica are associated to sets of rules that govern the recognition or disambiguation of the lexicon items. They contribute to building links between the text and the lexicon on the one hand, and between the lexicon and the ontology on the other hand. The various types of knowledge are traditionally considered as distinct resources although they partially overlap, maintaining the coherence between them remains an open question.The importance of lexical resources has raised the problem of their acquisition. Indeed the development of applications in specific domains generally requires the adaptation of available knowledge resources needed for the various linguistic processing. Thus the issues of the re-usability, the acquisition and the formalisation of knowledge become central.IE State of the Art
In the 1990s, IE quickly became operational for extracting simple information pieces from short and homogeneous documents such as conference announcements. But extracting relational information (e.g. gene-protein interaction) from free texts (e.g. abstracts of scientific papers) remained challenging.The IE field then evolved since the beginning of the 2000s toward semantic processing, knowledge acquisition and ontologies. This has led to the development of a new generation of IE systems (e.g. Two major phenomena have made this progress possible:• An increasing number of operational linguistic tools, and even whole integrated NLP pipelines, are available for people outside the NLP field. These tools achieve deeper and sounder linguistic analysis. They are now widely used by IE research. Section 3 presents an example of these NLP-based IE systems. • Since knowledge resources are scarcely available in specific domains, knowledge acquisition has become an important issue in IE since 1998 Corpus-based Machine Learning (ML) was soon recognised by the IE field as a relevant alternative to costly manual knowledge acquisition and adaptation in particular for the acquisition of information extraction rules and ontologies Current IE systems therefore evolved to sophisticated platforms that combine various NLP and ML steps, IE as a Knowledge-Based NLP Process
This section describes IE as a knowledge-based NLP process in more detail by outlining a generic IE architecture.Architecture of a Linguistic-Enabled IE System
We illustrate the role of text and ontology processing in IE by the Alvis semantic analysis pipeline. Figure Semantic-Based Text Analysis
The linguistic steps that were presented in Sect. 2.3 are implemented in Alvis NLP pipeline. We illustrate in Table Relevant named entities or terms are first identified as semantic units by the dotted line-framed components of Fig. This process is knowledge intensive. The components use linguistic resources as figured by the middle layer boxes. They are typically domaindependent and application-driven. The clear distinction between the process and the knowledge bases (KB) reduces the adaptation to new domains to the revision of the following knowledge bases: named entity dictionaries, terminologies, ontologies and IE rules. Two specialised versions of the Ogmios platform have been deployed to develop IR applications for scientific papers Coupling Semantic Annotation and Knowledge Acquisition
Acquisition methods are closely integrated into the Alvis pipeline as figured by the bottom layer of Fig. In production mode the pipeline is applied to a corpus in order to feed an external application, typically IE or IR with annotated documents. The components and the KB are stable and their choice is driven toward this application. In that mode, the pipeline is a critical element of an external service and usually processes massive amounts of data, so it must be reliable, stable, fast and scalable.In acquisition mode, relevant components of the pipeline are applied to a corpus in order to build training examples for an ML algorithm that aims at acquiring the KB of other components. As the amount of documents processed in acquisition mode is typically smaller than in production mode scalability and speed performance are less vital. However, the flexibility and the modularity of the pipeline are critical, since KB acquisition requires several experiments and fine tuning of the intermediate representations of the training corpus.The following sections explain the role of the various types of knowledge in IE by detailing how they are exploited in the text annotation process of the Alvis IE system and how they are acquired.Handling Named Entities
In its usual meaning, the term named entity (NE) designates proper nouns but it is also often used for other types of invariant terms (e.g. dates and chemical formulae). The proper names are rigid designators that designate a referential entity in an unambiguous way Named entity recognition (NER) identifies the named entities in texts and associates a canonical form and a semantic category to them. A canonical form is a unique representative of a set of forms denoting the same entity. The semantic type is a rough ontological knowledge about the NE. NER relies on named entity dictionaries often tuned for a specific domain and a specific type of documents.In the general language, variant and ambiguous NE frequently occur and even more in sub-languages of technical and scientific domains. For instance, Paris is ambiguous since it alternatively refers to entities belonging to different semantic categories: either a person or a place. The gene name cat may also refer either to a protein or to the mammalian. Many different name variation types can be observed: acronyms (chloramphenicol acetyltransferase / CAT ), abbreviations (Bacillus subtilis / B. subtilis), ellipses (EPO mimetic peptide / EPO), typographic variations (sigma K / sigma(K) / sigma-K ), synonymy due to renaming (SpoIIIG / sigma G). Each type of variation is handled differently Named Entity Recognition
Because of the ambiguities and variations, named entity tagging cannot be achieved by simple dictionary matching; it also involves matching of the context of the candidate NE by NER rules. On one hand, disambiguation rules specify in which context a given NE candidate should be interpreted as belonging to a given category. On the other hand, variation rules enrich the dictionary with lists of synonyms or are applied on the fly to recognise variant forms.Active domains constantly produce documents containing new concepts and new NE, thus dictionaries are quite hard to keep fully up to date. Additional recognition rules are able to palliate the NE dictionaries incompleteness. These rules exploit the morphology of the candidate NE and various contextual clues in documents.The respective roles of the dictionary and the rules are illustrated by one experiment we did in biology In Alvis, NE tagging is a two-step process. The first one only involves matching dictionary entries on the text to identify known NE. This tagging is used afterwards for word and sentence segmentation to avoid the interpretation of abbreviation dots as sentence separators (e.g. Bacillus sp. BT1 ). The second NER step is achieved after documents have been segmented and lemmatised and word morphology have been analysed. The conditions of the NER rules are checked for each candidate phrase, then NE are disambiguated and associated with their semantic type. Dictionary-based annotations are removed when they correspond to ambiguous NE and new NE annotation are added.Corpus-Based Acquisition of Named Entities
Supervised ML methods can be applied to automatically acquire disambiguation and recognition rules in order to improve existing NE dictionaries. The reference training corpus is pre-annotated using existing NE dictionaries, then manually reviewed by human experts. Negative examples are automatically generated under the closed world assumption. The main features computed to describe the training examples of NE are usually typographic (length, case, presence of symbols and digits). Non-typographic features are based on neighbourhood words. In Alvis this acquisition is performed in two steps: feature selection, then induction of a decision tree by C4.5 from the Weka library The Alvis experiment on biology has shown that the quality of the manual annotation is critical for NER rule learnability. NE and non-NE should be clearly distinguished in the training corpus and the frontier should be strictly defined Term Analysis
Less widely acknowledged than NE recognition, term recognition is nevertheless crucial for further linguistic and semantic processing because terms often denote ontological concepts. Terminological analysis is a traditional step in sub-language analysis. It helps to identify the most relevant semantic units and it reduces the wording diversity.Term Tagging
In Alvis, term tagging consists in the projection of the terminology on the corpus. The text fragments that correspond to a given term are tagged with a canonical form, but no semantic category. Only flexion variations are possible at that stage (e.g. plural transformation). The simpler the tagging process is, the richer the resource must be. This calls for powerful acquisition methods.The Role of Terminologies
A terminology is a knowledge source that describes the specific vocabulary of a given domain. It is composed of a list of terms, single or multi-word lexical units. For instance, the well-known medical terminological resource MeSH thesaurus (Medical Subject Headings)For a given domain, there are as many terminologies as application goals. Although specific, terms of scientific and technical terminologies hardly ever match the actual document text. For instance we observed that MeSH and Gene Ontology (GO) lexicons, although useful and widely recognised biomedical resources, have a very poor coverage on our corpus of 16,000 sentences of PubMed paper abstracts: less than 1% of the GO and MeSH terms. The reason is that existing terminologies have often been designed for other purposes than automatic text analysis and do not reflect writing usages in corpora.Term Acquisition
Terminological knowledge acquisition tools have been proposed since the 1990s The results of term extractors remain noisy, however. Expert knowledge is necessary to filter out irrelevant terms and to validate the most relevant ones. For instance, YaTeA extracts the two terms "heterologous polypeptide" and "suitable polypeptide" from biological documents. Both terms match the same morpho-syntactic pattern adj noun, but the second one must be filtered out by manual validation, because the adjective "suitable" does not convey additional relevant information to "polypeptide".The automatic acquisition of term variants greatly increases existing terminological resource coverage on the corpora. In the same way as for candidate terms, candidate variants must be validated. The term list is then organised into synonym classes of term variants (similar to WordNet synsets) and the most representative of them is chosen as the canonical form. We have integrated a separate term variation computing tool, FASTR Semantic Typing with Conceptual Hierarchies
Once the semantic units (named entities and terms) have been identified, they must be related to the concepts of an ontology by semantic tagging and the concepts play the role of semantic types. Compared to NE broad typing, finergrained ontological categories are considered in this task. In the case where concepts are organised into generality hierarchies, semantic tagging selects the generality level relevant to a given application. The tagging should both highlight contrasts among critical objects (e.g. protein and genes in genomics) and attenuate or remove unessential differences (e.g. rhetoric or stylistic considerations in scientific documents, the result indicates or the result shows).The Lexicon-Ontology Mapping
In the simplest case, ontology concept labels can be mapped to the text semantic units (e.g. protein as a concept maps to protein as a word) or through a one-to-one relation with a term or named entity lexicon entry. However, this process is not straightforward because some semantic units are ambiguous and can be assigned different ontological types. A typical example is star that denotes both an astronomical object and a famous actor. Contextual disambiguation rules associate the concepts of the ontology to the relevant lexical knowledge, in a similar way to NE type disambiguation rules. Various strategies involve various degrees of linguistic analysis and ontological inference in order to build the relevant context.Available ontologies are scarcely used for automatic text analysis. They are usually designed for domain modelling and inference, without the task of text analysis in mind so they are hardly usable for that purpose. In the best case, ontologies are used for manual text indexing such as MeSH indexing of MedLine abstracts or gene annotation by Gene Ontology entries Semantic Type Disambiguation
Disambiguation rules mainly rely on two types of contextual information: sets of neighbour words or syntactic dependencies. In the first case, each alternative meaning of an ambiguous term is attached to a set of usual neighbour words. An occurrence of this term is then interpreted according to the closest set. For instance, for disambiguating the word tiger in Flickr legends of photos as being a name of mammal or of a Mac OS version, word sets such as (mac, apple, OSX, computer / animal, cat, zoo, Sumatra) can be mapped to its context Finer disambiguation is achieved by taking into account the syntactic relations that connect the ambiguous term to its context. In the Alvis pipeline, disambiguation rules take advantage of the results of a syntactic dependency parsing that must match constraints defined along with ontology nodes. For instance, the word cat has many different meanings in biology, among which, a mammalian species or a gene name, though both senses are not found in the same contexts. Given the relevant ontology, in the sentence of Fig. This strategy greatly improves the quality of the disambiguation compared to simple neighbourhood-based strategies. However syntactic parsing is hardly applicable to very large datasets for computational performance reasons. It is appropriate for rather small specific collections.Acquisition of Conceptual Hierarchies
Corpus-based learning methods assist the acquisition of ontological hierarchies and disambiguation rules. Two main classes of acquisition methods can be applied: distributional semantics and lexico-syntactic patterns (see chapter "Ontology Learning").Distributional semantics identifies sets of terms frequently occurring in similar contexts in the training corpora. The definitions of context are the same as used in disambiguation: either word windows or syntactic dependencies. Various distance metrics have been proposed, all of which are based on co-occurrence frequency measures. Sets of close terms are supposed to be semantic classes and the generality relation is derived from set inclusions. The learning result must be manually validated; it happens that the distance does not denote a semantic proximity but a weaker relation. Linguistic phenomena like metonymy and ellipsis are typical sources of erroneous classes. Distributional semantics is considered robust on large corpora such as Web collections, but machine learning is more efficient when applied to homogeneous corpora with a limited vocabulary, reduced polysemy and limited syntactic variability. In the case of heterogeneous corpora, syntactic context is preferred over neighbourhood because the generated classes are of higher quality Research on lexico-syntactic patterns is largely inspired by traditional terminological methods • Indefinite apposition: the pattern "SU(X), a SU(Y)" where SU means semantic unit, gives X as an instance of Y, if Y is a concept. From the sentence "csbB, a putative membrane-bound glucosyl transferase", csbB is interpreted as an instance of transferase because csbB is a named entity and transferase is defined as a concept. • Copula construction: "SU(X) be one of SU(Y)" or "SU(X), e.g. SU(Y)".The fact that the NE abrB is an instance of gene concept is extracted from "to repress certain genes, e.g. abrB".The quality of the relations depends on the patterns. Pattern matches may be rare and precise (e.g. X also known as Y ) as well as frequent and weak (e.g. X is a Y may denote a property of X instead of a specialisation relation between X and Y ). Dedicated corpora such as textbooks, dictionaries or on-line encyclopedia are more productive although smaller than large Web document sets. The patterns may be automatically learnt from training examples annotated by hand or by bootstrapping learning from known pairs Pattern-based approaches are less productive than distributional semantics approaches, because of the low number of patterns matches in the corpus while distributional semantics potentially relate all significant words and phrases of the domain. However, the type of the relation is better specified and easier to interpret.Corpus-based learning is an efficient and operational way to assist the acquisition of lexicon-based ontologies. The synonymy and hyperonymy links extracted from text represent important lexical knowledge. But their modelling into the ontology strongly requires human interpretation and validation. One has to decide what should be considered as a property or a class (e.g. is four wheels vehicle a property or a type of vehicle?). The distinction between instances and abstract concepts cannot be automated. The independent knowledge bits must be properly integrated (e.g. if X is a Y and X is a Z, what can be said about the relationship between X and Y ?). Moreover the methods rely on the assumption that the learnt concepts are represented by explicit semantic units and that the formulation variations can be handled at the lexicon level. They are not applicable if the link between the text and the ontology is more complex and requires the application of inference rules at the ontology level.Identification of Ontological Specific Relations
Information extraction of events consists in identifying domain specific ontological relations in documents between instances of concepts represented by semantic units. The domain specific relations are defined in the ontology and reflected by the IE template slots (see for instance, the slot Interaction Agent in Fig. The recognition of the relation instances in the text consists in first checking the actual occurrence of candidate arguments in the text: there should be semantic units in the text with the same semantic types as the relation arguments (e.g. protein and gene in the interaction relation) The second step checks the presence of a relation between them by using IE rules as described in Sect. 2.2. Thus it does not consist in just tagging the text with entity couples that are known to hold a given relation.Designing Relation Extraction Rules
In complex cases, the arguments are not well-defined semantic units in a way that contextual explanations and evidence can be easily provided Then conditions of the IE rules usually include clues difficult to interpret in terms of ontological knowledge. For example, neighbourhood does not necessarily denote semantics but may capture some shallow knowledge that is useful in certain limited contexts. Rules often combine various matching conditions that pertain to different levels of text annotation (e.g. mixing conceptual, typographic, positional and syntactic criteria as in the examples of Figs. The design of efficient IE rules becomes a complex problem that remains open after many years of active research. Manual design is tedious, rarely comprehensive and unreliable (Sect. 7.1). Acquiring extraction rules by Machine Learning from training corpora saves expert time but was limited to rather simple cases until recently. Learning relational extraction rules remains challenging (Sect. 7.2) but the availability of new text analysis tools promises a lot of progress. The recent progress in performance and availability of syntactic dependency parsers had also a very positive effect on the system abstraction ability. When syntactic parsing conditions are combined with ontology-based semantic types, it may be easier to relate the rule conditions to the ontological definition of the objects IE Rule Learning
Learning IE rules for identifying specific domain relations is done by supervised learning applied on a training corpus where the target information was manually tagged. The abstraction degree of the learnt rules strictly depends on the representation of the training examples. Their features are derived from the linguistic analysis of the training corpus. The number of errors in abstract features like syntactic dependencies tends to be higher than in low level information such as word segmentation. The Machine Learning methods (e.g. ILP) applied to complex representations such as relational representation are also more sensitive to the errors occurring in the example description as opposed to statistics-based methods.Pre-processing the training examples by feature selection or more complex inference may reduce the number of errors, while preserving discriminant features. This is the track followed by LP-Propal method based on the Propal algorithm The application of LP-Propal to one of the LLLDiscussion
As mentioned above, IE has made significant progress and powerful IE systems are now operational. The previous sections have described on which principles a generic and modular IE system should be founded. This last section focuses on the key issues that remain to be solved in order to fully ground IE on ontologies.Beyond the Development of NLP Toolboxes
By acknowledging the needs for domain-specific applications, the IE field has been exploring horizons outside the frame of MUC, which was rather generalist. This called for a more sophisticated linguistic analysis to take into account the diversity of sub-language formulations and to improve the richness and reliability of the extracted information. The IE performances greatly improve in consequence as shown in Sect. 7.2. The NLP underlying analysis is more expensive in term of computational time, but the IE is also more robust.The availability of NLP toolboxes and pipelines helped IE system designers to achieve these results by exploiting and integrating various natural language processes into a unique IE system. An important effort in software integration was necessary, because NLP tools are usually developed independently by different teams and may have partially overlapping roles. For instance syntactic taggers, such as the popular TreeTagger, perform their own word segmentation and lemmatisation. The integration of a POS tagger with a third-party segmenter raises complex token alignment problems. The integration of each processing step in the NLP pipeline raises similar questions that should be properly solved for avoiding concurrent annotations and inconsistencies.However, focus has been put on software integration rather than on knowledge integration and several problems remain to be addressed. More fundamentally, IE approaches correspond to a relatively narrow form of textunderstanding:• The analysis is mostly limited to the scope of sentences. IE does not take the whole document discourse into consideration to the exception of anaphora resolution that extends the analysis to neighbour sentences. • Sophisticated ontology-based inference models beyond generality tree climbing are rarely involved. The conditions of the extraction rules are usually considered as independent.Lexical Knowledge as a Mediator Between Text and Ontology
We have argued in Sect. 2.3 that for text interpretation the lexical knowledge plays a necessary role of mediator between the text and the conceptual model. We have shown that a lexical base is composed of a lexicon and a set of rules. Their relative importance varies from one source to the other. The terminology represents the simplest case where the variants are listed in the lexicon and no rule is used. The domain specific relations represent an opposite case where the lexicon is quasi-absent, all the knowledge being embodied in the rules.To be fully operational, maintainable and reusable, this complex knowledge structure should be properly represented in expressive knowledge representation languages. Lexicon and ontology representations have drawn a lot of attention the last years Toward Formalised and Integrated Knowledge Resources
With the progress of formalisation, IE research cannot longer consider ontologies as organised vocabulary or hierarchies of terms as thoroughly demonstrated in chapter "Ontology and the Lexicon".While formal languages for ontology representation have made great advances, there are few formal or operational proposals to tie ontologies to linguistic knowledge. This gap severely hinders the progress of IE and more generally of all textual content analysis technologies (e.g. IR, Q/A, summarising). As illustrated in Sect. 3, sophisticated and operational IE pipelines are available for developing new applications. However the cost of maintaining and reconfiguring them exponentially increases with the complexity of the linguistic knowledge. The field would gain a lot in moving the focus from software integration to knowledge integration.Another open question comes from the partial overlap between the various types of knowledge, which are traditionally considered as distinct resources. For instance, it is sometimes difficult to distinguish named entities and terms. From an ontological point of view, they have different status. NE correspond to instance labels while terms correspond to concepts and concept labels. NE rather appear in the leaves of the ontology, while terms appear in internal nodes. The distinction is also useful from a pragmatic operational point of view but it is not sound from a linguistic point of view. In the same manner, NE dictionaries and ontologies often overlap, because NE dictionaries include NE semantic types that should be related to the ontology. Developing a coherent set of knowledge source or integrating these various knowledge sources into a single knowledge base (KB) requires that the specific scope of each one is clearly defined.A third problem concerns the integration the learnt lexical knowledge into the available knowledge bases. This question is particularly critical for ontologies as reflected by the ontology population and ontology alignment issues.From a research point of view, the IE field has quickly evolved towards the integration of research results from natural language processing, knowledge acquisition and ontology domains. The results on ontology formalisation and the development of new representation languages has a very positive effect on IE modelling effort while linguistic processing and knowledge acquisition methods increase the operationality of IE systems.Introduction
The Web is often seen as one of the fundamental inventions of the twentieth century, which helped to shape the notion of the networked resources, networked economy, and ultimately networked world. The Web matured and became a fairly user-oriented information space -mainly as an effect of emerging interactive applications collectively known as "Web 2.0". Although Social Web and Web 2.0 are not subjects of this chapter (for interesting insights, see, e.g., Despite this vast user base, the Web (and also its recent social enrichment) has some limitations -most of them related to the issues of conveying knowledge (rather than merely data) and interpreting it (rather than merely retrieving). To address this limitation the vision of Semantic Web Once such conceptual commitments are established, one gains an opportunity to request much richer information from the Web (here "the Web" is used as a large-scale data repository). For instance, instead of merely finding a list of typically collocated key words, thanks to semantic markup and conceptual models, one would be able to tell that "carbohydrates" are a specific group of "organic compounds", and as such they share some generic characteristics of all organic materials, but at the same time "sugars" or "sugar acids" are narrower and conceptually more specific terms. Admittedly, this is a fairly trivial enrichment, but this neighbourhood of conceptually related terms may be used, e.g., to expand the user's original search query. In a different domain, instead of merely retrieving a list of articles containing term "user modelling" among keywords, we can combine the conceptual annotations and data from several ontological models, so as to obtain, for example, a list of leading experts publishing on that topic or a list of publishing outlets where such topic may be appearing.However, there is a certain three-way tension between (1) the dependence of the Semantic Web on semantic annotations (done on a large scale), (2) the cost and complexity of providing these semantic annotations, and (3) the cognitive complexity for human user to interact with the semantic annotations. In this chapter we briefly touch on the third aspect of this tension; with an emphasis given to the user aspects. We consider how a user can interact with semantic mark-up -by means of turning it into web-browseable resources and by means of combining it with standard web pages or other (textual) documents: If a user cannot or does not know how to access knowledge that takes form of those rich conceptual connections that form Semantic Web, as described above, then the knowledge has very little value.In this chapter, we touch on the issue of semantic navigation on three levels. First, we look at recent approaches to semantic browsing and navigation in general, trying to identify four families of user interaction styles. Second, we sum up our experiences with a tool from one of the reviewed families -Magpie, and highlight what functional features are valuable from the end user's viewpoint. Finally, we devote the rest of the chapter to exploring the future of semantic navigation on the Web.Existing Semantic Web Browsing Applications
We start by looking at characteristics and evolution of Semantic Web browsing tools in general. In terms of desired functionality, Quan and Karger Early Semantic Web Browsing Approaches
First prototypes of tools that claimed to support some aspects of Semantic Web browsing appeared around year 2002-2003. One common trait of these early tools was a close relationship with the Web. Indeed, in the absence of the key ingredient -semantic markup -these tools looked to the available web pages and featured a range of entity recognition algorithms Another system, this time getting inspired by the hypertext and the web browsing paradigm, COHSE Recent Advances in Semantic Web Browsing
More recent approaches to facilitating access to semantic markup explored a wide range of other metaphors reused from other domains. In particular, style sheets became one of such approaches that inspired several tools supporting Semantic Web browsing. For example, PiggyBank Another user interaction metaphor that got exploited when the amount of semantic data reached larger proportions was faceted navigation Further details on different metaphors and tools are provided in the subsequent sections -here we tried to sketch how the idea of accessing semantic markup started and evolved. Also, a differently scoped review and humancomputer interaction focused analysis of tools for navigating the ontologies and other semantic content can be found in our other publications What Is Semantic Web Browsing?
We have mentioned Quan and Karger's view on this topic earlier, but the requirement to separate content from its presentation is more of a philosophy than an actual definition. If we wanted to define what Semantic Web browsing is about, we suggest the following: Semantic Web browsing (or navigation) is a family of user interaction styles that rely on techniques for rendering all information that can be found in semantic markup data stores about a specific resource for the purpose of exposing the information space(s) or context(s) around a specific resource. In addition to this generic and abstract definition, a few specific points apply:• Data is usually expressed in a standard, web-compatible formalism, such as RDF(S) or OWL (the same can be extended to techniques that are usually expressed in formalisms like SOAP or WSDL). • Data is usually meant as a combination of schema/ontology-level model and assertions about specific facts. • Information space is usually equivalent to Data -Link -Data structures that can be dereferenced to hyperlinks comprehensible to standard web servers.Based on our experiences and feedback we obtained with the demonstrators of Magpie technology, we suggested in Before continuing let us briefly say what applications do not satisfy the definition above. First, tools like IsaViz or CropCircles are schema visualization techniques, not semantic browsers per se: (1) they mainly visualize the schema but not the data, and (2) they consider hierarchical links (as in "a category of all lions is classified under category of mammals"). Second, tools like FlickrThird, there are other approaches to accessing Semantic Web data that may feed into browsing, but are, in principle, different user interaction paradigms. For example, a data store can be queried (with a user formulating a query in SPARQL, RDQL, etc.), and appropriate data rendered (e.g., with a style sheet). This is a more controlled user interaction, whereas browsing has an element of serendipity (cf. with searching and browsing on the Web The research addressing the above criteria is ongoing and new tools are produced. In terms of user interaction, while there are XML style sheet formalisms applicable also to Semantic Web languages, more generic challenge of how a human user can (and wants to) interact with the Semantic Web content is only now getting a more significant attention. The issue of using multiple ontological frames to open up the interpretation choices is even less developed. Nevertheless, let us consider a few specific user interaction metaphors that satisfy the above definition and can be seen as browsers for semantically marked up data.In this overview, we consider three dimensions of data presentation, as defined in a broader framework for classifying semantic search tools:Navigation in graph structures: the focus is on the organization of nested data in a form of trees or graphs with expansible and clickable nodes. Faceted navigation: primary feature here is the opportunity for a continuous query refinement and step-by-step formulation of the user need. Navigation with templates: primary feature of this family is (1) strong focus on selecting data properties, and (2) using a rich repertoire of visualization metaphors to present nested data records. Navigation with semantic overlays: primary feature here is (1) data selection being dynamic and relying on a loaded schema, and (2) data organization done in a form of embedding data into a plain text (e.g., web page).Navigating in Semantic Graphs
Tabulator One interesting proposal coming from the Tabulator project is about the requirement to include some form of user interface "tips" in the ontologies that can be interpreted by generic applications, such as Tabulator, effectively choose appropriate and most useful user interaction components to data from unfamiliar domains.Faceted Navigation
Large datasets (e.g., libraries or museums) have many dimensions along which they can be browsed, searched or navigated. One interaction strategy for such data -in addition to simple searching and browsing -is faceted browsing, where users filter an item sub-set by progressively selecting from valid dimensions of an appropriate classification. On a non-semantic level, the strategy was piloted in Flamenco A number of data-centric semantic browsers draw on a popular metaphor of faceted browsing. For RDF data, Longwell Another generic suite of faceting techniques originating in mSpace Navigation Using Styles and Templates
This approach to browsing emerged from the fact that much information present on the Web is already stored in relational form, in the database-driven web sites. Therefore, another way to resolve the aforementioned knowledge acquisition bottleneck is to take advantage of the structural clues of this structured web content to re-create the original information stored in the databases backing this content.Thresher A notable trend related to templates is the push away from the heavy clients towards lightweight clients -often in a form of plugins or bookmarklets. A lightweight equivalent of Haystack is PiggyBank Once information has been extracted, one way to reuse it is to re-publish it back to the Web. Here, Exhibit, another tool from the same family as the above tools, is a JavaScript-based approach that exposes structured data to the Web using styles and templates. Rather than directly showing graph structures, Exhibit, PiggyBank and others emphasize the need to make the semantic content human-friendly -hence, templates and styles serve to "prettify" the graphs and show them in a variety of familiar metaphors (e.g., timelines, maps, tables, etc.) One shortcoming of this approach is its focus on presenting simple annotations -they rely on the fact that inference support and additional personalization by the users are not needed in most browsing scenarios.Semantic Layering and Service-Based Navigation
Semantic layering is a notion that was introduced in connection with semantic browsing and navigation by Magpie To illustrate the notion of semantic layering take Magpie, as a web browser plugin it has to be initialized with a user-selected (or downloaded) lexicon. Lexicon-or gazetteer-based parsing is one of the approaches to an entity Fig. Annotated and highlighted concepts become hotspots that allow the user to request a menu with a set of actions (formally coded as web services ) for a relevant item. Here it suffices to say that web service choices depend on the ontological classification of a particular concept in the selected ontology and on what services are available for a given ontology. Magpie plugin is wrapping a user's click (i.e., the request for a particular service) into a URI, which is then unwrapped to communicate with the actual web service using SOAP over HTTP. The results from the individual web service may be constructed based on data retrieval (Fig. One feature of this approach to semantic navigation is the notion of ontological perspective and its selection by the user. It can be seen in other tools of this family; e.g., VIeWs Semantic Web Browsing: Experiences with Magpie
In the previous publications (e.g., Positive Experiences with Magpie
As shown in previous publications, Magpie is a generic and flexible semantic browser -in terms of supporting any ontological viewpoint the user is willing to commit to and interpret or annotate web pages. In Fig. One feature that came out of evaluations as positive was the opportunity for the user to access data even in situations, which would otherwise require the user to formulate a complicated and lengthy query. Second aspect that affected users' performance in our tests was the actual support for navigation; i.e., semantic data was not only retrieved, the framework enabled composing partial data retrieval services into a more complex service that showed the user how to apply analytic or synthetic compositions to obtain more valuable information. When compared with established techniques, such as Google Scholar in the domain of academic support, the value of semantically enriched platform showed in identifying similar researchers or topics, in identifying groups of researchers formed around a theme rather than explicitly joining any specific mailing list, discussion board, or working group.Another positive aspect is the capability to interact with the user via semantic web services -these can even be derived automatically based on a given ontology. Services can be obviously composed, and thus a more natural and richer user experience can be achieved. For example, the content of service response in Fig. Semantic shortcutting is also useful -merely highlighting concepts from a user-selected lexicon (which may be built by combining chunks from several ontologies) in a text gives an indication of its relevance. Combining this with services acting as an inference shortcut, even fairly sophisticated data relationships can be accessed with a single click. While this might not be useful for every user, in analytic and synthetic tasks (such as compilation of expertise sources on a given topic), Magpie shortcuts can cut the processing time from hours to a few seconds (for other analyses see also Shortcomings of the Magpie Approach
Although the "single ontology = single interpretative perspective" paradigm used by Magpie reduces the size of the problem space, this reduction is not always helpful. Although it focuses the user's attention (as intended), it also unduly restricts the breadth of the acquired knowledge (this was clearly not intended). For instance, during a study session a student may come across a few similar but semantically not entirely identical study materials. This means that at each page, the student would benefit from minor tuning of the used ontology, glossary and/or service menu. These tunings reflect slight shifts within a broader problem space, which is a fairly common tactic we use everyday to deal with the open situations. Thus, Magpie's design actually features a gap between the inherent notion of a single, formal, sound but 'semantically closed ontology guaranteeing a certain precision within the domain, and the desire to open up the interaction by supporting multiple services, as well as multiple ontologies (at the same time, without explicit user's reloading step).Another shortcoming of the "layering" in Magpie was the limitation of categories fitting the screen estate -hence, most Magpie applications were limited to between four and seven top-level categories (buttons) and six to eight web services forming a menu. Since, screen limits are unlikely to change, new approaches to presenting semantic data need to be explored; in particular, when one extends the "single ontology" perspective to multiple ontologies, more content becomes available, more services may be found and invoked. Yet, exploration without guidance and tracking may quickly degrade to chaotic and blind clicking.Future of Semantic Browsing
As we highlighted in Sect. 2, out of the four criteria for an application enabling the user to browse the Web using the semantic links, the least advanced is the second -the capability to apply multiple ontological perspectives in multiple user contexts. Therefore, we first touch on the issue of acquiring ontologies from an open, distributed environment of the Web. Then we suggest how multiple ontologies may be interacted with on the level of user interfaces.Finding Distributed Ontologies
Coping with multiple ontologies on the user level depends, to some extent, on an infrastructure supporting quick and efficient selection of ontologies. However, as the number of ontologies and semantically marked up data is growing at a rapid pace, it outpaced our understanding of the quality of this generated and designed content in the distributed Semantic Web resources. Our recent advances in infrastructure known as Watson Watson offers a scalable infrastructure for discovering and selecting ontologies distributed over the Web. It is a stand-alone (i.e., semantic browsing independent) infrastructure with several benefits over similar tools. For instance, Swoogle For the purpose of browsing on the open Web, the added value of infrastructures like Watson is in the fact that semantic and qualitative analysis of the harvested content is done independently of the semantic browser -on the infrastructural level. In addition to basic analytic information (e.g., data format or expressiveness), one can learn from Watson about the topological and networked relationships among ontologies and semantic data sets. All this helps us to acquire a heterogenous volume of semantic content applicable to any given web page; next, we briefly describe how the support for multiple ontological frames can be realized on the user level.Semantic Browsing Using Multiple Ontologies
Our new semantic browser ("PowerMagpie") relies on the generic Watson framework introduced in Sect. 4.1. Watson extends our semantic layering technique (Sect. 2.7) by feeding multiple ontologies to it. Thus, PowerMagpie may make different use of the retrieved ontologies, based on their quality, topic coverage, or expressiveness, rather than merely finding any semantic content containing a given keyword.Unlike the previous versions of Magpie that were restricted to semantic layering based on categories specific to a single ontology, the new framework is more flexible. Apart from offering multiple ontologies for any web page, PowerMagpie can discover additional semantically related content, which is not directly or indirectly referred by the user-selected ontology. This capability uses the fact that each ontology models a certain aspect of the world, from a particular, non-exhaustive perspective. Hence, it makes sense to view one ontology in the context, i.e., in a relation with other ontologies on the Web.The strategy of finding semantic similarities is common, e.g., in query expansion, but not in search engines. The majority of search engines bases the similarity on the lexical proximity of resources, which, in turn, draws upon the underlying search index. When such a similarity-computing service is implemented outside the search engine scope -i.e., it cannot exploit the document index to explore the resource neighbourhood -a dynamic, document-specific descriptive vector of terms needs to be computed. In our framework, this capability is referred to as document fingerprint, and it is somewhat resembling a summary of the document, a set of key defining concepts. The fingerprint terms are submitted to the Watson Semantic Web Gateway (Sect. 4.1). The key idea of interfacing Watson rather than generic engines, such as Google, is to reuse already formalized and represented conceptual commitments captured in numerous ontologies that Watson harvested on the Web.The technique is inherently iterative: the web browser plugin starts with an initial document fingerprint and tests its conceptual fitness against the existing ontologies. From the most relevant ontologies one can calculate semantic neighbours of the matched concepts, which, when returned to the PowerMagpie plugin serve as candidate fingerprint extensions. The plugin attempts to find matches to these fingerprint extensions, thus disambiguating between the different perspectives in which a document might be interpreted.Different ontologies not only facilitate different navigational paths for the document interpretation, they also offer opportunities for an implicit annotation of the page and for an implicit ontology population. In many semantic web browsers in the past, annotations were merely visual and transient. Now, with discovering new ontologies it makes sense to store the annotations locally. One formalism that has been recently agreed upon to facilitate this reuse is RDFa Functional Overview
PowerMagpie is implemented as a web browser bookmarklet, Term selection and ranking exploits the structure of a web page. For example, assigning the term appearing in the title or in a heading more weight than to a term randomly found in a document paragraph. This traditional filtering technique is extended by a calculation of weights from the popularity and frequency of these terms and of lexically similar terms in the actual ontology index maintained by Watson and Yahoo search engine.Web page processing is very simple: upon invocation, the document object model (DOM) of the page is serialized into XML and shared with the backend. The back-end carries out term extraction using TF/IDFOntology selection draws on the generic Watson Semantic Web Gateway, which pre-computes ontological indexes and thus simplifies and accelerates the selection process. There are several challenges on this level, due to the requirement on the web browser to respond to the user's requests in real time. Hence, ontology selection and processing must be also done in real time.Semantic matches are then returned to the web browser, where every matched entity is associated with a location in the web page and is expressed as an XPath expression Semantic layering is basically a visualization of matches in the web browser. For example, one can see the matches on the level of entire ontologies or on the level of concepts shared by the discovered ontologies but conceptualized differently. These different visual views on the semantic content then create a dedicated semantic layer (or a skin) over the web page, and three types of visualized content are shown in Fig. Discussion
The Semantic Web is gaining momentum and more semantic data is available online. This has an impact on the application development strategies. The original Magpie as described in the previous edition of this book The idea of exploiting the Web (and the Semantic Web) as a large source of background knowledge has appeared in several recent works concerning generic tasks (e.g., sense disambiguation or ontology matching). For example, Alani proposed a method for ontology learning that relies on reusing ontology modules from online ontologies relevant to keywords from a user query The use of Semantic Web at large as a resource in its own right introduces several new challenges. For example, in the open Semantic Web, it is unlikely that all ontologies and various lexicons derived from those ontologies would reside at the same location. Ontological resources are geographically dispersed, networked and richly interlinked. Given this, it is no longer sufficient for the user to choose ontology. Users may want to create their individual viewpoint from many networked ontological components. They may want to do it dynamically and without bringing any knowledge engineers into the loop.Moreover, one may need to combine semantic mark-up available within the web page with external semantic assertions coming, e.g., from third-party ontologies discovered by Watson Gatewayor similar engines. As we suggest in this chapter, tools like "PowerMagpie" may benefit from this emergence of a large body of semantic content Benefits of Using Multiple Ontologies
The key issue with older version of Magpie (and also other semantic web browsing tools) is the requirement upon the user to select, load and activate an ontology that would drive the application. For instance, in Magpie this has been achieved by means of ontology-derived lexicons, from which the application toolbar and semantic menus were created. The shortcoming is that such an approach assumes the user knows which ontology to use and where to load it from. Obviously, loading an inappropriate ontology would yield false positives (e.g., identifying terms in text such as "Cork" being an instance of "Tropical Wood" rather than "City" and a part of "Ireland").Moreover, the user rarely has suitable means to assess the fitness of a particular ontology to the semantic annotation and interpretation of any given web page. Hence, the approach of combining Watson ontology discovery, analysis and access engine (Sect. 4.1) with an iterative matching between the document and the ontologies (Sect. 4.2) automates these key decision tasks for the user. The automation is achieved by means of:• Ranking terms in the resource with an intention to produce an initial document fingerprint with semantic commitments • Selecting ontologies by matching the set of key descriptive terms identified in the previous step to the index of harvested semantic content • Ordering discovered ontologies to assist the user with assessing their fitness in terms of domain coverage, richness, and expressiveness • Creating dynamic semantic layers based on serializing selected semantic content from the discovered ontologies and visualizing it in text Semantic browsing is promoted in this chapter as a process of constructing and using semantic layers that are expressive and flexible in nature. Rather than using solely instances for semantic browsing (as, e.g., in faceted browsers and in semantic layering approaches), the proposed approach reminds the skins that visually amend user interfaces of many software applications. The "skinning" approach to semantic browsing embeds semantics onto any background text and thus supports advanced semantic analyses; for example:• On the level of ontologies it supports the identification of different perspectives and their role in facilitating different routes in semantic browsing. • On the level of conceptual entities the technique enables the user to compare what are the different meanings (and implications) of particular commitments in different ontologies. • On the level of concept links the technique shifts the user's attention away from singular entities and presents ontological relationships among the entities in the web document.• On the level of instances it supports ontology population and extension by mashing up and merging conceptual commitments from several sources into one (possibly persistent) skin representation.Advances in Semantic Browsing
The idea of interlinking semantic annotation, semantic browsing and semantic services is gaining popularity. Annotation is no longer a separate objective in its own right; new annotation tools aim to offer additional services, e.g., validation or consistency checking. A major challenge in the domain of semantic browsing stems from the need to make the association between semantic services and semantic mark-up more open and more flexible. Furthermore, a good motivator with usable semantic browsing techniques are needed to make semantic browsing a mainstream user activity.A potentially interesting input is likely to come from the deployment of modular ontologies and specialized services as opposed to monolithic ontologies with tightly integrated web services. The modular approach allows some of the services to be involved in evolving general knowledge captured on the Semantic Web. Some methods may use statistical techniques, whereas other services may rely more on social trust. It seems that knowledge evolution may provide a good test case and a motivator for semantic browsing toolswhether it is a formal evolution of knowledge within specific ontologies or repositories or a social, user-driven evolution based on the annotations and tags of users relying on similar ontologies in their browsing.Another motivator for semantic browsing tools might stem from a rise of new techniques for information extraction, text analysis, knowledge validation or relationship discovery. While the low-level techniques rapidly change and become outdated, semantic browsers with sufficiently flexible architectures may benefit from those changes -a web browser is a very low-cost tool to be upgraded by ordinary users. Thus, the visual components of a semantic browser may hide the flux of underlying technologies -one would be able to use the latest knowledge technologies without any major re-design of the existing user interaction techniques. Semantic browsers may thus become a bridge to enable a shift from closed, single perspective application development to a smarter, on-demand knowledge construction.From the very beginning of KM, two streams of research and applications could be identified, following the process-centered and the product-centered view on KM, respectively. Technical solutions in this area comprise, e.g., yellow page and expertfinder systems for determining the right communication partner, Computer-Supported Collaborative Work (CSCW) systems for effective collaboration between geographically separated people, or Skill Management systems for the systematic and planned acquisition and development of human skills.In this view, organizational measures play a particularly important role, e.g., the installation of expert networks, the running of training courses, the facilitation of virtual teams and communities of practice, and all kinds of cultural KM support.(2) The product-centered view assumes that knowledge can exist outside of people and can be treated as an object within IT systems. It focuses on knowledge documents, their creation, storage, and reuse in computer-based organizational memories (OMs). It is based on the idea of explicating, documenting, and formalizing knowledge in order to have it as a tangible resource, and on the idea of supporting the user's individual knowledge development/usage by presenting the right information sources at the appropriate time.The transition from intangible (implicit and tacit) to tangible (explicit) knowledge in the form of standardized processes and templates, of FAQs, lessons learned, best practices documents, etc., allows a company to enhance its structural capital to some extent -maybe at the price of reducing creativity and flexibility. Basic techniques for this approach come from Document Management, Knowledge-Based and Information Systems.In this view, organizational measures aim at fostering the use and improving the value of information systems by bonuses, or by installing organizational roles/processes for high-quality knowledge-content management.Figure Figure • Incorporates data and information from manifold sources • Organizes it according to a common corporate knowledge map • Provides collaboration and discovery services working upon these organizational knowledge sources • Feeds these services through a common knowledge portal into operational business processes and into KM processes Type 1 applications often do not maintain a knowledge-rich, explicit ontological basis; nevertheless, the box "Knowledge Map" in the middle of the picture points out the central role of a shared language to connect people to people, people to information, and information to information, in an organization. This is the target area for more "heavy-weight," knowledge-based approaches in order to improve KM systems and services by ontologies.Type 2 Applications: Intelligent Software Basis
Type 2a Applications: Intelligence-Enhanced Solutions. While Type 1 applications are based on "conventional" IT, we here subsume applications based on Artificial Intelligence methods, including ontologies as a core enabler. Figure R1: Minimalization of Upfront Knowledge Engineering
Since KM is considered an additional organizational task, orthogonal to the "productive" work, expensive start-up activities would be a major barrier for successful KM initiatives. On the other hand, it seems clear that no ontologybased approach can be introduced without an explicit commitment of all people involved and without their contributions to ontology engineering. Hence, all topics dealing with a smooth and cost-efficient introduction of ontologybased applications are particularly important for ontology-based KM:• Method-Driven Ontology Engineering. There are many far-developed ontology modeling and management tools. R3: Dealing with Heterogeneous Kinds of Information
Looking for a practical definition of "knowledge" (in contrast to data and information), it seems important that knowledge is always oriented towards action -this aspect is already treated above; other aspects concern the fact that knowledge is strongly related to context and that it has a network character -showing how pieces belong together. Technically, this leads to the requirement that KM applications often have to process data, information, and information sources created for capturing knowledge (like lessons-learned entries or best-practice documents) in a highly integrated manner. As a solution approach, such knowledge documents are annotated with metadata which can be processed automatically and set into relation with application data. Hence, a KM application should be built upon an Information Ontology • Which types of documents occur.• What metadata attributes they have and which ontologies determine the value ranges of these attributes, where: -This may differ from document type to document type: a lesson learned may have a pointer to the project it was created in and the question how successful this project was; whereas a technical report may have an attribute for the location of the hardcopy of the document in the library, or links to experts for the technology described. -Such metadata attributes may also be application specific; e.g., in an e-Learning application (which can be seen as a specific KM task) it might be important to specify how difficult to understand a document is and which prior knowledge is required; whereas in a Knowledge Trading scenario These ideas have been applied in the field of Experience Management in Software Engineering where sophisticated domain-specific information ontologies have been developed in order to identify those facets of a softwaredevelopment experience which are important for assessing its later reusability in another situation Of course, real-world KM applications (and their ontology aspects) have not only to meet the requirements described above, but also hold a rigorous cost-benefit analysis. A detailed analysis of an expected ontology life cycle can be a powerful guide to achieve an optimal level of formalization in terms of costs and benefits. Likewise an explicit handling of an ontology's sharing scope helps minimizing negotiation costs as well as the complexity of revision processes in case of ontology evolution (these dimensions and their trade-offs are theoretically discussed in Ontologies in Intelligence-Enhanced Applications
Knowledge Portals, or Community Portals act as an information intermediary which structures all aspects relevant to a given, specific topic, in order to allow a community of users to flexibly and easily access a huge amount of information in different formats (today, usually text documents) related to this topic, to exchange information and communicate about the topic in quest, and to maintain and extend the content base accessed via this Internet (or, Intranet) portal (2) Organizational Memories. An Organizational Memory Information System OMIS, or, for short, Organizational Memory OM (3) Lessons Learned Archives. A Lesson Learned (LL) is a piece of knowledge gained through experience, which if shared, would benefit the work of others. Usage and Benefits of Ontologies
In the above mentioned, major knowledge-based KM applications, ontologies are mainly used for the following three general purposes (see also O1: Ontologies Support Knowledge Search, Retrieval, and Personalization
The most important application of ontologies in KM -besides browsing interfaces in Knowledge Portals -is certainly to improve search and retrieval of documents by exploiting ontological background knowledge about the application domain.In In the Electronic Fault Recording system for structured documentation and retrieval of maintenance experiences for a complex and large mechanical device, The authors of In general, the more specifically a domain is described, the more powerful inferences for query expansion and query reformulation are possible; however, detailed models are expensive to acquire and maintain, such that we have the typical KM trade-off asking for economic rationality when deciding between "high-tech" and "low-tech" approaches.While the approaches above usually increase recall of IR, precision is not so often treated explicitly. The KonArc prototype (for storage and retrieval of experience in a database for software-solution designs) used domain-specific information about incompatibilities of search constraints (e.g., between operating systems and specific software packages) for early detecting empty answers sets (and also explaining the contradictions to the user) The so-far discussed approaches all describe information pull situations; of course, ontologies are also a means to provide the vocabulary for expressing personal interest profiles for information push services which automatically deliver knowledge and information for categories a user is interested in -be it in personalized knowledge portals that are offered by many KM tool-suite vendors, in KM-oriented RSS feeds, or in mobile KM scenarios which need a proactive knowledge supply. An example is the myPlanet system which creates personalized news with the help of an ontology-based user profile In general, the issues of sophisticated, ontology-based representations and processing of (life and work) context, user profiles, and user activities in order to realize high-precision retrieval, proactive, context-dependent knowledge supply, personalization of retrieval and presentation, collaborative retrieval, usage mining, proactive knowledge collection, group-knowledge sharing, etc., are still active and promising research topics in the intersection of KM and ontology research.O2: Ontologies Serve as the Basis for Information Gathering, Integration, and Organization
KM deals with knowledge resources of different degree of formality, often informal text documents. On the other hand, the more formally represented information we have, the more and better formal inferences are possiblefor query answering and passage retrieval, for derivation of new knowledge, or for comparing and integrating facts and documents from different sources. More formalized information (i.e., facts related to a predefined schema) allows, e.g., to partially automate problem solving or to integrate IR results into operative business applications. The basis for such inferences are the information ontology structuring the metadata of informal knowledge sources, and the domain ontologies structuring the content area of documents and providing background knowledge for inferences. This background knowledge may comprise information search knowledge as well as domain-specific application knowledge. Information Extraction (IE) algorithms (see chapter "Information Extraction" in this book) for (semi-)automatically annotating metadata to documents and Text Categorization techniques For realizing Business Intelligence applications in a KM context, domain ontologies provide the target data structures for gathering information from different sources in the Internet or a corporate Intranet. For example, in The Ontobroker O3: Ontologies Support Knowledge Visualization
Different aspects of visualization for information search have been discussed in the literature on Human-Computer Interaction (HCI) and in the Digital Library community (see, e.g., Such methods can be used for inspecting the metadata and content descriptions of knowledge stocks in order to create new knowledge by analysis and recombination of existing knowledge. In such cases visualization may help to illustrate structure (e.g., content density) and distribution of content in a document corpus, as well as relationships between specific metadata attributes (like time or geographic relationships regarding document content or document creation, as well as co-authorship relations between people). Visualization of content structures can even be useful for intra-document analysis for long documents like government reports, classical literature, socio-economic almanacs, etc. -in order to get a rough overview of topics discussed, of their textual manifestation, and their interrelationships, or in order to have a quick, topics-based access to document parts. Visualization is also valuable for finding useful knowledge items in vaguely specified search situations where (partially) exploring the information space is a part of problem-solving and helps clarifying the problem specification and/or its solution space.In the meanwhile, visualization for topic-oriented document access went into commercial practice. 7 A number of commercial companies offer tools for knowledge and information visualization, for instance:• USU AG (http://www.usu.de) or intelligent views GmbH (http://www. i-views.de/), among others, use a semantic network interface for browsing, navigating, and exploring the major topics and topic interrelationships in a collection of text documents, in combination with a search engine or for enterprise knowledge portals. • ADUNA (http://www.aduna-software.com/) offers a visualization of hierarchically classified objects which can be used to show instantiated 7 One enabling factor for commercially successful visualization suites for knowledge organization and access, may have been the IEEE Topic Map standard, see http://www.topicmaps.org/. Topic maps are often seen a competitor to ontologies because they serve partially similar purposes, but have different roots, some incompatible basic design decisions, a different research community. However, they have partially similar goals and application areas and complementary strengths to the mainstream ontology approaches -in particular, the design for human understanding and manipulation -such that the authors see them allies in the long term, rather than competitors.taxonomies or ontologies -this is used, e.g., to display how search results of a desktop search can be grouped according to their relevance for certain keywords and keyword combinations Altogether, visual approaches can be a great support for understanding, searching, and investigating huge amounts of information and metadata. An overview of research and practice of visualization for the Semantic Web (e.g., RDF Graph visualization) can be found in Independent from the question which visualization approach is used (even with a simple, tree-structured browsing interface), KM usually deals with sharing complex knowledge content between people with quite different background and interests; this may often lead to the requirement that multiple views onto the same knowledge base should be provided. This is to some extent contradicting to the goal of creating a widely shared ontology for enabling communication between people; nevertheless, this requirement should not be neglected in practice -in particular, regarding the future trends of more distributed KM scenarios (see Sect. 5). Preliminary considerations about the technical support for such scenarios are presented in Challenges for Ontologies in KM
Since Type 2A applications more or less represent the state-of-the-art in using ontologies for KM, we summarize some challenges which we see for the near future of research and technology transfer in this area: Evaluation: It is already an indispensable need for KM applications to show their economic benefits to the project sponsors -which is not easy. In order to be successful, we have to find success criteria and develop metrics to assess whether ontology-based applications are more useful than solutions with "low tech" approaches. Although there exist already methodologies for ontology-based KM projects (for instance Ontologies Towards Enhanced Integrated Solutions
We mentioned already that exploiting synergy effects between different applications in the complex KM scenario can be an interesting source of innovation -for both new ideas and improved effectiveness of existing software functionalities. This area -especially with respect to ontologies -is not yet explored very well; but, we give some examples for work into this direction:• We reported in Future Trends
Comprehensive KM frameworks emphasize that Knowledge Management can take place at the individual, the group, organizational, and interorganizational level. The software functionalities discussed in Sects. 3 and 4 are mostly used to support the group and organizational level. Focussing on the personal and the interorganizational level, are logical next steps. Economically, the transition to interorganizational KM is driven by the movements towards the Extended Enterprise which tries to integrate logistics and production processes along the whole production chain (cp. Technically, the concepts of Distributed Organizational Memory (DOM) A possible approach to realize AMKM or DOMs, is Peer-to-Peer technology (P2P, Some other recent trends, only enumerated in a sketchy manner:• The idea the semantic desktop is to use ontology-based, topic-oriented structuring mechanisms in the background for organizing and finding information from everyday-applications in the personal, private information space. The idea of the social semantic desktop transcends this from the personal towards the group information space. The NEPOMUK project investigates how such mechanisms can be used for personal and for community knowledge management (http://nepomuk.semanticdesktop.org/). • Folksonomies exploit the power of large user communities with lightweight semantic technologies to achieve nevertheless a good quality of indexing for information retrieval. The transition between such lightweight social software approaches and more heavyweight, ontology-based approaches is an open question with a particular importance for KM because it addresses the trade-off between costs and quality. • Process knowledge slowly becomes a topic of interest in KM, and in advanced, ontology-based information management projects. On one hand, business tasks and business processes are a source of context for knowledge creation and search; on the other hand, process and task execution knowledge itself may be a shareable, reusable asset in an organization; lastly, knowledge workers' productivity depends much on sensible task management support. Hence, manifold research topics can be found in this area and its combination with more traditional information management issues (see, for example, Introduction
In this chapter, we explore the uses within bioinformatics of ontologies and other ontology-like artefacts, some of which were described in chapter "Ontologies for Formal Representation of Biological System". That chapter provided a motivation for the use of an ontology and described the range now available. In the first edition of this volume, we explored why bioinformaticians have become so interested in the development and use of ontologies The need for a common reference for the functional attributes of gene products, by the genome projects for different organisms motivated the development of the Gene Ontology (GO) 1. A discipline of philosophy concerned with the description of that which exists 2. A shared understanding of what a community understands about a domain that allows machine reasoning In essence, these are both concerned with descriptions of the "things" in the world or the description of those entities as they appear within information.The emphasis of the second, however, is that of the shared use of the description and its use by computers. As we will see, defining what it is to be a member of a class, then agreeing the label for that class assists both human and computers in data processing. In a knowledge-based discipline, such as bioinformatics, having a machine-processable form of knowledge to allow a wide range of scientific inferences is vital. We claim, however, that description for the sake of description, without including the computer is potentially highly restrictive. An ontology, according to the philosophers who coined the term, is a description of the categories and membership criteria of those things which exist. Computer scientists have latterly taken this term and shifted its meaning somewhat In Sect. 2 we classify the uses to which ontologies have been put within bioinformatics. Then in Sect. 3 we look at some case studies of these uses. In Sect. 4 we discuss the current state and future directions for ontologies within bioinformatics.Classifying Uses of Bio-Ontologies
Ontologies, whether from the computer science or philosophical perspective, are all about description. The applications of ontologies within biology are therefore all rooted in description. Figure Other uses of ontologies exploit the structure of the relationships between the concepts. Having annotated data with a controlled vocabulary, the structure of the ontology can be used to query instance data or navigate instance data. To move from a shared understanding which is fit for humans to use towards one exploitable by machines, it is necessary to introduce a more strict semantics (a precise description of the relationships between concepts) and is facilitated by a richer expressivity (the ability to express different kinds of relationships). Additional semantic strictness and expressivity does not necessarily enable new uses per se, but can allow more extensive uses in the same area.The uses to which ontological description can be put include, but are not limited to:Reference ontology: Defining the classes of entities within a domain, hopefully both logically and in a human orientated fashion can be of utility in its own right. Simply affording a community of discourse an encyclopaedia of that which is known acts as a reference source for that domain. The Foundational Model of Anatomy There are many legitimate ways to describe the world; models are, after all, virtually neither complete nor wholly correct. This can be due to different perspectives on the same issues, e.g., taking either a developmental or structural view of anatomy will give different categories Guidance and decision trees: Ontologies, by capturing knowledge about a domain and encapsulating constraints about class membership, can offer guidance around a domain and support decision making processes. In query formulation, for instance, an ontology can inform an application or human operator information about what can be said about an entity There are a range of potential uses for bio-ontologies within bioinformatics. We have presented a simple classification scheme of their uses in order to help orientation and navigation within the field. All uses can be traced back to the description of entities in a domain which is an end in and of itself. Many of the uses are minor variations on major themes of controlled vocabulary, controlled structure and the querying that such knowledge models support. In the next section we take examples from biomedicine to illustrate this scheme.Case Studies
Using Controlled Vocabulary
The single most common use of ontology in bioinformatics is to provide a controlled vocabulary, which is then used to provide annotation for database entries. The pre-eminent example for this is the Gene Ontology (GO) The Gene Ontology is focused on describing three features or aspects of biology: the molecular function defined as the biochemical (or molecular) activity of a gene; the cellular component defined the location in the cell that a gene product is active; and the biological process defined as the biological objective, or the series of events to which the molecular function contributes As well as the ontology, there are also a large number of annotationsdatabase records describing various gene products (generally proteins or genes as proxies for proteins). At the time of writing, there were around 3,000,000 Uniprot For a GO annotation, the association between a term and the proteins is supplemented with "Evidence Codes"; this is a term from an additional controlled vocabulary that describe the kind of evidence that was used to suggest the association. These range from "TAS" or traceable author statement; in paraphrase this means that the evidence came From a statement in a review paper, rather than a primary research paper, which suggests that it is well enough acknowledged in the community. Other codes, such as IEP -Inferred from Expression Pattern -describe the kind of experimental evidence that has been used.The success of the Gene Ontology has spawned a large number of tools for its use. Perhaps the best known of these are Amigo -a website which functions as a browser for the Gene Ontology, shown in Fig. Perhaps the most common use of GO or its annotations is for the analysis of microarray results. Statistical Uses of Ontologies
We now describe the application of statistics applied to and with the Gene Ontology. The continued development of GO means that it now has around 24,000 terms. This large ontological structure has meant that GO has become difficult to present to users, particularly in the context of an expanded hierarchical viewer. The GO consortium's response to this was the introduction of "GO Slims" -defined subsets of GO. As well as a general purpose slim (the Generic GO slim), there are others tailored for specific purposes; for example the yeast and plant GO slim focuses on those terms which are important for the given organism; both contain "cell wall", (GO:0005616) for example, while only the plant slim contains "thylakoid", (GO:0009579). While these provide a partial solution to the problem, they retain difficulties; mostly that the size of the subset they provide is fixed; it cannot be changed to suit the purpose (although a new GO Slim could be created).One standard solution to automate the sub-setting of GO is to use a "level". By using terms, say 1 deep from the root of molecular function, a small number (18 at the time of writing) of GO terms can be used to summarize the others. This is unsatisfying, firstly from a theoretical perspective, GO is structured as a DAG not a tree -it does not really have levels as there are multiple paths to many terms; additionally, it is not clear that levels actually represent specificity. For example, "ice binding", (GO:0050825) is three levels below molecular function, while "high-affinity tryptophan transmembrane transporter activity", (GO:0005300) is 10 levels deep. One solution to this is provided by the GO Partition Database As well as summarizing GO, there are many applications that need a numerical measure of the semantic similarity between two GO terms or, in more common use, two entities annotated with one or more GO terms. Initial attempts to develop these measures came from WordNet Schema Reconciliation
There is great heterogeneity in how bioinformatics data are organized -that is, the schema of the databases differ. Complete integration of resources necessarily involves reconciliation at the level of data and the schema in which the data are held. Ontologies can be used with great effect to provide a structure for how data should be organized. In schema reconciliation, the general idea is that the differing representations of the data are re-modeled to fit with the ontological description of the data. Again, this is the general idea of a shared description of an understanding of what exists in a domain and the community members committing to use that description.One widely used and current examples of schema reconciliation in bioinformatics using ontology is the BioPAX project There are two top level classes in the BioPAX ontology: entity and utility-Class. Entities describe the biology while the utility classes record knowledge about the pathway data such as cross-references to other databases, evidence codes, and experimental conditions. Pathways are a subclass of entity, along with two sibling classes, interaction and physicalEntity. A pathway has components that are of the class pathwayStep, a utility class. Each pathwayStep contains a set of stepInteractions that describe the physical interactions, such as catalysis, modulation, biochemical reaction, complex assembly, and transport that make up that step in the pathway, or another pathway. A pathway, such as glycolysis (the conversion of glucose to pyruvate), MAPK (the intra-cellular transmission of growth factor signals), or apoptosis (biochemical events leading to a programmed cell death)is composed of instances of interactions. Interactions can occur between entities so that interactions of interactions and interactions of pathway can be represented.Figure Once the physicalEntities that participate in the reactions are identified, together with the interaction roles they play in the reaction, we can represent For each database, committing to the BioPAX ontology, a converter is made that maps that resource to elements of the ontology. The BioPAX ontology is general enough in how it models the elements of pathways to capture a wide range of the existing resources. It does this by modeling at a high level of abstraction. BioPAX does not attempt to make a canonical view of bio-pathways -a standard view, for instance, of glycolysis. Rather it describes the elements of pathways, their steps, types of interaction and so on. This means the actual pathways, enzymes, small molecules, and so on, are instances of these classes. This works in practice, although it is not ontologically rigorous The mapping in Fig. It is clear that not every resource to be mapped into BioPAX will have a schema element equivalent to all classes in the ontology. Here, the constraint based nature of OWL can help. An OWL class describes what is known about instances of that class. Simply asserting an instance to be a member of that class implies that the restriction on that class apply to the instance. So, for example, BioPAX states that all PhysicalEntity have a Cel-lularLocation, but if a client database does not give cellular location, it is simply assumed to exist. OWL's ability to describe under-specified knowledge in this way is of great utility for this kind of modeling as it means resources can be compliant without over-committing Once data are mapped into a common schema, it provides another level of query. The BioPAX initiative is rare within bioinformatics for being schema reconciliation as an end in itself. Schema reconciliation as described for BioPAX is a common factor in many systems, but is rarely done purely for the sake of schema reconciliation. TAMBIS (see Sect. 3.4) uses an ontology as a common schema, integrating through queries diverse and distributed bioinformatics data resources. Here, the schema reconciliation is part of the query answering process. Similarly, in the work of the Health Care and Life Sciences working group of the W3CClassifying Instances
Ontologies are description and definitions of instances in the world and we have already seen their utility in de facto integration with increased recall and precision in queries across diverse resources. In this section, we take this theme further by looking at broader ways of using ontologies to query data in the form of instances described by an ontology and to enable the recognition of the types of instances present. These are both forms of classification:1. An ontological class has an extent of instances. By creating a class, a set of instances is being described. A query also describes a set of instances.In this way, a query classifies instance -a query puts instances into a class. 2. A defined class captures the properties that are sufficient to recognize an instance as being a member of that class In this section we will concentrate on the second form of classification. Our chapter in the first edition In the second form of classification, we are moving much more towards ontology capturing knowledge for computational use. As described, the most straight-forward way of doing this is to recognise when an instance belongs to a particular class.Bioinformatics is rich with tools designed to detect features on proteins and DNA sequences. From the features detected, a human bioinformatician is given clues by which data can be interpreted and classified. Typical of this is the classification of protein sequences by the presence of a certain configuration of features that, for example, suggest a certain catalytic or other behavior. Tools such as InterPro and InterProScan We are beginning to see examples of this very general technique in the bioinformatics arena. For example, genome complements of protein phosphatases have been classified The proteins from a genome were analyzed with InterProScan to detect their sequence features. The scan results were processed and transformed to produce a collection of OWL individuals for each named protein, along with assertions as to which protein sequence features were present.The instances are classified against the ontology, producing a catalog of the phosphatases present in the genome. Note that in Fig. FungalWeb In FungalWeb, the instances are: The species of fungus; named proteins, as individuals, are classified by their reaction; chemical names represent individuals of enzyme substrate and product; and industrial applications of enzymes were modeled as individuals. Properties from the ontology allows these individuals to be related by assertions. An example set of individuals can be seen in Fig. Both these examples show ontologies written in OWL being used to reason over bioinformatics data in the form of OWL instances. In the case of the phosphatases, an ontology has been used to drive biological discoveries. OWL ontologies provide a method, through their necessary and sufficient conditions on classes, by which the features for recognition of class membershipdomain knowledge -can be computationally encoded. Bioinformatics, through tools such as InterProScan, provide the computational means for recognizing features on data. If we have the means to encode first, those features in an ontology; second, the class definitions in terms of those features by which an individual can be recognized to be a member of a class; and third, the means by which features can be detected an encoded as OWL individuals; then we have a general mechanism for classifying data.Discussion
We see many inter-related uses of ontological description of entities in the world. The overwhelming use of ontologies in bioinformatics is still the annotation of data to provide a common way of describing these data and then to enable the querying, clustering and further analysis of these data. This has been enormously powerful. It has enabled a large range and quantity of biological queries and insights to be gained. It has made the very expensively generated biological data much more useful.Whilst an ontological description of a domain is useful as a way to capture knowledge and stimulate thinking about a domain, a more exciting prospect is the opportunity to capture domain knowledge such that we can make computational use of knowledge in a symbolic form. The mass annotation of biological data has begun this action with clustering for the analyses of experimental data. Statistical measures over the annotated data, exploiting the structure in which the ontology terms are held provides more sophisticated analysis of these data.Drawing together diverse data into a common setting at both the level of schema and value, whether transiently or in a more sustained fashion, enables richer queries and analysis. The richness of and high-fidelity of ontological description makes it a good candidate for such reconciliation.Within the life-sciences, real computational use of knowledge is still in its infancy. Bioinformatics still has much to gain from basic annotation of its data with names supplied by an ontology. So much is enabled by this simple device that more complex analyzes from reasoning over symbolic knowledge are not yet demanded by biologists themselves. We have seen the beginnings of such computational use and it will test the scalability of current Semantic Web technologies and languages. All the current activities, however, are laying the foundation for a much deeper exploitation of bioinformatics data through the application of ontologies.Cultural heritage is a promising application domain for semantic portals • Global view to heterogeneous, distributed contents. The contents of different content providers can accessed through one service as a single, seamless, and homogenous repository Here semantic associations between search objects can be exposed to the end-user as recommendation links, possibly with explicit explanations. • Other intelligent services. Also other kind of intelligent services can be created based on machine interpretable content, such as knowledge and association discovery Semantic portals are very attractive from the content publishers viewpoint, too:• Distributed content creation. Portal content is usually created in a centralized fashion by using a content management system (CMS). This approach is costly and not feasible if content is created in a distributed fashion by independent publishers, e.g., by different museums and other memory organizations. Semantic technologies can be used for harvesting and aggregating distributed heterogenous content (semi-)automatically into global content portals • Shared content publication channel. In the cultural domain the publishers usually share the common goal of promoting cultural knowledge in public and among professionals. A semantic portal can provide the participating organizations with a shared, cost-effective publication channel A cultural semantic information portal includes several major components. First, we need a content model for representing cultural metadata, ontologies, and rules. Second, a content creation system is needed for creating and harvesting content. Third, the portal publishes semantic services for (1) human end-users as intelligent user interfaces and possibly for (2) other portals and applications as web services. In the following these components are explained in more detail.Content Models for Semantic Cultural Portals
The semantic web "layer cake model" makes the distinction between a syntactic data level based on XML• Metadata level. The RDF data modelIn the following, metadata, ontology, and logic layers are considered from the viewpoint of semantic cultural portals. Issues related to trust on the semantic web in the cultural heritage domain have thus far not been discussed much in the literature.Metadata Schemas
Cultural content in museum collections, libraries, and other content repositories is usually described using metadata schemas (also called annotation schemas or annotation ontologies). These templates specify a set of obligatory and optional elements, i.e., properties, by which the metadata for content items should be described. For example, the Dublin Core (DC) Metadata Element SetDC is used as a basis in more detailed cultural metadata schemas, such as the Visual Resource Association's (VRA) Core Categories. Another approach to creating metaportals is to first harvest the content into a global database, and search the global repository. Protocols such as Open Access Initiative Protocol for Metadata Harvesting (OAI-PMH) 14 can be used for distributed content publishing and harvesting.Schema definitions tackle the problems of syntactic and semantic interoperability of content objects. Syntactic interoperability can be obtained by harmonizing encoding conventions (e.g., a date format) and other structural forms for representing data (e.g., an XML schema). Semantic interoperability is obtained by shared conventions for interpreting the syntactic representations, e.g., that the property dc:subject describes the subject matter of a document as a set of keywords taken from a thesaurus. Making different metadata schemas semantically interoperable includes two subtasks. First, semantic interoperability of element values has to be addressed using (shared) vocabularies and ontologies, and second, if multiple metadata schemas are involved, interoperability problems between different schema elements has to be solved. In below, these two issues are discussed in more detail.Vocabularies and Ontologies
Metadata schemas specify data formats but do not tell how to fill the element values in the formats. Additional standards and guidelines are necessary to guide the choice of terms or words (data values) as well as the selection, organization, and formatting of those words (data content). Data value standards have been traditionally specified by constructing controlled vocabularies and thesauri For example, there are many problems in utilizing the Broader Term (BT) relations of thesauri Several domain ontologies are used in describing cultural metadata. This raises up the problem of making ontologies mutually interoperable. There are solution approaches for this, such as ontology mapping and alignment Metadata Schema Interoperability
If a portal aggregates cultural contents described using different kind of schemas (e.g., for artifacts, music, maps, books, cultural sites), the schema element structures have to be made interoperable in one way or another, including the element values. If the element structures in the schemas refine each other, then using subproperties and the dumb-down principle of DC applications may be applied. In other cases, the metadata schemas can be made interoperable by transforming them into a shared underlying form.An approach to this is the CIDOC Conceptual Reference Model (CIDOC CRM) Another approach to semantic metadata schema interoperability has been developed in the CultureSampo portalLogic Rules for Cultural Heritage
A collection of cultural metadata and related ontologies constitute a knowledge base. On the logical level, rules can be used for deriving new facts and knowledge based on the repository, i.e., for explicating the implicit content of the repository, and enriching the content semantically. Some examples illustrating different ways of using rules in semantic cultural portals and systems are given below:• Explicating content of metadata schemas. Many metadata formats contain implicit knowledge embedded, e.g., in the relational meaning of the element names. In • Association discovery. Association discovery can be based on rules trying to find paths between resources in a knowledge base Cultural Content Creation
Several kinds of content need to be created for a semantic portal, including ontologies, terminologies, and semantic annotations. Also creating rules for, e.g., semantic recommendations can be seen as a form of content to be created. In below, ontology, terminology, and annotation creation are discussed in some more detail.The core of a semantic cultural heritage portal is typically a set of domain ontologies that are used for annotating cultural contents. Many vocabularies and ontologies, such as AAT, are used for defining universals, i.e., general concepts, classes, or types of individuals, such as "chair" (artifact ontology), "wood" (material ontology), "painter" (actor types), or "city" (geographical concepts). In creating ontologies, it is advisable to try to reuse existing ontologies or transform existing thesauri into semantic web formats, as discussed earlier. The ontologies can also be created or enhanced manually using an ontology editor such as Protégé. The terminology used in a portal is typically defined by associating ontological resources with preferable and alternative labels (e.g., using properties rdfs:label or skos:altLabel). Resource identifiers (URIs) of concepts, used by the machine, refer to concepts that are in principle language independent. However, labels used by humans can be multi-lingual, based on XML markup (e.g., xml:lang). This is essential when creating multilingual portals.The content providers often use different literal terms to refer to the same resources when describing metadata in legacy systems. For example, literals "United States" and "US" may be used to refer to the same country. This problem of synonymy can be approached by using alternative labels. On the other hand, the same term may be used to refer to different concepts, such as river "bank" and financial "bank". In order to eliminate such homonymy in terminology, it is advisable that an ontology uses a unique labeling of terms for concepts (e.g., "bank (financial)"). However, this does not solve the problem disambiguating meanings of terms occurring in natural language descriptions. Content in memory organizations is usually available as relational legacy databases, whose annotations are literal terms and free text descriptions. Such annotations are often intended for human usage, use various syntactic conventions, are often semantically ambiguous, and may contain syntactic typing errors. When transforming legacy metadata into semantic web formats, a key problem is how to map textual descriptions in the metadata with ontological concepts, e.g., how to determine that the string "bank" in a dc:subject description of a photograph refers to the concept "river bank" and not "financial bank". In below, the task of transforming literal element values used in legacy systems into ontological references needed on the semantic web is discussed. The semantic portal MuseumFinland A major problem in the RDF transformation above is how to disambiguate the meanings of homonyms (e.g., "bank") that may occur as keywords, free indexing terms, or in free text descriptions in different element values. Several methods can be applied here. For example, the type of the metadata element in which a homonymous expression is used, can often be used for semantic disambiguation effectively Another practical problem is spelling errors in metadata, and the variance of synonyms and correct syntactic encoding practices used at different organizations at different times, in different languages, and even by different catalogers. For example, the name of Ivan Ayvazovsky (Russian painter, 1817-1900) has 13 different labels in ULAN (Ajvazovskij, Aivazovski, Aiwasoffski, etc.), and the first, middle, and last names can be ordered and shortened in many different ways.Still another problem of transforming literals into URIs is complicated free text descriptions that may be used as element values, such as the material description "cow leather with decorations". Free text descriptions in metadata are in general difficult to search for due their syntactic variance, and for the same reason, difficult to transform into URI references automatically. The problem can be approached by using in indexing controlled vocabularies or ontologies. However, even then the problem remains when dealing with free indexing terms. These terms are, by definition, legal keywords of a thesaurus that are not listed as entries. For example, plant and animal types as well as person and location names can be used as free indexing terms. When encountering such a term, it cannot usually be associated with the underlying ontologies without human help.In a distributed content creation environment, free indexing concepts pose a challenge for ontology maintenance, too. In many cases new concepts should to populated into the ontologies and be shared, too. For example, when a painting of a new, formerly unknown artist is cataloged in a museum, the other catalogers and organizations should be made aware of her/him in order to prevent creation of multiple identifiers for the artist and later confusion of identities.A solution approach to this is to connect annotation creation tools to centrally maintained ontology library services that provide the clients with upto-date information about the vocabulary resources available, and facilitates creation and sharing of new resources collaboratively. An implementation of such a service is the ONKI Ontology ServerSharing unique URIs for concepts is preferable on the semantic web, but in practice there will be multiple URIs referring to a single resource. Creation of multiple identifiers for free indexing concepts cannot be eliminated totally in practice, and multiple identifiers will be created purposefully, too. Global dereferencing services will be needed in the future telling, e.g., that the concept of "London" in UK refers to the same thing as "Londres" in France.After creating semantically interoperable RDF metadata, content harvesting and aggregation can be done either (1) off-line before starting the portal or (2) on-line dynamically when answering end-user queries. The on-line approach is more dynamic. However, from the viewpoint of creating intelligent end-user services, the off-line approach seems more promising: (1) By creating a global knowledge base first off-line, reasoning can be easily done at the global scale across local contents, which facilitates, e.g., generation of recommendation links between the content of different content providers. (2) Knowledge can be compiled and critical reasoning tasks performed off-line beforehand for faster response times. For example, the rdf:type instance-class-relations can be explicated as RDF-triples based on the transitive closures of the subclass-of hierarchies. (3) The portal is independent of the content providers' possibly unreliable web services when running the system.Semantic Portal Services
The goal of semantic information portals for cultural heritage is to provide the end-user with intelligent services for finding and learning the right information based on her own preferences and the context of using the system. In the following, some possibilities of providing the end-users with intelligent services using semantically annotated metadata are shortly explored.Semantic Search
In information retrieval With non-textual cultural documents, such as paintings, photographs, and videos, metadata-based search techniques are a must in practice, although also content-based information retrieval methods Here the idea is to utilize actual document features (at the data level), such as color, texture, and shape in images, as a basis for information retrieval. For example, an image of Abraham Lincoln could be used as a query for finding other pictures of him, or a piece of music could be searched for by humming it. Bridging the "semantic gap" between low level image and multimedia features and semantic annotations is an important but challenging research theme A key problem of semantic search is mapping the literal search words, used by humans, to underlying ontological concepts, used by the computer. Depending on the application, only meaningful queries expressed by terms that are relevant to the domain and content available, other queries result in frustrating "no hits" answers. A way to solve the problem is to provide the end-user with a vocabulary as a subject heading category tree, a facet, as in Yahoo! and dmoz.org. By selecting a category, related documents are retrieved. Faceted search Faceted search has been integrated with the idea of ontologies and the semantic web The faceted search paradigm is based on facet analysis Semantic Autocompletion
Keyword search can be integrated with semantic search by extending search to the labels of ontological resources or facet categories. For example, in The idea of searching ontologies and facet categories for disambiguating intended meanings and roles has been generalized into the notion of semantic autocompletion Autocompletion has become a popular way to find meaningful keywords in large search vocabularies after Google SuggestSemantic Browsing and Recommending
In addition to semantic search, semantic content facilitates semantic browsing. Faceted search is already a kind of combination of searching and browsing because search is based on selecting links on facets. However, in semantic browsing the general idea is not to constrain the result set but rather to expand it by trying to find objects of potential interest outside of the hit list. The idea is to support browsing documents through associative links that are created based on the underlying metadata and ontologies, not on hardwired anchor links encoded by humans in HTML pages.A simple form of a semantic browser are RDF browsers and tabulators A more developed related idea is recommendation systems Relational Search
Semantic recommending is related to relational search, where the idea is to try to search and discover serendipitous semantic associations between different content items Personalization and Context Awareness
In many occasions the functioning of a semantic portal should not be static but adapt dynamically according to the (1) personal interests of the end-user Fig. An example of location-based adaptability is the mobile phone user interface of Also time is an important parameter for contextualizing portal services. For example, recommending the end-user to visit a site in the nature during winter may not be wise due to snow, or to direct her to a museum when it happens to be closed.Visualization and Mash-Ups
Visualization is an important aspect of the semantic web dealing with semantically complicated and interlinked contents Maps are useful in both searching content and in visualizing the results. A widely used approach to using maps in portals is to use mash-up map services. For example, In the cultural heritage domain, historical maps are of interest of their own. For example, they depict old place names and borders not available anymore in contemporary maps. An approach to visualize historical changes is developed in the Temp-O-Map system Another important dimension for visualizing cultural content is time. A standard approach for temporal visualization is to project search objects on a time line, as in Cross-Portal Reuse of Content
Portal contents can be reused in other web applications and portals due to semantic web standards. Reusing semantic content in this way is a kind of generalization of the idea of "multi-channel publication" of XML, where a single syntactic structure can be rendered in different ways. In a similar vein, semantic metadata can be reused without modifying it through multi-application publication.One possibility to facilitate cross-portal reuse is to merge triple stores, and provide services to end-users based on the extended knowledge base. For example, the learning object video portal Conclusions
Cultural heritage provides a semantically rich application domain in which useful vocabularies and collection contents are available, and where the organizations are eager to make their content publicly accessible. A major application type in the area has been semantic portals, often aggregating content from different collections, thus providing cultural organizations with a shared cost-effective publication channel and the possibility of enriching collaboratively the contents of each other's collections. For the end-user, new kinds of intelligent semantic services and ways of visualizing content can be provided. It can be envisioned that in the near future ever larger cultural semantic portals crossing geographical, cultural, and linguistic barriers of content providers at different countries will be developed, such as Europeana A major practical hinder for publishing cultural content on the semantic web is that current legacy cataloging system do not support creation of ontology-based annotations. If semantic annotations cannot be created in memory organization when cataloging content, then costly manual work is needed when transforming and disambiguating literal legacy metadata into ontological references in semantic portals. A solution approach to this fundamental problem is to provide ontologies as publicly available ontology services, and to reuse them-as well as semantically annotated portal contents-as ready-to-use functionalities (widgets) in legacy systems using mash-up techniques Recommender Systems
People find articulating exactly what they want difficult, but they are good at recognizing it when they see it. This insight has led to the utilization of relevance feedback, where people rate items as interesting or not interesting and the system tries to find items that match the "interesting", positive examples and do not match the "not interesting", negative examples. With sufficient positive and negative examples, modern machine learning techniques can classify new pages with impressive accuracy. Recommender systems can recommend many types of item, including web pages, new articles, music CDs and books.Unobtrusive monitoring provides positive examples of what the user is looking for, without interfering with the user's normal work activity. Heuristics can also be applied to infer negative examples from observed behaviour, although generally with less confidence. This idea has led to content-based recommender systems, which unobtrusively watch user behaviour and recommend new items that correlate with a user's profile.Another way to recommend items is based on the ratings provided by other people who have liked the item before. Collaborative recommender systems do this by asking people to rate items explicitly and then recommend new items that similar users have rated highly. An issue with collaborative filtering is that there is no direct reward for providing examples since they only help other people. This leads to initial difficulties in obtaining a sufficient number of ratings for the system to be useful, a problem known as the cold-start problem Hybrid systems, attempting to combine the advantages of content-based and collaborative recommender systems, have also proved popular to-date. The feedback required for content-based recommendation is shared, allowing collaborative recommendation as well.User Profiling
User profiling is typically either knowledge-based or behaviour-based. Knowledge-based approaches use static models of users and dynamically match users to the closest model. Questionnaires and interviews are often employed to obtain this user knowledge. Once a model is selected for a user, specialist domain knowledge for that user type can be applied to help the user. Behaviour-based approaches use the user's behaviour as a model, commonly using machine-learning techniques to discover useful patterns in the behaviour. Behavioural logging is employed to obtain the data necessary from which to extract patterns. Kobsa The user profiling approach used by most recommender systems is behavioural-based, commonly using a binary class model to represent what users find interesting and not interesting. Machine-learning techniques are then used to find potential items of interest in respect to the binary model, recommending items that match the positive examples and do not match the negative examples. There are a lot of effective machine learning algorithms based on two classes. A binary profile does not, however, lend itself to sharing examples of interest or integrating any domain knowledge that might be available. Sebastiani Ontologies
An ontology is a conceptualisation of a domain into a human-understandable, but machine-readable format consisting of entities, attributes, relationships, and axioms Ontologies help extend recommender systems to a multi-class environment, allowing knowledge-based approaches to be used alongside classical machine learning algorithms. Section 2 provides an in-depth overview of how ontologies are integrated into the techniques used for recommendation. Part IV of this book contains details on the current best practice for supporting infrastructures and for ontologies, especially chapters "Ontology Repositories" and "Ontology Mapping".Chapter Structure
In this chapter we show how ontologies are used in recommender systems today, providing an overview of the technology space and some further reading on specific approaches. We then examine in some depth a case study of two recommender systems that were among the first to adopt ontological techniques. In these case studies the problem domain, algorithms and results are detailed along with a discussion that highlights some of the practical difficulties experienced running a recommender system for real.Ontology Use in Recommender Systems
Ontologies are now used routinely in recommender systems in combination with machine learning, statistical correlations, user profiling and domain specific heuristics. Commercial recommender systems generally either maintain simple product ontologies (e.g. books) that they can then utilize via heuristics or have a large community of users actively rating content (e.g. movies) suitable for collaborative filtering. More research oriented recommender systems use a much wider variety of techniques that offer advantages such as improved accuracy coupled with constraints such as requiring explicit relevance feedback or intrusive monitoring of user behaviour over prolonged periods of time.Recommendation of new items to users can be performed by looking at item to item similarity (content-based filtering), item reviews within a community of users (collaborative filtering), semantic relationships between items (heuristic-based recommendation) or a hybrid approach. In many cases the type of approach adopted will depend heavily on how much metadata is available about the items and how much user feedback is available, both implicit and explicit. Content-based techniques work well if training data is available in advance. Collaborative techniques work well when a system has a large community of users. There are, however, no definitive rules to decide on an approach and normally experience and expertise is required to pick the best approach for a given problem domain.Content-Based Recommendation
Early recommender systems used content-based binary classification approaches looking at training sets of what was, and what was not interesting to a specific user. Machine learning techniques were employed to perform supervised learning based on sets of observed training examples that a user labelled either as "good" or "bad". A classic example of a content-based recommender system is Fab To enhance binary classification domain ontologies were introduced allowing multi-class classification and hence multi-class recommendation. Typically the classes in a domain ontology, such as a product ontology defining all the products of an e-commerce website, would be used to classify the previously observed products / web pages a user had purchased / viewed. A good example of multi-class recommendation is RAAP Once a domain has been classified in terms of ontological concepts the relationships defined by the domain ontology can be used to infer interest and relevance of one concept from observed interest in another. A knowledge-based system can use expert system rules to infer probabilistic interest in classes of item with a semantic connection to an observed item of interest. Typically the semantic distance (number of relationships away one topic is from another) is used to calculate semantic similarity, and this is used to weight likely interest. Entre Clustering and Topic Diversification
Some domains do not have well identified classes of item from which content can be classified. In these cases recommender systems have employed clustering techniques to identify within groups of items potentially similar classes. Hierarchical clustering has been used to categorize document collections for recommender algorithms Distance-based clustering Concept-based clustering takes items represented as attribute-pairs and builds relationships based on the probability of occurrence of attribute-pairs within nodes. An early example of concept-based clustering is the COBWEB Often recommender systems will recommend clusters of items that are very similar, or variants of the same item (e.g. different formats of the film/DVD). To avoid this topic diversification Collaborative Filtering
Collaborative filtering works by using the ratings provided by a community of users to recommend items for a specific user. There are two complementary approaches available, user-based or item-based collaborative filtering. Userbased collaborative filtering is where similar users are found and items recommended that these similar users also liked. Item-based collaborative filtering is where items are grouped if people rate them similarly.In order to perform collaborative filtering a user profile must be created from the available historical records of what items people have reviewed and rated. Often a 5-point scale is used for ratings (very good to very bad). A common user profile representation is a weighted vector if interest with as many dimensions as the domain has classes. Vectors can also be used for item to item similarity. Domains where item metadata is not accessible as ontological terms will usually apply pre-processing techniques to compute word/document/metadata term frequencies, remove common words and merge similar words using a thesaurus like WordNet.User-based collaborative filtering is the most popular recommendation algorithm due to its simplicity and excellent quality of recommendation. First neighbourhoods are formed using a similarity metric, such as a statistical correlation metric like Pearson-r correlation. Second a set of rating predictions are created using profiles that are within the same neighbourhood as the user's own profile. Recommendations are created from the top-N items. The GroupLens project Item-based collaborative filtering has become popular in the last 5 years since it decouples the model computation from the prediction process; Amazon Use of the Semantic Web and Web 2.0 Approaches
Recent work has also used some of the emerging Web 2.0 resources from the Semantic Web to help identify classes of item. One such system Case Study: Two Ontological Recommender Systems
For a case study two experimental recommender systems are presented, Quickstep and Foxtrot, that explored the novel idea of using an ontological approach to user profiling in the context of recommender systems. Representing user interests in ontological terms involves losing some of the fine grained information held in the raw examples of interest, but in turn allows inference to assist user profiling, communication with other external ontologies and visualization of the profiles using ontological terms understandable to users. Figure A research paper topic ontology is shared between all system processes, allowing both classifications and user profiles to use a common terminology. The ontology itself contains is-a relationships between appropriate topic classes; a section from the topic ontology is shown in Fig. Classification Using a Research Paper Topic Ontology
Sharing training examples, within the structure of an ontology, allows for much larger training sets than would be possible if a single user just provided examples of personal interest. Larger training sets improve classifier accuracy. However, multi-class classification is inherently less accurate than binary class classification, so the increased training set size has to be weighed along with the reduction in accuracy that occurs with every extra class the system supports.Both the Quickstep and Foxtrot recommender systems use the research paper topic ontology to base paper classifications upon. A set of labelled example papers is manually provided for each class within the ontology, and then used by the classifier as a labelled training set. In the Quickstep system users can add new examples of papers as time goes by, allowing the training set to reflect the continually changing needs of the users.In addition to larger training sets, having users share a common ontology enforces a consistent conceptual model, which removes some of the subjective nature of selecting categories for research papers. A common conceptual model also helps users to understand how the recommender system works, which helps form reasonable user expectations and assists in building trust and a feeling of control over what the system is doing.Ontological Inference to Assist User Profiling
Ontological inference is a powerful tool to assist user profiling. An ontology could contain all sorts of useful knowledge about users and their interests, such as related research subjects, technologies behind each subject area, projects people are working on, etc. This knowledge can be used to infer more interests than can be seen by just observation.Our two experimental recommender systems both use ontological inference to enhance user profiles. Is-a relationships within the research paper topic ontology are used to infer interest in more general, super-class topics. We add 50% of the interest in a specific class to the super-class. This inference has the effect of rounding out profiles, making them more inclusive and attuning them to the broad interests of a user. The profiling algorithm used is shown in Fig. The event interest values were chosen to balance the feedback in favour of explicitly provided feedback, which is likely to be the most reliable. The 50% inference value was chosen to reflect the reduction in certainty you get the further away from the observed behaviour you move. Determining optimal values for these parameters would require further empirical evaluation.Bootstrapping with an External Ontology
Recommender systems suffer from the cold-start problem In one of our experiments we take an external ontology containing publication and personnel data about academic researchers and integrate it with the Quickstep recommender system. The knowledge held within the external ontology is used to bootstrap initial user profiles, with the aim of reducing the cold-start effect. The external ontology uses the same research topic ontology as the Quickstep system, providing a firm basis for communication. The external ontology contains publications and authorship relationships, projects and project membership, staff and their roles and other such knowledge. Knowledge of publications held within the external ontology is used to infer historical interests for new users, and network analysis of ontological relationships is used to discover similar users whose own interests might be used to bootstrap a new user's profile.The two bootstrapping algorithms used in our experiment are shown in Figs. In addition to using the ontology to bootstrap the recommender system, our experiment uses the interest profiles held within the recommender system to continually update the external ontology. Interest acquisition is a problematic task for ontologies that are based on static knowledge sources, and this synergistic relationship provides a useful source of personal knowledge about individual researchers.  Profile Visualization Using Ontological Concepts
Since users can understand the topics held within the ontology, the user profiles can be visualized. These visualizations allow users to see what the system thinks they are interested in and hence allow them to gain an insight into how the system works. Profile visualization thus provides users with a conceptual model of how the profiling algorithm works, allowing users to gain trust in the system and providing users with a feeling of control over what's going on. With a better conceptual model user expectations should be more realistic.The Foxtrot recommender system visualizes profiles using a time/interest graph. In addition to simply visualizing profiles, a drawing package metaphor is used to allow users to draw interest bars directly onto the time/interest graph. This allows the system to acquire direct profile feedback, which can be used by the profiler to improve profile accuracy and hence recommendation accuracy. Figure Case Study: Experimentation Results
We have conducted three experiments with our two recommender systems. The Quickstep recommender system is used to measure the performance gain seen when using profile inference, and the reduction in the cold-start seen when an external ontology is used for bootstrapping. The Foxtrot recommender system is used to measure the effect profile visualization has on profile accuracy, and to perform a large-scale assessment of our overall ontological approach to recommender systems.A more in-depth statistical investigation of this approach has been performed using the datasets gathered in our user trials (260 subjects, 15,792 documents) and is published in Using Ontological Inference to Improve Recommendation Accuracy
Our first experiment used the Quickstep recommender system to compare subjects whose profiles were computed using ontological inference with subjects whose profiles did not use ontological inference. Two identical trials were conducted, the first with 14 subjects and the second with 24 subjects, both over 1.5 months; some interface improvements were made for the second trial. Subjects were taken from researchers in a computer science laboratory and split into two groups; one group used a topic ontology and profile inference while the other group used an unstructured flat list of topics with no profile inference. An overall evaluation of the Quickstep recommender system was also performed. This experiment is published in more detail in This experiment found that ontological profile users provided more favourable feedback and had superior recommendation accuracy. Figures Since 10 recommendations were provided at a time, a recommendation accuracy of 10% means that on average there was one good recommendation in each set presented to the user. We regard providing one good recommendation upon each visit to the recommendation web site as demonstrating significant utility.While not statistically significant due to sample size, the results suggest how using ontological inference in the profiling process results in superior performance over using a flat list of unstructured topics. The ontology users tended to have more "rounder" profiles, including topics of interest that were not directly browsed. This increased the accuracy of the profiles, and hence usefulness of the recommendations.Ontological Bootstrapping to Reduce the Cold-Start Problem
Our second experiment integrated the Quickstep recommender system with an external ontology to evaluate how using ontological knowledge could reduce the cold-start problem . The external ontology used was based on a publication database and personnel database, coupled with a tool for performing network analysis of ontological relationships to discover similar users. The behavioural log data from the previous experiment was used to simulate the bootstrapping effect both the new-system and new-user initial profiling algorithms would have. This experiment is published in more detail in Subjects were selected from those in the previous experiment who had entries within the external ontology. We selected nine subjects in total and their URL browsing logs were broken up into weekly log entries. Seven weeks of browsing behaviour were taken from the start of the Quickstep trials, and an empty log created to simulate the very start of the trial where no behaviour has yet been recorded.Two bootstrapping algorithms were tested, the new-system and new-user initial profile algorithms described earlier. As the new-system algorithm bootstraps a completely cold-start we tested from week 0 to week 7. The new-user algorithm requires the system to have been running for a while, so we added the new user on week 7, after the new-system cold-start was over.Two measurements were made to measure the reduction in the cold-start. The first, profile precision, measures how many topics were mentioned in both the bootstrapped profile and benchmark profile. Profile precision is an indication of how quickly the profile is converging to the final state, and thus how quickly the effects of the cold-start are overcome. The second, profile error rate, measures how many topics appeared in the bootstrapped profile that did not appear within the benchmark profile. Profile error rate is an indication of the errors introduced by the two bootstrapping algorithms. Figure The new-system algorithm produced profiles with a low error rate of 0.06 and a reasonable precision of 0.35. This reflects that previous publications are a good indication of users current interests, and so can produce a good starting point for a bootstrap profile. The new-user algorithm achieved good precision of 0.84 at the expense of a significant 0.55 error rate.This experiment suggests that using an ontology to bootstrap user profiles can significantly reduce the impact of the recommender system coldstart problem. It is particularly useful for the new-system cold-start problem, where the alternative is to start with no information at all and hence a profile precision of zero.Visualizing Profiles to Improve Profile Accuracy
Our third experiment used the Foxtrot recommender system to compare subjects who could visualize their profiles and provide profile feedback with subjects who could only use traditional relevance feedback. An overall evaluation of the Foxtrot recommender system was also performed.This experimental trial took place over the academic year 2002, starting in November and ending in July. Of the 260 subjects registered to use the system, 103 used the web page, and of these 37 subjects used the system three or more times. All 260 subjects used the web proxy and hence their browsing was recorded and daily profiles built. By the end of the trial the research paper database had grown from 6,000 to 15,792 documents as a result of subject web browsing.Subjects were divided into two groups. The first "profile feedback" group had full access to the system and its profile visualization and profile feedback options; the second "relevance feedback" group were denied access to the profile interface. A total of nine subjects provided profile feedback.Towards the end of the trial an additional email feature was added to the recommender system. This email feature sent out weekly emails to all users who had used the system at least once, detailing the top three papers in their current recommendation set. Email notification was started in May and ran for the remaining 3 months of the trial.Recommendation accuracy, profile accuracy and profile predictive accuracy was measured. Profile accuracy measures the number of papers jumped to or browsed that match the top three profile topics each day. This is a good measure of the accuracy of the current interests within a profile at any given time. Profile predictive accuracy measures the number of papers jumped to or browsed that match the top three profile topics in a 4-week period after the day the profile was created. This measures the ability of a profile to predict subject interests. Figures The "profile feedback" group outperformed the "relevance feedback" group for most of the metrics, and the experimental data revealed several trends. Email recommendation appeared to be preferred by the "relevance feedback" group, and especially by those users who did not regularly check their web page recommendations. A reason for this could be that since the "profile feedback" group used the web page recommendations more, they needed to use the email recommendations less. There is certainly a limit to how many recommendations any user needs over a given time period; in our case nobody regularly checked for recommendations more than once a week. The overall recommendation accuracy was about 1%, or 2-5% for the profile feedback group.This third experiment shows that both profile visualization and profile feedback can significantly improve the profiling accuracy and the  Case Study: Conclusions
Through our three experiments we have demonstrated that using an ontological approach to user profiling offers significant benefits to recommender systems.Ontological inference, even simple inference such as using is-a relationships to infer general interests, can improve profiling process and hence the recommendation accuracy of a recommender system. We achieve a 7-15% increase recommendation accuracy using just is-a relationships, and we feel it is clear that a more complete domain ontology, with more informative relationships, could perform significantly better.External ontologies can be used to reduce significantly the cold-start problem recommender systems face. We have shown that a bootstrap profile precision of 35% is achievable given the right ontological knowledge to drawn upon. While further experimentation is required to determine exactly how good a bootstrap profile needs to be before a cold-start is avoided, it is clear that external knowledge sources offer a practical way to achieve this.Most recommender systems hold user profiles in cryptic formats generated by techniques such as neural networks or Bayesian learners. Using an ontological approach to user profiling allows the visualization of user profiles using ontological terms users understand, and hence a way to elicit feedback on the profiles themselves. This profile feedback can be used to adjust profiles, improving their accuracy significantly. We have demonstrated increases in profiling accuracy of up to 50% of that which is achievable by traditional relevance feedback. These three features are implemented in our two experimental recommender systems. Overall recommendation accuracy, for individual recommendations, of 7-11% for a laboratory based subject group and 2-5% recommendation accuracy for a larger department based group is demonstrated. This gives an average of one good recommendation per set of recommendations provided for the small group of about 20 users, and one every other set for the larger group of about 200 users. Both these systems compare favourably with other systems in the literature when the problem domains are taken into account. D
Definition 2 . 1 (
•Fig. 1 .
Example 2 . 2
Definition 2 . 4 (Example 2 . 3 = {Person 1 ,
Fig. 3 .
Fig. 5 .
1 .
Definition 3 .Definition 4 .
( 6 )
( 7 )
?
Fig. 2 .
Fig. 1 .
Fig. 4 .
Definition 4 .Definition 5 .
4 : 1 .
5 .
Definition 8 .
Fig. 6 .
Fig. 7 .
Fig. 1 .
Fig. 4 .
Fig. 1 .
Fig. 2 .
Fig. 3 .
Fig. 4 .
Fig. 5 .
Fig. 1 .
Fig. 2 .
S
Fig. 1 .
Definition 1 .
Fig. 3 .
Fig. 4 .
Definition 4 .
Fig. 5 .
Fig. 7 .
Fig. 1 .
Fig. 3 .
Fig. 4 .
Fig. 1 .
Fig. 3 .
Fig. 1 .
Example 1 .
Fig. 1 .
Fig. 1 .
Fig. 2 .
Fig. 3 .
Fig. 4 .
2
Fig. 1 .
Fig. 2 .
Fig. 4 .
Fig. 1 .
Fig. 1 .
(Fig. 2 .
(
(
(
(
(
Fig. 3 .
(
(
Fig. 4 .
Fig. 5 .
Fig. 1 .
Fig. 2 .
Fig. 1 .
Fig. 2 . 1 , o punch 2 , o press 7 , o press 4 , o punch 11 }
Fig. 3 .Fig. 4 .Fig. 5 .
Fig. 1 .
Fig. 1 .
Fig. 3 .
Fig. 1 .
Fig. 2 .
3 …Fig. 3 .
Fig. 1 .
Definition 4 .
Fig. 1 .
Fig. 2 .
Fig. 1 .
Fig. 3 .
Fig. 4 .
b
Fig. 4 .
Fig. 6 .
Fig. 1 .
Fig. 4 .
Fig. 5 .
Fig. 1 .
Fig. 2 .
O'
Fig. 1 .
Fig. 2 .
Fig. 3 .
Fig. 1 .
Fig. 1 .Fig. 2 .
Fig. 3 .
Fig. 4 .Fig. 5 .
Fig. 6 .
Fig. 7 .Fig. 8 .
Fig. 9 .
Fig. 10 .
Fig. 11 .
3 = O 2 ∪ {cooperates-with(x, y) ↔ cooperates-with(y, x)} Transitivity. Although arguable, we specify reports-to as a transitive relation: O 4 = O 3 ∪ {reports-to(x, z) ← reports-to(x, y) ∧ reports-to(y, z)} Disjointness There is no Person who is both a Researcher and a Manager: O 5 = O 4 ∪ {Manager(x) → ¬Researcher(x)}Table 1 .
// structural inheritance of signatures defaulttype(?C1, ?A, ?T) :-sub(?C1, ?C2) and defaulttype(?C2, ?A, ?T). type(?O, ?A, ?T) :-isa(?O, ?C) and defaulttype(?C, ?A, ?T).Definition 3. (Datatype Map)
1. For p ∈ V ∪ rdfV, IS(p) ∈ IP iff IS(p), IS(rdf:Property)Proposition 1 .
have IS(x) ∈ IR. Due to IR = ICEXT (IS(rdfs:Resource)) and Condition 1 in Definition 6, IS(x), IS(rdfs:Resource) ∈ IEXT (IS(rdf:type)). For [rdfs:Class rdfs:subClassOf rdfs:Resource .]:•
Semantics is a prerequisite for reasoning support: Derivations such as the above can be made mechanically, instead of being made by hand. Reasoning support is important because it allows one to check the consistency of the ontology and the knowledge, check for unintended relationships between classes, and automatically classify instances in classes. Checks like the above are valuable for designing large ontologies, where multiple authors are involved, and integrating and sharing ontologies from various sources.•
Table 1 .
Table 2 .
Table 3 .
t∈T At, t∈T Bt and t∈T (At, Bt) = t∈T At , t∈T Bt .Fig. 6. Luxenburger basis for the Mushrooms data set
u :=Wolfgang May and Michael Kifer, editors, Proceedings of the 4th European Semantic Web Conference (ESWC'07), Innsbruck, Austria, June 2007, volume 4519, pages 311-325. Springer. 52. Denny Vrandecic, York Sure, Raul Palma, and Francisco Santana. Ontology repository and content evaluation. Deliverable D1.2.10v2, KnowledgeWeb project, 2007.Table 1 .
Hearst5: NP hyper including {NP hypo ,}* NP hypo {(and | or)} NP hypo Hearst6: NP hyper especially {NP hypo ,}* {(and|or)} NP hypoTable 1 .
specific environments: In several domains, such as the Semantic Web, bioinformatics, medical science, agent technology, and software development, ontology development tools specialized to each domain are developed. For instance, OBO-Edit and DAG-Edit are ontology editors for Gene Ontology (GO) in bioinformatics, CliniClue is an ontology browsing tool for SNOMED CT in the medical domain, and Zeus is an agent development tool kit including an ontology editing tool. ontology storage with an inference system based on RDBMS. WSMO Studio is Eclipse-based integrated environments to edit the Semantic Web service for WSMO 3 (Web Service Modeling Ontology). It can be used with other tools for WSMO such as a reasoner, a validator, API for web services and so on.Table 2 .
include management of the well-known steps in software development process, that is, requirement specification, conceptual design, implementation and evaluation. OntoEdit is based on the On-To-Knowledge methodology, and WebODE is based on the METHONTOLOGY. Others have no such a methodology. some mechanisms for access management to ontologies, SWOOP and Protégé can manage histories of change with annotation for supporting collaborative construction.Table 1 .
Table 2 .
Table 3 .
Table 4 .
Table 5 .
Table 1 .
mel.nist.gov/psl/psl-ontology/. Core theories are indicated by a .th suffix and definitional extensions are indicated by a .def suffix.Table 1 .
Table 2 .
Table 3 .
Table 4 .
3rd Party Fig. 1. The GORF architecture 4 Cf. http://watson.kmi.open.ac.uk/•
2 1
Table 1 .
Table 1 .
inKoblenz & Karlsruhe, Steffen Staab October 2008Rudi StuderThe name of a person could also be assigned via relations, e.g., firstname(I046758,'Daniel') and lastname(I046758,'Oberle').To underly their link with conceptualizations, Guarino has proposed to call such intensional relations "conceptual relations" in[10].It is important to note that, if we want to provide a well-founded, grounded account of meaning, this system needs to be first of all a physical system, and not an abstract entity.Of course, properly speaking, it is an agent who commits to a conceptualization while using a certain language: what we call the language commitment is an account of the competent use of the language by an agent who adopts a certain conceptualization.This was later elaborated to "a formal specification of a shared conceptualisation"[21].S. Staab and R. Studer (eds.), Handbook on Ontologies, International Handbookson Information Systems, DOI 10.1007/978-3-540-92673-3, c Springer-Verlag Berlin Heidelberg 2009http://www.w3.org/2001/sw/WebOnt/To be exact, it is based on SHOIN .The more expressive qualified number restrictions are not supported by OWL, but are featured in the proposed OWL 2 extension (see Sect. 4).In this approach, so-called SEP-triplets are used both to compensate for the absence of transitive roles in ALC, and to express the propagation of properties across a distinguished "part-of" role.We refer the interested reader to[60]  for a definition of simple roles: roughly speaking, a role is simple if it is neither transitive nor has a transitive sub-role. Only simple roles are allowed in number restrictions to ensure decidability[60].When the qualifying concept is , this is equivalent to an unqualified restriction, and it will often be written as such, e.g. ( 2 hasParent).In order to give cyclic definitions definitional impact, one would need to use fixpoint semantics for them[1, 75].In addition to the role hasParent, which relates children to their parents, we use the concept Parent, which describes all humans having children.http://www.w3.org/TR/2008/WD-owl2-syntax-20081202/http://www.w3.org/2005/rules/wg.htmlhttp://projects.semwebcentral.org/projects/forum/Java permits application of static methods to instances of the classes for which those methods are defined. However, this is a (perhaps unfortunate) syntactic sugar. The result of such an application is the same as applying the static method to the corresponding class.Details can be found at http://projects.semwebcentral.org/projects/forum/ forum-syntax.html.http://projects.semwebcentral.org/projects/forum/forum-syntax.htmlhttp://www.w3.org/Submission/2005/SUBM-SWSF-SWSL-20050909/http://www.w3.org/Submission/WRL/http://www.w3.org/2005/rules/wg.htmlhttp://www.wsmo.org/index.htmlhttp://www.w3.org/Submission/SWSF/See Sect.for precise definitions of RDF syntax.See the RDF semantics document[9]  for the complete list of RDF(S) built-in datatypes.Readers are referred to[9]  for the list of the RDF axiomatic statements.We only focus on the core RDFS primitives, i.e. the RDFS predefined metaproperties introduced on page 75.  5  Again, readers are referred to[9]  for a list of the RDFS axiomatic statements, which includes, e.g. [rdf:type rdfs:range rdfs:Class.].Another reason that OWL-Full is undecidable is that it does not impose restrictions on the use of transitive properties[12].Classes can be regarded as mega-objects in upper strata of the meta-modelling architecture.Here we use the N3 syntax, instead of the RDF/XML syntax, as it is more compact.In[15], the subscripts are not used.owl:Class is a subclass of rdfs:Class.http://www.w3.org/Submission/SWRL/http://www.ruleml.org/In addition to the forms directly below, SWRL also allows for "built-in" atoms which are discussed in Sect. 2.3.As in the SWRL submission.For a basic discussion of the standard translation in a modal logic context, see chapter "Ontologies in F-Logic" of[1]. For variants of the standard translation for SHOIN (D) and other description logics see[32]  and[16].Class and property equivalence axioms can be defined as a pair of inclusion axioms.Thanks to an anonymous reviewer for some clarifications on this point.We presume normal Datalog safety, i.e. that every variable in the head appears in the body.http://www.w3.org/Submission/2006/SUBM-owl11-tractable-20061219/http://www.w3.org/2007/OWL/We would like to mention that it has been argued whether this analysis is appropriate.  12  http://www.aifb.uni-karlsruhe.de/about.html 13 http://ebiquity.umbc.edu/blogger/2007/06/15/how-owlimport-is-used/14  In our terminology, the set of OWL Lite statements {C D E, D ≡ E} would not qualify as a set of DLP statements, although it is semantically equivalent to {C D, D ≡ E}, which is expressible in DLP. We are well aware of this restriction, but will not be concerned with it in the moment, because this more general notion of semantic equivalence is not readily accessible by syntactic means. Note, however, that C D D qualifies as a DLP statement, since it is semantically equivalent to C D.Note that -since we have non-disjunctive rules -the only effect of integrity constraints is that they can render the knowledge base to be inconsistent. Equality also needs some explanations: The theorem assumes that we do not use the unique name assumption, and equality thus has the same meaning as it has under OWL DL, e.g. two different constants can be equal, meaning that they denote the same individual.However, Horn-SHIQ lies within the proposed OWL 2 language, see http:// www.w3.org/Submission/owl11-tractable/http://kaon2.semanticweb.org/http://www.cs.man.ac.uk/∼bmotik/HermiT/Using, e.g. XSB-Prolog, http://xsb.sourceforge.net/See http://www.businessrulesgroup.org/brmanifesto.htm for a taste of business rule concerns.http://www.w3.org/2005/rules/wghttp://www.ontoknowledge.orghttp://dagstuhl-km-2000.aifb.uni-karlsruhe.de/Thanks to Urs Gisler, Valentin Schoeb and Patrick Shann from Swiss Life for their efforts during the ontology modelling.OntoWeb, a European thematic network, see http://www.ontoweb.org for further information.EU Integrated Project NeOn Lifecycle Support for Networked Ontologies, see www.neon-project.org for further information.http://www.geneontology.orghttp://www.virtuelle-fabrik.comhttp://swap.semanticweb.orghttp://suo.ieee.org/In our terminology, a methodology for an engineering artefact is a tested and validated process template abstracting over all possible successful engineering processes for engineering the artefact.Maintenance is supported by later stages of DILIGENT.Ideally the board should have access to all users' ontologies. However, in some settings it may only have access to requests for changes.In the revision stage.http://www.ibit.orgAfter a discussion of the three levels, DINfocusses on guidelines for good namings and definitions. It is thus a valuable resource for ontology engineers. An alternative source is the related international standard 'ISO 704: Terminology Work -Principles and Methods'[41].http://www.informatik.uni-trier.de/∼ley/db/conf/icfca/http://www.informatik.uni-trier.de/∼ley/db/conf/iccs/http://www.bibsonomy.org/tag/fcaThis is equivalent to requiring that A × B ⊆ I such that neither A nor B can be enlarged without validating this condition.That is, each subset of concepts has a unique greatest common subconcept (called its infimum) and a unique least common superconcept (called its supremum).http://toscanaj.sourceforge.net/, see also[37]  for its foundations and a description of its predecessor TOSCANA.http://www.citeulike.orghttp://www.connotea.orghttp://www.43things.comhttp://www.bibsonomy.orghttp://www.bibsonomy.org/tag/triadic+fcahttp://www.bibsonomy.orghttp://www.informatik.uni-trier.de/∼ley/db/BibSonomy benchmark datasets are available for scientific purposes, see http:// www.bibsonomy.org/faq.See http://www.mail-sleuth.com/ for a commercial follow-up.http://www.bibsonomy.org/user/stumme/OntologyHandbook+FCAFor example, in the projects FOS : http://www.fao.org/agris/aos/, WonderWeb: http://wonderweb.semanticweb.org, Metokis: http://metokis.salzburgresearch.at, and NeOn: http://www.neon-project.orgWith the exception of Logical OPs.In software engineering, formal approaches to design patterns, based on dedicated ontologies, are being investigated, e.g. in so-called semantic middleware[34].Eclipse (http://www.eclipse.org/) is a programming environment used for developing Java projects.  5  http://whole.sourceforge.net/ 6 See http://www.w3.org/2001/sw/BestPractices/In the pragmatics of an ontology designer, the fact that all modelling solutions are representable as higher-order logic expressions is hardly relevant, and such implicit reengineering has been never documented as actually happening.http://www.martinfowler.com/articles/writingPatterns.html#CommonPattern Formshttp://www.ontologydesignpatterns.org/schema/cpannotationschema.owlSee[36, 37].http://www.ontologydesignpatterns.org/ont/dul//DUL.owlhttp://www.ontologydesignpatterns.org/cp/owl/informationrealization.owlhttp://ontologydesignpatterns.org/cp/owl/timeindexedpersonrole.owlAn interesting review of evaluation, selection and reuse methods in ontology engineering is in[40].http://www.neon-toolkit.orgSee http://www.cis.upenn.edu/∼treebankKnowledge management: the process of capturing value, knowledge and understanding of corporate information, using IT systems, in order to mantain, re-use and re-deploy that knowledge.  4  Practice: knowledge of how something is customarily done.From a linguistic point of view, a term t1 is a hyponym of a term t2 if we can say a t1 is a kind of t2. Correspondingly, t2 is then a hypernym of t1.http://www.xml.com/pub/a/2004/07/14/onto.htmlSome of them are listed in the web sites such as ESW Wiki SemanticWebTools (http://esw.w3.org/topic/SemanticWebTools), Ontology Tool Survey and so on.http://www.wsmo.org/http://wordnet.princeton.edu/http://www.hozo.jp/http://www.mindswap.org/2004/SWOOP/Inline editing in RDF/XML and Turtle is supported by SWOOP ver.2.3.http://protege.stanford.edu/http://ontogen.ijs.si/http://www.opengroup.org/togaf/An exception from this statement is the IDEF integrated definition method. The IDEF approach defines a function modeling method (IDEF0), informationhttp://www.isaca.org/Content/ContentGroups/Val IT1/Val IT.htmReference[10]  for a discussion on the relation between this process model and the IEEE standards[12].Linear regression is a mathematical method to calculate the parameters of a linear equation so that the squared differences between the predictions from the linear equation and the observations are minimal[19].There are other positions like, e.g., the bundle theory[23].We limit this presentation to properties. The arguments, mutatis mutandis, hold for relations as well.Recall the notion of property given in Sect. 2.1. One should refrain from considering boolean combinations of predicates, like 'not being present', as possible values for F .  7  We use the set-theoretical ∈ predicate to indicate that here F stands for the class of tropes that satisfy F.8  We could do as in (a6) as well but we do not investigate this option here.Endurantists do not refuse the existence of temporal parts and temporal slices in general. They do not accept that all the persistent entities necessarily have temporal slices at each time of their existence.Analogously for temporary parthood even though, of course, this relation requires a notion of 'existence in time'.All these statements are easily stated in logic. Here we omit their formal characterization.Perdurantists read CC(x, y, t) as the identity of the temporal slices x@t and y@t.This claim has to be taken with a grain of salt since one should not consider properties that constrain x before or after t itself, e.g., 'being red an year after t' (provided this actually counts as a property).Analogously, the ontology comprises the quality kind 'being space-located' which is not presented here.The reader may note, that we occasionally use concept and association names (written in sans serif and preceded by a namespace to clarify their origin) as subjects, objects, and predicates of the sentences in the text.The OoP:Plan of the Class contains all Plans of its Methods as alternatives.The OoP:predecessor and OoP:successor associations hold between OoP:Tasks, and are different from OoP:precondition and OoP:postcondition associations, which hold between OoP:Plans and DnS:SituationDescriptions.Both are specializations of DnS:modalTarget, viz., the generic association holding between DnS:Roles and DnS:Courses.Note that DnS:unifies is the generic association between DnS:SituationDescriptions and DnS:Collections.http://cohse.semanticweb.org/software.htmlhttp://introspector.sourceforge.nethttp://gcc.gnu.orghttp://www.reuters.com/news/video/summitVideo?videoId=56114See also the forthcoming W3C Media Fragments Working Group http://www.w3. org/2008/01/media-fragments-wg.htmlhttp://www.annodex.net/TR/URI fragments.htmlhttp://image.ece.ntua.gr/∼gstoil/VDOSans serif font indicates ontology concepts.digital-data entities are DOLCE endurants, i.e. entities which exist in time and space.Events, processes or phenomena are examples of perdurants. endurants participate in perdurants.Examples of the axiomatization are available on the COMM website.The scheme used in Fig.3is instance:Concept, the usual UML notation.In this example, the domain ontology corresponds to a collection of wikipedia URI's.The Java API is available at http://multimedia.semanticweb.org/COMM/api/.http://www.w3.org/2005/Incubator/mmsem/XGR-interoperability/PSL has been published as the International Standard ISO 18629 by the International Organisation of Standardisation.This terminology is used in[1]. The treatment of objects as continuants is also known as endurantism or 3D-ontology, while the treatment of objects as occurrents is also known as perdurantismor 4D-ontology.High-throughput technologies are large-scale, usually automated, methods to purify, identify, and characterize DNA, RNA, proteins and other molecules. They allow rapid analysis of very large numbers of samples.For this current discussion, a formal representation means a computerinterpretable standardized form that can be the basis for creating unambiguous descriptions of biological systems 2.1.  4  We use "models" to mean a schematic description of a system or phenomenon that accounts for its known or inferred properties and can be used for further study of its characteristics.Source public domain, non copy righted image.The cell cycle is a complicated biological process and comprises of the progression of events that occur in a cell during successive cell replication. The process can be described at varying level of details ranging from a high level qualitative description to a detailed system of differential equations. However, for most biological processes the representation is primarily in terms of qualitative interactions.S. Staab and R. Studer (eds.), Handbook on Ontologies, International Handbookshttp://www.icom.orghttp://www.ifla.orghttp://www.ica.orgFor instance, those published by the conference series Computer Applications & Quantitative Methods in Archaeology http://caa.leidenuniv.nl/ proceedings/  5  Round Tablediscussionat the 8th EAA ANNUAL MEETING, 24-29 September 2002, Thessaloniki-Hellas, http://www.symvoli.com.gr/eaa8/mple.htm#P5http://dublincore.org/http://www.iconclass.nl/See http://download-east.oracle.com/otndocs/tech/semantic web/pdf/rdfrm. pdfSee http://www.w3.org/TR/xquery/http://esw.w3.org/topic/LargeTripleStoresFor instance, if numbers are chosen for the unique identifier, the unique identifier of the negation of a (non-negated) concept with number n could be n + 1. The encoding process must assign numbers accordingly. If (pointers to) objects are used for representing concepts, a field with a pointer to the negated concept provides for a fast implementation of neg at the cost of memory requirements probably being a little bit higher.We use a way to construct the canonical interpretation that already considers additional concept constructors such as, say, number restrictions. In case of ALC it would be possible to map i to its witness w in the canonical interpretation.http://kaon2.semanticweb.org/Cf. http://www.daml.org/ontologies/Cf. http://www.onthology.org/Cf. http://omv.ontoware.org/Cf. http://www.epinions.comCf. http://www.slashdot.orgCf. http://www.amazon.comCf. http://www.apple.com/itunesCf. http://www.w3.org/Metadata/http://dublincore.org/dcregistry/Cf. http://oyster.ontoware.org/Cf. http://www.onthology.org/http://www.openrdf.org/http://kaon.semanticweb.org/For a complete information and for downloading Oyster system we refer the reader to http://ontoware.org/projects/Oyster/biointology.org S. Staab and R. Studer (eds.), Handbook on Ontologies, International Handbookshttp://www.daml.org/services/owl-s/1.0/http://www.w3.org/TR/wsdlhttp://oaei.ontologymatching.orgToday's methodologies follow incremental and iterative software development.http://www.spinroot.com/spin/Doc/course/Standish Survey.htmhttp://blogs.sun.com/bblfish/entry/java annotations the semantic webSee http://www.w3.org/RDF and chapter "Resource Description Framework" of this handbook.http://aws.amazon.com/http://www.w3.org/TR/soap/http://www.w3.org/TR/wsdl S. Staab and R. Studer (eds.), Handbook on Ontologies, International HandbooksNote that WSMO allows including multiple interfaces in a Web service description, thereby facilitating interaction with the service in different ways.For a complete definition of the functionality of information-providing services, the relation between the inputs and outputs should be specified as well.http://www.daml.org/services/swsl/http://www.mel.nist.gov/psl/http://swoogle.umbc.edu/ S. Staab and R. Studer (eds.), Handbook on Ontologies, International HandbooksThe stopword list of the SMART project which is available at ftp://ftp.cs. cornell.edu/pub/smart/english.stop is commonly used for English.http://cosco.hiit.fi/search/alvis.htmlhttp://search.cpan.org/ ~thhamon/Alvis-NLPPlatform-0.3/https://www.epipagri.org/index.cgi?rm=mode\_srLink Grammar Parser has been tuned to parse biological texts[38].http://www.nlm.nih.gov/mesh/Learning Language in Logic.Flickr (http://www.flickr.com) is a registered trademark of Yahoo!For background and the schema see section "Example" in http://swuiwiki. webscience.org/swuiwiki/index.php?title=Semantic Search SurveyFurther details about this thesaurus are at http://www.fao.org/agrovocSwoogle's "web" view leads to semantic duplicates or near-duplicates, i.e., files with different URLs but semantically equivalent. Not taking semantic duplication into account may skew the performance of applications like semantic browsers.A bookmarklet is a small application stored as a bookmark URL in a web browser or as a hyperlink on a web page.This is a common strategy in portal development, where small sub-applications can be quickly linked to create more complex applications, and our formalism of choice, RDFa, allows this kind of integration client-side.TF/IDF or "term frequency/inverse document frequency" is a well-known method for weighing terms in a text corpus.A joint work with Takayuki Goto, Knowledge-as-Media Research Group, National Institute of Informatics and University of Tokyo.The same dichotomy was also called personalization vs. codification strategy, organic vs. mechanistic approach, or community model vs. cognitive model view, see[1, 44].Like, for instance, the NeOn toolkit (http://www.neon-toolkit.org/), KAON (http://kaon.semanticweb.org), or Protégé (http://protege.stanford.edu), cp.[29].Cp. Tadzebao, WebODE, or DILIGENT[29, 75].See http://www.aic.nrl.navy.mil/ ~aha/lessons/, cp.[76].Log entries of maintenance experience comprise fault events, maintenance measures, repair actions, etc.Due to space limitations, we cannot include references for all the named systems; but they can be found in[70].Here we take a broad definition of bioinformatics to mean the storage, management and analysis of biological data by computational means to answer biological questions.http://www.geneontology.orgThe less common term "poset" or partially ordered set is also used.http://www.geneontology.org/GO.current.annotations.shtmlAt least judged by the number of tools available; microarray tools form the largest subsection of related tools on the GO website (http://www.geneontology.org/ GO.tools.shtml).In this section, we will talk exclusively about GO, as it is the ontology which has been statistically analysed most widely. Most of the techniques could also apply to other ontologies.The left-hand side of the reaction.The right-hand side of the reaction.The enzyme classification number.http://www.w3.org/2001/sw/HCLSThe sequence of amino acid residues in a protein determine how the protein "folds" into a three-dimensional shape. This shape determines the functionality of the protein. Biologists have determined some patterns of amino acid residue that indicate certain features of these "shapes" that are diagnostic for functionality.http://www.w3.org/2001/SW/ S. Staab and R. Studer (eds.), Handbook on Ontologies, International Handbookshttp://www.w3.org/XML/http://www.w3.org/RDF/http://www.w3.org/2004/OWL/http://dublincore.org/documents/1998/09/dces/NISO Standard Z39.85-2001 and ISO Standard 15836-2003.http://www.vraweb.org/http://www.dajobe.org/2004/01/turtle/http://en.wikipedia.org/wiki/Metasearch_enginehttp://www.getty.edu/research/conducting_research/vocabularies/tgn/http://authorities.loc.gov/http://www.mda.org.uk/stand.htmhttp://www.vraweb.org/ccoweb/cco/index.htmlhttp://www.w3.org/2004/02/skos/http://www.loa-cnr.it/DOLCE.htmlhttp://suo.ieee.org/Since 2006, CIDOC CRM has been an official ISO standard 21127:2006.http://cidoc.ics.forth.gr/http://www.kulttuurisampo.fi/http://protege.stanford.edu/http://www.seco.tkk.fi/services/onki/http://www.google.com/webhp?complete=1&hl=enhttp://simile.mit.edu/timeline/http://www.google.com/adsense/http://www.europeana.eu/http://www.steve.com/http://www.powerhousemuseum.com/collection/database/Ontology Requirements Specification Document, 140 Ontology visualization, 691 Ontology, contrasted with lexicon, 276 Ontology, definition, 736 Ontology-based Knowledge Management, 137 Ontology-Driven Architecture, 594 Ontology-like, 737 OntoLT, 259 OntoStudio, 320 Open Biomedical Ontologies (OBO),Software models, 602 Software patterns, 604 Software requirements, 595 Software retirement phase, 596 Soundness, 515 Space of properties, 365, 373, 376 SPARQL, 501, 609 Spatio-temporal inclusion, 370, 373 Specialization, 235 Specification, 303 Splitting, 541 SQL, 193, 499 SROIQ, 123, 548 SROIQ Rules, 124 Stability (metrics), 302 Stage theory, 366 State transitions, 474, 475 Statistical Relational Learning, 653 Statistical correlation, 781 Statistical uses of ontology, 743 Stopword, 642 Strata, 84 Stratum, 84 String, 77 Structural OPs, 225 Structural transformation, 534 Subconcept, 179 Substitution, 532 Substitutivity, 375 Subsumes, 511 Subsumption, 511 SUMO, 578 Superconcept, 179 Support Vector Machine, 645, 648 Supervised learning, 782 Swoogle, 700 SWOOP, 36, 326 SWRC, 119 SWRL, 112 Synonym, near-, 277, 284