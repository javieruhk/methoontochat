Introduction
In the Software Engineering field, conformance testing refers to techniques that determine to what extent an implementation of a particular standard conforms to the requirements of that standard Currently, several standardisation bodies are generating standard ontologies, i.e., ontological specifications that are published as a standard or as a part of one, to maximise semantic interoperability. A standard provides an agreed baseline approved by the community that can be used by other developers who want to describe the related area of concern. This standard can be used by developers to identify potential areas of conflict between the standard and their ontology as the result of embedded differences when describing the domain. Therefore, it should be possible to ensure that the ontologies of a particular domain conform to a particular standard, ensuring quality and interoperability when describing such domain. This analysis of conformance would reflect the absence of inconsistencies and the presence of overlaps according to the ontologies and the standard specification. As examples of standard specifications, on the Internet of Things To define this method, the translation between requirements from standards into tests was analysed to generate tests from a standard specification. These standard tests must be executed on ontologies that are not related to the standard specification. Therefore, such tests must be reusable. Moreover, it was also analysed which is the information related to the conformance testing process that must be included in the conformance report, allowing to identify the inconsistencies and overlaps found between an ontology and the standard.Intending to analyse the overlap between a set of standards in a particular domain, this work also presents a method for identifying the minimum common knowledge between ontologies. Therefore, it enables to determine to what extent a set of standard ontologies represents the same knowledge and which are their differences. This approach for analysing ontology conformance through ontological requirements aims at addressing the following research questions to be aware about the shared knowledge between ontologies related to the same domain: RQ1. What is the level of overlap in terms of requirements between an ontology and a standard from the same domain? RQ2. Which types of requirements support interoperability, i.e., are shared by ontologies from the same domain?The remainder of this paper is structured as follows. Section 2 presents the state of the art related to existing conformance testing methods. Section 3 describes the proposed method for conformance testing in Ontology Engineering. Section 4 describes the method to analyse the minimum common knowledge between ontologies, while Section 5 describes validation of the proposed research questions. Finally, Section 6 outlines some conclusions and future work.State of the art
This section presents the most well-known conformance testing approaches in the Software Engineering field, which are used as the basis for the work presented in this paper. Moreover, although in the Ontology Engineering field there is a lack of approaches for analysing conformance between ontologies and standards, this section also presents the most relevant ontology testing approaches, which can be considered as the basis of any ontology conformance method.Conformance testing methods
The following sub-sections present a summary of the most relevant conformance testing approaches for industry and software, which are supported by several standardisation bodies and are used as the basis of this work. This section presents the norm for conformance testing as published by the International Organisation for Standardisation (ISO) and the International Electrotechnical Commission (IEC), as well as the conformance approaches developed by the ETSI and the W3C.Conformance testing by the ISO/IEC
ISO/IEC 9646 (ISO/IEC 9646-1, 1994) is a multi-part international standard which specifies a general methodology for testing the conformance of products to Open Systems Interconnection specifications. However, the concepts for testing their implementations have a broader applicability and can also be used in testing of other kinds of protocol systems. The standard also defines TTCN The process of conformance testing in this ISO/IEC norm begins with the collation and categorisation of the features and options to be tested into a tabular form which is normally referred to as the implementation conformance statement (ICS). All implemented capabilities supported by the implementation under test are listed by the implementer in the ICS, so that the tester knows which options have to be tested.The next step is to collect the requirements. For each requirement, one or more tests should be identified and expressed in the form of test purposes (TP), which describe a well-defined objective of testing. The TP describe in plain language the actions required to reach a verdict on whether an implementation passes or fails the test. Then, the tests are classified into a number of groups to provide the test suite structure (TSS). The test cases are combined into an abstract test suite (ATS) using a specific testing language such as TTCN. It is worth mentioning that the test suite is abstract in the sense that the tests are developed independently of any implementation.Based on the ATS, a set of executable test cases (ETS) is generated. Such ETSs are then verified against a number of implementations to test (IUT) for correct operation according to some agreed procedures. An implementation extra information for test (IXIT) associated to the ATS should be produced to help executing protocol conformance testing. The results of this verification process are documented in a conformance report. Fig. Conformance testing by the ETSI
The ETSI conformance testing specifications are developed according to the method described in the ISO/IEC 9646. The specification provided by the ETSI Similarly as in the ISO/IEC 9646, the ATS represents the entire collection of test cases. In the ETSI conformance testing method, each test case specifies the detailed coding of the TP written using the standardised test specification language TTCN-3 All the test specifications developed by ETSI are available online and can be searched and downloaded via the ETSI Work Programme application. Conformance testing by the W3C
The conformance testing approach presented by the W3C is focused on the requirements and good practices for including conformance in W3C specifications The W3C proposes conformance requirements such as the addition of conformance clauses in every specification and the identification of which conformance requirements are mandatory, recommended and optional. Additionally, it also requires to use a consistent style for conformance requirements and to explain how to distinguish them. An example of detailed conformance clauses following the W3C guidelines is included in the Scalable Vector Graphics 1.1 specification Regarding good practices, the W3C proposes the definition of the specification conformance model and the specification of how to distinguish normative from informative content, as well as the description of the wording for conformance claims.Ontology testing methods
In the Ontology Engineering field, there is active research on ontology evaluation Blomqvist and colleagues' testing approach
Blomqvist and colleagues In order to keep tests separated from the ontology to be tested, this proposal represents a test case as an OWL ontology, which includes properties for describing each test case.Although this methodology proposes different types of tests to verify requirements, which could be considered as a set of TPs, it does not describe the relation between such requirements and the tests. The tests in this approach are not abstract and, therefore, they cannot be executed on several ontologies, because the SPARQL query is not independent of the ontology from which it is extracted.CQChecker
CQCheckerUnlike Blomqvist and colleagues, which proposed a set of the generic types of tests, the authors identified three types of requirements written as competency questions to be verified based on how they are specified:• Competency questions which work over classes and their relations. • Decision problems expressed as competency questions. In this type, the answer permitted to the question can only be true or false. • Competency questions are expressed in an interrogative form which works only over instances.This tool allows translating these three types of competency questions, which represent the ontology requirements, into SPARQL queries and executing them on an ontology. The set of SPARQL queries could represent a set of TPs associated to an ontology. Moreover, since the translation between the requirement and the SPARQL query is automatic depending on the ontology in which the SPARQL query is executed, the set of TPs can be considered abstract. However, authors are focused on the technological support to execute such tests rather on the methodology for performing the testing process. Therefore, there is a lack of a formal process to generate the tests and to analyse the obtained results. Test-driven development of ontologies
Keet and Ławrynowicz proposed a test-driven development (TDD) of ontologies The TDD method is supported by the TDDOnto tool,This approach is oriented to the definition of a set of TPs and their execution on an ontology. A potential input to write such tests are the competency questions, i.e., the ontology requirements; however, how to write such tests from requirements and how to analyse the results are aspects that are out of scope of the method. The authors do not state whether the tests generated by the users are abstract and, therefore, reusable.Fernández-Izquierdo and García-Castro's testing method
The same authors of this paper presented an ontology testing method based on requirements Ontology testing approaches are usually divided into two activities, i.e., test implementation and test execution. However, in this testing method the test design activity is needed due to the ambiguity and assumptions inherent to the natural language It is worth mentioning that the tests defined during the test design activity and that are written using the language proposed in this method do not include information related to the ontology from which the tests are extracted. Consequently, these tests can be considered as abstract, since they are independent of any ontology.Moreover, this testing method proposes the storage of the results of each activity as RDF files that follow the Verification Test Case vocabulary. Comparison between conformance testing approaches
Conformance testing approaches developed by standardisation bodies agree on the fact that conformance testing is a key activity for ensuring the quality of products and for increasing the confidence in the correct functioning of an implementation with respect to a given specification.Table Table As it was previously mentioned, in the Ontology Engineering field there is a lack of works on analysing conformance to check whether an ontology is compliant with the specification of a standard. However, an initial step may be the different ontology testing methods that allow to execute tests on ontologies. Table Table The work presented in this paper is inspired by the conformance testing methods in the state of the art for Software Engineering and industry and, taking as input existing ontology testing methods, aims at providing a first step in analysing conformance in the Ontology Engineering field. Similarly to the ISO/IEC 9464 and the ETSI approaches, the ontology conformance testing and the minimum common identification methods proposed in this work are focused on the language definition for generating homogeneous tests and on their reusability.Method for ontology conformance testing
Borrowing the concept from the Software Engineering field Following the conformance concepts described in the already introduced ISO/IEC 9646, any ontology conformance testing approach should include a testing process for designing, implementing and executing tests associated with a standard. Such tests, which are extracted from the standard specification to check a set of selected features, should be abstract in the sense that they do not include any information about the ontology in which the test will be executed, being independent of any ontology and, consequently, reusable. In the context of Ontology Engineering, a separated activity for analysing the results should be included, in order to determine up to what extent an analysed ontology is conformant to a particular standard and to generate a conformance report with the results.This paper advances the state of the art in conformance testing for ontologies by proposing a conformance testing for OWL ontologies method based on functional requirements.This ontology conformance testing method based on ontology requirements includes five activities, namely, functional ontology requirements extraction, test design, test implementation, test execution and test results analysis. Fig. Since ontology verification methods already cover the design, implementation and execution of tests, this work builds on top of them. Hence, the ontology conformance testing approach proposed in this work is grounded on the existing testing method for ontology verification Activities within the conformance testing method
The following sub-sections detail the testing activities that should be carried out in the proposed conformance testing framework. The definition of the test design, test implementation and test execution activities are reused from the testing method for ontology verification proposed by the same authors of this work, which provides an automatic procedure to implement and execute tests. The activities related to the extraction of requirements, the analysis of the results and the conformance report are defined in this paper for supporting the conformance analysis.The conformance testing method is supported by the online tool Themis (Fernández-Izquierdo and García-Castro, 2019), which allows test implementation and test execution on one or multiple ontologies. However, the requirements extraction, the test design and the analysis of the results should be done manually.Functional ontology requirements extraction
Functional ontology requirements extraction refers to the activity of collecting the functional ontology requirements that the standard specifies. These requirements should be related to the domain that the standard should model. To analyse conformance, they should define the concepts, relations and restrictions that must be defined in the domain described by the standard. This activity produces a set of functional ontology requirements that could be materialised in one of the following forms:• Competency questions. This technique was proposed by To ease the requirements proposal, the CORAL Corpus (Fernández-Izquierdo et al., 2019) can be used. CORAL includes a dictionary of lexico-syntactic patterns that identifies different types of ontology requirements and how they are specified, e.g., a requirement with the structure What is NP⟨class⟩?, asks about the existence of a class in the ontology named NP⟨class⟩. Moreover, CORAL also identifies a set of ambiguous expressions that can lead to multiple implementations in the ontology and, therefore, they should be avoided in the specification of requirements. As an example, the use of the verb ''to have'' in a requirement can be translated both as a datatype property and an object property in the ontology. In addition to these lexico-syntactic patterns, CORAL collects 834 requirements extracted from real-world ontologies that are annotated according to their lexico-syntactic patterns. Such requirements can be used as examples of how to write requirements.Ontology requirements should be grouped in one or more topics related to them, e.g., the requirement ''What is a gateway?'' can be associated with the topic ''Gateway''. This grouping process facilitates the analysis of conformance once the test results are obtained, and allows to check conformance with regards to topics related to the ontology. To that end, each requirement should be tagged according to its associated topic.Ontology developers must guarantee that the defined requirements are correct and complete since they will be used to check conformance with the associated ontology. However, the CORAL corpus shows that multiple requirements that are published online have ambiguous expressions that can lead to several implementations in an ontology. In that situation, the results obtained by the conformance analysis could be incorrect. Therefore, the following criteria defined by • A set of requirements is correct if each requirement refers to some features of the ontology to be developed. • A set of requirements is internally consistent if no conflicts exist between them. • A set of requirements is concise if each and every requirement is relevant, and no duplicated or irrelevant requirements exist.• A set of requirements is realistic if each and every requirement meaning makes sense in the domain. • Each requirement in the set of requirements is understandable to end-users and domain experts. • Each requirement in the set of requirements is unambiguous if it has only one meaning; that is, if it does not admit any doubt or misunderstanding.In a conformance testing scenario, the higher the quantity of requirements and the more concise and unambiguous the requirements are, the more accurate the identification of overlaps and conflicts between the standard and the ontology to be analysed.Test design
During the test design activity, a set of test designs are created for each of the topics defined in the previous step. To that end, the desired behaviour of each standard requirement, i.e., their goal in the standard, is extracted and formalised into a set of supported test purposes (TPs), which describe the objective of testing. These TPs are defined without any information related to the ontology in which such TP will be executed, e.g., URIs or labels. Therefore, they can be considered as abstract TPs that allow their execution on multiple ontologies, including those that are not related to the standard from which they are extracted. Each test design includes the identifier, the TP, the requirement and the associated topic.The set of test designs constitutes the Abstract Test Suite (ATS), which is the output of this test design activity. The ATS must be stored in an RDF file by using the Verification Test Case ontology,To generate such ATS, each TP uses keywords that indicate its goal. These keywords are inspired by the software engineering approach called keyword-based testing The complete set of TPs is presented in Table As an example, the requirement included in the ISO/IEC 30141 specification that states ''An IoT gateway is a digital entity'' is associated with the TP ''Gateway subClassOf DigitalEntity''. This TP has the goal of representing a subsumption relation between two classes, i.e., Gateway and DigitalEntity, as shown in Table Test implementation
The ATS defined during the test design activity must be implemented into queries or axioms to be executed on an ontology and to check whether it satisfies the standard requirement. The set of test implementations associated with the ATS constitutes the Executable Test Suite (ETS). The tests in the ETS are still abstract since they do not include any information about the ontology on which the tests are going to be executed; the same implementation can be executed on multiple ontologies.The separation between the test design and the test implementation allows increasing the maintainability of tests. It is possible to change the implementation of tests without changing the test designs and, therefore, without changing the ATSs associated with standards. The ETS can also be stored in an RDF file by using the Verification Test Case ontology, which describes all the information related to how a test design is implemented.The structure of the test implementation of each test design in the ATS is reused from the verification testing method • A precondition, which is a SPARQL query that checks whether the terms involved in the test design are defined in the ontology to be analysed. This precondition allows to check whether the terms defined in the test design associated with the standard are defined in the ontology. If these terms are not defined in the ontology, then the test design is not satisfied. • A set of axioms to declare auxiliary terms, which are a set of temporary axioms added to the ontology to declare the auxiliary terms needed for executing the TP included in the test design. • A set of assertions to check whether the ontology behaves as determined in the test design, which are a set of pairs of axioms and expected results that represent different ontology scenarios.For each pair, the axiom is temporary added to the ontology to force a scenario, after which a reasoner is executed. The expected result determines if the ontology status (i.e., inconsistent ontology, unsatisfiable class or consistent ontology) after the addition is the expected one in case the test design was satisfied. If the ontology status concurs with the expected status, then the test design is satisfied. This assertion allows to check whether the knowledge expected by the standard requirement is included in the ontology.Table Test execution
Once the ETS is generated, each test implementation can be executed on an ontology. This execution activity consists of three steps:(1) the execution of the query that represents the preconditions, (2) the addition of the axioms which declare the auxiliary terms, and (3) the addition of the assertions. After the addition of each axiom, a reasoner is executed to report the status of the ontology, i.e., whether the ontology is consistent, inconsistent or has unsatisfiable classes. The addition of the auxiliary axioms needs to always lead to a consistent ontology. However, in the case of the assertions, the agreement between the reasoner status after the addition of all the axioms and the status indicated in the test implementation determines whether the ontology satisfies the TP.During this activity, each test implementation in the ETS should be first completed with the information related to the ontology to be executed. To that end, a glossary of terms must be generated manually or automatically to map each term in the standard test with a term in the ontology. Therefore, the terms in the TP that are defined in the ontology in which the test implementation will be executed, e.g., Gateway, are collected and associated with a URI in the ontology, e.g., http://example.org/ontology#Gateway. Then, using these associations, the terms in the test implementation are translated into a term in the ontology. Consequently, the ETS can be executed on the ontology although it was extracted from a standard specification document.Following the ontology verification testing method (Fernández-Izquierdo and García-Castro, 2018), the conformance testing method provides four possible results for each test implementation in the ETS and each ontology:1. Passed: if the ontology passes the preconditions defined in the test implementation and the results of the assertions are the expected ones. This result indicates that the ontology satisfies the test design associated with the standard. 2. Undefined terms: if the ontology does not pass the preconditions.This result indicates that the terms included in the test design associated with the standard are not defined in the ontology. 3. Absent relation: if the ontology passes the preconditions and the results of the assertion are not the expected ones but there are no conflicts in the ontology. This result indicates that there is a relation defined in the test design associated with the standard that is not defined in the ontology. 4. Conflict : if the ontology passes the preconditions and the results of the assertion are not the expected ones, and the addition of the axioms related to the test design included in the associated test design leads to a conflict in the ontology. This result indicates that there is a conflict between what is defined in the standard and what is defined in the ontology.As an example, the test implementation described in Table Test results analysis
To extract more information from the tests and to determine up to what extent an ontology conforms to a standard, a new activity is defined in this paper, i.e., test results analysis. During this activity, the results of the test execution are analysed to identify the degree of conformance of the ontology regarding the standard.   To analyse such conformance, the following information extracted from different artefacts involved in the verification process, i.e., the requirements, the tests and the ontology, can be obtained:• How many tests included in the ATS associated with the standard are satisfied by the ontology. The Passed Test Percentage metric should be calculated to obtain this information. The higher the percentage of this metric, the greater the coverage of the ontology regarding the standard specification document. • How many terms defined in the ATS associated with the standard are out of scope of the ontology. The Undefined Test Percentage metric should be calculated to obtain this information. The higher the percentage of this metric, the greater the number of standard terms that are out of scope of the ontology. • How many relations in the ATS associated with the standard are not included in the ontology. The Absent Test Percentage metric should be calculated to obtain this information. The higher the percentage of this metric, the greater the absent relations and restrictions in the ontology.• How many tests in the ATS associated with the standard are not passed by the ontology. The Failed Test Percentage metric should be calculated to obtain this information. The higher the percentage of this metric, the greater the conflicts between the standard and the ontology. • How many requirements in a standard specification document are satisfied by the ontology, since sometimes a requirement needs more than one test to be checked. The Covered Requirements Percentage metric should be calculated to obtain this information. The higher the Covered Requirements Percentage metric, the greater the coverage between the standard and the ontology. • How many requirements associated with a standard specification have a conflict with the ontology. The Requirements Fault Percentage metric should be calculated to obtain this information, which in this case refer to those requirements of the standard that have a conflict with the ontology. Similarly to the Test Failed Percentage metric, the higher the percentage of this metric, the greater the conflicts between the standard and the ontology.• How many terms included in the standard are also defined in the ATS. In the conformance testing scenario, having this information helps analysing the completeness of the requirements specification used in the testing process. The Number of Tested Vocabulary Terms metric should be calculated in order to obtain this information. The higher the percentage of this metric, the more complete the requirements specification will be. However, it should be considered that due to modelling decisions or ontology design patterns Using the previous metrics, the conformance degree can be determined through the following steps. Each step can be related to the requirements topics (topic level) or to the entire set of requirements (general level):1. To identify the requirements of the standard whose tests are passed by the ontology, i.e., the coverage between the standard and the ontology. The Passed Test Percentage and Covered Requirements Percentage metrics facilitate the identification of such coverage. The Number of Tested Vocabulary Terms metric provides an overview of completeness of the requirements specification, which can also be included in the coverage analysis. 2. To identify the requirements whose tests result in undefined terms, i.e., requirements that include information that is included in the standard but is out of the scope of the ontology to be analysed. 3. To identify the requirements whose tests result in absence, i.e., requirements that include information related to restrictions and relations that are defined in the standard but not in the ontology.The Absent Test Percentage metric provides an overview of the percentage of tests with absent relations. 4. To identify the requirements whose tests result in conflict, i.e., the incompatibilities between the standard and the ontology.The Failed Test Percentage and Requirement Fault Percentage metrics identify the percentage of incompatible tests and requirements between the standard and the ontology to be analysed.As an example of the degree of conformance that can be obtained with this information, if a set of requirements for a standard is divided into three topics, e.g., (1) Devices, (2) Users and (3) Services, it could be determined that the ontology to be analysed has a Covered Requirements Percentage of 60% for the first topic, but a 0% for the second and third topic. In this case and with this information it can be concluded that there is coverage between the standard and the ontology only for the specification of devices.Based on these results, the conformance report is generated. The goal of this conformance report is to inform about the degree of conformance of a given ontology by using all the information obtained during the conformance analysis. Inspired by the ISO/IEC 9646, such report includes the following fields:• The ontology to be analysed.• The standard for which the analysis has been performed.• The ATS together with its results.• The ETS generated from the ATS.• The glossary of terms used during the execution of the ETS.• The obtained metrics for each topic or for the complete list of requirements. • A clause that states the conformance status, which could include the overall conformance and the conformance divided by topic. • A clause that provides information about those requirements related to the standard that are in conflict with the ontology.The information provided by the conformance report can be used for requesting changes or for identifying conflicts between the developed ontology and the standard. Moreover, it can also be used to identify mappings and potential terms for reuse, as well as for guaranteeing interoperability between the standard, or standard topics, and the ontology.Minimum common knowledge identification method
When analysing a particular domain, it can be observed that several standards coexist. As an example, in the IoT field the ETSI developed the SAREF ontology for smart applications Since reusing other ontologies is a key activity during ontology development, ontology developers should be aware of how a set of standards covers their domain, identifying their scope, their overlaps, their differences and their conflicts. This information is valuable for identifying potential mappings and needs that are not supported by existing standards or that are defined in more than one. To that end, this section introduces a method for common knowledge identification between a set of standards which identifies the overlaps that are shared among their specifications. These standards can be ontologies or non-ontological specification documents that define data models.This minimum common knowledge identification method is grounded on the conformance testing method described in the previous section. The activities related to the test design, implementation and execution are reused from the testing method for ontology verification. However, the test execution activity is adapted to the particular scenario of minimum common knowledge identification to support the execution of all the ATSs on all the standards. The generation of ontologies for data models without an ontology associated, the generation of shared glossaries of terms and the analysis of the results are novel activities defined in this paper. A more detailed description related to the differences between these two methods is described in the following sub-sections.Activities within the minimum common knowledge identification method
To carry out this minimum common knowledge identification method, the functional ontology requirements associated with all the standards to be analysed should be extracted to generate an ATS from each of them. Then, these ATSs are executed on all the standards, and the obtained results are analysed to determine the overlaps and differences between them.The overview of the method for identifying the minimum common knowledge between standards is summarised in Fig. Generation of ontologies from data models
The main challenge of this method is to execute the ATSs on every standard, even on those without any associated ontology, e.g., data models described in ISO/IEC norms. Therefore, an ontology is built from the ATS related to the standards that are not specified through an ontology to solve this issue.The goals of the supported TPs that can be included in an ATS (Table These associations between a TP and a set of OWL axioms allows to automatically generate an ontology from an ATS for those standards that do not have an ontology related. The collection of all the OWL axioms retrieved by all the TPs in an ATS constitutes the ontology to be used in the method.To support part of the method, an extension of Themis 7 has been developed that, before executing the tests, automatically generates ontologies from the associated ATS for those standards without a related ontology. This extension of Themis also generates automatically a shared glossary of terms that should be used for the execution of the ATSs. The requirements extraction, the test design and the analysis of the results are out of scope of this extension of Themis and should be performed manually.Generation of a shared glossary of terms
A shared glossary of terms maps the terms in each TP included in the ATSs and the terms in each standard. This glossary of terms is required to automatically execute all ATSs on all the standards since it provides the required information to complete the abstract tests with the corresponding URIs for each standard. Table This glossary of terms should be generated either automatically or manually by the person in charge of the conformance testing process, who has to ensure that the glossary of terms is correct because an incorrect glossary of terms can lead to mistakes in the method results. It should be noted that the glossary of terms is a crucial element in the minimum common knowledge identification method since it allows to map each term in the ATSs with the terms in the standard.Analysis of the results
The output of the execution activity consists of the results obtained after the execution of each ATS in each standard. The amount of results hinders the test result analysis since more results have to be 7 The code and distribution of the extension are available in the GitHub repository: https://github.com/albaizq/ThemisForConformance. analysed. However, the same steps that were detailed in Section 3.1.5 can be followed for analysing those results. The adoption of this method allows the developers to identify different layers of knowledge between standards, namely • The common knowledge, which represents the knowledge shared by the set of analysed models. In this case, the common knowledge represents the requirements that are satisfied by all the ontologies to be analysed. • The variant-domain knowledge, which represents the knowledge that is common to more than one model, even though it is not shared by all of them. In this case, the variant-domain knowledge represents the requirements that are satisfied by more than two ontologies of the set of ontologies to be analysed. • The domain-task knowledge, which represents the lower and most specific knowledge. In this case, the domain-task knowledge represents the requirements that are satisfied by only one ontology of the set of ontologies to be analysed.These layers determine the knowledge that is common and the knowledge that is particular for each standard, identifying the scope and the commonalities between a set of standards. In addition to these layers, during the analysis of results the requirements that create conflicts between standards can also be identified.Validation
To provide an assessment of the conformance and the minimum common identification methods and to address the research questions presented in Section 1, an empirical analysis has been carried out in a concrete use case. From the research questions, two hypotheses have been proposed that aim to be validated with this analysis. Considering the numerous standards that are proposed for the same domain, and that each standard describes a particular area of such domain, the hypotheses to be validated state:H1. The common and variant-domain knowledge between ontologies
and standards related to the same domain represent a small percentage of the requirements in the requirement specification documents of the standards. To validate the hypotheses, two experiments were performed: (1) following the conformance testing method described in Section 3, it was analysed the conformance between an ontology and a set of standards related to the same domain; and (2) following the minimum common knowledge identification method proposed in Section 4, the minimum common knowledge between the aforementioned standards was analysed. The first experiment provided insights about the hypotheses, while the latter one confirmed them.Conformance testing
The goal of this experiment was to determine which type of requirements related to standards were satisfied by a given ontology. To that end, an ontology network already published on the Web was collected, together with a set of standards related to the same domain.These standards needed to have associated requirements that identify the knowledge defined in them or documentation that allows extracting the requirements. However, the standards did not need to have an ontology associated. Afterwards, the tests related to these requirements were defined following the conformance testing method described in Section 3. Moreover, as explained in Section 3, these tests were abstract and did not have any information about ontologies, such as URIs, making them independent of the ontology from which they are extracted. Therefore, these tests can be executed on any ontology even though they were extracted from standards.To analyse the type of shared requirements, the conformance testing method described in Section 3 was applied. In this method, tests associated with the requirements of each standard are executed on an ontology. Those requirements that are satisfied by the ontology are considered as shared requirements.The following steps summarise the actions performed during the experiment:1. To gather the functional requirements related to the standards. 2. To group them according to the topic associated, as described in Section 3.1.1. 3. To generate the ATS associated with these requirements following the testing process and testing language described in Section 3.1.2. 4. To group the tests designs in the ATS according to the type of TP and topic associated. 5. To execute the ATS on the ontology network. 6. To analyse the results in order to identify classes and properties that are shared between each standard and the ontology.The following information was retrieved from the performance of these steps for each ontology in the ontology network:• The types of the tests associated with the requirements of the standards that are passed by the ontology. • The topics related to the tests associated with the requirements of the standards that are passed by the ontology. • The results of the tests associated with the requirements of the standards that were executed on the ontology. • The classes and properties that are shared between each standard and the ontology to be analysed based on the results of the tests.To support the execution of tests, the Themis tool was used. Themis executed all the tests associated with the standards on each ontology and identified which were the tests that were passed by the ontology and which were not. Afterwards, a manual analysis to identify the shared types, topics and terms was performed.Results
The ontology network that was developed in the European VICIN-ITY project was selected for this experiment.The VICINITY network is related to the IoT field. Therefore, the following IoT standards were selected for the experiment: (1) the ETSI SAREF ontology, which describes smart applications; (2) the W3C SSN ontology, which describes sensors, their observations, and related procedures; and (3) the oneM2M base ontology, which provides syntactic and semantic interoperability of oneM2M platforms with external systems. Moreover, the conformance with the following nonontological standard data models in the IoT field was analysed: (1) the ISO/IEC 30141: The W3C SSN, the OCF and the ISO/IEC 30131 do not have requirements defined in their official documentation and specifications. Therefore, based on the documentation associated with the standards and following the process defined in Section 3.1.1, a set of requirements were extracted for each one. These requirements identified the classes, properties and relations between classes that are defined in each standard. For the W3C SSN standard, the requirements are mainly related to observation and actuation, therefore, not all the classes and properties defined in the standard are included in the list of requirements.Themis was used to check whether the VICINITY ontology network satisfies the 197 tests defined for the IoT standards, which would indicate the conformance of the ontology network with regards to those standards. Since the tests of such standards were generated following the testing method proposed in Section 3, the tests are abstract and do not have any information of ontologies such as URIs, making them independent of the ontology from which they are extracted. Therefore, these tests can be executed on the five ontologies in the VICINITY ontology network. Table From the results obtained by following this ontology conformance testing method, it can be deduced that the VICINITY ontology network passes 42 tests related to the standards, but does not take into consideration some concepts related to them as it is shown in the 138 requirements with the undefined term result, which determine those requirements that include concepts that are not defined in the ontology. However, it can also be concluded that there are no conflicts between the VICINITY ontology network and these standards since there are no tests with the conflict result, even though there are some absences, i.e., the terms specified in the standard are defined in the VICINITY ontology network but the relations between the terms are 10 http://vicinity.iot.linkeddata.es/vicinity/conformance.html.not. Therefore, it can be concluded that there are no inconsistencies between the domain defined in the standards and the domain defined in the VICINITY ontology network.Table In the case of the ISO/IEC 30141, the OCF and the W3C SSN the Passed Test Percentage is higher than 30%, indicating that there are overlaps between them and the VICINITY ontology network. It should be considered that the VICINITY ontology network imports the SOSA ontology, which is a module included in the SSN ontology and, therefore, there is more overlap between them.Table Additionally, Table From Table Finally, for illustrating the conformance between the VICINITY ontology network and the IoT standards, Fig. After analysing all the results obtained by the conformance method, it can be concluded that there are several requirements shared between the analysed ontology and each standard. However, these shared requirements are mostly related to the definition of classes, as it is confirmed in Fig. Not only those requirements related to the definition of tests are satisfied by the ontology, but also those related to relations between terms. However, the number of passed tests associated with these requirements is smaller. In this use case ontologies and standards share those simple requirements that are related to classes or relations. This situation can stem because more complex restrictions are defined for particular cases when describing a field, and, therefore, they are not common to any scenario of such field. Bear in mind that these results depend on the quality of the specification of requirements: the more requirements, the more accurate the conformance analysis.To conclude, the results provided by Tables These results can be considered both by ontology developers and by users that use the VICINITY ontology network since they can check that such ontology network conforms to the definitions of the terms of well-known standards and that no conflicts were found, which increases its interoperability. Moreover, they can also be informed about which topics of the standards are covered by the ontology network and which are not. As an example, while the VICINITY ontology is conforming to the device specification provided by the oneM2M ontology, it does not consider any term related to security, although it is a topic included in the standard.The developers of the VICINITY ontology could also benefit from the conformance analysis to define new requirements for the ontology, taking into account that the more interoperable ones might be those related to the definition of classes and properties rather than those requirements that are more restrictive. However, as shown in Table The results gathered from this use case indicate that hypotheses H1 and H2 could be validated. However, to have more information to validate them, the minimum common knowledge analysis has been performed.Performance
To analyse the performance of the method, it was calculated the time spent loading and executing the tests using Themis. To only calculate the load and execution time, and not the time spent retrieving the data from the corresponding servers, both the tests and the VICINITY ontology were uploaded as files in Themis, rather than using the URIs. Fig. The execution time of the ATS is divided into the execution time for those tests that result in undefined terms in the ontology and the execution time for those tests with full execution, i.e., those that result in passed by the ontology, absent or conflict. This division was made because those tests that result in undefined terms only execute the first of the three steps of the execution algorithm, i.e., the execution of the precondition, while the other ones execute the three steps.To check the scalability of the method, a linear regression model Concerning the scalability of the test execution, the execution times for the tests with full execution adjusted the linear model with a pvalue of 0.0121, while the execution times for the tests with undefined results adjusted the linear model with a p-value of 0.003. Since both p-values are lower than 0.05 and the confidence level is 95%, it can be concluded that the method scaled linearly for the test execution since its results fit a linear regression model when the number of tests grew. This linear behaviour can be observed thanks to the blue line depicted in Fig. Minimum common knowledge between ontologies
In addition to the experiment related to the conformance of the VICINITY ontology network with regards to the IoT standards, it was also analysed the minimum common knowledge between them. The aim of this experiment was to identify the common knowledge shared by a set of standards to check whether there is an overlap in the domain they describe.As for the previous experiment, the standards needed to have associated requirements that identify the knowledge defined in them or documentation that allows extracting the requirements. However, they do not need to have an ontology associated. As it was performed for the previous experiment, the tests associated with these requirements were defined following the testing process and the testing language described in Section 3. In this scenario, all the tests were also abstract, i.e., independent of the ontology from which they are extracted, and were executed on all the standards.The following steps summarise the actions performed during the experiment:1. To gather the functional requirements related to the standards. 2. To group them according to the topic associated, as described in Section 3.1.1. 3. To generate the ATS associated with these requirements following the conformance testing process and testing language described in Section 3.1.2. 4. To group the test designs in the ATS according to the type of TP and topic associated. 5. To execute all the ATSs of all the standards on all the standards. 6. To analyse the results to identify classes and properties that are shared between more than one standard.The following information was retrieved from the performance of the previous steps:• The requirements that are shared between more than one standard. • The types of the requirements shared between more than one standard. • The topics related to the requirements shared between more than one standard. • The classes and properties that are shared between more than one standard. These results allow to identify the layers of knowledge described in Section 4, namely, the common knowledge, which represents the knowledge shared by the set of analysed standards, and variant-domain knowledge, which represents the knowledge that is common to more than one standard.To execute the tests, the extension of the tool Themis that automatically generates ontologies from their associated ATS for those standards without a related ontology was used. This automatic generation of ontologies allows the execution of the tests on any standard.For this experiment, the IoT standards used in the previous experiment, i.e., the ETSI SAREF ontology, the W3C SSN ontology, the oneM2M base ontology, the ISO/IEC 30141 and the OCF standard, were selected to be analysed. As mentioned in the previous section, neither the OCF standard nor the ISO/IEC 30141 have related ontologies.Results
After executing all the tests on all the IoT standards, the test results analysis activity was performed. During this test results analysis, it was checked which requirements, and from which type and topic, are shared among the standards.This analysis showed that none of the requirements and no terms were shared by all of the standards, i.e., the common knowledge. However, the variant-domain knowledge, which refers to the knowledge shared by more than one standard, could be identified. The results of such analysis are summarised in Table Table Moreover, from Table To check if there are more classes and properties shared by the standards, but not included in the requirements, the shared glossary of terms was analysed. Table From Tables Fig. Based on the results obtained after the application of the method in this use case, ontology developers and users can be aware of how these different standards cover the IoT domain, as well as of which are their overlaps and which are their differences. Developers and users can employ these results for checking that there is a consensus between these standards regarding the terms they describe and that there are no conflicts between them. Moreover, developers could also benefit from these results for selecting which standard better fits the developers' needs in ontology reuse tasks. As an example, SAREF and oneM2M can be useful for the definition of functions related to a device, while SAREF and SSN can be useful for the definition of measurements.Performance
The time spent during the execution of all the test suites on each standard using Themis was calculated. Fig. To check the scalability of the method, a linear regression model was also applied with a confidence level of 95%. As a result, the execution times adjusted the model with a p-value of 0.0042. As mentioned in Section 5.1.2, to be a relevant result, its p-value should be below 0.05 due to the confidence level of 95%. Therefore, it can be concluded that the method scaled linearly since its results fit a linear regression model when the size of the ontology grew. This linear behaviour can be observed thanks to the blue line depicted in Fig. Conclusions and future work
The adoption of conformance techniques in the Ontology Engineering field could help to ensure the quality of ontologies by verifying whether they conform to well-known standards. Nowadays, standardisation bodies develop and publish standard ontologies and data models, but they do not provide any method or technique to analyse conformance. As it is recommended by the W3C In this paper, a method for analysing conformance between an ontology and a standard is proposed, which can be used by either ontology developers or users for checking the conformance of an ontology with regards to a standard ontology or to a non-ontological standard data model. Moreover, this method allows identifying areas of conflict between an ontology and such standard, as well as the topics of the standard that are not covered by the ontology. In terms of ontology reuse, the conformance testing method could also be used by ontology developers for identifying potential mappings or terms for reuse between the ontology and the standard.Furthermore, this paper also proposed a method for determining the minimum knowledge shared by a set of standard ontologies and data models. This method can be used by ontology developers and users to analyse how a set of standards covers a particular domain, checking whether there is a consensus concerning the definition of such domain and whether there are conflicts between them.Both methods were applied to a particular use case, where the ontology network developed in the VICINITY project was analysed regarding its conformance with a set of standard ontologies related to the IoT domain.The analysis of conformance in this use case shows how the shared requirements between the ontology network and the standards, as well as the variant-domain knowledge between standards in a particular domain are mostly related to the definition of classes. Those restrictive requirements, e.g., those related to cardinalities, are only satisfied by the ontology or standard from which they are extracted. These restrictive requirements refer to domain-task knowledge that is defined for particular aspects of the described domain.Besides, it has also been observed that the analysed standards in the IoT domain (i.e., the SAREF ontology, the SSN ontology, the oneM2M ontology, the ISO/IEC 30141 and the OCF standard) do not share any minimum commitment between all of them, i.e., common knowledge, although some of them share some terms and requirements. These results show that even though these IoT standards are related to the same domain, they were created to provide support to different areas of concern in the same IoT field and, therefore, there is a minimum overlap between them.During the validation of the conformance and the minimum common identification methods, it was observed that the results depend on the quality of the requirements specification. Therefore, to provide an accurate conformance analysis, standardisation bodies should spend effort in providing precise requirements specifications. Moreover, although the requirements specification should include all the different types of requirements, standardisation bodies should also consider including requirements related to the definition of classes in their specifications, since there is a small number of them included.Future work will be directed to provide guidelines on how to extract requirements from standard specifications, as well as to study the automatic translation from ontology requirements into tests, to ease the conformance testing process. Moreover, future work will be also directed to the adoption of certification practices in the Ontology Engineering field, to propose techniques to issue certificates for ontologies based on their conformance with standards.Fig. 1 .
Fig. 2 .
Fig. 3 .
Fig. 4 .
Fig. 5 .
Fig. 6 .
Fig. 7 .
Fig. 8 .
Fig. 9 .
Table 1
Table 2
Table 3
Table 4
Table 5
Table 7
Table 8
Table 9
Table 10
Table 11
Table 12
https://portal.etsi.org/webapp/WorkProgram/SimpleSearch/QueryForm. asp.CQChecker is available in the following URL: https://sourceforge.net/ projects/cqchecker. The last update was on December 5, 2016.The last version of TDDOnto is available in the following URL: https: //github.com/kierendavies/tddonto2. The last update was on August 23, 2018.https://w3id.org/def/vtc#.Through this paper functional ontology requirements are referred to as ontology requirements for clarity.https://w3id.org/def/vtc#.http://vicinity.iot.linkeddata.es/.https://openconnectivity.org.Note that a requirement can belong to one or more topics.