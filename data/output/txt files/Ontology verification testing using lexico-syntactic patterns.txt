Introduction
Ontology verification refers to the ontology evaluation activity where the ontology is compared against its ontology requirements, ensuring that the ontology is built correctly in compliance with the ontology requirements specification Ontology Engineering has been inspired over the years by Software Engineering practices but, regarding ontology verification, there is still much to learn from software verification approaches. In this field, software verification practices and techniques are widely integrated into the software development process to ensure the quality of software products. However, in the Ontology Engineering field, ontology verification has been neglected, and only a few approaches deal with this activity (e.g., Blomqvist and colleagues' testing methodology Inspired by the Software Engineering field and since manual ontology verification can be a time-consuming and repetitive task, the latest works in Ontology Engineering propose the use of tests to automate and facilitate the ontology verification Consequently, the contributions of this paper are the following:C1. A verification testing method that uses lexico-syntactic patterns and that systematises the verification activity. The method considers the participation of ontology engineers, domain experts, and users during the verification activity. Moreover, the method stores the requirements, tests and their results in a machine-readable format, enabling traceability between the different ontology artefacts involved in the verification activity. C2. A testing language that expresses its syntax using lexico-syntactic patterns to facilitate the definition of tests for different types of users. Moreover, to generate the testing language, representative keywords based on ontology axioms have been extracted from each lexico-syntactic pattern to propose the set of test designs. C3. A comparison between verification tools that use testing languages with tools that do not. This comparison allows analysing whether using a testing language reduces errors in the identification of verified requirements. Moreover, it also allows analysing whether using testing languages reduces the reply time during the verification activity.The remainder of this paper is structured as follows. Section 2 presents the state of the art related to existing verification testing methods. Section 3 describes the proposed method for verification testing, together with the developed ontology for promoting traceability and technological support. Section 4 describes the validation of the proposed research questions and hypotheses. Finally, Section 5 outlines some conclusions and future lines of work.State of the art
Several approaches which defend the importance of verifying ontologies through their ontology requirements have been developed to date. These approaches focus on some aspects: requirements analysis, verification testing methodology, query execution, and integration of ontology testing into ontology development methodologies. This section summarises the existing approaches that can be used to verify whether particular functional requirements are satisfied in an ontology. It should be noted that these methods are oriented to the verification of OWL ontologies.Requirements analysis
Several works are focused on the analysis of how requirements are specified, with the ultimate goal of helping their translation into formal queries that can be executed on an ontology. These works are not focused on providing a process for automatically translating natural language requirements into formal queries, but on providing lists of linguistic patterns that can be used to make such translations.One of these works is the approach presented by Ren and colleagues Based on these features, this work categorises competency questions into archetypes, which are shown in Table Another work oriented to requirements analysis is the one proposed by Wis ´niewski and colleagues To perform the analysis, the authors proposed a method that includes the following steps:1. Chunks and pattern candidates. In order to identify regularities among the collected questions, the authors studied the linguistic structure of every requirement in the dataset. Because the requirement dataset does not contain any pair of questions consisting of identical sequences of words, they proposed a pattern detection procedure to identify more general groups. 2. Pattern semantics. The previous steps produced some patterns that are semantically the same but that differ in minor aspects such as using plural and singular verbs, using synonyms or using words that could be removed from the requirement without changing its meaning. The authors semantically joined the same patterns and unify all cases, e.g., Is there CE1 for CE2? and Are there any CE1 for CE2? represent the same pattern.In this work, the authors concluded that there can be multiple SPARQL-OWL queries for one distinct linguistic pattern, which is due to the different ways an ontology engineer can represent knowledge in the ontology. Moreover, there can be multiple linguistic patterns for a single SPARQL-OWL query, because there are different ways to formulate the same thing in natural language.Ontology verification testing methods
Several testing approaches have been developed to date which defend the importance of verifying ontologies through their ontology requirements. Each of these approaches is focused on some testing aspect, such as methodological background, test implementation, or traceability between the ontology and the tests.Blomqvist and colleagues Although this methodology proposes types of tests to verify requirements, it does not describe the relation between such requirements and the tests, e.g., when to use each type of test. Moreover, the analysis of the requirements is out of the scope of this methodology.CQCheckerThe authors identify three types of competency questions based on how they are specified:1. Competency questions that work with classes and their relations.2. Decision problems expressed as competency questions. In this type, the answer permitted to the question can only be true or false. 3. Competency questions expressed in an interrogative form that works only over instances.The tool analyses the competency question submitted by the user to classify it into one of the three types of competency questions according to the possible answer it is supposed to retrieve. Then, the system processes it and checks whether it is satisfied.Ontology testing methods for query execution
While in the previous sections several testing approaches and tools based on competency questions have been reviewed, this section focuses on tools that query an ontology and that can be used for ontology verification.These approaches are not oriented to the definition of types of tests based on different types of requirements, but to the definition of different types of queries that can be executed on an ontology. Although these approaches are not directly oriented to the ontology verification activity, they can be useful for analysing how they support the execution of queries to test an ontology.OntologyTestInstantiation test. It specifies whether or not an individual belongs to a given class. An example of query could be Paracetamol(paracetamol102), which checks whether the instance paracetamol102 is of class Paracetamol. Recovery test. It allows the user to specify a list with all instances that must belong to a particular class. An example of query could be Paracetamol, and an example of expected result could be [paracetamol101, paracetamol102, paraceta-mol103], which represent all the instances of the Paracetamol class. Realisation test. It specifies the most specific class that must be instantiated by an individual. For instance, the query paracetamol101 and the expected result Paracetamol, check whether paracetamol101 is an instance of the class Paracetamol.Satisfaction test. It specifies whether an inconsistency should occur in the ontology after adding a new instance of a class. An example of query could be Paracetamol(paracetamol102), to check whether the addition of the instance paracetamol102 leads to an inconsistency. Classification test. It specifies a list with all the classes that an individual must belong to. For instance, the query paraceta-mol101 with the expected result [paracetamol, chemicalSubstance, thing] indicates all the classes to which the instance paracetamol101 belongs to. SPARQL test. The query is written in SPARQL, and the results are associated with the variables of the query.VoCol. Although VoCol provides a service that allows users to execute SPARQL queries on an ontology, how to create such queries is out of scope of the tool.Integrated ontology testing into ontology development methodologies
Keet and Ławrynowicz proposed a test-driven development (TDD) method for ontologies The steps to be performed within the TDD testing approach are summarised as follows:1. To check whether the vocabulary elements of the axiom x are in the ontology O (itself a TDD test). (c) To run the test again, which should pass, checking that there is no new inconsistency or undesirable deduction. 3. To run all previous successful tests, which should pass. This step represents the regression testing. This TDD methodology is supported by the TDDOnto tool, which has been first introduced by Ławrynowicz and Keet Conclusions
Based on the state of the art analysis on ontology verification, it can be concluded that in the current testing approaches and tools there is a lack of guidelines to help developers and practitioners to create tests from such requirements.Moreover, the majority of these testing approaches do not consider traceability between requirements, tests, and ontologies, and are oriented to ontology engineers, e.g., developing plugins for ontology engineers tools or generating SPARQL queries. Therefore, they pay little attention to how both domain experts and users should also become part of the ontology verification activity.Table The method proposed in this paper aims at addressing these current lacks by providing a testing method that uses a testing language based on how ontology requirements are specified using lexico-syntactic patterns. Such a testing language facilitates the definition of tests not only for ontology engineers but also for users and domain experts. Moreover, the method proposes the storage of the results of the verification testing process in an RDF file, which enables traceability between the tests, requirements, and the ontology.A method for ontology verification testing
The method for ontology verification testing proposed in this paper aims to involve different roles in the ontology verification activity in addition to ontology engineers, i.e., domain experts and users. These roles can be defined as follows:Ontology engineer: An ontology engineer is a member of the ontology development team who has high knowledge about ontology development and knowledge representation. The ontology engineer is usually the person who defines and executes the tests. Domain expert: A domain expert is an expert in the domains covered by the ontology. This role does not need to be knowledgeable about ontology development, but a domain expert can use the verification testing method to check whether the ontology satisfies the domain that is expected to be covered. Ontology user: An ontology user is a potential end user of the ontology. This actor also includes software developers who will use the ontology in their applications. This role can also use the verification testing method to check whether the ontology satisfies their needs.In this method, although usually the ontology engineer is the person who defines the set of tests, the domain experts and users could also participate in it and use the method. To this end, it is necessary to use a language that is understandable by all of these roles. Therefore, inspired by software testing approaches such as keyword-driven testing Furthermore, this testing method aims at systematising the design, implementation, and execution of tests extracted from ontology requirements to verify an ontology, as well as at providing traceability between ontology requirements, tests, and ontologies. Consequently, this method describes the set of activities to be carried out in a verification testing process and proposes an ontology to store and publish the tests. This section provides details of the proposed ontology verification testing method.Following agile and iterative methodologies, such as LOT, Ontology requirements specification analysis
In recent years, the definition of ontology requirements However, accurately defining ontology requirements is not a trivial task and, therefore, neither is their automatic translation into a formal language. Due to the fact that some requirements are ambiguous In this dictionary, the LSPs are categorised according to their correspondences, which could also be used to identify whether the LSP patterns are polysemous, i.e., 1 to 1 correspondence if one LSP corresponds to one possible implementation in the ontology or 1 to N correspondence if one LSP can result in more than one possible implementation in the ontology. The LSPs that have more than one correspondence are considered polysemous LSPs.Table Activities within the ontology testing method
This section details the activities to be carried out during the testing method for verifying an ontology, which were first introduced in The following sections present each of these testing activities, which are also summarised in Fig. Test design activity
Ontology engineers, domain experts, and users should be involved in the ontology verification testing process, where tests are generated from the requirements in order to ensure that the ontology satisfies all expected needs. However, writing tests in formal languages such as SPARQL, which is a language whose semantics are not easy to understand for people without a background on it In Software Engineering, there are several techniques and languages to ease the test design process. An example is keyword-driven testing To that end, representative keywords have been extracted from each LSP to define the syntax of the test designs. These keywords are based on the ontology axioms (e.g., rdfs:subClassOf) and on the different types of LSP Due to the fact that polysemous LSPs do not have a direct translation from requirements into an ontology, since multiple ontology implementations can be correct, they are not considered for the time being. Additionally, it is worth mentioning that a small set of tests and keywords was also added to the catalogue on the demand of ontology experts.The catalogue of test expressions is shown in Table The tests are defined without any information, e.g., URIs or labels, related to the ontology in which such a test will be executed. With this separation between the test and the ontology, the reuse of tests in different ontologies is allowed. However, the terms included in the test must be present in the glossary of terms of the ontology on which the test will be executed, as shown in Fig. As an example of test design, the requirement that states ''An IoT gateway is a digital entity" has as goal to model a subsumption relation between two entities. Therefore, the test to be used should refer to ''T2: Subsumption relation between classes A and B". Consequently, the test expression for T2 determined in the catalogue, i.e., ''[ClassA] subClassOf [ClassB]", is completed with the information of the requirement. The requirement ''An IoT gateway is a digital entity" is then associated with the test ''Gateway subClassOf DigitalEntity", since Gateway and DigitalEntity are terms that describe the concepts in the test. Then, the terms Gateway and DigitalEntity must be included into the glossary of terms as keywords to be associated with terms in the ontology during the test execution activity, e.g., Gateway and DigitalEntity could be associated with the terms in the ontology named <http://www.example.org/ontology#Gateway> and <http://www.example.org/ontology#Digi-talEntity>, respectively.Test implementation activity
During this activity, the tests should be implemented to be executed on an ontology. To this end, each test is formalised into a precondition, a set of auxiliary term declarations, and a set of assertions to check the behaviour. This testing method proposes a test implementation for each test design included in Table The precondition is a SPARQL query that checks whether the terms involved in the test are defined in the ontology. To execute the tests, these terms need to be declared in the ontology. Otherwise, the test fails and the requirement is not satisfied.The axioms to declare auxiliary terms (i.e., test preparation) are a set of temporary axioms added to the ontology to declare the auxiliary terms needed to carry out the assertions.Table To check the equivalence between two classes, a set of auxiliary terms are defined, i.e., the classes that complement A (i.e., :A) and B (i.e., :B). After their definition, a set of assertions that force the ontology to present unsatisfiable classes or inconsistencies are also defined. The first, associated with axiom 'E 2' in Table Test execution activity
Taking as input the test implementation, the test execution activity consists of three steps: (1) the execution of the query that represents the preconditions, (2) the addition of the axioms that declare the auxiliary terms, and (3) the addition of the assertions. After the addition of each axiom, a reasoner is executed to report the status of the ontology, i.e., whether the ontology is consistent, inconsistent, or has unsatisfiable classes. The addition of auxiliary axioms should always lead to a consistent ontology. However, in the case of assertions, the agreement between the reasoner status after the addition of all axioms and the status indicated in the test implementation determines whether the ontology satisfies the desired behaviour and, consequently, the requirement. The steps carried out during the execution activity are summarised in Algorithm 1.During this activity, the test implementation should be first completed with the information related to the ontology to be executed. To that end, a glossary of terms must be generated manually or automatically. As mentioned before, the glossary of terms maps each term in the test to a term in the ontology to be analysed. Therefore, the terms that are defined in the ontology, e.g., Gateway, are collected and associated with a URI in the ontology, e.g., <http://www.example.org/ontology#Gateway>. Then, using these associations, the terms in the test implementation are translated into terms in the ontology. Consequently, these test implementations can then be executed on the ontology. This requires that the terms in the test expressions be included in the glossary. However, it is possible that a term in the test expression is not included in the glossary of terms of the ontology. In this case, the ontology does not include the terms asked by the test and, therefore, the test is not passed.There are four possible results of the execution step for each test and each ontology:1. Passed: if the ontology passes, the preconditions and the results of the assertions are the expected ones.2. Undefined term: if the ontology does not pass the preconditions, i.e., some of the terms in the test expression are not defined in the ontology. 3. Absent: if the ontology passes the preconditions and the results of the assertion are not the expected ones but there are no conflicts in the ontology, i.e., there is a missing relation between terms in the ontology. 4. Conflict: if the ontology passes the preconditions and the results of the assertion are not the expected ones, and the addition of the axioms related to the test expression leads to a conflict in the ontology. Traceability between tests, requirements, and ontologies
An ontology for modelling tests could provide not only a guide on how to create tests for ontology verification but also a procedure for creating reusable tests and for allowing traceability between ontologies, requirements, tests, and test results. In addition, the test suite written following the ontology with all the information related to the requirements and tests can be considered as a formalised documentation. Having all this information stored in the RDF format could also allow having a knowledge graph on which users could execute queries to obtain information about which requirements are satisfied for each ontology. For this reason, the Test Case Verification ontology, Listing 1 presents a short illustrative example of a test design, generated from a requirement that states ''An IoT gateway is a digital entity", which is classified into one requirement type: ''T2. Subsumption relation between classes A and B". Because this test does not have URIs related to the ontology in which the tests are going to be executed, it can be reused in other ontologies.This traceability between the requirements, and ontology allows users to interact with the verification results by, for example, querying the data to obtain: Which ontologies satisfy a particular requirement. Which requirements are defined for a particular ontology. Which tests are related to a particular concept. As an example of use, Themis publishes the set of test designs available on its website 13 using the Helio tool, 14 which also provides a SPARQL endpoint 15 to execute queries on the dataset. As an example of SPARQL query in this dataset, Listing 2 shows how to search for those tests that are defined for the WoT ontology. 16   Technological support
Themis 17 Themis is implemented in Java and uses well-known open-source frameworks and libraries. The code of the tool is available in Github 18 under Apache License 2.0. 19   Evaluation
This section describes the empirical evaluation performed to validate the research questions and hypotheses presented in Section 1, which are focused on the comparison between tools that use testing languages and tools that do not with regard to errors and response time during the verification activity.With the aim of determining whether the use of a testing language for defining tests facilitates the ontology testing method in terms of errors and time, Themis, the tool presented in Section 3.4 that supports the verification testing method described in Section 3, was compared through a user evaluation with other tools of the state of the art for ontology verification. Some of these tools also use a testing language for the definition of tests.Different participants with different ontology development expertise participated in such evaluation, providing feedback regarding the verification tools and their usability.Experimental design
The goal of this analysis was to compare ontology verification tools that use testing languages with tools that do not. To this end, each participant in this experiment had to verify whether a given ontology satisfied a set of ontology requirements with one of these tools. Depending on the tool, participants had to execute tests or to browse the terms and axioms in the ontology to check whether the requirements were satisfied.For this evaluation, a set of 30 ontology requirements written in natural language was defined and, based on them, an ontology was developed and published. 20 These requirements included diverse restrictions and had different complexities, such as simple requirements that require a hierarchy and complex requirements associated with cardinality or with several relations between terms. Moreover, only some of the requirements were satisfied by the ontology. The participants had to identify which of them were satisfied by the ontology and which were not. The list of requirements used in this experiment is shown in Table Based on the results of the participants, the verification tools can be compared according to two aspects, namely, the errors made by the participants and the time spent in the verification process.To check the errors made by the participants, the results provided, i.e., the set of satisfied or unsatisfied requirements, were analysed. To identify for each tool:Incorrect answers, i.e., to affirm that a requirement is satisfied by an ontology when it is not or vice versa. 13 http://themis.linkeddata.es/catalogue.html. 14 https://oeg-upm.github.io/helio/. 15 https://helio.vicinity.iot.linkeddata.es/sparql. 16 http://iot.linkeddata.es/def/wot. 17 http://themis.linkeddata.es. 18 https://github.com/oeg-upm/Themis. 19 https://www.apache.org/licenses/LICENSE-2.0. 20 https://w3id.org/def/themisEval#.Correct answers, i.e., to affirm that a requirement is not satisfied by an ontology and it is not or that a requirement is satisfied by an ontology and it is. Unsolvable requirements, which are requirements that could not be verified by participants. In this analysis, participants were asked to leave a requirement as 'unsolvable' if they had to spend more than 5 min to check if it was satisfied or not. In addition, unsolvable requirements refer to those requirements that were not understood by the participants.To check the time spent by the participants for each tool during the experiment, the time spent by each participant in verifying each requirement was calculated.With the aim of comparing the tools, a web application with an online questionnaire was developed to be completed by the participants in the experiment. This questionnaire collected data from participants and their results regarding the verification process.First, the questionnaire asked participants to add their demographic data, including expertise in the OWL language and software development. Furthermore, participants were asked about their previous experience with the tools analysed in the experiment.Subsequently, the questionnaire asked the participants about the verification process for each requirement in the experiment. The following information had to be added by each participant for each requirement:The test or technique that was used to verify the requirement (e.g., the use of a reasoner or an editor browser). Based on the results of the test or the technique used, participants had to indicate for each requirement: (1) if the requirement is satisfied; (2) if there are terms in the requirement that are not included in the ontology; (3) if there is any absent restriction; (4) if there is a conflict between the requirement and the analysed ontology; or (5) if the participant does not know how to verify the requirement. Feedback or comments that the participants wanted to report related to the tool, the requirement, or the tests associated with it.In addition, the questionnaire automatically collects the time spent by each participant in identifying whether each requirement is satisfied by the ontology or not. Once the participants completed the questionnaire, a USE questionnaire Results and discussion
To perform this experiment, participants used the tools Themis (described in Section 3.4), TDDOnto2 (described in Section 2) and Protégé. In this evaluation, 30 participants were involved (10 participants per tool), among which there were experts and nonexperts in the OWL language (participants that were familiar and not familiar with OWL). Participants in the experiment are required to have a minimum of knowledge in Semantic Web technologies. The set of participants also included undergraduate and master students. Tables As shown in these tables, most of the participants are familiar with the OWL language, but they do not have a background in the OWL Manchester Syntax. In addition, there are several participants who are also familiar with software development, which can help to understand the testing process. Finally, the majority of the participants had not used any of the testing tools until this experiment.Time analysis
The experiment was planned to last around one hour and a half. It should be noted that not all participants answered the 30 requirements due to time problems, since they spent more than the planned time.Figs. From these figures, it can be observed that, regardless of the expertise of the participants, Protégé has a stable time spent per requirement during the verification process, while both Themis and TDDOnto2 have a learning curve. As an example, in Fig. Correctness analysis
During this experiment, the answers (incorrect and correct) given by the users were also collected. Checking the summarised results presented in Table For those users that were not familiar with OWL, there is not a high variability in the results of the tools (even if they are different). It should also be noted that Protégé had more participants that are not familiar with the OWL language than the other tools.Tables 10-12 From these results, it can be concluded that, although Protégé is the tool with the most stable time spent per requirement, it is also the tool with the worst results regarding the errors made during the verification process. In contrast, Themis and TDDOnto2 have a learning curve, but yield better results in terms of correctness for those users that are familiar and experts in OWL.Apart from this, it should be noted that in the three tools there were some requirements that the participants could not test, although the number of these unsolved requirements is low. The requirements that had a higher percentage of unresolved results were R.12, R.13, R.16, and R.25 in the case of Themis, R.13, R.15, R.16 and R.17 in the case of TDDOnto2 and R.20, R.25 in the case of Protégé. Requirements R.12, R.13, and R.25 are associated with relations between terms in the ontology, R.15 and R.16 also include a cardinality restriction, R.17 is related to a hierarchy between classes, and R.20 is associated with the definition of an instance of a class. This motivates the analysis of the correct results in terms of the type of requirement.  Most of the requirements with a higher percentage of unresolved results are related to term relations since the participants could not identify which restrictions, e.g., existential or universal ones, were associated with such requirements.Tables Regarding Themis, for the participants who were not familiar with OWL, the requirements that had the highest percentage of correct answers were those related to instances or related both to a relation and a hierarchy between terms, which had 100% of correct results. For the participants who were familiar with OWL, the requirements related to disjoint classes had the highest percentage of correct answers (i.e., 100% of correct results). Finally, for the participants that were experts, the requirements that had the highest percentage of correct answers were those related to disjoint classes and to relations between terms, and those requirements that had more than one restriction such as a relation and a hierarchy or union between classes. These three types of requirements also had 100% of correct results.Concerning incorrect results, in the case of Themis, the requirements with the lowest percentage of correct answers for the participants that were not familiar with OWL were those related to hierarchies and to relations and unions between terms, which both had 100% of incorrect results. Regarding participants who were familiar with the OWL language, those requirements related to relations and hierarchies had the lowest percentage of correct answers, although it is higher than 50%. Finally, regarding OWL experts, the requirements with the lowest percentage of correct answers were those related to hierarchies and disjoint classes, which had 50% of incorrect results.In the case of TDDOnto2, the highest percentage of correct answers for the participants that were not familiar with OWL were related to instances, cardinality and hierarchy and disjointness. All these requirements had 100% of correct results. Concerning participants who were familiar with the OWL language, the requirements related to hierarchy and the requirements that define both hierarchies and disjoint classes were the ones with the highest percentage. These requirements had 86% and 83% of correct results, respectively. Finally, concerning experts, the requirements related to instances, as well as those related to both relations and hierarchies, had 100% of correct answers.Regarding incorrect results, for TDDOnto2, the requirements with the lowest percentage of correct results for all the participants were those related to several restrictions. For participants who were not familiar with OWL, the requirements related to relations and cardinality between terms, to relations between terms and hierarchies, and to relations and unions between terms, had 0% of correct answers. Regarding participants that were familiar with OWL, the requirements related to relations and unions between terms had the lowest percentage of correct answers. Finally, for those participants experts in  OWL, the requirements related to relations and cardinality, to hierarchy and disjointness, and to relations and unions between terms had 67% of correct answers, which was the lowest percentage of correct answers for this type of participants.For Protégé, the type of requirement with the highest percentage of correct answers for all the participants was the one related to relations and unions between terms, which was not a trivial requirement for participants using the other tools. This could happen due to the visualisation of the restrictions provided by Protégé.The requirements related to a hierarchy in Protégé had a low percentage of correct answers compared to the other tools. For participants that are familiar with OWL only 33% of this type of requirement had correct answers, while for experts only 40%. This situation might arise because Protége only shows the direct superclass and, consequently, users cannot see the entire hierarchy unless they navigate through the classes. This might also happen with those requirements related to disjoint classes and to relations between terms.Table It must also be remarked that the variability in the results for the different tools is high. This makes sense because the testing approaches provided by each tool facilitate dealing with certain types of test above others. For example, the percentage of incorrect results regarding disjointness is quite low for Themis, since the tool only requires writing a simple test expression; on the other hand, checking disjointness with Protégé requires browsing the disjointness axioms in the ontology and even running a reasoner when those axioms are not explicit.It was also analysed using a correlation coefficient Figs. Conclusions of the experiment
Based on the information gathered from the experiment, the following conclusions regarding the hypotheses were obtained:H1. Using a testing language to define tests based on functional requirements facilitates the ontology testing process regarding the reduction of time in users who are familiar and experienced in OWL. Considering time, although it is true that the learning curve is higher in Themis and TDDOnto2, once the user gets familiar with the syntax, the time spent per requirement is similar in all the analysed tools. However, Protégé is the tool with a more stable average time. For the participants that were not familiar with OWL, Protégé is the tool with a shorter average time per requirement. For participants familiar with OWL, TDDOnto2 is the tool with a shorter average time spent per requirement and for participants experts in the OWL language, Themis is the one with a shorter average time spent. Therefore, this hypothesis is rejected. H2. Using a testing language to define tests based on functional requirements facilitates the ontology testing process regarding the reduction of errors in users that are familiar and experienced in OWL.Themis and TDDOnto2, both based on a testing language, had a significantly higher percentage of correct results in comparison to Protégé for users that are familiar and expert in the OWL language. Moreover, although both tools have a learning curve in terms of the time spent per requirement, they had a lower number of incorrect results for these types of users. Therefore, this hypothesis is true. H3. Using a testing language to define tests based on functional requirements increases the usability of the tools during the verification activity.Both Themis and Protégé had very similar results in the USE questionnaire. Regarding usefulness and satisfaction, Themis was the tool with the best results, while regarding ease of use (e.g., user friendliness or effort), Protégé had better ones. Therefore, usability does not depend only on the fact of whether the tools use testing languages or not, but also on how the user can interact with them. On the basis of these results, this hypothesis is rejected.Conclusions and future work
This paper presents an ontology verification testing method that considers the participation and feedback of domain experts and users during the verification process. To define the tests, a testing language based on lexico-syntactic patterns is proposed as part of this testing method, which was defined after an analysis of how requirements are specified. Besides, following the verification testing method, the results of the testing process are proposed to be stored in a machine-readable format, which also provides traceability between the requirements, the tests, and the ontology implementation. During the development of the CORAL corpus, significant difficulties to find available real-world requirements were found, which hindered the analysis of the requirements specifications and, consequently, limited the defined testing language. Publishing ontology requirements online would help the analysis of their specification and, therefore, would facilitate research related to their definition, formalisation, and verification.In the evaluation analysis, Themis and TDDOnto2, both tools that use a testing language for the ontology verification process, were compared to Protégé, which is a tool that does not use a testing language. As shown in the empirical analysis (Section 4), both Themis and TDDOnto2 reduced the number of errors during the verification process for those users that are familiar with the OWL language, as well as for developers who are experts in it, compared to Protégé. Moreover, Protégé is the tool that had a stable time spent per requirement, but also the worst results with regard to errors made during the verification process.From the results obtained during the evaluation of the testing process, it was concluded that users without any ontology background found it difficult to understand the restrictions that can be described in an ontology. Consequently, it is difficult for them to go beyond asking for the presence of classes and properties, which usually is not enough for the verification process. Moreover, ontology visualisation or examples of use are needed by them to understand the structure of the ontology. Furthermore, domain experts and users that have knowledge about ontologies, even if they are not experts, managed to verify an ontology using tests and to analyse the restrictions included in it. Therefore, it was concluded that ontology verification testing processes should consider ontology engineers and practitioners with a minimum of knowledge regarding ontologies as potential users since a manual review of the verification status and tests is needed.During the experiment, it was also found that all types of participants made mistakes during the verification process, even those that are experts in the OWL language. These mistakes refer to incorrect answers, i.e., to affirm that a requirement is satisfied by an ontology when it is not or vice versa. Having incorrect answers during the verification activity can affect the development of the ontology, as it helps to ensure that all the expected requirements are satisfied. Therefore, this fact reinforces the need for testing approaches that ontology engineers should use to develop ontologies that satisfy their expected requirements.This verification testing method was used as the basis in the conformance testing method proposed by Fernández-Izquierdo and García-Castro 2 .
Fig. 2 .
Fig. 4 .
Fig. 5 .
Fig. 6 .
Fig. 7 .
Fig. 8 .
Table 2
Table 3
10 Table 4
[ClassA] subclassof [PropertyP] min [num] [ClassB] and [ClassB] subclassof [PropertyP] some [ClassC]Table 5
Table 6
Table 7
Table 8
Table 9
Table 10
Table 11
Table 12
Table 13
Table 14
Table 15
Table 16
CQChecker is not available online.OntologyTest is not available online.VoCol is available in the following URL: https://www.vocoreg.com/. The last update was on 6 April 2021.The last version of TDDOnto is available at the following URL: https://github.com/kierendavies/tddonto2. The last update was on August 23, 2018.A. Fernández-Izquierdo and R. García-Castro Information Sciences 582 (2022) 89-113https://lot.linkeddata.es/.http://coralcorpus.linkeddata.es.https://doi.org/10.5281/zenodo.1967306.https://www.w3.org/TR/owl2-manchester-syntax.A. Fernández-Izquierdo and R. García-Castro Information Sciences 582 (2022) 89-113The Verification Test Case ontology is available online: https://w3id.org/def/vtc.https://protege.stanford.edu/.For the sake of readability, the numbers of all tables in this section are rounded. In addition, C refers to correct results, I to incorrect results and U to unresolvable results. The sum of the percentages of C, I and U needs to be 100%. A 0% value refers to the fact that there are 0 requirements with correct, incorrect, or unsolvable results, depending on the column.A. Fernández-Izquierdo and R. García-Castro Information Sciences 582 (2022) 89-113